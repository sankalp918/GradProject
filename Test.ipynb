{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-07T21:40:12.144734Z",
     "start_time": "2025-06-07T21:40:10.411020Z"
    }
   },
   "cell_type": "code",
   "source": "!pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu128",
   "id": "fd97499fe5ded2c4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://download.pytorch.org/whl/cu128\n",
      "Requirement already satisfied: torch in c:\\users\\sandy\\miniconda3\\envs\\gradproject\\lib\\site-packages (2.7.0+cu128)\n",
      "Requirement already satisfied: torchvision in c:\\users\\sandy\\miniconda3\\envs\\gradproject\\lib\\site-packages (0.22.0+cu128)\n",
      "Requirement already satisfied: torchaudio in c:\\users\\sandy\\miniconda3\\envs\\gradproject\\lib\\site-packages (2.7.0+cu128)\n",
      "Requirement already satisfied: filelock in c:\\users\\sandy\\miniconda3\\envs\\gradproject\\lib\\site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\sandy\\miniconda3\\envs\\gradproject\\lib\\site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\sandy\\miniconda3\\envs\\gradproject\\lib\\site-packages (from torch) (1.13.3)\n",
      "Requirement already satisfied: networkx in c:\\users\\sandy\\miniconda3\\envs\\gradproject\\lib\\site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\sandy\\miniconda3\\envs\\gradproject\\lib\\site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in c:\\users\\sandy\\miniconda3\\envs\\gradproject\\lib\\site-packages (from torch) (2024.6.1)\n",
      "Requirement already satisfied: setuptools in c:\\users\\sandy\\miniconda3\\envs\\gradproject\\lib\\site-packages (from torch) (78.1.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\sandy\\miniconda3\\envs\\gradproject\\lib\\site-packages (from torchvision) (2.1.2)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\sandy\\miniconda3\\envs\\gradproject\\lib\\site-packages (from torchvision) (11.0.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\sandy\\miniconda3\\envs\\gradproject\\lib\\site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\sandy\\miniconda3\\envs\\gradproject\\lib\\site-packages (from jinja2->torch) (3.0.2)\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-07T21:40:13.217234Z",
     "start_time": "2025-06-07T21:40:12.158048Z"
    }
   },
   "cell_type": "code",
   "source": "!pip install transformers numpy scikit-learn matplotlib seaborn Pillow",
   "id": "3a1d9bd0cb0b07fa",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\sandy\\miniconda3\\envs\\gradproject\\lib\\site-packages (4.52.4)\n",
      "Requirement already satisfied: numpy in c:\\users\\sandy\\miniconda3\\envs\\gradproject\\lib\\site-packages (2.1.2)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\sandy\\miniconda3\\envs\\gradproject\\lib\\site-packages (1.6.1)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\sandy\\miniconda3\\envs\\gradproject\\lib\\site-packages (3.10.3)\n",
      "Requirement already satisfied: seaborn in c:\\users\\sandy\\miniconda3\\envs\\gradproject\\lib\\site-packages (0.13.2)\n",
      "Requirement already satisfied: Pillow in c:\\users\\sandy\\miniconda3\\envs\\gradproject\\lib\\site-packages (11.0.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\sandy\\miniconda3\\envs\\gradproject\\lib\\site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in c:\\users\\sandy\\miniconda3\\envs\\gradproject\\lib\\site-packages (from transformers) (0.32.3)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\sandy\\miniconda3\\envs\\gradproject\\lib\\site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\sandy\\miniconda3\\envs\\gradproject\\lib\\site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\sandy\\miniconda3\\envs\\gradproject\\lib\\site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in c:\\users\\sandy\\miniconda3\\envs\\gradproject\\lib\\site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\sandy\\miniconda3\\envs\\gradproject\\lib\\site-packages (from transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\sandy\\miniconda3\\envs\\gradproject\\lib\\site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\sandy\\miniconda3\\envs\\gradproject\\lib\\site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\sandy\\miniconda3\\envs\\gradproject\\lib\\site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2024.6.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\sandy\\miniconda3\\envs\\gradproject\\lib\\site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.12.2)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\sandy\\miniconda3\\envs\\gradproject\\lib\\site-packages (from scikit-learn) (1.15.3)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\sandy\\miniconda3\\envs\\gradproject\\lib\\site-packages (from scikit-learn) (1.5.1)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\sandy\\miniconda3\\envs\\gradproject\\lib\\site-packages (from scikit-learn) (3.6.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\sandy\\miniconda3\\envs\\gradproject\\lib\\site-packages (from matplotlib) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\sandy\\miniconda3\\envs\\gradproject\\lib\\site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\sandy\\miniconda3\\envs\\gradproject\\lib\\site-packages (from matplotlib) (4.58.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\sandy\\miniconda3\\envs\\gradproject\\lib\\site-packages (from matplotlib) (1.4.8)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\sandy\\miniconda3\\envs\\gradproject\\lib\\site-packages (from matplotlib) (3.2.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\sandy\\miniconda3\\envs\\gradproject\\lib\\site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: pandas>=1.2 in c:\\users\\sandy\\miniconda3\\envs\\gradproject\\lib\\site-packages (from seaborn) (2.2.3)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\sandy\\miniconda3\\envs\\gradproject\\lib\\site-packages (from pandas>=1.2->seaborn) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\sandy\\miniconda3\\envs\\gradproject\\lib\\site-packages (from pandas>=1.2->seaborn) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\sandy\\miniconda3\\envs\\gradproject\\lib\\site-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\sandy\\miniconda3\\envs\\gradproject\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\sandy\\miniconda3\\envs\\gradproject\\lib\\site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\sandy\\miniconda3\\envs\\gradproject\\lib\\site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\sandy\\miniconda3\\envs\\gradproject\\lib\\site-packages (from requests->transformers) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\sandy\\miniconda3\\envs\\gradproject\\lib\\site-packages (from requests->transformers) (2025.4.26)\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "DATA AUGMENTATION AND MIXING. 1:1 ratio of Normal and Stroke classes.",
   "id": "6b9291576534a39d"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-06-07T22:01:56.379570Z",
     "start_time": "2025-06-07T21:40:13.312334Z"
    }
   },
   "source": [
    "import os\n",
    "import random\n",
    "import torchvision.transforms as T\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import DataLoader\n",
    "from PIL import Image\n",
    "\n",
    "IMAGE_SIZE = (256, 256)\n",
    "\n",
    "# --- Augmentation pipelines ---\n",
    "\n",
    "classic_augment = T.Compose([\n",
    "    T.RandomRotation(degrees=15),\n",
    "    T.RandomAffine(degrees=0, translate=(0.1, 0.1), scale=(0.9, 1.1), shear=5),\n",
    "    T.GaussianBlur(kernel_size=3, sigma=(0.1, 2.0)),\n",
    "    T.Normalize(mean=[0.5], std=[0.5])\n",
    "])\n",
    "\n",
    "unmix_augment = T.Compose([\n",
    "    T.RandomAdjustSharpness(sharpness_factor=2, p=0.5),\n",
    "    T.RandomAutocontrast(p=0.5),\n",
    "    T.Normalize(mean=[0.5], std=[0.5])\n",
    "])\n",
    "\n",
    "# Mixing function\n",
    "def mix_tensors(t1, t2, alpha=0.5):\n",
    "    return alpha * t1 + (1 - alpha) * t2\n",
    "\n",
    "# --- Paths ---\n",
    "input_folder = \"Dataset\"\n",
    "output_folder = \"Balanced_dataset\"\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# --- Dataset ---\n",
    "resize_and_tensor = T.Compose([\n",
    "    T.Resize(IMAGE_SIZE),\n",
    "    T.ToTensor()\n",
    "])\n",
    "dataset = ImageFolder(root=input_folder, transform=resize_and_tensor)\n",
    "dataloader = DataLoader(dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "# --- Class specific config ---\n",
    "augmentation_counts = {\n",
    "    \"Normal\": {\"classic\": 5, \"mixed\": 4},  # +1 original = 10\n",
    "    \"Stroke\": {\"classic\": 10, \"mixed\": 9}  # +1 original = 20\n",
    "}\n",
    "\n",
    "# --- Build class index map for mixing ---\n",
    "class_to_indices = {}\n",
    "for idx, (path, label) in enumerate(dataset.imgs):\n",
    "    class_name = dataset.classes[label]\n",
    "    class_to_indices.setdefault(class_name, []).append(idx)\n",
    "\n",
    "# --- Save function ---\n",
    "def save_image(tensor, path):\n",
    "    tensor = tensor * 0.5 + 0.5  # Unnormalize\n",
    "    pil = T.ToPILImage()(tensor.squeeze(0))\n",
    "    pil.save(path)\n",
    "\n",
    "def save_augmented_images(image_tensor, image_name, class_name, output_folder, classic_n, mixed_n):\n",
    "    class_folder = os.path.join(output_folder, class_name)\n",
    "    os.makedirs(class_folder, exist_ok=True)\n",
    "\n",
    "    # Save original\n",
    "    save_image(image_tensor, os.path.join(class_folder, f\"{image_name}_orig.png\"))\n",
    "\n",
    "    # Classic augmentations\n",
    "    for i in range(classic_n):\n",
    "        aug_tensor = classic_augment(image_tensor)\n",
    "        save_image(aug_tensor, os.path.join(class_folder, f\"{image_name}_aug_{i}.png\"))\n",
    "\n",
    "    # Mixing augmentations\n",
    "    indices_pool = class_to_indices[class_name]\n",
    "    for i in range(mixed_n):\n",
    "        other_idx = random.choice(indices_pool)\n",
    "        other_img_path = dataset.imgs[other_idx][0]\n",
    "        other_img = Image.open(other_img_path).convert(\"RGB\")\n",
    "        other_img = other_img.resize(IMAGE_SIZE)\n",
    "        other_tensor = T.ToTensor()(other_img).unsqueeze(0)\n",
    "\n",
    "        mixed = mix_tensors(image_tensor, other_tensor, alpha=random.uniform(0.4, 0.6))\n",
    "        mixed = unmix_augment(mixed)\n",
    "        save_image(mixed, os.path.join(class_folder, f\"{image_name}_mix_{i}.png\"))\n",
    "\n",
    "# --- Main loop ---\n",
    "print(\"ðŸš€ Starting image augmentation and mixing...\")\n",
    "\n",
    "for idx, (image_tensor, label) in enumerate(dataloader):\n",
    "    image_path = dataset.imgs[idx][0]\n",
    "    image_name = os.path.splitext(os.path.basename(image_path))[0]\n",
    "    class_name = dataset.classes[label.item()]\n",
    "    aug_config = augmentation_counts.get(class_name, {\"classic\": 0, \"mixed\": 0})\n",
    "\n",
    "    save_augmented_images(\n",
    "        image_tensor,\n",
    "        image_name,\n",
    "        class_name,\n",
    "        output_folder,\n",
    "        classic_n=aug_config[\"classic\"],\n",
    "        mixed_n=aug_config[\"mixed\"]\n",
    "    )\n",
    "\n",
    "print(\"âœ… Balanced classic + mixed augmentation done. ~120k images generated.\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸš€ Starting image augmentation and mixing...\n",
      "âœ… Balanced classic + mixed augmentation done. ~120k images generated.\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Dataset Augmentation and Mixing for Imbalanced Dataset",
   "id": "a34c8f5772fc8fec"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-07T22:21:44.263050Z",
     "start_time": "2025-06-07T22:01:56.561290Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import random\n",
    "import torchvision.transforms as T\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Classic augmentation pipeline\n",
    "classic_aug = T.Compose([\n",
    "    T.RandomRotation(degrees=15),\n",
    "    T.RandomAffine(degrees=0, translate=(0.1, 0.1), scale=(0.9, 1.1), shear=5),\n",
    "    T.GaussianBlur(kernel_size=3, sigma=(0.1, 2.0)),\n",
    "    T.Normalize(mean=[0.5], std=[0.5])\n",
    "])\n",
    "\n",
    "# Mixing augmentation (MixUp-style)\n",
    "def mixing_augmentation(img1, img2, alpha=0.4):\n",
    "    lam = random.uniform(alpha, 1.0)\n",
    "    return lam * img1 + (1 - lam) * img2\n",
    "\n",
    "# Paths\n",
    "input_folder = \"Dataset\"\n",
    "output_folder = \"Aug_dataset\"\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "resize_shape = (256, 256)\n",
    "\n",
    "base_transform = T.Compose([\n",
    "    T.Resize(resize_shape),\n",
    "    T.ToTensor()\n",
    "])\n",
    "\n",
    "classic_aug = T.Compose([\n",
    "    T.RandomRotation(degrees=15),\n",
    "    T.RandomAffine(degrees=0, translate=(0.1, 0.1), scale=(0.9, 1.1), shear=5),\n",
    "    T.GaussianBlur(kernel_size=3, sigma=(0.1, 2.0)),\n",
    "    T.Normalize(mean=[0.5], std=[0.5])\n",
    "])\n",
    "\n",
    "dataset = ImageFolder(root=input_folder, transform=base_transform)\n",
    "dataloader = DataLoader(dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "base_image_count = len(dataset)\n",
    "total_augmented_images = 120000\n",
    "aug_per_image = total_augmented_images // base_image_count\n",
    "\n",
    "# Save augmented images\n",
    "def save_augmented_images(image_tensor, image_name, class_name, output_folder, dataset, current_idx, num_augmentations=20):\n",
    "    class_folder = os.path.join(output_folder, class_name)\n",
    "    os.makedirs(class_folder, exist_ok=True)\n",
    "\n",
    "    for i in range(num_augmentations):\n",
    "        if i < num_augmentations // 2:\n",
    "            aug_tensor = classic_aug(image_tensor)\n",
    "        else:\n",
    "            rand_idx = random.randint(0, len(dataset) - 1)\n",
    "            if rand_idx == current_idx:\n",
    "                rand_idx = (rand_idx + 1) % len(dataset)\n",
    "            mix_tensor, _ = dataset[rand_idx]\n",
    "            aug_tensor = mixing_augmentation(image_tensor.squeeze(0), mix_tensor)\n",
    "            aug_tensor = T.Normalize(mean=[0.5], std=[0.5])(aug_tensor)\n",
    "\n",
    "        unnormalized_tensor = aug_tensor * 0.5 + 0.5\n",
    "        if unnormalized_tensor.dim() == 4:\n",
    "            unnormalized_tensor = unnormalized_tensor.squeeze(0)\n",
    "\n",
    "        augmented_pil = T.ToPILImage()(unnormalized_tensor)\n",
    "        aug_filename = f\"{image_name}_aug_{i}.png\"\n",
    "        augmented_pil.save(os.path.join(class_folder, aug_filename))\n",
    "\n",
    "\n",
    "# Process all images\n",
    "print(\"ðŸš€ Starting augmentation to generate ~120k images...\")\n",
    "\n",
    "for idx, (image_tensor, label) in enumerate(dataloader):\n",
    "    image_name = os.path.splitext(os.path.basename(dataset.imgs[idx][0]))[0]\n",
    "    class_name = dataset.classes[label.item()]\n",
    "    save_augmented_images(image_tensor, image_name, class_name, output_folder,\n",
    "                          dataset=dataset, current_idx=idx,\n",
    "                          num_augmentations=aug_per_image)\n",
    "\n",
    "print(\"âœ… All augmented images saved.\")"
   ],
   "id": "523dbb06c07ce4d6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸš€ Starting augmentation to generate ~120k images...\n",
      "âœ… All augmented images saved.\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "IMBALANCED DATASET",
   "id": "fed76070aece227a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-08T09:10:56.556082Z",
     "start_time": "2025-06-07T22:21:44.376516Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.model_selection import train_test_split  # <-- Add this\n",
    "from transformers import ViTForImageClassification, ViTConfig  # Required for ViT\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "class BrainCTDataset(Dataset):\n",
    "    def __init__(self, image_paths, labels, transform=None):\n",
    "        self.image_paths = image_paths\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_path = self.image_paths[idx]\n",
    "        image = Image.open(image_path).convert('RGB')\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label\n",
    "\n",
    "def load_dataset(data_dir):\n",
    "    \"\"\"\n",
    "    Load dataset from directory structure:\n",
    "    data_dir/\n",
    "        Stroke/\n",
    "            image1.jpg\n",
    "            image2.jpg\n",
    "        Normal/\n",
    "            image1.jpg\n",
    "            image2.jpg\n",
    "    \"\"\"\n",
    "    image_paths = []\n",
    "    labels = []\n",
    "    class_names = ['Normal', 'Stroke']  # 0: Normal, 1: Stroke\n",
    "\n",
    "    for class_idx, class_name in enumerate(class_names):\n",
    "        class_dir = os.path.join(data_dir, class_name)\n",
    "        if os.path.exists(class_dir):\n",
    "            for img_name in os.listdir(class_dir):\n",
    "                if img_name.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp', '.tiff')):\n",
    "                    image_paths.append(os.path.join(class_dir, img_name))\n",
    "                    labels.append(class_idx)\n",
    "\n",
    "    return image_paths, labels, class_names\n",
    "\n",
    "def create_data_augmentation():\n",
    "    \"\"\"Create comprehensive data augmentation for medical images\"\"\"\n",
    "\n",
    "    # Training augmentation - aggressive for small datasets\n",
    "    train_transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.RandomRotation(15),  # Small rotation for medical images\n",
    "        transforms.RandomHorizontalFlip(p=0.5),\n",
    "        transforms.RandomVerticalFlip(p=0.3),  # Medical images can be flipped vertically\n",
    "        transforms.RandomAffine(degrees=0, translate=(0.1, 0.1), scale=(0.9, 1.1)),\n",
    "        transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.1, hue=0.05),\n",
    "        transforms.RandomGrayscale(p=0.1),\n",
    "        transforms.ToTensor(),  # Convert to tensor BEFORE tensor-only transforms\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        transforms.GaussianBlur(kernel_size=3, sigma=(0.1, 2.0)),  # Move after ToTensor\n",
    "        transforms.RandomErasing(p=0.2, scale=(0.02, 0.1)),  # Move after ToTensor\n",
    "    ])\n",
    "\n",
    "    # Validation/Test transform - no augmentation\n",
    "    val_transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "\n",
    "    return train_transform, val_transform\n",
    "\n",
    "def split_data(image_paths, labels, test_size=0.2, val_size=0.2, random_state=42):\n",
    "    \"\"\"Split data into train, validation, and test sets\"\"\"\n",
    "\n",
    "    # First split: train+val vs test\n",
    "    X_temp, X_test, y_temp, y_test = train_test_split(\n",
    "        image_paths, labels, test_size=test_size,\n",
    "        random_state=random_state, stratify=labels\n",
    "    )\n",
    "\n",
    "    # Second split: train vs val\n",
    "    val_ratio = val_size / (1 - test_size)\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X_temp, y_temp, test_size=val_ratio,\n",
    "        random_state=random_state, stratify=y_temp\n",
    "    )\n",
    "\n",
    "    print(f\"Dataset split:\")\n",
    "    print(f\"Train: {len(X_train)} images\")\n",
    "    print(f\"Validation: {len(X_val)} images\")\n",
    "    print(f\"Test: {len(X_test)} images\")\n",
    "\n",
    "    return X_train, X_val, X_test, y_train, y_val, y_test\n",
    "\n",
    "class ViTBrainClassifier(nn.Module):\n",
    "    def __init__(self, num_classes=2, pretrained=True):\n",
    "        super(ViTBrainClassifier, self).__init__()\n",
    "\n",
    "        if pretrained:\n",
    "            # Use pre-trained ViT-Base\n",
    "            self.vit = ViTForImageClassification.from_pretrained(\n",
    "                'google/vit-base-patch16-224',\n",
    "                num_labels=num_classes,\n",
    "                ignore_mismatched_sizes=True\n",
    "            )\n",
    "        else:\n",
    "            # Create ViT from scratch (for very small datasets)\n",
    "            config = ViTConfig(\n",
    "                image_size=224,\n",
    "                patch_size=16,\n",
    "                num_channels=3,\n",
    "                num_classes=num_classes,\n",
    "                hidden_size=768,\n",
    "                num_hidden_layers=12,\n",
    "                num_attention_heads=12,\n",
    "                intermediate_size=3072\n",
    "            )\n",
    "            self.vit = ViTForImageClassification(config)\n",
    "\n",
    "    def forward(self, x):\n",
    "        outputs = self.vit(x)\n",
    "        return outputs.logits\n",
    "\n",
    "def train_model(model, train_loader, val_loader, num_epochs=25, learning_rate=1e-4):\n",
    "    \"\"\"Train the ViT model with early stopping\"\"\"\n",
    "\n",
    "    # Loss and optimizer\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=0.01)\n",
    "\n",
    "    # Learning rate scheduler\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, mode='min', patience=5, factor=0.5\n",
    "    )\n",
    "\n",
    "    # Training history\n",
    "    train_losses = []\n",
    "    train_accuracies = []\n",
    "    val_losses = []\n",
    "    val_accuracies = []\n",
    "\n",
    "    best_val_acc = 0.0\n",
    "    patience_counter = 0\n",
    "    patience = 10\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"Starting epoch {epoch+1}/{num_epochs}...\")\n",
    "\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        train_correct = 0\n",
    "        train_total = 0\n",
    "\n",
    "        print(f\"Starting training loop with {len(train_loader)} batches...\")\n",
    "\n",
    "        for batch_idx, (images, labels) in enumerate(train_loader):\n",
    "            if batch_idx == 0:\n",
    "                print(f\"Successfully loaded first batch: images shape {images.shape}, labels shape {labels.shape}\")\n",
    "\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            train_total += labels.size(0)\n",
    "            train_correct += (predicted == labels).sum().item()\n",
    "\n",
    "            if batch_idx % 10 == 0:\n",
    "                print(f'Epoch {epoch+1}/{num_epochs}, Batch {batch_idx+1}/{len(train_loader)}, Loss: {loss.item():.4f}')\n",
    "\n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for images, labels in val_loader:\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                val_loss += loss.item()\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                val_total += labels.size(0)\n",
    "                val_correct += (predicted == labels).sum().item()\n",
    "\n",
    "        # Calculate metrics\n",
    "        train_acc = 100 * train_correct / train_total\n",
    "        val_acc = 100 * val_correct / val_total\n",
    "        avg_train_loss = train_loss / len(train_loader)\n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "\n",
    "        train_losses.append(avg_train_loss)\n",
    "        train_accuracies.append(train_acc)\n",
    "        val_losses.append(avg_val_loss)\n",
    "        val_accuracies.append(val_acc)\n",
    "\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}:')\n",
    "        print(f'Train Loss: {avg_train_loss:.4f}, Train Acc: {train_acc:.2f}%')\n",
    "        print(f'Val Loss: {avg_val_loss:.4f}, Val Acc: {val_acc:.2f}%')\n",
    "        print('-' * 60)\n",
    "\n",
    "        # Learning rate scheduling\n",
    "        scheduler.step(avg_val_loss)\n",
    "\n",
    "        # Early stopping and model saving\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            torch.save(model.state_dict(), 'Aug_dataset_vit_stroke.pth')\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "\n",
    "        if patience_counter >= patience:\n",
    "            print(f'Early stopping at epoch {epoch+1}')\n",
    "            break\n",
    "\n",
    "    return train_losses, train_accuracies, val_losses, val_accuracies\n",
    "\n",
    "def evaluate_model(model, test_loader, class_names):\n",
    "    \"\"\"Evaluate the model on test set\"\"\"\n",
    "    model.eval()\n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "\n",
    "            all_predictions.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(all_labels, all_predictions)\n",
    "    print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(all_labels, all_predictions, target_names=class_names))\n",
    "\n",
    "    # Confusion Matrix\n",
    "    cm = confusion_matrix(all_labels, all_predictions)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                xticklabels=class_names, yticklabels=class_names)\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.show()\n",
    "\n",
    "    return accuracy\n",
    "\n",
    "def plot_training_history(train_losses, train_accuracies, val_losses, val_accuracies):\n",
    "    \"\"\"Plot training history\"\"\"\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "    # Loss plot\n",
    "    ax1.plot(train_losses, label='Training Loss')\n",
    "    ax1.plot(val_losses, label='Validation Loss')\n",
    "    ax1.set_title('Model Loss')\n",
    "    ax1.set_xlabel('Epoch')\n",
    "    ax1.set_ylabel('Loss')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True)\n",
    "\n",
    "    # Accuracy plot\n",
    "    ax2.plot(train_accuracies, label='Training Accuracy')\n",
    "    ax2.plot(val_accuracies, label='Validation Accuracy')\n",
    "    ax2.set_title('Model Accuracy')\n",
    "    ax2.set_xlabel('Epoch')\n",
    "    ax2.set_ylabel('Accuracy (%)')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def main():\n",
    "    # Configuration\n",
    "    DATA_DIR = \"Aug_dataset\"  # Update this path\n",
    "    BATCH_SIZE = 32  # Small batch size for small datasets\n",
    "    NUM_EPOCHS = 25\n",
    "    LEARNING_RATE = 1e-4\n",
    "\n",
    "    # Load dataset\n",
    "    print(\"Loading dataset...\")\n",
    "    image_paths, labels, class_names = load_dataset(DATA_DIR)\n",
    "    print(f\"Total images: {len(image_paths)}\")\n",
    "    print(f\"Classes: {class_names}\")\n",
    "    print(f\"Class distribution: Normal: {labels.count(0)}, Stroke: {labels.count(1)}\")\n",
    "\n",
    "    # Split dataset\n",
    "    X_train, X_val, X_test, y_train, y_val, y_test = split_data(image_paths, labels)\n",
    "\n",
    "    # Create data transforms\n",
    "    train_transform, val_transform = create_data_augmentation()\n",
    "\n",
    "    # Create datasets\n",
    "    train_dataset = BrainCTDataset(X_train, y_train, transform=train_transform)\n",
    "    val_dataset = BrainCTDataset(X_val, y_val, transform=val_transform)\n",
    "    test_dataset = BrainCTDataset(X_test, y_test, transform=val_transform)\n",
    "\n",
    "    # Create data loaders (set num_workers=0 for Windows compatibility)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=0)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
    "\n",
    "    # Create model\n",
    "    print(\"Creating ViT model...\")\n",
    "    model = ViTBrainClassifier(num_classes=len(class_names), pretrained=True)\n",
    "    model = model.to(device)\n",
    "\n",
    "    # Print model info\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print(f\"Total parameters: {total_params:,}\")\n",
    "    print(f\"Trainable parameters: {trainable_params:,}\")\n",
    "\n",
    "    # Train model\n",
    "    print(\"Starting training...\")\n",
    "    train_losses, train_accs, val_losses, val_accs = train_model(\n",
    "        model, train_loader, val_loader, NUM_EPOCHS, LEARNING_RATE\n",
    "    )\n",
    "\n",
    "    # Plot training history\n",
    "    plot_training_history(train_losses, train_accs, val_losses, val_accs)\n",
    "\n",
    "    # Load best model and evaluate\n",
    "    print(\"Loading best model for evaluation...\")\n",
    "    model.load_state_dict(torch.load('Aug_dataset_vit_stroke.pth'))\n",
    "    test_accuracy = evaluate_model(model, test_loader, class_names)\n",
    "\n",
    "    print(f\"Final Test Accuracy: {test_accuracy:.4f}\")\n",
    "    torch.save(model.state_dict(), 'Aug_dataset_vit_stroke.pth')\n",
    "    torch.save(model, 'Aug_dataset_vit_stroke.h5')\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n",
    "# Additional utility functions for small datasets\n",
    "\n",
    "def create_weighted_sampler(labels):\n",
    "    \"\"\"Create weighted sampler for imbalanced datasets\"\"\"\n",
    "    from torch.utils.data import WeightedRandomSampler\n",
    "\n",
    "    class_counts = np.bincount(labels)\n",
    "    class_weights = 1.0 / class_counts\n",
    "    sample_weights = [class_weights[label] for label in labels]\n",
    "\n",
    "    return WeightedRandomSampler(\n",
    "        weights=sample_weights,\n",
    "        num_samples=len(sample_weights),\n",
    "        replacement=True\n",
    "    )\n",
    "\n",
    "def apply_mixup(data, targets, alpha=0.2):\n",
    "    \"\"\"Apply MixUp augmentation for small datasets\"\"\"\n",
    "    if alpha > 0:\n",
    "        lam = np.random.beta(alpha, alpha)\n",
    "    else:\n",
    "        lam = 1\n",
    "\n",
    "    batch_size = data.size(0)\n",
    "    index = torch.randperm(batch_size)\n",
    "\n",
    "    mixed_data = lam * data + (1 - lam) * data[index, :]\n",
    "    targets_a, targets_b = targets, targets[index]\n",
    "\n",
    "    return mixed_data, targets_a, targets_b, lam\n",
    "\n",
    "def focal_loss(inputs, targets, alpha=1, gamma=2):\n",
    "    \"\"\"Focal loss for handling class imbalance\"\"\"\n",
    "    ce_loss = nn.CrossEntropyLoss(reduction='none')(inputs, targets)\n",
    "    pt = torch.exp(-ce_loss)\n",
    "    focal_loss = alpha * (1 - pt) ** gamma * ce_loss\n",
    "    return focal_loss.mean()\n",
    "\n",
    "# Example usage for very small datasets (< 100 images):\n",
    "def small_dataset_modifications():\n",
    "    \"\"\"\n",
    "    For very small datasets, consider these modifications:\n",
    "\n",
    "    1. Use stronger data augmentation\n",
    "    2. Reduce model size or use smaller ViT variants\n",
    "    3. Apply transfer learning with frozen early layers\n",
    "    4. Use techniques like MixUp, CutMix\n",
    "    5. Apply focal loss for class imbalance\n",
    "    6. Use k-fold cross-validation instead of single split\n",
    "    \"\"\"\n",
    "\n",
    "    # Example: Freeze early layers for transfer learning\n",
    "    def freeze_early_layers(model, num_layers_to_freeze=6):\n",
    "        for i, (name, param) in enumerate(model.vit.vit.encoder.layer.named_parameters()):\n",
    "            if i < num_layers_to_freeze:\n",
    "                param.requires_grad = False\n",
    "\n",
    "    # Example: Use smaller learning rate and longer training\n",
    "    optimizer_config = {\n",
    "        'lr': 5e-5,  # Smaller learning rate\n",
    "        'weight_decay': 0.01,\n",
    "        'eps': 1e-8\n",
    "    }\n",
    "\n",
    "    return optimizer_config"
   ],
   "id": "4ef139f130e7e326",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sandy\\miniconda3\\envs\\GradProject\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Loading dataset...\n",
      "Total images: 117000\n",
      "Classes: ['Normal', 'Stroke']\n",
      "Class distribution: Normal: 78000, Stroke: 39000\n",
      "Dataset split:\n",
      "Train: 70200 images\n",
      "Validation: 23400 images\n",
      "Test: 23400 images\n",
      "Creating ViT model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized because the shapes did not match:\n",
      "- classifier.bias: found shape torch.Size([1000]) in the checkpoint and torch.Size([2]) in the model instantiated\n",
      "- classifier.weight: found shape torch.Size([1000, 768]) in the checkpoint and torch.Size([2, 768]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters: 85,800,194\n",
      "Trainable parameters: 85,800,194\n",
      "Starting training...\n",
      "Starting epoch 1/25...\n",
      "Starting training loop with 2194 batches...\n",
      "Successfully loaded first batch: images shape torch.Size([32, 3, 224, 224]), labels shape torch.Size([32])\n",
      "Epoch 1/25, Batch 1/2194, Loss: 0.8152\n",
      "Epoch 1/25, Batch 11/2194, Loss: 0.6931\n",
      "Epoch 1/25, Batch 21/2194, Loss: 0.6668\n",
      "Epoch 1/25, Batch 31/2194, Loss: 0.5634\n",
      "Epoch 1/25, Batch 41/2194, Loss: 0.3838\n",
      "Epoch 1/25, Batch 51/2194, Loss: 0.6455\n",
      "Epoch 1/25, Batch 61/2194, Loss: 0.5443\n",
      "Epoch 1/25, Batch 71/2194, Loss: 0.7556\n",
      "Epoch 1/25, Batch 81/2194, Loss: 0.4902\n",
      "Epoch 1/25, Batch 91/2194, Loss: 0.5194\n",
      "Epoch 1/25, Batch 101/2194, Loss: 0.6798\n",
      "Epoch 1/25, Batch 111/2194, Loss: 0.5843\n",
      "Epoch 1/25, Batch 121/2194, Loss: 0.4084\n",
      "Epoch 1/25, Batch 131/2194, Loss: 0.7335\n",
      "Epoch 1/25, Batch 141/2194, Loss: 0.4558\n",
      "Epoch 1/25, Batch 151/2194, Loss: 0.4135\n",
      "Epoch 1/25, Batch 161/2194, Loss: 0.6423\n",
      "Epoch 1/25, Batch 171/2194, Loss: 0.3381\n",
      "Epoch 1/25, Batch 181/2194, Loss: 0.3822\n",
      "Epoch 1/25, Batch 191/2194, Loss: 0.4871\n",
      "Epoch 1/25, Batch 201/2194, Loss: 0.4905\n",
      "Epoch 1/25, Batch 211/2194, Loss: 0.4650\n",
      "Epoch 1/25, Batch 221/2194, Loss: 0.4708\n",
      "Epoch 1/25, Batch 231/2194, Loss: 0.3752\n",
      "Epoch 1/25, Batch 241/2194, Loss: 0.4163\n",
      "Epoch 1/25, Batch 251/2194, Loss: 0.4809\n",
      "Epoch 1/25, Batch 261/2194, Loss: 0.4579\n",
      "Epoch 1/25, Batch 271/2194, Loss: 0.4565\n",
      "Epoch 1/25, Batch 281/2194, Loss: 0.3591\n",
      "Epoch 1/25, Batch 291/2194, Loss: 0.4166\n",
      "Epoch 1/25, Batch 301/2194, Loss: 0.4615\n",
      "Epoch 1/25, Batch 311/2194, Loss: 0.4730\n",
      "Epoch 1/25, Batch 321/2194, Loss: 0.5566\n",
      "Epoch 1/25, Batch 331/2194, Loss: 0.4770\n",
      "Epoch 1/25, Batch 341/2194, Loss: 0.4525\n",
      "Epoch 1/25, Batch 351/2194, Loss: 0.5706\n",
      "Epoch 1/25, Batch 361/2194, Loss: 0.2821\n",
      "Epoch 1/25, Batch 371/2194, Loss: 0.5496\n",
      "Epoch 1/25, Batch 381/2194, Loss: 0.3565\n",
      "Epoch 1/25, Batch 391/2194, Loss: 0.3511\n",
      "Epoch 1/25, Batch 401/2194, Loss: 0.3817\n",
      "Epoch 1/25, Batch 411/2194, Loss: 0.4800\n",
      "Epoch 1/25, Batch 421/2194, Loss: 0.4363\n",
      "Epoch 1/25, Batch 431/2194, Loss: 0.3877\n",
      "Epoch 1/25, Batch 441/2194, Loss: 0.3125\n",
      "Epoch 1/25, Batch 451/2194, Loss: 0.3555\n",
      "Epoch 1/25, Batch 461/2194, Loss: 0.4993\n",
      "Epoch 1/25, Batch 471/2194, Loss: 0.5295\n",
      "Epoch 1/25, Batch 481/2194, Loss: 0.4539\n",
      "Epoch 1/25, Batch 491/2194, Loss: 0.2773\n",
      "Epoch 1/25, Batch 501/2194, Loss: 0.4714\n",
      "Epoch 1/25, Batch 511/2194, Loss: 0.4710\n",
      "Epoch 1/25, Batch 521/2194, Loss: 0.3884\n",
      "Epoch 1/25, Batch 531/2194, Loss: 0.3443\n",
      "Epoch 1/25, Batch 541/2194, Loss: 0.4099\n",
      "Epoch 1/25, Batch 551/2194, Loss: 0.3166\n",
      "Epoch 1/25, Batch 561/2194, Loss: 0.4074\n",
      "Epoch 1/25, Batch 571/2194, Loss: 0.5286\n",
      "Epoch 1/25, Batch 581/2194, Loss: 0.4173\n",
      "Epoch 1/25, Batch 591/2194, Loss: 0.2525\n",
      "Epoch 1/25, Batch 601/2194, Loss: 0.5181\n",
      "Epoch 1/25, Batch 611/2194, Loss: 0.5883\n",
      "Epoch 1/25, Batch 621/2194, Loss: 0.3424\n",
      "Epoch 1/25, Batch 631/2194, Loss: 0.3610\n",
      "Epoch 1/25, Batch 641/2194, Loss: 0.2794\n",
      "Epoch 1/25, Batch 651/2194, Loss: 0.4080\n",
      "Epoch 1/25, Batch 661/2194, Loss: 0.5389\n",
      "Epoch 1/25, Batch 671/2194, Loss: 0.4252\n",
      "Epoch 1/25, Batch 681/2194, Loss: 0.3971\n",
      "Epoch 1/25, Batch 691/2194, Loss: 0.5029\n",
      "Epoch 1/25, Batch 701/2194, Loss: 0.4409\n",
      "Epoch 1/25, Batch 711/2194, Loss: 0.3253\n",
      "Epoch 1/25, Batch 721/2194, Loss: 0.4676\n",
      "Epoch 1/25, Batch 731/2194, Loss: 0.3287\n",
      "Epoch 1/25, Batch 741/2194, Loss: 0.3426\n",
      "Epoch 1/25, Batch 751/2194, Loss: 0.4399\n",
      "Epoch 1/25, Batch 761/2194, Loss: 0.5765\n",
      "Epoch 1/25, Batch 771/2194, Loss: 0.5396\n",
      "Epoch 1/25, Batch 781/2194, Loss: 0.4204\n",
      "Epoch 1/25, Batch 791/2194, Loss: 0.2191\n",
      "Epoch 1/25, Batch 801/2194, Loss: 0.3656\n",
      "Epoch 1/25, Batch 811/2194, Loss: 0.3173\n",
      "Epoch 1/25, Batch 821/2194, Loss: 0.5897\n",
      "Epoch 1/25, Batch 831/2194, Loss: 0.3625\n",
      "Epoch 1/25, Batch 841/2194, Loss: 0.2535\n",
      "Epoch 1/25, Batch 851/2194, Loss: 0.3742\n",
      "Epoch 1/25, Batch 861/2194, Loss: 0.3183\n",
      "Epoch 1/25, Batch 871/2194, Loss: 0.3995\n",
      "Epoch 1/25, Batch 881/2194, Loss: 0.3864\n",
      "Epoch 1/25, Batch 891/2194, Loss: 0.4074\n",
      "Epoch 1/25, Batch 901/2194, Loss: 0.3294\n",
      "Epoch 1/25, Batch 911/2194, Loss: 0.3722\n",
      "Epoch 1/25, Batch 921/2194, Loss: 0.5289\n",
      "Epoch 1/25, Batch 931/2194, Loss: 0.4600\n",
      "Epoch 1/25, Batch 941/2194, Loss: 0.3528\n",
      "Epoch 1/25, Batch 951/2194, Loss: 0.3231\n",
      "Epoch 1/25, Batch 961/2194, Loss: 0.3569\n",
      "Epoch 1/25, Batch 971/2194, Loss: 0.3020\n",
      "Epoch 1/25, Batch 981/2194, Loss: 0.3991\n",
      "Epoch 1/25, Batch 991/2194, Loss: 0.3272\n",
      "Epoch 1/25, Batch 1001/2194, Loss: 0.4087\n",
      "Epoch 1/25, Batch 1011/2194, Loss: 0.4291\n",
      "Epoch 1/25, Batch 1021/2194, Loss: 0.3265\n",
      "Epoch 1/25, Batch 1031/2194, Loss: 0.4888\n",
      "Epoch 1/25, Batch 1041/2194, Loss: 0.2935\n",
      "Epoch 1/25, Batch 1051/2194, Loss: 0.4846\n",
      "Epoch 1/25, Batch 1061/2194, Loss: 0.5023\n",
      "Epoch 1/25, Batch 1071/2194, Loss: 0.3141\n",
      "Epoch 1/25, Batch 1081/2194, Loss: 0.3958\n",
      "Epoch 1/25, Batch 1091/2194, Loss: 0.2840\n",
      "Epoch 1/25, Batch 1101/2194, Loss: 0.4351\n",
      "Epoch 1/25, Batch 1111/2194, Loss: 0.5921\n",
      "Epoch 1/25, Batch 1121/2194, Loss: 0.1763\n",
      "Epoch 1/25, Batch 1131/2194, Loss: 0.2764\n",
      "Epoch 1/25, Batch 1141/2194, Loss: 0.5252\n",
      "Epoch 1/25, Batch 1151/2194, Loss: 0.4564\n",
      "Epoch 1/25, Batch 1161/2194, Loss: 0.5255\n",
      "Epoch 1/25, Batch 1171/2194, Loss: 0.2749\n",
      "Epoch 1/25, Batch 1181/2194, Loss: 0.6062\n",
      "Epoch 1/25, Batch 1191/2194, Loss: 0.2713\n",
      "Epoch 1/25, Batch 1201/2194, Loss: 0.4089\n",
      "Epoch 1/25, Batch 1211/2194, Loss: 0.3298\n",
      "Epoch 1/25, Batch 1221/2194, Loss: 0.3088\n",
      "Epoch 1/25, Batch 1231/2194, Loss: 0.3825\n",
      "Epoch 1/25, Batch 1241/2194, Loss: 0.3606\n",
      "Epoch 1/25, Batch 1251/2194, Loss: 0.2675\n",
      "Epoch 1/25, Batch 1261/2194, Loss: 0.4034\n",
      "Epoch 1/25, Batch 1271/2194, Loss: 0.6362\n",
      "Epoch 1/25, Batch 1281/2194, Loss: 0.4701\n",
      "Epoch 1/25, Batch 1291/2194, Loss: 0.3143\n",
      "Epoch 1/25, Batch 1301/2194, Loss: 0.3829\n",
      "Epoch 1/25, Batch 1311/2194, Loss: 0.2780\n",
      "Epoch 1/25, Batch 1321/2194, Loss: 0.1627\n",
      "Epoch 1/25, Batch 1331/2194, Loss: 0.3155\n",
      "Epoch 1/25, Batch 1341/2194, Loss: 0.3953\n",
      "Epoch 1/25, Batch 1351/2194, Loss: 0.3995\n",
      "Epoch 1/25, Batch 1361/2194, Loss: 0.4533\n",
      "Epoch 1/25, Batch 1371/2194, Loss: 0.3060\n",
      "Epoch 1/25, Batch 1381/2194, Loss: 0.2218\n",
      "Epoch 1/25, Batch 1391/2194, Loss: 0.2071\n",
      "Epoch 1/25, Batch 1401/2194, Loss: 0.2734\n",
      "Epoch 1/25, Batch 1411/2194, Loss: 0.3555\n",
      "Epoch 1/25, Batch 1421/2194, Loss: 0.5373\n",
      "Epoch 1/25, Batch 1431/2194, Loss: 0.3887\n",
      "Epoch 1/25, Batch 1441/2194, Loss: 0.2106\n",
      "Epoch 1/25, Batch 1451/2194, Loss: 0.3105\n",
      "Epoch 1/25, Batch 1461/2194, Loss: 0.3036\n",
      "Epoch 1/25, Batch 1471/2194, Loss: 0.3551\n",
      "Epoch 1/25, Batch 1481/2194, Loss: 0.2789\n",
      "Epoch 1/25, Batch 1491/2194, Loss: 0.4376\n",
      "Epoch 1/25, Batch 1501/2194, Loss: 0.5615\n",
      "Epoch 1/25, Batch 1511/2194, Loss: 0.5272\n",
      "Epoch 1/25, Batch 1521/2194, Loss: 0.5653\n",
      "Epoch 1/25, Batch 1531/2194, Loss: 0.4254\n",
      "Epoch 1/25, Batch 1541/2194, Loss: 0.1819\n",
      "Epoch 1/25, Batch 1551/2194, Loss: 0.3441\n",
      "Epoch 1/25, Batch 1561/2194, Loss: 0.2728\n",
      "Epoch 1/25, Batch 1571/2194, Loss: 0.2685\n",
      "Epoch 1/25, Batch 1581/2194, Loss: 0.5289\n",
      "Epoch 1/25, Batch 1591/2194, Loss: 0.5014\n",
      "Epoch 1/25, Batch 1601/2194, Loss: 0.2991\n",
      "Epoch 1/25, Batch 1611/2194, Loss: 0.3722\n",
      "Epoch 1/25, Batch 1621/2194, Loss: 0.3441\n",
      "Epoch 1/25, Batch 1631/2194, Loss: 0.5567\n",
      "Epoch 1/25, Batch 1641/2194, Loss: 0.3335\n",
      "Epoch 1/25, Batch 1651/2194, Loss: 0.4411\n",
      "Epoch 1/25, Batch 1661/2194, Loss: 0.4854\n",
      "Epoch 1/25, Batch 1671/2194, Loss: 0.3423\n",
      "Epoch 1/25, Batch 1681/2194, Loss: 0.3555\n",
      "Epoch 1/25, Batch 1691/2194, Loss: 0.4678\n",
      "Epoch 1/25, Batch 1701/2194, Loss: 0.4486\n",
      "Epoch 1/25, Batch 1711/2194, Loss: 0.2369\n",
      "Epoch 1/25, Batch 1721/2194, Loss: 0.2650\n",
      "Epoch 1/25, Batch 1731/2194, Loss: 0.4210\n",
      "Epoch 1/25, Batch 1741/2194, Loss: 0.2422\n",
      "Epoch 1/25, Batch 1751/2194, Loss: 0.2908\n",
      "Epoch 1/25, Batch 1761/2194, Loss: 0.4971\n",
      "Epoch 1/25, Batch 1771/2194, Loss: 0.3632\n",
      "Epoch 1/25, Batch 1781/2194, Loss: 0.4454\n",
      "Epoch 1/25, Batch 1791/2194, Loss: 0.2794\n",
      "Epoch 1/25, Batch 1801/2194, Loss: 0.2738\n",
      "Epoch 1/25, Batch 1811/2194, Loss: 0.2813\n",
      "Epoch 1/25, Batch 1821/2194, Loss: 0.2833\n",
      "Epoch 1/25, Batch 1831/2194, Loss: 0.2716\n",
      "Epoch 1/25, Batch 1841/2194, Loss: 0.2847\n",
      "Epoch 1/25, Batch 1851/2194, Loss: 0.6500\n",
      "Epoch 1/25, Batch 1861/2194, Loss: 0.2218\n",
      "Epoch 1/25, Batch 1871/2194, Loss: 0.2410\n",
      "Epoch 1/25, Batch 1881/2194, Loss: 0.4684\n",
      "Epoch 1/25, Batch 1891/2194, Loss: 0.4334\n",
      "Epoch 1/25, Batch 1901/2194, Loss: 0.2853\n",
      "Epoch 1/25, Batch 1911/2194, Loss: 0.2574\n",
      "Epoch 1/25, Batch 1921/2194, Loss: 0.4002\n",
      "Epoch 1/25, Batch 1931/2194, Loss: 0.2184\n",
      "Epoch 1/25, Batch 1941/2194, Loss: 0.3128\n",
      "Epoch 1/25, Batch 1951/2194, Loss: 0.2259\n",
      "Epoch 1/25, Batch 1961/2194, Loss: 0.8685\n",
      "Epoch 1/25, Batch 1971/2194, Loss: 0.4305\n",
      "Epoch 1/25, Batch 1981/2194, Loss: 0.3269\n",
      "Epoch 1/25, Batch 1991/2194, Loss: 0.2827\n",
      "Epoch 1/25, Batch 2001/2194, Loss: 0.5253\n",
      "Epoch 1/25, Batch 2011/2194, Loss: 0.2970\n",
      "Epoch 1/25, Batch 2021/2194, Loss: 0.3319\n",
      "Epoch 1/25, Batch 2031/2194, Loss: 0.2890\n",
      "Epoch 1/25, Batch 2041/2194, Loss: 0.4345\n",
      "Epoch 1/25, Batch 2051/2194, Loss: 0.4024\n",
      "Epoch 1/25, Batch 2061/2194, Loss: 0.2219\n",
      "Epoch 1/25, Batch 2071/2194, Loss: 0.4006\n",
      "Epoch 1/25, Batch 2081/2194, Loss: 0.3178\n",
      "Epoch 1/25, Batch 2091/2194, Loss: 0.3264\n",
      "Epoch 1/25, Batch 2101/2194, Loss: 0.2932\n",
      "Epoch 1/25, Batch 2111/2194, Loss: 0.2948\n",
      "Epoch 1/25, Batch 2121/2194, Loss: 0.3483\n",
      "Epoch 1/25, Batch 2131/2194, Loss: 0.3629\n",
      "Epoch 1/25, Batch 2141/2194, Loss: 0.4120\n",
      "Epoch 1/25, Batch 2151/2194, Loss: 0.2399\n",
      "Epoch 1/25, Batch 2161/2194, Loss: 0.2442\n",
      "Epoch 1/25, Batch 2171/2194, Loss: 0.2572\n",
      "Epoch 1/25, Batch 2181/2194, Loss: 0.4031\n",
      "Epoch 1/25, Batch 2191/2194, Loss: 0.2765\n",
      "Epoch 1/25:\n",
      "Train Loss: 0.4004, Train Acc: 81.73%\n",
      "Val Loss: 0.3082, Val Acc: 86.39%\n",
      "------------------------------------------------------------\n",
      "Starting epoch 2/25...\n",
      "Starting training loop with 2194 batches...\n",
      "Successfully loaded first batch: images shape torch.Size([32, 3, 224, 224]), labels shape torch.Size([32])\n",
      "Epoch 2/25, Batch 1/2194, Loss: 0.3669\n",
      "Epoch 2/25, Batch 11/2194, Loss: 0.3796\n",
      "Epoch 2/25, Batch 21/2194, Loss: 0.3091\n",
      "Epoch 2/25, Batch 31/2194, Loss: 0.4984\n",
      "Epoch 2/25, Batch 41/2194, Loss: 0.2673\n",
      "Epoch 2/25, Batch 51/2194, Loss: 0.3921\n",
      "Epoch 2/25, Batch 61/2194, Loss: 0.2977\n",
      "Epoch 2/25, Batch 71/2194, Loss: 0.3066\n",
      "Epoch 2/25, Batch 81/2194, Loss: 0.2340\n",
      "Epoch 2/25, Batch 91/2194, Loss: 0.4458\n",
      "Epoch 2/25, Batch 101/2194, Loss: 0.2203\n",
      "Epoch 2/25, Batch 111/2194, Loss: 0.2361\n",
      "Epoch 2/25, Batch 121/2194, Loss: 0.2738\n",
      "Epoch 2/25, Batch 131/2194, Loss: 0.4201\n",
      "Epoch 2/25, Batch 141/2194, Loss: 0.2475\n",
      "Epoch 2/25, Batch 151/2194, Loss: 0.2915\n",
      "Epoch 2/25, Batch 161/2194, Loss: 0.3891\n",
      "Epoch 2/25, Batch 171/2194, Loss: 0.2511\n",
      "Epoch 2/25, Batch 181/2194, Loss: 0.3281\n",
      "Epoch 2/25, Batch 191/2194, Loss: 0.6701\n",
      "Epoch 2/25, Batch 201/2194, Loss: 0.3195\n",
      "Epoch 2/25, Batch 211/2194, Loss: 0.3702\n",
      "Epoch 2/25, Batch 221/2194, Loss: 0.2871\n",
      "Epoch 2/25, Batch 231/2194, Loss: 0.1982\n",
      "Epoch 2/25, Batch 241/2194, Loss: 0.3065\n",
      "Epoch 2/25, Batch 251/2194, Loss: 0.5997\n",
      "Epoch 2/25, Batch 261/2194, Loss: 0.4661\n",
      "Epoch 2/25, Batch 271/2194, Loss: 0.2516\n",
      "Epoch 2/25, Batch 281/2194, Loss: 0.2864\n",
      "Epoch 2/25, Batch 291/2194, Loss: 0.2182\n",
      "Epoch 2/25, Batch 301/2194, Loss: 0.1937\n",
      "Epoch 2/25, Batch 311/2194, Loss: 0.3551\n",
      "Epoch 2/25, Batch 321/2194, Loss: 0.2219\n",
      "Epoch 2/25, Batch 331/2194, Loss: 0.2599\n",
      "Epoch 2/25, Batch 341/2194, Loss: 0.2646\n",
      "Epoch 2/25, Batch 351/2194, Loss: 0.3725\n",
      "Epoch 2/25, Batch 361/2194, Loss: 0.4429\n",
      "Epoch 2/25, Batch 371/2194, Loss: 0.2764\n",
      "Epoch 2/25, Batch 381/2194, Loss: 0.4194\n",
      "Epoch 2/25, Batch 391/2194, Loss: 0.3382\n",
      "Epoch 2/25, Batch 401/2194, Loss: 0.3808\n",
      "Epoch 2/25, Batch 411/2194, Loss: 0.1544\n",
      "Epoch 2/25, Batch 421/2194, Loss: 0.4341\n",
      "Epoch 2/25, Batch 431/2194, Loss: 0.3652\n",
      "Epoch 2/25, Batch 441/2194, Loss: 0.2723\n",
      "Epoch 2/25, Batch 451/2194, Loss: 0.5330\n",
      "Epoch 2/25, Batch 461/2194, Loss: 0.1382\n",
      "Epoch 2/25, Batch 471/2194, Loss: 0.3777\n",
      "Epoch 2/25, Batch 481/2194, Loss: 0.2333\n",
      "Epoch 2/25, Batch 491/2194, Loss: 0.3063\n",
      "Epoch 2/25, Batch 501/2194, Loss: 0.4242\n",
      "Epoch 2/25, Batch 511/2194, Loss: 0.3712\n",
      "Epoch 2/25, Batch 521/2194, Loss: 0.5509\n",
      "Epoch 2/25, Batch 531/2194, Loss: 0.2237\n",
      "Epoch 2/25, Batch 541/2194, Loss: 0.3385\n",
      "Epoch 2/25, Batch 551/2194, Loss: 0.1575\n",
      "Epoch 2/25, Batch 561/2194, Loss: 0.2922\n",
      "Epoch 2/25, Batch 571/2194, Loss: 0.1396\n",
      "Epoch 2/25, Batch 581/2194, Loss: 0.2903\n",
      "Epoch 2/25, Batch 591/2194, Loss: 0.4320\n",
      "Epoch 2/25, Batch 601/2194, Loss: 0.3245\n",
      "Epoch 2/25, Batch 611/2194, Loss: 0.1646\n",
      "Epoch 2/25, Batch 621/2194, Loss: 0.1903\n",
      "Epoch 2/25, Batch 631/2194, Loss: 0.2152\n",
      "Epoch 2/25, Batch 641/2194, Loss: 0.2447\n",
      "Epoch 2/25, Batch 651/2194, Loss: 0.2511\n",
      "Epoch 2/25, Batch 661/2194, Loss: 0.3548\n",
      "Epoch 2/25, Batch 671/2194, Loss: 0.3097\n",
      "Epoch 2/25, Batch 681/2194, Loss: 0.2688\n",
      "Epoch 2/25, Batch 691/2194, Loss: 0.1890\n",
      "Epoch 2/25, Batch 701/2194, Loss: 0.3106\n",
      "Epoch 2/25, Batch 711/2194, Loss: 0.4574\n",
      "Epoch 2/25, Batch 721/2194, Loss: 0.4635\n",
      "Epoch 2/25, Batch 731/2194, Loss: 0.3563\n",
      "Epoch 2/25, Batch 741/2194, Loss: 0.3496\n",
      "Epoch 2/25, Batch 751/2194, Loss: 0.2625\n",
      "Epoch 2/25, Batch 761/2194, Loss: 0.3184\n",
      "Epoch 2/25, Batch 771/2194, Loss: 0.2792\n",
      "Epoch 2/25, Batch 781/2194, Loss: 0.4372\n",
      "Epoch 2/25, Batch 791/2194, Loss: 0.2734\n",
      "Epoch 2/25, Batch 801/2194, Loss: 0.3514\n",
      "Epoch 2/25, Batch 811/2194, Loss: 0.2693\n",
      "Epoch 2/25, Batch 821/2194, Loss: 0.4590\n",
      "Epoch 2/25, Batch 831/2194, Loss: 0.1670\n",
      "Epoch 2/25, Batch 841/2194, Loss: 0.2821\n",
      "Epoch 2/25, Batch 851/2194, Loss: 0.2900\n",
      "Epoch 2/25, Batch 861/2194, Loss: 0.2442\n",
      "Epoch 2/25, Batch 871/2194, Loss: 0.5039\n",
      "Epoch 2/25, Batch 881/2194, Loss: 0.2321\n",
      "Epoch 2/25, Batch 891/2194, Loss: 0.2551\n",
      "Epoch 2/25, Batch 901/2194, Loss: 0.3336\n",
      "Epoch 2/25, Batch 911/2194, Loss: 0.3775\n",
      "Epoch 2/25, Batch 921/2194, Loss: 0.3756\n",
      "Epoch 2/25, Batch 931/2194, Loss: 0.2008\n",
      "Epoch 2/25, Batch 941/2194, Loss: 0.2012\n",
      "Epoch 2/25, Batch 951/2194, Loss: 0.2785\n",
      "Epoch 2/25, Batch 961/2194, Loss: 0.6802\n",
      "Epoch 2/25, Batch 971/2194, Loss: 0.3040\n",
      "Epoch 2/25, Batch 981/2194, Loss: 0.4636\n",
      "Epoch 2/25, Batch 991/2194, Loss: 0.2095\n",
      "Epoch 2/25, Batch 1001/2194, Loss: 0.3392\n",
      "Epoch 2/25, Batch 1011/2194, Loss: 0.3377\n",
      "Epoch 2/25, Batch 1021/2194, Loss: 0.2636\n",
      "Epoch 2/25, Batch 1031/2194, Loss: 0.2245\n",
      "Epoch 2/25, Batch 1041/2194, Loss: 0.1922\n",
      "Epoch 2/25, Batch 1051/2194, Loss: 0.2051\n",
      "Epoch 2/25, Batch 1061/2194, Loss: 0.3735\n",
      "Epoch 2/25, Batch 1071/2194, Loss: 0.2285\n",
      "Epoch 2/25, Batch 1081/2194, Loss: 0.2783\n",
      "Epoch 2/25, Batch 1091/2194, Loss: 0.2940\n",
      "Epoch 2/25, Batch 1101/2194, Loss: 0.2867\n",
      "Epoch 2/25, Batch 1111/2194, Loss: 0.3408\n",
      "Epoch 2/25, Batch 1121/2194, Loss: 0.2334\n",
      "Epoch 2/25, Batch 1131/2194, Loss: 0.2822\n",
      "Epoch 2/25, Batch 1141/2194, Loss: 0.2465\n",
      "Epoch 2/25, Batch 1151/2194, Loss: 0.2536\n",
      "Epoch 2/25, Batch 1161/2194, Loss: 0.2294\n",
      "Epoch 2/25, Batch 1171/2194, Loss: 0.3270\n",
      "Epoch 2/25, Batch 1181/2194, Loss: 0.3393\n",
      "Epoch 2/25, Batch 1191/2194, Loss: 0.1506\n",
      "Epoch 2/25, Batch 1201/2194, Loss: 0.4310\n",
      "Epoch 2/25, Batch 1211/2194, Loss: 0.3059\n",
      "Epoch 2/25, Batch 1221/2194, Loss: 0.3053\n",
      "Epoch 2/25, Batch 1231/2194, Loss: 0.2173\n",
      "Epoch 2/25, Batch 1241/2194, Loss: 0.1499\n",
      "Epoch 2/25, Batch 1251/2194, Loss: 0.4373\n",
      "Epoch 2/25, Batch 1261/2194, Loss: 0.3442\n",
      "Epoch 2/25, Batch 1271/2194, Loss: 0.2523\n",
      "Epoch 2/25, Batch 1281/2194, Loss: 0.3836\n",
      "Epoch 2/25, Batch 1291/2194, Loss: 0.2829\n",
      "Epoch 2/25, Batch 1301/2194, Loss: 0.1336\n",
      "Epoch 2/25, Batch 1311/2194, Loss: 0.2766\n",
      "Epoch 2/25, Batch 1321/2194, Loss: 0.1173\n",
      "Epoch 2/25, Batch 1331/2194, Loss: 0.4070\n",
      "Epoch 2/25, Batch 1341/2194, Loss: 0.3430\n",
      "Epoch 2/25, Batch 1351/2194, Loss: 0.2442\n",
      "Epoch 2/25, Batch 1361/2194, Loss: 0.4952\n",
      "Epoch 2/25, Batch 1371/2194, Loss: 0.2318\n",
      "Epoch 2/25, Batch 1381/2194, Loss: 0.3903\n",
      "Epoch 2/25, Batch 1391/2194, Loss: 0.1852\n",
      "Epoch 2/25, Batch 1401/2194, Loss: 0.3671\n",
      "Epoch 2/25, Batch 1411/2194, Loss: 0.2578\n",
      "Epoch 2/25, Batch 1421/2194, Loss: 0.2470\n",
      "Epoch 2/25, Batch 1431/2194, Loss: 0.2607\n",
      "Epoch 2/25, Batch 1441/2194, Loss: 0.1998\n",
      "Epoch 2/25, Batch 1451/2194, Loss: 0.2842\n",
      "Epoch 2/25, Batch 1461/2194, Loss: 0.4268\n",
      "Epoch 2/25, Batch 1471/2194, Loss: 0.2959\n",
      "Epoch 2/25, Batch 1481/2194, Loss: 0.3246\n",
      "Epoch 2/25, Batch 1491/2194, Loss: 0.2081\n",
      "Epoch 2/25, Batch 1501/2194, Loss: 0.1987\n",
      "Epoch 2/25, Batch 1511/2194, Loss: 0.3010\n",
      "Epoch 2/25, Batch 1521/2194, Loss: 0.2373\n",
      "Epoch 2/25, Batch 1531/2194, Loss: 0.4389\n",
      "Epoch 2/25, Batch 1541/2194, Loss: 0.2517\n",
      "Epoch 2/25, Batch 1551/2194, Loss: 0.3848\n",
      "Epoch 2/25, Batch 1561/2194, Loss: 0.1644\n",
      "Epoch 2/25, Batch 1571/2194, Loss: 0.3382\n",
      "Epoch 2/25, Batch 1581/2194, Loss: 0.1796\n",
      "Epoch 2/25, Batch 1591/2194, Loss: 0.2557\n",
      "Epoch 2/25, Batch 1601/2194, Loss: 0.4108\n",
      "Epoch 2/25, Batch 1611/2194, Loss: 0.3758\n",
      "Epoch 2/25, Batch 1621/2194, Loss: 0.3068\n",
      "Epoch 2/25, Batch 1631/2194, Loss: 0.2124\n",
      "Epoch 2/25, Batch 1641/2194, Loss: 0.2276\n",
      "Epoch 2/25, Batch 1651/2194, Loss: 0.3248\n",
      "Epoch 2/25, Batch 1661/2194, Loss: 0.2502\n",
      "Epoch 2/25, Batch 1671/2194, Loss: 0.2623\n",
      "Epoch 2/25, Batch 1681/2194, Loss: 0.2141\n",
      "Epoch 2/25, Batch 1691/2194, Loss: 0.3782\n",
      "Epoch 2/25, Batch 1701/2194, Loss: 0.2699\n",
      "Epoch 2/25, Batch 1711/2194, Loss: 0.1503\n",
      "Epoch 2/25, Batch 1721/2194, Loss: 0.2442\n",
      "Epoch 2/25, Batch 1731/2194, Loss: 0.3726\n",
      "Epoch 2/25, Batch 1741/2194, Loss: 0.1381\n",
      "Epoch 2/25, Batch 1751/2194, Loss: 0.2465\n",
      "Epoch 2/25, Batch 1761/2194, Loss: 0.3162\n",
      "Epoch 2/25, Batch 1771/2194, Loss: 0.2905\n",
      "Epoch 2/25, Batch 1781/2194, Loss: 0.2794\n",
      "Epoch 2/25, Batch 1791/2194, Loss: 0.2633\n",
      "Epoch 2/25, Batch 1801/2194, Loss: 0.2994\n",
      "Epoch 2/25, Batch 1811/2194, Loss: 0.1520\n",
      "Epoch 2/25, Batch 1821/2194, Loss: 0.1171\n",
      "Epoch 2/25, Batch 1831/2194, Loss: 0.2046\n",
      "Epoch 2/25, Batch 1841/2194, Loss: 0.4599\n",
      "Epoch 2/25, Batch 1851/2194, Loss: 0.4141\n",
      "Epoch 2/25, Batch 1861/2194, Loss: 0.2270\n",
      "Epoch 2/25, Batch 1871/2194, Loss: 0.2377\n",
      "Epoch 2/25, Batch 1881/2194, Loss: 0.1891\n",
      "Epoch 2/25, Batch 1891/2194, Loss: 0.2099\n",
      "Epoch 2/25, Batch 1901/2194, Loss: 0.3531\n",
      "Epoch 2/25, Batch 1911/2194, Loss: 0.1724\n",
      "Epoch 2/25, Batch 1921/2194, Loss: 0.1119\n",
      "Epoch 2/25, Batch 1931/2194, Loss: 0.2001\n",
      "Epoch 2/25, Batch 1941/2194, Loss: 0.3602\n",
      "Epoch 2/25, Batch 1951/2194, Loss: 0.2822\n",
      "Epoch 2/25, Batch 1961/2194, Loss: 0.2868\n",
      "Epoch 2/25, Batch 1971/2194, Loss: 0.4906\n",
      "Epoch 2/25, Batch 1981/2194, Loss: 0.4730\n",
      "Epoch 2/25, Batch 1991/2194, Loss: 0.2651\n",
      "Epoch 2/25, Batch 2001/2194, Loss: 0.2678\n",
      "Epoch 2/25, Batch 2011/2194, Loss: 0.1741\n",
      "Epoch 2/25, Batch 2021/2194, Loss: 0.4512\n",
      "Epoch 2/25, Batch 2031/2194, Loss: 0.3847\n",
      "Epoch 2/25, Batch 2041/2194, Loss: 0.2057\n",
      "Epoch 2/25, Batch 2051/2194, Loss: 0.2606\n",
      "Epoch 2/25, Batch 2061/2194, Loss: 0.1039\n",
      "Epoch 2/25, Batch 2071/2194, Loss: 0.3950\n",
      "Epoch 2/25, Batch 2081/2194, Loss: 0.2872\n",
      "Epoch 2/25, Batch 2091/2194, Loss: 0.2379\n",
      "Epoch 2/25, Batch 2101/2194, Loss: 0.3490\n",
      "Epoch 2/25, Batch 2111/2194, Loss: 0.2262\n",
      "Epoch 2/25, Batch 2121/2194, Loss: 0.3913\n",
      "Epoch 2/25, Batch 2131/2194, Loss: 0.3760\n",
      "Epoch 2/25, Batch 2141/2194, Loss: 0.2968\n",
      "Epoch 2/25, Batch 2151/2194, Loss: 0.3971\n",
      "Epoch 2/25, Batch 2161/2194, Loss: 0.3400\n",
      "Epoch 2/25, Batch 2171/2194, Loss: 0.3165\n",
      "Epoch 2/25, Batch 2181/2194, Loss: 0.3993\n",
      "Epoch 2/25, Batch 2191/2194, Loss: 0.2976\n",
      "Epoch 2/25:\n",
      "Train Loss: 0.3057, Train Acc: 86.58%\n",
      "Val Loss: 0.2499, Val Acc: 89.15%\n",
      "------------------------------------------------------------\n",
      "Starting epoch 3/25...\n",
      "Starting training loop with 2194 batches...\n",
      "Successfully loaded first batch: images shape torch.Size([32, 3, 224, 224]), labels shape torch.Size([32])\n",
      "Epoch 3/25, Batch 1/2194, Loss: 0.3440\n",
      "Epoch 3/25, Batch 11/2194, Loss: 0.2819\n",
      "Epoch 3/25, Batch 21/2194, Loss: 0.2473\n",
      "Epoch 3/25, Batch 31/2194, Loss: 0.2605\n",
      "Epoch 3/25, Batch 41/2194, Loss: 0.1309\n",
      "Epoch 3/25, Batch 51/2194, Loss: 0.4512\n",
      "Epoch 3/25, Batch 61/2194, Loss: 0.3198\n",
      "Epoch 3/25, Batch 71/2194, Loss: 0.2634\n",
      "Epoch 3/25, Batch 81/2194, Loss: 0.1329\n",
      "Epoch 3/25, Batch 91/2194, Loss: 0.1080\n",
      "Epoch 3/25, Batch 101/2194, Loss: 0.1975\n",
      "Epoch 3/25, Batch 111/2194, Loss: 0.1364\n",
      "Epoch 3/25, Batch 121/2194, Loss: 0.1302\n",
      "Epoch 3/25, Batch 131/2194, Loss: 0.2817\n",
      "Epoch 3/25, Batch 141/2194, Loss: 0.2238\n",
      "Epoch 3/25, Batch 151/2194, Loss: 0.2804\n",
      "Epoch 3/25, Batch 161/2194, Loss: 0.2845\n",
      "Epoch 3/25, Batch 171/2194, Loss: 0.1661\n",
      "Epoch 3/25, Batch 181/2194, Loss: 0.2632\n",
      "Epoch 3/25, Batch 191/2194, Loss: 0.1491\n",
      "Epoch 3/25, Batch 201/2194, Loss: 0.1314\n",
      "Epoch 3/25, Batch 211/2194, Loss: 0.6343\n",
      "Epoch 3/25, Batch 221/2194, Loss: 0.3341\n",
      "Epoch 3/25, Batch 231/2194, Loss: 0.5443\n",
      "Epoch 3/25, Batch 241/2194, Loss: 0.1361\n",
      "Epoch 3/25, Batch 251/2194, Loss: 0.1791\n",
      "Epoch 3/25, Batch 261/2194, Loss: 0.2704\n",
      "Epoch 3/25, Batch 271/2194, Loss: 0.3178\n",
      "Epoch 3/25, Batch 281/2194, Loss: 0.2411\n",
      "Epoch 3/25, Batch 291/2194, Loss: 0.1385\n",
      "Epoch 3/25, Batch 301/2194, Loss: 0.2264\n",
      "Epoch 3/25, Batch 311/2194, Loss: 0.2812\n",
      "Epoch 3/25, Batch 321/2194, Loss: 0.5814\n",
      "Epoch 3/25, Batch 331/2194, Loss: 0.4075\n",
      "Epoch 3/25, Batch 341/2194, Loss: 0.2517\n",
      "Epoch 3/25, Batch 351/2194, Loss: 0.2473\n",
      "Epoch 3/25, Batch 361/2194, Loss: 0.2645\n",
      "Epoch 3/25, Batch 371/2194, Loss: 0.2695\n",
      "Epoch 3/25, Batch 381/2194, Loss: 0.1406\n",
      "Epoch 3/25, Batch 391/2194, Loss: 0.1386\n",
      "Epoch 3/25, Batch 401/2194, Loss: 0.2099\n",
      "Epoch 3/25, Batch 411/2194, Loss: 0.2876\n",
      "Epoch 3/25, Batch 421/2194, Loss: 0.1394\n",
      "Epoch 3/25, Batch 431/2194, Loss: 0.3743\n",
      "Epoch 3/25, Batch 441/2194, Loss: 0.2459\n",
      "Epoch 3/25, Batch 451/2194, Loss: 0.2483\n",
      "Epoch 3/25, Batch 461/2194, Loss: 0.3338\n",
      "Epoch 3/25, Batch 471/2194, Loss: 0.3292\n",
      "Epoch 3/25, Batch 481/2194, Loss: 0.2712\n",
      "Epoch 3/25, Batch 491/2194, Loss: 0.2722\n",
      "Epoch 3/25, Batch 501/2194, Loss: 0.3163\n",
      "Epoch 3/25, Batch 511/2194, Loss: 0.2552\n",
      "Epoch 3/25, Batch 521/2194, Loss: 0.2020\n",
      "Epoch 3/25, Batch 531/2194, Loss: 0.4222\n",
      "Epoch 3/25, Batch 541/2194, Loss: 0.3729\n",
      "Epoch 3/25, Batch 551/2194, Loss: 0.2418\n",
      "Epoch 3/25, Batch 561/2194, Loss: 0.2453\n",
      "Epoch 3/25, Batch 571/2194, Loss: 0.2646\n",
      "Epoch 3/25, Batch 581/2194, Loss: 0.3677\n",
      "Epoch 3/25, Batch 591/2194, Loss: 0.2505\n",
      "Epoch 3/25, Batch 601/2194, Loss: 0.2363\n",
      "Epoch 3/25, Batch 611/2194, Loss: 0.1897\n",
      "Epoch 3/25, Batch 621/2194, Loss: 0.2784\n",
      "Epoch 3/25, Batch 631/2194, Loss: 0.3687\n",
      "Epoch 3/25, Batch 641/2194, Loss: 0.1482\n",
      "Epoch 3/25, Batch 651/2194, Loss: 0.3221\n",
      "Epoch 3/25, Batch 661/2194, Loss: 0.2550\n",
      "Epoch 3/25, Batch 671/2194, Loss: 0.1708\n",
      "Epoch 3/25, Batch 681/2194, Loss: 0.1832\n",
      "Epoch 3/25, Batch 691/2194, Loss: 0.3119\n",
      "Epoch 3/25, Batch 701/2194, Loss: 0.2802\n",
      "Epoch 3/25, Batch 711/2194, Loss: 0.3647\n",
      "Epoch 3/25, Batch 721/2194, Loss: 0.1834\n",
      "Epoch 3/25, Batch 731/2194, Loss: 0.5887\n",
      "Epoch 3/25, Batch 741/2194, Loss: 0.1905\n",
      "Epoch 3/25, Batch 751/2194, Loss: 0.2219\n",
      "Epoch 3/25, Batch 761/2194, Loss: 0.1596\n",
      "Epoch 3/25, Batch 771/2194, Loss: 0.1868\n",
      "Epoch 3/25, Batch 781/2194, Loss: 0.4337\n",
      "Epoch 3/25, Batch 791/2194, Loss: 0.1684\n",
      "Epoch 3/25, Batch 801/2194, Loss: 0.2674\n",
      "Epoch 3/25, Batch 811/2194, Loss: 0.1324\n",
      "Epoch 3/25, Batch 821/2194, Loss: 0.3211\n",
      "Epoch 3/25, Batch 831/2194, Loss: 0.2581\n",
      "Epoch 3/25, Batch 841/2194, Loss: 0.3112\n",
      "Epoch 3/25, Batch 851/2194, Loss: 0.3672\n",
      "Epoch 3/25, Batch 861/2194, Loss: 0.2546\n",
      "Epoch 3/25, Batch 871/2194, Loss: 0.3666\n",
      "Epoch 3/25, Batch 881/2194, Loss: 0.3013\n",
      "Epoch 3/25, Batch 891/2194, Loss: 0.2001\n",
      "Epoch 3/25, Batch 901/2194, Loss: 0.3215\n",
      "Epoch 3/25, Batch 911/2194, Loss: 0.2667\n",
      "Epoch 3/25, Batch 921/2194, Loss: 0.1621\n",
      "Epoch 3/25, Batch 931/2194, Loss: 0.1997\n",
      "Epoch 3/25, Batch 941/2194, Loss: 0.4543\n",
      "Epoch 3/25, Batch 951/2194, Loss: 0.3254\n",
      "Epoch 3/25, Batch 961/2194, Loss: 0.1387\n",
      "Epoch 3/25, Batch 971/2194, Loss: 0.1732\n",
      "Epoch 3/25, Batch 981/2194, Loss: 0.2338\n",
      "Epoch 3/25, Batch 991/2194, Loss: 0.4366\n",
      "Epoch 3/25, Batch 1001/2194, Loss: 0.2390\n",
      "Epoch 3/25, Batch 1011/2194, Loss: 0.2784\n",
      "Epoch 3/25, Batch 1021/2194, Loss: 0.2060\n",
      "Epoch 3/25, Batch 1031/2194, Loss: 0.2122\n",
      "Epoch 3/25, Batch 1041/2194, Loss: 0.2485\n",
      "Epoch 3/25, Batch 1051/2194, Loss: 0.2135\n",
      "Epoch 3/25, Batch 1061/2194, Loss: 0.2129\n",
      "Epoch 3/25, Batch 1071/2194, Loss: 0.2436\n",
      "Epoch 3/25, Batch 1081/2194, Loss: 0.1663\n",
      "Epoch 3/25, Batch 1091/2194, Loss: 0.4754\n",
      "Epoch 3/25, Batch 1101/2194, Loss: 0.2812\n",
      "Epoch 3/25, Batch 1111/2194, Loss: 0.3000\n",
      "Epoch 3/25, Batch 1121/2194, Loss: 0.2226\n",
      "Epoch 3/25, Batch 1131/2194, Loss: 0.1976\n",
      "Epoch 3/25, Batch 1141/2194, Loss: 0.2299\n",
      "Epoch 3/25, Batch 1151/2194, Loss: 0.4755\n",
      "Epoch 3/25, Batch 1161/2194, Loss: 0.2110\n",
      "Epoch 3/25, Batch 1171/2194, Loss: 0.1020\n",
      "Epoch 3/25, Batch 1181/2194, Loss: 0.2309\n",
      "Epoch 3/25, Batch 1191/2194, Loss: 0.3087\n",
      "Epoch 3/25, Batch 1201/2194, Loss: 0.1559\n",
      "Epoch 3/25, Batch 1211/2194, Loss: 0.1414\n",
      "Epoch 3/25, Batch 1221/2194, Loss: 0.3086\n",
      "Epoch 3/25, Batch 1231/2194, Loss: 0.1998\n",
      "Epoch 3/25, Batch 1241/2194, Loss: 0.1778\n",
      "Epoch 3/25, Batch 1251/2194, Loss: 0.2090\n",
      "Epoch 3/25, Batch 1261/2194, Loss: 0.2407\n",
      "Epoch 3/25, Batch 1271/2194, Loss: 0.3622\n",
      "Epoch 3/25, Batch 1281/2194, Loss: 0.1707\n",
      "Epoch 3/25, Batch 1291/2194, Loss: 0.2466\n",
      "Epoch 3/25, Batch 1301/2194, Loss: 0.2201\n",
      "Epoch 3/25, Batch 1311/2194, Loss: 0.3585\n",
      "Epoch 3/25, Batch 1321/2194, Loss: 0.3383\n",
      "Epoch 3/25, Batch 1331/2194, Loss: 0.3456\n",
      "Epoch 3/25, Batch 1341/2194, Loss: 0.2940\n",
      "Epoch 3/25, Batch 1351/2194, Loss: 0.2314\n",
      "Epoch 3/25, Batch 1361/2194, Loss: 0.1890\n",
      "Epoch 3/25, Batch 1371/2194, Loss: 0.3371\n",
      "Epoch 3/25, Batch 1381/2194, Loss: 0.3495\n",
      "Epoch 3/25, Batch 1391/2194, Loss: 0.2209\n",
      "Epoch 3/25, Batch 1401/2194, Loss: 0.3655\n",
      "Epoch 3/25, Batch 1411/2194, Loss: 0.2824\n",
      "Epoch 3/25, Batch 1421/2194, Loss: 0.1455\n",
      "Epoch 3/25, Batch 1431/2194, Loss: 0.2116\n",
      "Epoch 3/25, Batch 1441/2194, Loss: 0.2468\n",
      "Epoch 3/25, Batch 1451/2194, Loss: 0.3735\n",
      "Epoch 3/25, Batch 1461/2194, Loss: 0.3178\n",
      "Epoch 3/25, Batch 1471/2194, Loss: 0.0734\n",
      "Epoch 3/25, Batch 1481/2194, Loss: 0.1515\n",
      "Epoch 3/25, Batch 1491/2194, Loss: 0.1475\n",
      "Epoch 3/25, Batch 1501/2194, Loss: 0.3663\n",
      "Epoch 3/25, Batch 1511/2194, Loss: 0.1581\n",
      "Epoch 3/25, Batch 1521/2194, Loss: 0.1578\n",
      "Epoch 3/25, Batch 1531/2194, Loss: 0.1502\n",
      "Epoch 3/25, Batch 1541/2194, Loss: 0.4452\n",
      "Epoch 3/25, Batch 1551/2194, Loss: 0.2859\n",
      "Epoch 3/25, Batch 1561/2194, Loss: 0.3538\n",
      "Epoch 3/25, Batch 1571/2194, Loss: 0.3828\n",
      "Epoch 3/25, Batch 1581/2194, Loss: 0.2982\n",
      "Epoch 3/25, Batch 1591/2194, Loss: 0.4106\n",
      "Epoch 3/25, Batch 1601/2194, Loss: 0.4050\n",
      "Epoch 3/25, Batch 1611/2194, Loss: 0.1516\n",
      "Epoch 3/25, Batch 1621/2194, Loss: 0.4626\n",
      "Epoch 3/25, Batch 1631/2194, Loss: 0.2613\n",
      "Epoch 3/25, Batch 1641/2194, Loss: 0.2838\n",
      "Epoch 3/25, Batch 1651/2194, Loss: 0.2502\n",
      "Epoch 3/25, Batch 1661/2194, Loss: 0.3594\n",
      "Epoch 3/25, Batch 1671/2194, Loss: 0.1562\n",
      "Epoch 3/25, Batch 1681/2194, Loss: 0.1467\n",
      "Epoch 3/25, Batch 1691/2194, Loss: 0.2323\n",
      "Epoch 3/25, Batch 1701/2194, Loss: 0.3404\n",
      "Epoch 3/25, Batch 1711/2194, Loss: 0.2427\n",
      "Epoch 3/25, Batch 1721/2194, Loss: 0.1908\n",
      "Epoch 3/25, Batch 1731/2194, Loss: 0.1714\n",
      "Epoch 3/25, Batch 1741/2194, Loss: 0.2103\n",
      "Epoch 3/25, Batch 1751/2194, Loss: 0.3805\n",
      "Epoch 3/25, Batch 1761/2194, Loss: 0.1907\n",
      "Epoch 3/25, Batch 1771/2194, Loss: 0.3471\n",
      "Epoch 3/25, Batch 1781/2194, Loss: 0.1798\n",
      "Epoch 3/25, Batch 1791/2194, Loss: 0.1086\n",
      "Epoch 3/25, Batch 1801/2194, Loss: 0.2048\n",
      "Epoch 3/25, Batch 1811/2194, Loss: 0.0781\n",
      "Epoch 3/25, Batch 1821/2194, Loss: 0.2241\n",
      "Epoch 3/25, Batch 1831/2194, Loss: 0.2063\n",
      "Epoch 3/25, Batch 1841/2194, Loss: 0.4068\n",
      "Epoch 3/25, Batch 1851/2194, Loss: 0.2023\n",
      "Epoch 3/25, Batch 1861/2194, Loss: 0.3064\n",
      "Epoch 3/25, Batch 1871/2194, Loss: 0.1178\n",
      "Epoch 3/25, Batch 1881/2194, Loss: 0.2522\n",
      "Epoch 3/25, Batch 1891/2194, Loss: 0.2601\n",
      "Epoch 3/25, Batch 1901/2194, Loss: 0.1798\n",
      "Epoch 3/25, Batch 1911/2194, Loss: 0.2304\n",
      "Epoch 3/25, Batch 1921/2194, Loss: 0.2525\n",
      "Epoch 3/25, Batch 1931/2194, Loss: 0.4086\n",
      "Epoch 3/25, Batch 1941/2194, Loss: 0.2416\n",
      "Epoch 3/25, Batch 1951/2194, Loss: 0.3947\n",
      "Epoch 3/25, Batch 1961/2194, Loss: 0.1611\n",
      "Epoch 3/25, Batch 1971/2194, Loss: 0.2200\n",
      "Epoch 3/25, Batch 1981/2194, Loss: 0.3546\n",
      "Epoch 3/25, Batch 1991/2194, Loss: 0.1991\n",
      "Epoch 3/25, Batch 2001/2194, Loss: 0.2473\n",
      "Epoch 3/25, Batch 2011/2194, Loss: 0.1964\n",
      "Epoch 3/25, Batch 2021/2194, Loss: 0.2424\n",
      "Epoch 3/25, Batch 2031/2194, Loss: 0.2675\n",
      "Epoch 3/25, Batch 2041/2194, Loss: 0.4791\n",
      "Epoch 3/25, Batch 2051/2194, Loss: 0.1524\n",
      "Epoch 3/25, Batch 2061/2194, Loss: 0.2151\n",
      "Epoch 3/25, Batch 2071/2194, Loss: 0.2095\n",
      "Epoch 3/25, Batch 2081/2194, Loss: 0.0909\n",
      "Epoch 3/25, Batch 2091/2194, Loss: 0.1695\n",
      "Epoch 3/25, Batch 2101/2194, Loss: 0.1824\n",
      "Epoch 3/25, Batch 2111/2194, Loss: 0.3275\n",
      "Epoch 3/25, Batch 2121/2194, Loss: 0.4488\n",
      "Epoch 3/25, Batch 2131/2194, Loss: 0.2358\n",
      "Epoch 3/25, Batch 2141/2194, Loss: 0.2899\n",
      "Epoch 3/25, Batch 2151/2194, Loss: 0.3192\n",
      "Epoch 3/25, Batch 2161/2194, Loss: 0.5662\n",
      "Epoch 3/25, Batch 2171/2194, Loss: 0.0875\n",
      "Epoch 3/25, Batch 2181/2194, Loss: 0.2660\n",
      "Epoch 3/25, Batch 2191/2194, Loss: 0.1955\n",
      "Epoch 3/25:\n",
      "Train Loss: 0.2592, Train Acc: 88.78%\n",
      "Val Loss: 0.2273, Val Acc: 89.88%\n",
      "------------------------------------------------------------\n",
      "Starting epoch 4/25...\n",
      "Starting training loop with 2194 batches...\n",
      "Successfully loaded first batch: images shape torch.Size([32, 3, 224, 224]), labels shape torch.Size([32])\n",
      "Epoch 4/25, Batch 1/2194, Loss: 0.1772\n",
      "Epoch 4/25, Batch 11/2194, Loss: 0.1361\n",
      "Epoch 4/25, Batch 21/2194, Loss: 0.3746\n",
      "Epoch 4/25, Batch 31/2194, Loss: 0.1055\n",
      "Epoch 4/25, Batch 41/2194, Loss: 0.1521\n",
      "Epoch 4/25, Batch 51/2194, Loss: 0.3936\n",
      "Epoch 4/25, Batch 61/2194, Loss: 0.3420\n",
      "Epoch 4/25, Batch 71/2194, Loss: 0.3820\n",
      "Epoch 4/25, Batch 81/2194, Loss: 0.2136\n",
      "Epoch 4/25, Batch 91/2194, Loss: 0.3397\n",
      "Epoch 4/25, Batch 101/2194, Loss: 0.2760\n",
      "Epoch 4/25, Batch 111/2194, Loss: 0.4053\n",
      "Epoch 4/25, Batch 121/2194, Loss: 0.4465\n",
      "Epoch 4/25, Batch 131/2194, Loss: 0.2303\n",
      "Epoch 4/25, Batch 141/2194, Loss: 0.0931\n",
      "Epoch 4/25, Batch 151/2194, Loss: 0.2958\n",
      "Epoch 4/25, Batch 161/2194, Loss: 0.2675\n",
      "Epoch 4/25, Batch 171/2194, Loss: 0.2632\n",
      "Epoch 4/25, Batch 181/2194, Loss: 0.1967\n",
      "Epoch 4/25, Batch 191/2194, Loss: 0.1991\n",
      "Epoch 4/25, Batch 201/2194, Loss: 0.1115\n",
      "Epoch 4/25, Batch 211/2194, Loss: 0.4286\n",
      "Epoch 4/25, Batch 221/2194, Loss: 0.2741\n",
      "Epoch 4/25, Batch 231/2194, Loss: 0.1420\n",
      "Epoch 4/25, Batch 241/2194, Loss: 0.1870\n",
      "Epoch 4/25, Batch 251/2194, Loss: 0.1513\n",
      "Epoch 4/25, Batch 261/2194, Loss: 0.2207\n",
      "Epoch 4/25, Batch 271/2194, Loss: 0.3177\n",
      "Epoch 4/25, Batch 281/2194, Loss: 0.3204\n",
      "Epoch 4/25, Batch 291/2194, Loss: 0.3315\n",
      "Epoch 4/25, Batch 301/2194, Loss: 0.3445\n",
      "Epoch 4/25, Batch 311/2194, Loss: 0.1859\n",
      "Epoch 4/25, Batch 321/2194, Loss: 0.2696\n",
      "Epoch 4/25, Batch 331/2194, Loss: 0.1546\n",
      "Epoch 4/25, Batch 341/2194, Loss: 0.1691\n",
      "Epoch 4/25, Batch 351/2194, Loss: 0.3161\n",
      "Epoch 4/25, Batch 361/2194, Loss: 0.2322\n",
      "Epoch 4/25, Batch 371/2194, Loss: 0.2814\n",
      "Epoch 4/25, Batch 381/2194, Loss: 0.1994\n",
      "Epoch 4/25, Batch 391/2194, Loss: 0.4686\n",
      "Epoch 4/25, Batch 401/2194, Loss: 0.2298\n",
      "Epoch 4/25, Batch 411/2194, Loss: 0.2287\n",
      "Epoch 4/25, Batch 421/2194, Loss: 0.0787\n",
      "Epoch 4/25, Batch 431/2194, Loss: 0.2434\n",
      "Epoch 4/25, Batch 441/2194, Loss: 0.2605\n",
      "Epoch 4/25, Batch 451/2194, Loss: 0.1420\n",
      "Epoch 4/25, Batch 461/2194, Loss: 0.4079\n",
      "Epoch 4/25, Batch 471/2194, Loss: 0.2535\n",
      "Epoch 4/25, Batch 481/2194, Loss: 0.2017\n",
      "Epoch 4/25, Batch 491/2194, Loss: 0.2030\n",
      "Epoch 4/25, Batch 501/2194, Loss: 0.1229\n",
      "Epoch 4/25, Batch 511/2194, Loss: 0.3305\n",
      "Epoch 4/25, Batch 521/2194, Loss: 0.2791\n",
      "Epoch 4/25, Batch 531/2194, Loss: 0.2720\n",
      "Epoch 4/25, Batch 541/2194, Loss: 0.0842\n",
      "Epoch 4/25, Batch 551/2194, Loss: 0.2503\n",
      "Epoch 4/25, Batch 561/2194, Loss: 0.1774\n",
      "Epoch 4/25, Batch 571/2194, Loss: 0.1854\n",
      "Epoch 4/25, Batch 581/2194, Loss: 0.2130\n",
      "Epoch 4/25, Batch 591/2194, Loss: 0.1556\n",
      "Epoch 4/25, Batch 601/2194, Loss: 0.1781\n",
      "Epoch 4/25, Batch 611/2194, Loss: 0.3297\n",
      "Epoch 4/25, Batch 621/2194, Loss: 0.2478\n",
      "Epoch 4/25, Batch 631/2194, Loss: 0.1217\n",
      "Epoch 4/25, Batch 641/2194, Loss: 0.2714\n",
      "Epoch 4/25, Batch 651/2194, Loss: 0.2019\n",
      "Epoch 4/25, Batch 661/2194, Loss: 0.3422\n",
      "Epoch 4/25, Batch 671/2194, Loss: 0.1132\n",
      "Epoch 4/25, Batch 681/2194, Loss: 0.1809\n",
      "Epoch 4/25, Batch 691/2194, Loss: 0.2500\n",
      "Epoch 4/25, Batch 701/2194, Loss: 0.1901\n",
      "Epoch 4/25, Batch 711/2194, Loss: 0.1263\n",
      "Epoch 4/25, Batch 721/2194, Loss: 0.1848\n",
      "Epoch 4/25, Batch 731/2194, Loss: 0.2558\n",
      "Epoch 4/25, Batch 741/2194, Loss: 0.2011\n",
      "Epoch 4/25, Batch 751/2194, Loss: 0.2576\n",
      "Epoch 4/25, Batch 761/2194, Loss: 0.2940\n",
      "Epoch 4/25, Batch 771/2194, Loss: 0.1820\n",
      "Epoch 4/25, Batch 781/2194, Loss: 0.2952\n",
      "Epoch 4/25, Batch 791/2194, Loss: 0.2357\n",
      "Epoch 4/25, Batch 801/2194, Loss: 0.1870\n",
      "Epoch 4/25, Batch 811/2194, Loss: 0.2080\n",
      "Epoch 4/25, Batch 821/2194, Loss: 0.2788\n",
      "Epoch 4/25, Batch 831/2194, Loss: 0.2318\n",
      "Epoch 4/25, Batch 841/2194, Loss: 0.2982\n",
      "Epoch 4/25, Batch 851/2194, Loss: 0.2274\n",
      "Epoch 4/25, Batch 861/2194, Loss: 0.3385\n",
      "Epoch 4/25, Batch 871/2194, Loss: 0.4918\n",
      "Epoch 4/25, Batch 881/2194, Loss: 0.1649\n",
      "Epoch 4/25, Batch 891/2194, Loss: 0.0407\n",
      "Epoch 4/25, Batch 901/2194, Loss: 0.1508\n",
      "Epoch 4/25, Batch 911/2194, Loss: 0.2525\n",
      "Epoch 4/25, Batch 921/2194, Loss: 0.3044\n",
      "Epoch 4/25, Batch 931/2194, Loss: 0.2398\n",
      "Epoch 4/25, Batch 941/2194, Loss: 0.1548\n",
      "Epoch 4/25, Batch 951/2194, Loss: 0.2827\n",
      "Epoch 4/25, Batch 961/2194, Loss: 0.2318\n",
      "Epoch 4/25, Batch 971/2194, Loss: 0.1204\n",
      "Epoch 4/25, Batch 981/2194, Loss: 0.0719\n",
      "Epoch 4/25, Batch 991/2194, Loss: 0.4008\n",
      "Epoch 4/25, Batch 1001/2194, Loss: 0.3240\n",
      "Epoch 4/25, Batch 1011/2194, Loss: 0.2266\n",
      "Epoch 4/25, Batch 1021/2194, Loss: 0.2497\n",
      "Epoch 4/25, Batch 1031/2194, Loss: 0.1170\n",
      "Epoch 4/25, Batch 1041/2194, Loss: 0.2235\n",
      "Epoch 4/25, Batch 1051/2194, Loss: 0.2139\n",
      "Epoch 4/25, Batch 1061/2194, Loss: 0.1567\n",
      "Epoch 4/25, Batch 1071/2194, Loss: 0.1322\n",
      "Epoch 4/25, Batch 1081/2194, Loss: 0.3952\n",
      "Epoch 4/25, Batch 1091/2194, Loss: 0.2915\n",
      "Epoch 4/25, Batch 1101/2194, Loss: 0.1737\n",
      "Epoch 4/25, Batch 1111/2194, Loss: 0.2409\n",
      "Epoch 4/25, Batch 1121/2194, Loss: 0.1384\n",
      "Epoch 4/25, Batch 1131/2194, Loss: 0.3661\n",
      "Epoch 4/25, Batch 1141/2194, Loss: 0.2361\n",
      "Epoch 4/25, Batch 1151/2194, Loss: 0.2399\n",
      "Epoch 4/25, Batch 1161/2194, Loss: 0.2273\n",
      "Epoch 4/25, Batch 1171/2194, Loss: 0.2938\n",
      "Epoch 4/25, Batch 1181/2194, Loss: 0.2455\n",
      "Epoch 4/25, Batch 1191/2194, Loss: 0.1330\n",
      "Epoch 4/25, Batch 1201/2194, Loss: 0.3217\n",
      "Epoch 4/25, Batch 1211/2194, Loss: 0.2477\n",
      "Epoch 4/25, Batch 1221/2194, Loss: 0.3334\n",
      "Epoch 4/25, Batch 1231/2194, Loss: 0.1488\n",
      "Epoch 4/25, Batch 1241/2194, Loss: 0.1470\n",
      "Epoch 4/25, Batch 1251/2194, Loss: 0.2613\n",
      "Epoch 4/25, Batch 1261/2194, Loss: 0.1033\n",
      "Epoch 4/25, Batch 1271/2194, Loss: 0.2702\n",
      "Epoch 4/25, Batch 1281/2194, Loss: 0.2583\n",
      "Epoch 4/25, Batch 1291/2194, Loss: 0.1493\n",
      "Epoch 4/25, Batch 1301/2194, Loss: 0.0714\n",
      "Epoch 4/25, Batch 1311/2194, Loss: 0.1525\n",
      "Epoch 4/25, Batch 1321/2194, Loss: 0.1938\n",
      "Epoch 4/25, Batch 1331/2194, Loss: 0.2053\n",
      "Epoch 4/25, Batch 1341/2194, Loss: 0.2188\n",
      "Epoch 4/25, Batch 1351/2194, Loss: 0.1614\n",
      "Epoch 4/25, Batch 1361/2194, Loss: 0.1923\n",
      "Epoch 4/25, Batch 1371/2194, Loss: 0.0745\n",
      "Epoch 4/25, Batch 1381/2194, Loss: 0.3656\n",
      "Epoch 4/25, Batch 1391/2194, Loss: 0.3611\n",
      "Epoch 4/25, Batch 1401/2194, Loss: 0.2603\n",
      "Epoch 4/25, Batch 1411/2194, Loss: 0.2274\n",
      "Epoch 4/25, Batch 1421/2194, Loss: 0.2754\n",
      "Epoch 4/25, Batch 1431/2194, Loss: 0.2617\n",
      "Epoch 4/25, Batch 1441/2194, Loss: 0.1324\n",
      "Epoch 4/25, Batch 1451/2194, Loss: 0.1948\n",
      "Epoch 4/25, Batch 1461/2194, Loss: 0.1750\n",
      "Epoch 4/25, Batch 1471/2194, Loss: 0.1435\n",
      "Epoch 4/25, Batch 1481/2194, Loss: 0.1147\n",
      "Epoch 4/25, Batch 1491/2194, Loss: 0.1970\n",
      "Epoch 4/25, Batch 1501/2194, Loss: 0.2117\n",
      "Epoch 4/25, Batch 1511/2194, Loss: 0.2264\n",
      "Epoch 4/25, Batch 1521/2194, Loss: 0.2739\n",
      "Epoch 4/25, Batch 1531/2194, Loss: 0.1526\n",
      "Epoch 4/25, Batch 1541/2194, Loss: 0.3302\n",
      "Epoch 4/25, Batch 1551/2194, Loss: 0.2400\n",
      "Epoch 4/25, Batch 1561/2194, Loss: 0.1978\n",
      "Epoch 4/25, Batch 1571/2194, Loss: 0.2256\n",
      "Epoch 4/25, Batch 1581/2194, Loss: 0.3038\n",
      "Epoch 4/25, Batch 1591/2194, Loss: 0.2801\n",
      "Epoch 4/25, Batch 1601/2194, Loss: 0.2368\n",
      "Epoch 4/25, Batch 1611/2194, Loss: 0.3400\n",
      "Epoch 4/25, Batch 1621/2194, Loss: 0.1724\n",
      "Epoch 4/25, Batch 1631/2194, Loss: 0.3280\n",
      "Epoch 4/25, Batch 1641/2194, Loss: 0.1232\n",
      "Epoch 4/25, Batch 1651/2194, Loss: 0.3408\n",
      "Epoch 4/25, Batch 1661/2194, Loss: 0.2544\n",
      "Epoch 4/25, Batch 1671/2194, Loss: 0.1998\n",
      "Epoch 4/25, Batch 1681/2194, Loss: 0.1286\n",
      "Epoch 4/25, Batch 1691/2194, Loss: 0.0880\n",
      "Epoch 4/25, Batch 1701/2194, Loss: 0.2851\n",
      "Epoch 4/25, Batch 1711/2194, Loss: 0.4470\n",
      "Epoch 4/25, Batch 1721/2194, Loss: 0.0782\n",
      "Epoch 4/25, Batch 1731/2194, Loss: 0.3547\n",
      "Epoch 4/25, Batch 1741/2194, Loss: 0.1543\n",
      "Epoch 4/25, Batch 1751/2194, Loss: 0.2603\n",
      "Epoch 4/25, Batch 1761/2194, Loss: 0.1567\n",
      "Epoch 4/25, Batch 1771/2194, Loss: 0.1853\n",
      "Epoch 4/25, Batch 1781/2194, Loss: 0.3108\n",
      "Epoch 4/25, Batch 1791/2194, Loss: 0.1833\n",
      "Epoch 4/25, Batch 1801/2194, Loss: 0.3099\n",
      "Epoch 4/25, Batch 1811/2194, Loss: 0.0857\n",
      "Epoch 4/25, Batch 1821/2194, Loss: 0.3616\n",
      "Epoch 4/25, Batch 1831/2194, Loss: 0.4826\n",
      "Epoch 4/25, Batch 1841/2194, Loss: 0.1821\n",
      "Epoch 4/25, Batch 1851/2194, Loss: 0.2594\n",
      "Epoch 4/25, Batch 1861/2194, Loss: 0.1832\n",
      "Epoch 4/25, Batch 1871/2194, Loss: 0.3208\n",
      "Epoch 4/25, Batch 1881/2194, Loss: 0.1871\n",
      "Epoch 4/25, Batch 1891/2194, Loss: 0.1374\n",
      "Epoch 4/25, Batch 1901/2194, Loss: 0.2871\n",
      "Epoch 4/25, Batch 1911/2194, Loss: 0.2885\n",
      "Epoch 4/25, Batch 1921/2194, Loss: 0.3134\n",
      "Epoch 4/25, Batch 1931/2194, Loss: 0.5882\n",
      "Epoch 4/25, Batch 1941/2194, Loss: 0.1443\n",
      "Epoch 4/25, Batch 1951/2194, Loss: 0.2220\n",
      "Epoch 4/25, Batch 1961/2194, Loss: 0.0969\n",
      "Epoch 4/25, Batch 1971/2194, Loss: 0.1421\n",
      "Epoch 4/25, Batch 1981/2194, Loss: 0.0888\n",
      "Epoch 4/25, Batch 1991/2194, Loss: 0.2740\n",
      "Epoch 4/25, Batch 2001/2194, Loss: 0.1398\n",
      "Epoch 4/25, Batch 2011/2194, Loss: 0.2237\n",
      "Epoch 4/25, Batch 2021/2194, Loss: 0.1732\n",
      "Epoch 4/25, Batch 2031/2194, Loss: 0.5334\n",
      "Epoch 4/25, Batch 2041/2194, Loss: 0.3004\n",
      "Epoch 4/25, Batch 2051/2194, Loss: 0.2032\n",
      "Epoch 4/25, Batch 2061/2194, Loss: 0.1770\n",
      "Epoch 4/25, Batch 2071/2194, Loss: 0.1909\n",
      "Epoch 4/25, Batch 2081/2194, Loss: 0.1599\n",
      "Epoch 4/25, Batch 2091/2194, Loss: 0.1727\n",
      "Epoch 4/25, Batch 2101/2194, Loss: 0.2602\n",
      "Epoch 4/25, Batch 2111/2194, Loss: 0.2333\n",
      "Epoch 4/25, Batch 2121/2194, Loss: 0.1594\n",
      "Epoch 4/25, Batch 2131/2194, Loss: 0.1223\n",
      "Epoch 4/25, Batch 2141/2194, Loss: 0.2235\n",
      "Epoch 4/25, Batch 2151/2194, Loss: 0.2235\n",
      "Epoch 4/25, Batch 2161/2194, Loss: 0.2920\n",
      "Epoch 4/25, Batch 2171/2194, Loss: 0.1190\n",
      "Epoch 4/25, Batch 2181/2194, Loss: 0.1020\n",
      "Epoch 4/25, Batch 2191/2194, Loss: 0.1821\n",
      "Epoch 4/25:\n",
      "Train Loss: 0.2277, Train Acc: 90.12%\n",
      "Val Loss: 0.2520, Val Acc: 89.99%\n",
      "------------------------------------------------------------\n",
      "Starting epoch 5/25...\n",
      "Starting training loop with 2194 batches...\n",
      "Successfully loaded first batch: images shape torch.Size([32, 3, 224, 224]), labels shape torch.Size([32])\n",
      "Epoch 5/25, Batch 1/2194, Loss: 0.0427\n",
      "Epoch 5/25, Batch 11/2194, Loss: 0.1937\n",
      "Epoch 5/25, Batch 21/2194, Loss: 0.1276\n",
      "Epoch 5/25, Batch 31/2194, Loss: 0.1264\n",
      "Epoch 5/25, Batch 41/2194, Loss: 0.1901\n",
      "Epoch 5/25, Batch 51/2194, Loss: 0.3272\n",
      "Epoch 5/25, Batch 61/2194, Loss: 0.2792\n",
      "Epoch 5/25, Batch 71/2194, Loss: 0.3050\n",
      "Epoch 5/25, Batch 81/2194, Loss: 0.2988\n",
      "Epoch 5/25, Batch 91/2194, Loss: 0.1860\n",
      "Epoch 5/25, Batch 101/2194, Loss: 0.2785\n",
      "Epoch 5/25, Batch 111/2194, Loss: 0.0914\n",
      "Epoch 5/25, Batch 121/2194, Loss: 0.2586\n",
      "Epoch 5/25, Batch 131/2194, Loss: 0.3191\n",
      "Epoch 5/25, Batch 141/2194, Loss: 0.3074\n",
      "Epoch 5/25, Batch 151/2194, Loss: 0.1468\n",
      "Epoch 5/25, Batch 161/2194, Loss: 0.1917\n",
      "Epoch 5/25, Batch 171/2194, Loss: 0.2100\n",
      "Epoch 5/25, Batch 181/2194, Loss: 0.2552\n",
      "Epoch 5/25, Batch 191/2194, Loss: 0.1221\n",
      "Epoch 5/25, Batch 201/2194, Loss: 0.2389\n",
      "Epoch 5/25, Batch 211/2194, Loss: 0.1674\n",
      "Epoch 5/25, Batch 221/2194, Loss: 0.1414\n",
      "Epoch 5/25, Batch 231/2194, Loss: 0.2513\n",
      "Epoch 5/25, Batch 241/2194, Loss: 0.2303\n",
      "Epoch 5/25, Batch 251/2194, Loss: 0.2343\n",
      "Epoch 5/25, Batch 261/2194, Loss: 0.4008\n",
      "Epoch 5/25, Batch 271/2194, Loss: 0.3600\n",
      "Epoch 5/25, Batch 281/2194, Loss: 0.2508\n",
      "Epoch 5/25, Batch 291/2194, Loss: 0.1205\n",
      "Epoch 5/25, Batch 301/2194, Loss: 0.2234\n",
      "Epoch 5/25, Batch 311/2194, Loss: 0.3144\n",
      "Epoch 5/25, Batch 321/2194, Loss: 0.2372\n",
      "Epoch 5/25, Batch 331/2194, Loss: 0.2932\n",
      "Epoch 5/25, Batch 341/2194, Loss: 0.0907\n",
      "Epoch 5/25, Batch 351/2194, Loss: 0.1479\n",
      "Epoch 5/25, Batch 361/2194, Loss: 0.3687\n",
      "Epoch 5/25, Batch 371/2194, Loss: 0.1800\n",
      "Epoch 5/25, Batch 381/2194, Loss: 0.3076\n",
      "Epoch 5/25, Batch 391/2194, Loss: 0.2039\n",
      "Epoch 5/25, Batch 401/2194, Loss: 0.1865\n",
      "Epoch 5/25, Batch 411/2194, Loss: 0.2470\n",
      "Epoch 5/25, Batch 421/2194, Loss: 0.3116\n",
      "Epoch 5/25, Batch 431/2194, Loss: 0.1226\n",
      "Epoch 5/25, Batch 441/2194, Loss: 0.2819\n",
      "Epoch 5/25, Batch 451/2194, Loss: 0.2063\n",
      "Epoch 5/25, Batch 461/2194, Loss: 0.1702\n",
      "Epoch 5/25, Batch 471/2194, Loss: 0.1323\n",
      "Epoch 5/25, Batch 481/2194, Loss: 0.1632\n",
      "Epoch 5/25, Batch 491/2194, Loss: 0.1225\n",
      "Epoch 5/25, Batch 501/2194, Loss: 0.1795\n",
      "Epoch 5/25, Batch 511/2194, Loss: 0.1139\n",
      "Epoch 5/25, Batch 521/2194, Loss: 0.3110\n",
      "Epoch 5/25, Batch 531/2194, Loss: 0.1834\n",
      "Epoch 5/25, Batch 541/2194, Loss: 0.1703\n",
      "Epoch 5/25, Batch 551/2194, Loss: 0.2967\n",
      "Epoch 5/25, Batch 561/2194, Loss: 0.2759\n",
      "Epoch 5/25, Batch 571/2194, Loss: 0.1792\n",
      "Epoch 5/25, Batch 581/2194, Loss: 0.1281\n",
      "Epoch 5/25, Batch 591/2194, Loss: 0.2099\n",
      "Epoch 5/25, Batch 601/2194, Loss: 0.0800\n",
      "Epoch 5/25, Batch 611/2194, Loss: 0.5269\n",
      "Epoch 5/25, Batch 621/2194, Loss: 0.1170\n",
      "Epoch 5/25, Batch 631/2194, Loss: 0.1236\n",
      "Epoch 5/25, Batch 641/2194, Loss: 0.2169\n",
      "Epoch 5/25, Batch 651/2194, Loss: 0.2303\n",
      "Epoch 5/25, Batch 661/2194, Loss: 0.4675\n",
      "Epoch 5/25, Batch 671/2194, Loss: 0.1520\n",
      "Epoch 5/25, Batch 681/2194, Loss: 0.2426\n",
      "Epoch 5/25, Batch 691/2194, Loss: 0.3272\n",
      "Epoch 5/25, Batch 701/2194, Loss: 0.2869\n",
      "Epoch 5/25, Batch 711/2194, Loss: 0.1672\n",
      "Epoch 5/25, Batch 721/2194, Loss: 0.1483\n",
      "Epoch 5/25, Batch 731/2194, Loss: 0.1781\n",
      "Epoch 5/25, Batch 741/2194, Loss: 0.2106\n",
      "Epoch 5/25, Batch 751/2194, Loss: 0.2098\n",
      "Epoch 5/25, Batch 761/2194, Loss: 0.1055\n",
      "Epoch 5/25, Batch 771/2194, Loss: 0.2205\n",
      "Epoch 5/25, Batch 781/2194, Loss: 0.1996\n",
      "Epoch 5/25, Batch 791/2194, Loss: 0.1706\n",
      "Epoch 5/25, Batch 801/2194, Loss: 0.1999\n",
      "Epoch 5/25, Batch 811/2194, Loss: 0.0936\n",
      "Epoch 5/25, Batch 821/2194, Loss: 0.1249\n",
      "Epoch 5/25, Batch 831/2194, Loss: 0.3208\n",
      "Epoch 5/25, Batch 841/2194, Loss: 0.1523\n",
      "Epoch 5/25, Batch 851/2194, Loss: 0.1151\n",
      "Epoch 5/25, Batch 861/2194, Loss: 0.1466\n",
      "Epoch 5/25, Batch 871/2194, Loss: 0.1597\n",
      "Epoch 5/25, Batch 881/2194, Loss: 0.1654\n",
      "Epoch 5/25, Batch 891/2194, Loss: 0.1748\n",
      "Epoch 5/25, Batch 901/2194, Loss: 0.1319\n",
      "Epoch 5/25, Batch 911/2194, Loss: 0.1365\n",
      "Epoch 5/25, Batch 921/2194, Loss: 0.1677\n",
      "Epoch 5/25, Batch 931/2194, Loss: 0.0829\n",
      "Epoch 5/25, Batch 941/2194, Loss: 0.1093\n",
      "Epoch 5/25, Batch 951/2194, Loss: 0.4807\n",
      "Epoch 5/25, Batch 961/2194, Loss: 0.1364\n",
      "Epoch 5/25, Batch 971/2194, Loss: 0.1615\n",
      "Epoch 5/25, Batch 981/2194, Loss: 0.1201\n",
      "Epoch 5/25, Batch 991/2194, Loss: 0.1383\n",
      "Epoch 5/25, Batch 1001/2194, Loss: 0.2035\n",
      "Epoch 5/25, Batch 1011/2194, Loss: 0.1767\n",
      "Epoch 5/25, Batch 1021/2194, Loss: 0.3018\n",
      "Epoch 5/25, Batch 1031/2194, Loss: 0.1182\n",
      "Epoch 5/25, Batch 1041/2194, Loss: 0.4335\n",
      "Epoch 5/25, Batch 1051/2194, Loss: 0.1392\n",
      "Epoch 5/25, Batch 1061/2194, Loss: 0.3563\n",
      "Epoch 5/25, Batch 1071/2194, Loss: 0.1225\n",
      "Epoch 5/25, Batch 1081/2194, Loss: 0.1579\n",
      "Epoch 5/25, Batch 1091/2194, Loss: 0.2300\n",
      "Epoch 5/25, Batch 1101/2194, Loss: 0.2658\n",
      "Epoch 5/25, Batch 1111/2194, Loss: 0.4087\n",
      "Epoch 5/25, Batch 1121/2194, Loss: 0.4314\n",
      "Epoch 5/25, Batch 1131/2194, Loss: 0.0865\n",
      "Epoch 5/25, Batch 1141/2194, Loss: 0.1653\n",
      "Epoch 5/25, Batch 1151/2194, Loss: 0.1606\n",
      "Epoch 5/25, Batch 1161/2194, Loss: 0.3088\n",
      "Epoch 5/25, Batch 1171/2194, Loss: 0.3542\n",
      "Epoch 5/25, Batch 1181/2194, Loss: 0.2137\n",
      "Epoch 5/25, Batch 1191/2194, Loss: 0.1313\n",
      "Epoch 5/25, Batch 1201/2194, Loss: 0.2557\n",
      "Epoch 5/25, Batch 1211/2194, Loss: 0.2290\n",
      "Epoch 5/25, Batch 1221/2194, Loss: 0.2161\n",
      "Epoch 5/25, Batch 1231/2194, Loss: 0.1572\n",
      "Epoch 5/25, Batch 1241/2194, Loss: 0.1914\n",
      "Epoch 5/25, Batch 1251/2194, Loss: 0.1222\n",
      "Epoch 5/25, Batch 1261/2194, Loss: 0.2698\n",
      "Epoch 5/25, Batch 1271/2194, Loss: 0.0976\n",
      "Epoch 5/25, Batch 1281/2194, Loss: 0.1625\n",
      "Epoch 5/25, Batch 1291/2194, Loss: 0.3146\n",
      "Epoch 5/25, Batch 1301/2194, Loss: 0.1924\n",
      "Epoch 5/25, Batch 1311/2194, Loss: 0.1421\n",
      "Epoch 5/25, Batch 1321/2194, Loss: 0.1406\n",
      "Epoch 5/25, Batch 1331/2194, Loss: 0.1632\n",
      "Epoch 5/25, Batch 1341/2194, Loss: 0.3359\n",
      "Epoch 5/25, Batch 1351/2194, Loss: 0.1477\n",
      "Epoch 5/25, Batch 1361/2194, Loss: 0.1537\n",
      "Epoch 5/25, Batch 1371/2194, Loss: 0.2505\n",
      "Epoch 5/25, Batch 1381/2194, Loss: 0.1649\n",
      "Epoch 5/25, Batch 1391/2194, Loss: 0.2568\n",
      "Epoch 5/25, Batch 1401/2194, Loss: 0.1124\n",
      "Epoch 5/25, Batch 1411/2194, Loss: 0.4740\n",
      "Epoch 5/25, Batch 1421/2194, Loss: 0.2445\n",
      "Epoch 5/25, Batch 1431/2194, Loss: 0.1556\n",
      "Epoch 5/25, Batch 1441/2194, Loss: 0.2430\n",
      "Epoch 5/25, Batch 1451/2194, Loss: 0.1618\n",
      "Epoch 5/25, Batch 1461/2194, Loss: 0.1587\n",
      "Epoch 5/25, Batch 1471/2194, Loss: 0.1287\n",
      "Epoch 5/25, Batch 1481/2194, Loss: 0.1876\n",
      "Epoch 5/25, Batch 1491/2194, Loss: 0.2518\n",
      "Epoch 5/25, Batch 1501/2194, Loss: 0.3964\n",
      "Epoch 5/25, Batch 1511/2194, Loss: 0.1537\n",
      "Epoch 5/25, Batch 1521/2194, Loss: 0.1693\n",
      "Epoch 5/25, Batch 1531/2194, Loss: 0.2398\n",
      "Epoch 5/25, Batch 1541/2194, Loss: 0.4098\n",
      "Epoch 5/25, Batch 1551/2194, Loss: 0.1010\n",
      "Epoch 5/25, Batch 1561/2194, Loss: 0.1824\n",
      "Epoch 5/25, Batch 1571/2194, Loss: 0.3380\n",
      "Epoch 5/25, Batch 1581/2194, Loss: 0.1871\n",
      "Epoch 5/25, Batch 1591/2194, Loss: 0.2174\n",
      "Epoch 5/25, Batch 1601/2194, Loss: 0.3520\n",
      "Epoch 5/25, Batch 1611/2194, Loss: 0.3054\n",
      "Epoch 5/25, Batch 1621/2194, Loss: 0.3708\n",
      "Epoch 5/25, Batch 1631/2194, Loss: 0.2126\n",
      "Epoch 5/25, Batch 1641/2194, Loss: 0.0714\n",
      "Epoch 5/25, Batch 1651/2194, Loss: 0.1958\n",
      "Epoch 5/25, Batch 1661/2194, Loss: 0.3740\n",
      "Epoch 5/25, Batch 1671/2194, Loss: 0.0695\n",
      "Epoch 5/25, Batch 1681/2194, Loss: 0.2613\n",
      "Epoch 5/25, Batch 1691/2194, Loss: 0.2333\n",
      "Epoch 5/25, Batch 1701/2194, Loss: 0.0757\n",
      "Epoch 5/25, Batch 1711/2194, Loss: 0.1158\n",
      "Epoch 5/25, Batch 1721/2194, Loss: 0.2221\n",
      "Epoch 5/25, Batch 1731/2194, Loss: 0.2827\n",
      "Epoch 5/25, Batch 1741/2194, Loss: 0.1885\n",
      "Epoch 5/25, Batch 1751/2194, Loss: 0.1329\n",
      "Epoch 5/25, Batch 1761/2194, Loss: 0.4768\n",
      "Epoch 5/25, Batch 1771/2194, Loss: 0.3384\n",
      "Epoch 5/25, Batch 1781/2194, Loss: 0.2859\n",
      "Epoch 5/25, Batch 1791/2194, Loss: 0.1175\n",
      "Epoch 5/25, Batch 1801/2194, Loss: 0.1584\n",
      "Epoch 5/25, Batch 1811/2194, Loss: 0.2391\n",
      "Epoch 5/25, Batch 1821/2194, Loss: 0.2428\n",
      "Epoch 5/25, Batch 1831/2194, Loss: 0.2592\n",
      "Epoch 5/25, Batch 1841/2194, Loss: 0.2772\n",
      "Epoch 5/25, Batch 1851/2194, Loss: 0.1123\n",
      "Epoch 5/25, Batch 1861/2194, Loss: 0.2851\n",
      "Epoch 5/25, Batch 1871/2194, Loss: 0.1627\n",
      "Epoch 5/25, Batch 1881/2194, Loss: 0.2416\n",
      "Epoch 5/25, Batch 1891/2194, Loss: 0.3313\n",
      "Epoch 5/25, Batch 1901/2194, Loss: 0.1635\n",
      "Epoch 5/25, Batch 1911/2194, Loss: 0.1312\n",
      "Epoch 5/25, Batch 1921/2194, Loss: 0.4310\n",
      "Epoch 5/25, Batch 1931/2194, Loss: 0.0862\n",
      "Epoch 5/25, Batch 1941/2194, Loss: 0.1731\n",
      "Epoch 5/25, Batch 1951/2194, Loss: 0.0755\n",
      "Epoch 5/25, Batch 1961/2194, Loss: 0.2850\n",
      "Epoch 5/25, Batch 1971/2194, Loss: 0.1824\n",
      "Epoch 5/25, Batch 1981/2194, Loss: 0.2298\n",
      "Epoch 5/25, Batch 1991/2194, Loss: 0.1998\n",
      "Epoch 5/25, Batch 2001/2194, Loss: 0.1970\n",
      "Epoch 5/25, Batch 2011/2194, Loss: 0.2542\n",
      "Epoch 5/25, Batch 2021/2194, Loss: 0.2780\n",
      "Epoch 5/25, Batch 2031/2194, Loss: 0.2221\n",
      "Epoch 5/25, Batch 2041/2194, Loss: 0.3342\n",
      "Epoch 5/25, Batch 2051/2194, Loss: 0.3235\n",
      "Epoch 5/25, Batch 2061/2194, Loss: 0.1849\n",
      "Epoch 5/25, Batch 2071/2194, Loss: 0.1155\n",
      "Epoch 5/25, Batch 2081/2194, Loss: 0.1096\n",
      "Epoch 5/25, Batch 2091/2194, Loss: 0.2030\n",
      "Epoch 5/25, Batch 2101/2194, Loss: 0.3840\n",
      "Epoch 5/25, Batch 2111/2194, Loss: 0.1872\n",
      "Epoch 5/25, Batch 2121/2194, Loss: 0.2455\n",
      "Epoch 5/25, Batch 2131/2194, Loss: 0.2242\n",
      "Epoch 5/25, Batch 2141/2194, Loss: 0.2056\n",
      "Epoch 5/25, Batch 2151/2194, Loss: 0.2676\n",
      "Epoch 5/25, Batch 2161/2194, Loss: 0.1488\n",
      "Epoch 5/25, Batch 2171/2194, Loss: 0.0766\n",
      "Epoch 5/25, Batch 2181/2194, Loss: 0.1822\n",
      "Epoch 5/25, Batch 2191/2194, Loss: 0.1392\n",
      "Epoch 5/25:\n",
      "Train Loss: 0.2031, Train Acc: 91.09%\n",
      "Val Loss: 0.1884, Val Acc: 91.63%\n",
      "------------------------------------------------------------\n",
      "Starting epoch 6/25...\n",
      "Starting training loop with 2194 batches...\n",
      "Successfully loaded first batch: images shape torch.Size([32, 3, 224, 224]), labels shape torch.Size([32])\n",
      "Epoch 6/25, Batch 1/2194, Loss: 0.1132\n",
      "Epoch 6/25, Batch 11/2194, Loss: 0.2631\n",
      "Epoch 6/25, Batch 21/2194, Loss: 0.1189\n",
      "Epoch 6/25, Batch 31/2194, Loss: 0.3392\n",
      "Epoch 6/25, Batch 41/2194, Loss: 0.0965\n",
      "Epoch 6/25, Batch 51/2194, Loss: 0.0731\n",
      "Epoch 6/25, Batch 61/2194, Loss: 0.1347\n",
      "Epoch 6/25, Batch 71/2194, Loss: 0.1517\n",
      "Epoch 6/25, Batch 81/2194, Loss: 0.1485\n",
      "Epoch 6/25, Batch 91/2194, Loss: 0.2441\n",
      "Epoch 6/25, Batch 101/2194, Loss: 0.1950\n",
      "Epoch 6/25, Batch 111/2194, Loss: 0.1202\n",
      "Epoch 6/25, Batch 121/2194, Loss: 0.0431\n",
      "Epoch 6/25, Batch 131/2194, Loss: 0.4557\n",
      "Epoch 6/25, Batch 141/2194, Loss: 0.1485\n",
      "Epoch 6/25, Batch 151/2194, Loss: 0.0926\n",
      "Epoch 6/25, Batch 161/2194, Loss: 0.1408\n",
      "Epoch 6/25, Batch 171/2194, Loss: 0.2119\n",
      "Epoch 6/25, Batch 181/2194, Loss: 0.4244\n",
      "Epoch 6/25, Batch 191/2194, Loss: 0.2489\n",
      "Epoch 6/25, Batch 201/2194, Loss: 0.3556\n",
      "Epoch 6/25, Batch 211/2194, Loss: 0.1632\n",
      "Epoch 6/25, Batch 221/2194, Loss: 0.2444\n",
      "Epoch 6/25, Batch 231/2194, Loss: 0.1700\n",
      "Epoch 6/25, Batch 241/2194, Loss: 0.1817\n",
      "Epoch 6/25, Batch 251/2194, Loss: 0.2613\n",
      "Epoch 6/25, Batch 261/2194, Loss: 0.1773\n",
      "Epoch 6/25, Batch 271/2194, Loss: 0.3297\n",
      "Epoch 6/25, Batch 281/2194, Loss: 0.2423\n",
      "Epoch 6/25, Batch 291/2194, Loss: 0.2165\n",
      "Epoch 6/25, Batch 301/2194, Loss: 0.4004\n",
      "Epoch 6/25, Batch 311/2194, Loss: 0.2706\n",
      "Epoch 6/25, Batch 321/2194, Loss: 0.1330\n",
      "Epoch 6/25, Batch 331/2194, Loss: 0.1710\n",
      "Epoch 6/25, Batch 341/2194, Loss: 0.1406\n",
      "Epoch 6/25, Batch 351/2194, Loss: 0.3269\n",
      "Epoch 6/25, Batch 361/2194, Loss: 0.1295\n",
      "Epoch 6/25, Batch 371/2194, Loss: 0.1634\n",
      "Epoch 6/25, Batch 381/2194, Loss: 0.1170\n",
      "Epoch 6/25, Batch 391/2194, Loss: 0.1180\n",
      "Epoch 6/25, Batch 401/2194, Loss: 0.2130\n",
      "Epoch 6/25, Batch 411/2194, Loss: 0.2552\n",
      "Epoch 6/25, Batch 421/2194, Loss: 0.2277\n",
      "Epoch 6/25, Batch 431/2194, Loss: 0.2271\n",
      "Epoch 6/25, Batch 441/2194, Loss: 0.1771\n",
      "Epoch 6/25, Batch 451/2194, Loss: 0.1648\n",
      "Epoch 6/25, Batch 461/2194, Loss: 0.2521\n",
      "Epoch 6/25, Batch 471/2194, Loss: 0.1241\n",
      "Epoch 6/25, Batch 481/2194, Loss: 0.1101\n",
      "Epoch 6/25, Batch 491/2194, Loss: 0.2022\n",
      "Epoch 6/25, Batch 501/2194, Loss: 0.2894\n",
      "Epoch 6/25, Batch 511/2194, Loss: 0.3135\n",
      "Epoch 6/25, Batch 521/2194, Loss: 0.2295\n",
      "Epoch 6/25, Batch 531/2194, Loss: 0.3495\n",
      "Epoch 6/25, Batch 541/2194, Loss: 0.0798\n",
      "Epoch 6/25, Batch 551/2194, Loss: 0.2334\n",
      "Epoch 6/25, Batch 561/2194, Loss: 0.0662\n",
      "Epoch 6/25, Batch 571/2194, Loss: 0.2112\n",
      "Epoch 6/25, Batch 581/2194, Loss: 0.0801\n",
      "Epoch 6/25, Batch 591/2194, Loss: 0.1827\n",
      "Epoch 6/25, Batch 601/2194, Loss: 0.2256\n",
      "Epoch 6/25, Batch 611/2194, Loss: 0.0981\n",
      "Epoch 6/25, Batch 621/2194, Loss: 0.1644\n",
      "Epoch 6/25, Batch 631/2194, Loss: 0.1529\n",
      "Epoch 6/25, Batch 641/2194, Loss: 0.1808\n",
      "Epoch 6/25, Batch 651/2194, Loss: 0.1589\n",
      "Epoch 6/25, Batch 661/2194, Loss: 0.0849\n",
      "Epoch 6/25, Batch 671/2194, Loss: 0.1285\n",
      "Epoch 6/25, Batch 681/2194, Loss: 0.1500\n",
      "Epoch 6/25, Batch 691/2194, Loss: 0.1776\n",
      "Epoch 6/25, Batch 701/2194, Loss: 0.3482\n",
      "Epoch 6/25, Batch 711/2194, Loss: 0.1348\n",
      "Epoch 6/25, Batch 721/2194, Loss: 0.2107\n",
      "Epoch 6/25, Batch 731/2194, Loss: 0.0876\n",
      "Epoch 6/25, Batch 741/2194, Loss: 0.2018\n",
      "Epoch 6/25, Batch 751/2194, Loss: 0.1029\n",
      "Epoch 6/25, Batch 761/2194, Loss: 0.1698\n",
      "Epoch 6/25, Batch 771/2194, Loss: 0.0360\n",
      "Epoch 6/25, Batch 781/2194, Loss: 0.0906\n",
      "Epoch 6/25, Batch 791/2194, Loss: 0.1871\n",
      "Epoch 6/25, Batch 801/2194, Loss: 0.2577\n",
      "Epoch 6/25, Batch 811/2194, Loss: 0.2420\n",
      "Epoch 6/25, Batch 821/2194, Loss: 0.1681\n",
      "Epoch 6/25, Batch 831/2194, Loss: 0.2036\n",
      "Epoch 6/25, Batch 841/2194, Loss: 0.1006\n",
      "Epoch 6/25, Batch 851/2194, Loss: 0.2818\n",
      "Epoch 6/25, Batch 861/2194, Loss: 0.1283\n",
      "Epoch 6/25, Batch 871/2194, Loss: 0.2342\n",
      "Epoch 6/25, Batch 881/2194, Loss: 0.1639\n",
      "Epoch 6/25, Batch 891/2194, Loss: 0.3030\n",
      "Epoch 6/25, Batch 901/2194, Loss: 0.4587\n",
      "Epoch 6/25, Batch 911/2194, Loss: 0.3675\n",
      "Epoch 6/25, Batch 921/2194, Loss: 0.1641\n",
      "Epoch 6/25, Batch 931/2194, Loss: 0.1037\n",
      "Epoch 6/25, Batch 941/2194, Loss: 0.2246\n",
      "Epoch 6/25, Batch 951/2194, Loss: 0.2519\n",
      "Epoch 6/25, Batch 961/2194, Loss: 0.1695\n",
      "Epoch 6/25, Batch 971/2194, Loss: 0.3211\n",
      "Epoch 6/25, Batch 981/2194, Loss: 0.1384\n",
      "Epoch 6/25, Batch 991/2194, Loss: 0.2216\n",
      "Epoch 6/25, Batch 1001/2194, Loss: 0.0604\n",
      "Epoch 6/25, Batch 1011/2194, Loss: 0.1190\n",
      "Epoch 6/25, Batch 1021/2194, Loss: 0.2693\n",
      "Epoch 6/25, Batch 1031/2194, Loss: 0.2279\n",
      "Epoch 6/25, Batch 1041/2194, Loss: 0.2486\n",
      "Epoch 6/25, Batch 1051/2194, Loss: 0.1027\n",
      "Epoch 6/25, Batch 1061/2194, Loss: 0.2213\n",
      "Epoch 6/25, Batch 1071/2194, Loss: 0.1073\n",
      "Epoch 6/25, Batch 1081/2194, Loss: 0.2563\n",
      "Epoch 6/25, Batch 1091/2194, Loss: 0.1790\n",
      "Epoch 6/25, Batch 1101/2194, Loss: 0.1354\n",
      "Epoch 6/25, Batch 1111/2194, Loss: 0.1228\n",
      "Epoch 6/25, Batch 1121/2194, Loss: 0.1789\n",
      "Epoch 6/25, Batch 1131/2194, Loss: 0.1501\n",
      "Epoch 6/25, Batch 1141/2194, Loss: 0.0992\n",
      "Epoch 6/25, Batch 1151/2194, Loss: 0.1119\n",
      "Epoch 6/25, Batch 1161/2194, Loss: 0.1736\n",
      "Epoch 6/25, Batch 1171/2194, Loss: 0.2896\n",
      "Epoch 6/25, Batch 1181/2194, Loss: 0.0667\n",
      "Epoch 6/25, Batch 1191/2194, Loss: 0.1718\n",
      "Epoch 6/25, Batch 1201/2194, Loss: 0.1129\n",
      "Epoch 6/25, Batch 1211/2194, Loss: 0.2938\n",
      "Epoch 6/25, Batch 1221/2194, Loss: 0.1197\n",
      "Epoch 6/25, Batch 1231/2194, Loss: 0.1103\n",
      "Epoch 6/25, Batch 1241/2194, Loss: 0.1700\n",
      "Epoch 6/25, Batch 1251/2194, Loss: 0.1214\n",
      "Epoch 6/25, Batch 1261/2194, Loss: 0.2007\n",
      "Epoch 6/25, Batch 1271/2194, Loss: 0.1784\n",
      "Epoch 6/25, Batch 1281/2194, Loss: 0.3770\n",
      "Epoch 6/25, Batch 1291/2194, Loss: 0.1362\n",
      "Epoch 6/25, Batch 1301/2194, Loss: 0.1299\n",
      "Epoch 6/25, Batch 1311/2194, Loss: 0.0624\n",
      "Epoch 6/25, Batch 1321/2194, Loss: 0.2226\n",
      "Epoch 6/25, Batch 1331/2194, Loss: 0.0751\n",
      "Epoch 6/25, Batch 1341/2194, Loss: 0.1643\n",
      "Epoch 6/25, Batch 1351/2194, Loss: 0.2415\n",
      "Epoch 6/25, Batch 1361/2194, Loss: 0.3175\n",
      "Epoch 6/25, Batch 1371/2194, Loss: 0.2321\n",
      "Epoch 6/25, Batch 1381/2194, Loss: 0.2910\n",
      "Epoch 6/25, Batch 1391/2194, Loss: 0.1467\n",
      "Epoch 6/25, Batch 1401/2194, Loss: 0.0644\n",
      "Epoch 6/25, Batch 1411/2194, Loss: 0.0438\n",
      "Epoch 6/25, Batch 1421/2194, Loss: 0.1439\n",
      "Epoch 6/25, Batch 1431/2194, Loss: 0.1390\n",
      "Epoch 6/25, Batch 1441/2194, Loss: 0.1190\n",
      "Epoch 6/25, Batch 1451/2194, Loss: 0.3078\n",
      "Epoch 6/25, Batch 1461/2194, Loss: 0.1705\n",
      "Epoch 6/25, Batch 1471/2194, Loss: 0.3048\n",
      "Epoch 6/25, Batch 1481/2194, Loss: 0.0930\n",
      "Epoch 6/25, Batch 1491/2194, Loss: 0.0572\n",
      "Epoch 6/25, Batch 1501/2194, Loss: 0.2108\n",
      "Epoch 6/25, Batch 1511/2194, Loss: 0.1103\n",
      "Epoch 6/25, Batch 1521/2194, Loss: 0.1024\n",
      "Epoch 6/25, Batch 1531/2194, Loss: 0.1360\n",
      "Epoch 6/25, Batch 1541/2194, Loss: 0.1368\n",
      "Epoch 6/25, Batch 1551/2194, Loss: 0.1265\n",
      "Epoch 6/25, Batch 1561/2194, Loss: 0.0921\n",
      "Epoch 6/25, Batch 1571/2194, Loss: 0.0675\n",
      "Epoch 6/25, Batch 1581/2194, Loss: 0.1562\n",
      "Epoch 6/25, Batch 1591/2194, Loss: 0.1727\n",
      "Epoch 6/25, Batch 1601/2194, Loss: 0.0803\n",
      "Epoch 6/25, Batch 1611/2194, Loss: 0.1797\n",
      "Epoch 6/25, Batch 1621/2194, Loss: 0.2003\n",
      "Epoch 6/25, Batch 1631/2194, Loss: 0.1146\n",
      "Epoch 6/25, Batch 1641/2194, Loss: 0.1834\n",
      "Epoch 6/25, Batch 1651/2194, Loss: 0.3468\n",
      "Epoch 6/25, Batch 1661/2194, Loss: 0.3083\n",
      "Epoch 6/25, Batch 1671/2194, Loss: 0.1977\n",
      "Epoch 6/25, Batch 1681/2194, Loss: 0.1230\n",
      "Epoch 6/25, Batch 1691/2194, Loss: 0.1513\n",
      "Epoch 6/25, Batch 1701/2194, Loss: 0.2898\n",
      "Epoch 6/25, Batch 1711/2194, Loss: 0.1443\n",
      "Epoch 6/25, Batch 1721/2194, Loss: 0.1808\n",
      "Epoch 6/25, Batch 1731/2194, Loss: 0.1834\n",
      "Epoch 6/25, Batch 1741/2194, Loss: 0.3953\n",
      "Epoch 6/25, Batch 1751/2194, Loss: 0.0596\n",
      "Epoch 6/25, Batch 1761/2194, Loss: 0.0852\n",
      "Epoch 6/25, Batch 1771/2194, Loss: 0.2494\n",
      "Epoch 6/25, Batch 1781/2194, Loss: 0.1838\n",
      "Epoch 6/25, Batch 1791/2194, Loss: 0.1173\n",
      "Epoch 6/25, Batch 1801/2194, Loss: 0.2257\n",
      "Epoch 6/25, Batch 1811/2194, Loss: 0.1512\n",
      "Epoch 6/25, Batch 1821/2194, Loss: 0.1877\n",
      "Epoch 6/25, Batch 1831/2194, Loss: 0.2765\n",
      "Epoch 6/25, Batch 1841/2194, Loss: 0.2323\n",
      "Epoch 6/25, Batch 1851/2194, Loss: 0.0941\n",
      "Epoch 6/25, Batch 1861/2194, Loss: 0.3020\n",
      "Epoch 6/25, Batch 1871/2194, Loss: 0.1559\n",
      "Epoch 6/25, Batch 1881/2194, Loss: 0.3055\n",
      "Epoch 6/25, Batch 1891/2194, Loss: 0.1195\n",
      "Epoch 6/25, Batch 1901/2194, Loss: 0.1758\n",
      "Epoch 6/25, Batch 1911/2194, Loss: 0.1192\n",
      "Epoch 6/25, Batch 1921/2194, Loss: 0.1736\n",
      "Epoch 6/25, Batch 1931/2194, Loss: 0.2241\n",
      "Epoch 6/25, Batch 1941/2194, Loss: 0.2645\n",
      "Epoch 6/25, Batch 1951/2194, Loss: 0.0986\n",
      "Epoch 6/25, Batch 1961/2194, Loss: 0.2945\n",
      "Epoch 6/25, Batch 1971/2194, Loss: 0.1736\n",
      "Epoch 6/25, Batch 1981/2194, Loss: 0.3063\n",
      "Epoch 6/25, Batch 1991/2194, Loss: 0.1474\n",
      "Epoch 6/25, Batch 2001/2194, Loss: 0.1827\n",
      "Epoch 6/25, Batch 2011/2194, Loss: 0.1285\n",
      "Epoch 6/25, Batch 2021/2194, Loss: 0.2115\n",
      "Epoch 6/25, Batch 2031/2194, Loss: 0.2938\n",
      "Epoch 6/25, Batch 2041/2194, Loss: 0.2152\n",
      "Epoch 6/25, Batch 2051/2194, Loss: 0.1938\n",
      "Epoch 6/25, Batch 2061/2194, Loss: 0.1872\n",
      "Epoch 6/25, Batch 2071/2194, Loss: 0.1581\n",
      "Epoch 6/25, Batch 2081/2194, Loss: 0.1249\n",
      "Epoch 6/25, Batch 2091/2194, Loss: 0.0966\n",
      "Epoch 6/25, Batch 2101/2194, Loss: 0.1205\n",
      "Epoch 6/25, Batch 2111/2194, Loss: 0.2824\n",
      "Epoch 6/25, Batch 2121/2194, Loss: 0.2627\n",
      "Epoch 6/25, Batch 2131/2194, Loss: 0.2212\n",
      "Epoch 6/25, Batch 2141/2194, Loss: 0.1648\n",
      "Epoch 6/25, Batch 2151/2194, Loss: 0.1121\n",
      "Epoch 6/25, Batch 2161/2194, Loss: 0.1381\n",
      "Epoch 6/25, Batch 2171/2194, Loss: 0.1107\n",
      "Epoch 6/25, Batch 2181/2194, Loss: 0.2727\n",
      "Epoch 6/25, Batch 2191/2194, Loss: 0.1911\n",
      "Epoch 6/25:\n",
      "Train Loss: 0.1869, Train Acc: 92.00%\n",
      "Val Loss: 0.2019, Val Acc: 90.66%\n",
      "------------------------------------------------------------\n",
      "Starting epoch 7/25...\n",
      "Starting training loop with 2194 batches...\n",
      "Successfully loaded first batch: images shape torch.Size([32, 3, 224, 224]), labels shape torch.Size([32])\n",
      "Epoch 7/25, Batch 1/2194, Loss: 0.2716\n",
      "Epoch 7/25, Batch 11/2194, Loss: 0.2158\n",
      "Epoch 7/25, Batch 21/2194, Loss: 0.2080\n",
      "Epoch 7/25, Batch 31/2194, Loss: 0.1353\n",
      "Epoch 7/25, Batch 41/2194, Loss: 0.2360\n",
      "Epoch 7/25, Batch 51/2194, Loss: 0.0975\n",
      "Epoch 7/25, Batch 61/2194, Loss: 0.1386\n",
      "Epoch 7/25, Batch 71/2194, Loss: 0.1117\n",
      "Epoch 7/25, Batch 81/2194, Loss: 0.2340\n",
      "Epoch 7/25, Batch 91/2194, Loss: 0.2029\n",
      "Epoch 7/25, Batch 101/2194, Loss: 0.3087\n",
      "Epoch 7/25, Batch 111/2194, Loss: 0.1322\n",
      "Epoch 7/25, Batch 121/2194, Loss: 0.1984\n",
      "Epoch 7/25, Batch 131/2194, Loss: 0.1521\n",
      "Epoch 7/25, Batch 141/2194, Loss: 0.1356\n",
      "Epoch 7/25, Batch 151/2194, Loss: 0.0960\n",
      "Epoch 7/25, Batch 161/2194, Loss: 0.0223\n",
      "Epoch 7/25, Batch 171/2194, Loss: 0.2157\n",
      "Epoch 7/25, Batch 181/2194, Loss: 0.1241\n",
      "Epoch 7/25, Batch 191/2194, Loss: 0.3249\n",
      "Epoch 7/25, Batch 201/2194, Loss: 0.2111\n",
      "Epoch 7/25, Batch 211/2194, Loss: 0.2039\n",
      "Epoch 7/25, Batch 221/2194, Loss: 0.2825\n",
      "Epoch 7/25, Batch 231/2194, Loss: 0.2112\n",
      "Epoch 7/25, Batch 241/2194, Loss: 0.4294\n",
      "Epoch 7/25, Batch 251/2194, Loss: 0.1530\n",
      "Epoch 7/25, Batch 261/2194, Loss: 0.2514\n",
      "Epoch 7/25, Batch 271/2194, Loss: 0.0590\n",
      "Epoch 7/25, Batch 281/2194, Loss: 0.0426\n",
      "Epoch 7/25, Batch 291/2194, Loss: 0.3483\n",
      "Epoch 7/25, Batch 301/2194, Loss: 0.2249\n",
      "Epoch 7/25, Batch 311/2194, Loss: 0.4175\n",
      "Epoch 7/25, Batch 321/2194, Loss: 0.3311\n",
      "Epoch 7/25, Batch 331/2194, Loss: 0.0944\n",
      "Epoch 7/25, Batch 341/2194, Loss: 0.2443\n",
      "Epoch 7/25, Batch 351/2194, Loss: 0.0861\n",
      "Epoch 7/25, Batch 361/2194, Loss: 0.0944\n",
      "Epoch 7/25, Batch 371/2194, Loss: 0.2596\n",
      "Epoch 7/25, Batch 381/2194, Loss: 0.1624\n",
      "Epoch 7/25, Batch 391/2194, Loss: 0.1621\n",
      "Epoch 7/25, Batch 401/2194, Loss: 0.3683\n",
      "Epoch 7/25, Batch 411/2194, Loss: 0.2547\n",
      "Epoch 7/25, Batch 421/2194, Loss: 0.1527\n",
      "Epoch 7/25, Batch 431/2194, Loss: 0.4060\n",
      "Epoch 7/25, Batch 441/2194, Loss: 0.1935\n",
      "Epoch 7/25, Batch 451/2194, Loss: 0.2704\n",
      "Epoch 7/25, Batch 461/2194, Loss: 0.1579\n",
      "Epoch 7/25, Batch 471/2194, Loss: 0.0583\n",
      "Epoch 7/25, Batch 481/2194, Loss: 0.3520\n",
      "Epoch 7/25, Batch 491/2194, Loss: 0.2024\n",
      "Epoch 7/25, Batch 501/2194, Loss: 0.3014\n",
      "Epoch 7/25, Batch 511/2194, Loss: 0.1683\n",
      "Epoch 7/25, Batch 521/2194, Loss: 0.2274\n",
      "Epoch 7/25, Batch 531/2194, Loss: 0.3707\n",
      "Epoch 7/25, Batch 541/2194, Loss: 0.1594\n",
      "Epoch 7/25, Batch 551/2194, Loss: 0.2653\n",
      "Epoch 7/25, Batch 561/2194, Loss: 0.1624\n",
      "Epoch 7/25, Batch 571/2194, Loss: 0.2227\n",
      "Epoch 7/25, Batch 581/2194, Loss: 0.1075\n",
      "Epoch 7/25, Batch 591/2194, Loss: 0.0596\n",
      "Epoch 7/25, Batch 601/2194, Loss: 0.1077\n",
      "Epoch 7/25, Batch 611/2194, Loss: 0.2381\n",
      "Epoch 7/25, Batch 621/2194, Loss: 0.0871\n",
      "Epoch 7/25, Batch 631/2194, Loss: 0.1493\n",
      "Epoch 7/25, Batch 641/2194, Loss: 0.2418\n",
      "Epoch 7/25, Batch 651/2194, Loss: 0.1819\n",
      "Epoch 7/25, Batch 661/2194, Loss: 0.1212\n",
      "Epoch 7/25, Batch 671/2194, Loss: 0.2438\n",
      "Epoch 7/25, Batch 681/2194, Loss: 0.1625\n",
      "Epoch 7/25, Batch 691/2194, Loss: 0.1783\n",
      "Epoch 7/25, Batch 701/2194, Loss: 0.1850\n",
      "Epoch 7/25, Batch 711/2194, Loss: 0.0334\n",
      "Epoch 7/25, Batch 721/2194, Loss: 0.2653\n",
      "Epoch 7/25, Batch 731/2194, Loss: 0.1508\n",
      "Epoch 7/25, Batch 741/2194, Loss: 0.2411\n",
      "Epoch 7/25, Batch 751/2194, Loss: 0.1751\n",
      "Epoch 7/25, Batch 761/2194, Loss: 0.2163\n",
      "Epoch 7/25, Batch 771/2194, Loss: 0.1900\n",
      "Epoch 7/25, Batch 781/2194, Loss: 0.1342\n",
      "Epoch 7/25, Batch 791/2194, Loss: 0.1196\n",
      "Epoch 7/25, Batch 801/2194, Loss: 0.1551\n",
      "Epoch 7/25, Batch 811/2194, Loss: 0.1746\n",
      "Epoch 7/25, Batch 821/2194, Loss: 0.2157\n",
      "Epoch 7/25, Batch 831/2194, Loss: 0.2231\n",
      "Epoch 7/25, Batch 841/2194, Loss: 0.2720\n",
      "Epoch 7/25, Batch 851/2194, Loss: 0.2534\n",
      "Epoch 7/25, Batch 861/2194, Loss: 0.3620\n",
      "Epoch 7/25, Batch 871/2194, Loss: 0.0460\n",
      "Epoch 7/25, Batch 881/2194, Loss: 0.0570\n",
      "Epoch 7/25, Batch 891/2194, Loss: 0.2646\n",
      "Epoch 7/25, Batch 901/2194, Loss: 0.2221\n",
      "Epoch 7/25, Batch 911/2194, Loss: 0.1786\n",
      "Epoch 7/25, Batch 921/2194, Loss: 0.0770\n",
      "Epoch 7/25, Batch 931/2194, Loss: 0.2735\n",
      "Epoch 7/25, Batch 941/2194, Loss: 0.1582\n",
      "Epoch 7/25, Batch 951/2194, Loss: 0.1052\n",
      "Epoch 7/25, Batch 961/2194, Loss: 0.3101\n",
      "Epoch 7/25, Batch 971/2194, Loss: 0.0943\n",
      "Epoch 7/25, Batch 981/2194, Loss: 0.1802\n",
      "Epoch 7/25, Batch 991/2194, Loss: 0.1255\n",
      "Epoch 7/25, Batch 1001/2194, Loss: 0.0925\n",
      "Epoch 7/25, Batch 1011/2194, Loss: 0.3021\n",
      "Epoch 7/25, Batch 1021/2194, Loss: 0.1982\n",
      "Epoch 7/25, Batch 1031/2194, Loss: 0.2418\n",
      "Epoch 7/25, Batch 1041/2194, Loss: 0.1905\n",
      "Epoch 7/25, Batch 1051/2194, Loss: 0.0918\n",
      "Epoch 7/25, Batch 1061/2194, Loss: 0.1012\n",
      "Epoch 7/25, Batch 1071/2194, Loss: 0.1932\n",
      "Epoch 7/25, Batch 1081/2194, Loss: 0.3155\n",
      "Epoch 7/25, Batch 1091/2194, Loss: 0.1176\n",
      "Epoch 7/25, Batch 1101/2194, Loss: 0.1919\n",
      "Epoch 7/25, Batch 1111/2194, Loss: 0.0597\n",
      "Epoch 7/25, Batch 1121/2194, Loss: 0.1404\n",
      "Epoch 7/25, Batch 1131/2194, Loss: 0.1616\n",
      "Epoch 7/25, Batch 1141/2194, Loss: 0.2841\n",
      "Epoch 7/25, Batch 1151/2194, Loss: 0.2526\n",
      "Epoch 7/25, Batch 1161/2194, Loss: 0.1281\n",
      "Epoch 7/25, Batch 1171/2194, Loss: 0.2174\n",
      "Epoch 7/25, Batch 1181/2194, Loss: 0.1450\n",
      "Epoch 7/25, Batch 1191/2194, Loss: 0.1652\n",
      "Epoch 7/25, Batch 1201/2194, Loss: 0.0629\n",
      "Epoch 7/25, Batch 1211/2194, Loss: 0.1557\n",
      "Epoch 7/25, Batch 1221/2194, Loss: 0.1740\n",
      "Epoch 7/25, Batch 1231/2194, Loss: 0.1070\n",
      "Epoch 7/25, Batch 1241/2194, Loss: 0.1390\n",
      "Epoch 7/25, Batch 1251/2194, Loss: 0.0748\n",
      "Epoch 7/25, Batch 1261/2194, Loss: 0.1293\n",
      "Epoch 7/25, Batch 1271/2194, Loss: 0.1860\n",
      "Epoch 7/25, Batch 1281/2194, Loss: 0.4257\n",
      "Epoch 7/25, Batch 1291/2194, Loss: 0.4010\n",
      "Epoch 7/25, Batch 1301/2194, Loss: 0.1908\n",
      "Epoch 7/25, Batch 1311/2194, Loss: 0.1691\n",
      "Epoch 7/25, Batch 1321/2194, Loss: 0.2896\n",
      "Epoch 7/25, Batch 1331/2194, Loss: 0.1704\n",
      "Epoch 7/25, Batch 1341/2194, Loss: 0.2338\n",
      "Epoch 7/25, Batch 1351/2194, Loss: 0.1387\n",
      "Epoch 7/25, Batch 1361/2194, Loss: 0.1984\n",
      "Epoch 7/25, Batch 1371/2194, Loss: 0.1003\n",
      "Epoch 7/25, Batch 1381/2194, Loss: 0.1808\n",
      "Epoch 7/25, Batch 1391/2194, Loss: 0.1391\n",
      "Epoch 7/25, Batch 1401/2194, Loss: 0.1173\n",
      "Epoch 7/25, Batch 1411/2194, Loss: 0.2079\n",
      "Epoch 7/25, Batch 1421/2194, Loss: 0.3234\n",
      "Epoch 7/25, Batch 1431/2194, Loss: 0.1205\n",
      "Epoch 7/25, Batch 1441/2194, Loss: 0.2957\n",
      "Epoch 7/25, Batch 1451/2194, Loss: 0.1408\n",
      "Epoch 7/25, Batch 1461/2194, Loss: 0.2136\n",
      "Epoch 7/25, Batch 1471/2194, Loss: 0.4355\n",
      "Epoch 7/25, Batch 1481/2194, Loss: 0.1132\n",
      "Epoch 7/25, Batch 1491/2194, Loss: 0.1054\n",
      "Epoch 7/25, Batch 1501/2194, Loss: 0.1037\n",
      "Epoch 7/25, Batch 1511/2194, Loss: 0.2954\n",
      "Epoch 7/25, Batch 1521/2194, Loss: 0.0748\n",
      "Epoch 7/25, Batch 1531/2194, Loss: 0.1104\n",
      "Epoch 7/25, Batch 1541/2194, Loss: 0.3724\n",
      "Epoch 7/25, Batch 1551/2194, Loss: 0.1954\n",
      "Epoch 7/25, Batch 1561/2194, Loss: 0.1985\n",
      "Epoch 7/25, Batch 1571/2194, Loss: 0.2230\n",
      "Epoch 7/25, Batch 1581/2194, Loss: 0.0496\n",
      "Epoch 7/25, Batch 1591/2194, Loss: 0.1768\n",
      "Epoch 7/25, Batch 1601/2194, Loss: 0.1670\n",
      "Epoch 7/25, Batch 1611/2194, Loss: 0.1824\n",
      "Epoch 7/25, Batch 1621/2194, Loss: 0.2246\n",
      "Epoch 7/25, Batch 1631/2194, Loss: 0.1273\n",
      "Epoch 7/25, Batch 1641/2194, Loss: 0.1219\n",
      "Epoch 7/25, Batch 1651/2194, Loss: 0.2384\n",
      "Epoch 7/25, Batch 1661/2194, Loss: 0.2862\n",
      "Epoch 7/25, Batch 1671/2194, Loss: 0.1801\n",
      "Epoch 7/25, Batch 1681/2194, Loss: 0.2332\n",
      "Epoch 7/25, Batch 1691/2194, Loss: 0.1942\n",
      "Epoch 7/25, Batch 1701/2194, Loss: 0.1660\n",
      "Epoch 7/25, Batch 1711/2194, Loss: 0.3877\n",
      "Epoch 7/25, Batch 1721/2194, Loss: 0.2547\n",
      "Epoch 7/25, Batch 1731/2194, Loss: 0.1771\n",
      "Epoch 7/25, Batch 1741/2194, Loss: 0.2350\n",
      "Epoch 7/25, Batch 1751/2194, Loss: 0.0523\n",
      "Epoch 7/25, Batch 1761/2194, Loss: 0.1994\n",
      "Epoch 7/25, Batch 1771/2194, Loss: 0.1346\n",
      "Epoch 7/25, Batch 1781/2194, Loss: 0.1443\n",
      "Epoch 7/25, Batch 1791/2194, Loss: 0.1350\n",
      "Epoch 7/25, Batch 1801/2194, Loss: 0.3397\n",
      "Epoch 7/25, Batch 1811/2194, Loss: 0.0383\n",
      "Epoch 7/25, Batch 1821/2194, Loss: 0.2963\n",
      "Epoch 7/25, Batch 1831/2194, Loss: 0.1649\n",
      "Epoch 7/25, Batch 1841/2194, Loss: 0.1100\n",
      "Epoch 7/25, Batch 1851/2194, Loss: 0.2580\n",
      "Epoch 7/25, Batch 1861/2194, Loss: 0.1107\n",
      "Epoch 7/25, Batch 1871/2194, Loss: 0.1831\n",
      "Epoch 7/25, Batch 1881/2194, Loss: 0.1837\n",
      "Epoch 7/25, Batch 1891/2194, Loss: 0.1196\n",
      "Epoch 7/25, Batch 1901/2194, Loss: 0.1849\n",
      "Epoch 7/25, Batch 1911/2194, Loss: 0.3023\n",
      "Epoch 7/25, Batch 1921/2194, Loss: 0.0820\n",
      "Epoch 7/25, Batch 1931/2194, Loss: 0.1079\n",
      "Epoch 7/25, Batch 1941/2194, Loss: 0.0872\n",
      "Epoch 7/25, Batch 1951/2194, Loss: 0.3519\n",
      "Epoch 7/25, Batch 1961/2194, Loss: 0.1314\n",
      "Epoch 7/25, Batch 1971/2194, Loss: 0.1249\n",
      "Epoch 7/25, Batch 1981/2194, Loss: 0.2210\n",
      "Epoch 7/25, Batch 1991/2194, Loss: 0.1093\n",
      "Epoch 7/25, Batch 2001/2194, Loss: 0.1656\n",
      "Epoch 7/25, Batch 2011/2194, Loss: 0.2008\n",
      "Epoch 7/25, Batch 2021/2194, Loss: 0.2603\n",
      "Epoch 7/25, Batch 2031/2194, Loss: 0.1932\n",
      "Epoch 7/25, Batch 2041/2194, Loss: 0.1911\n",
      "Epoch 7/25, Batch 2051/2194, Loss: 0.2233\n",
      "Epoch 7/25, Batch 2061/2194, Loss: 0.1568\n",
      "Epoch 7/25, Batch 2071/2194, Loss: 0.2037\n",
      "Epoch 7/25, Batch 2081/2194, Loss: 0.2278\n",
      "Epoch 7/25, Batch 2091/2194, Loss: 0.1249\n",
      "Epoch 7/25, Batch 2101/2194, Loss: 0.1451\n",
      "Epoch 7/25, Batch 2111/2194, Loss: 0.0960\n",
      "Epoch 7/25, Batch 2121/2194, Loss: 0.1291\n",
      "Epoch 7/25, Batch 2131/2194, Loss: 0.1478\n",
      "Epoch 7/25, Batch 2141/2194, Loss: 0.2326\n",
      "Epoch 7/25, Batch 2151/2194, Loss: 0.2885\n",
      "Epoch 7/25, Batch 2161/2194, Loss: 0.2620\n",
      "Epoch 7/25, Batch 2171/2194, Loss: 0.2287\n",
      "Epoch 7/25, Batch 2181/2194, Loss: 0.1250\n",
      "Epoch 7/25, Batch 2191/2194, Loss: 0.2999\n",
      "Epoch 7/25:\n",
      "Train Loss: 0.1805, Train Acc: 92.17%\n",
      "Val Loss: 0.1698, Val Acc: 92.50%\n",
      "------------------------------------------------------------\n",
      "Starting epoch 8/25...\n",
      "Starting training loop with 2194 batches...\n",
      "Successfully loaded first batch: images shape torch.Size([32, 3, 224, 224]), labels shape torch.Size([32])\n",
      "Epoch 8/25, Batch 1/2194, Loss: 0.2421\n",
      "Epoch 8/25, Batch 11/2194, Loss: 0.1361\n",
      "Epoch 8/25, Batch 21/2194, Loss: 0.1857\n",
      "Epoch 8/25, Batch 31/2194, Loss: 0.2131\n",
      "Epoch 8/25, Batch 41/2194, Loss: 0.3370\n",
      "Epoch 8/25, Batch 51/2194, Loss: 0.1566\n",
      "Epoch 8/25, Batch 61/2194, Loss: 0.2044\n",
      "Epoch 8/25, Batch 71/2194, Loss: 0.0539\n",
      "Epoch 8/25, Batch 81/2194, Loss: 0.1873\n",
      "Epoch 8/25, Batch 91/2194, Loss: 0.1444\n",
      "Epoch 8/25, Batch 101/2194, Loss: 0.1740\n",
      "Epoch 8/25, Batch 111/2194, Loss: 0.1658\n",
      "Epoch 8/25, Batch 121/2194, Loss: 0.1138\n",
      "Epoch 8/25, Batch 131/2194, Loss: 0.2192\n",
      "Epoch 8/25, Batch 141/2194, Loss: 0.0656\n",
      "Epoch 8/25, Batch 151/2194, Loss: 0.2045\n",
      "Epoch 8/25, Batch 161/2194, Loss: 0.0604\n",
      "Epoch 8/25, Batch 171/2194, Loss: 0.3444\n",
      "Epoch 8/25, Batch 181/2194, Loss: 0.0773\n",
      "Epoch 8/25, Batch 191/2194, Loss: 0.1012\n",
      "Epoch 8/25, Batch 201/2194, Loss: 0.2520\n",
      "Epoch 8/25, Batch 211/2194, Loss: 0.1875\n",
      "Epoch 8/25, Batch 221/2194, Loss: 0.3335\n",
      "Epoch 8/25, Batch 231/2194, Loss: 0.1641\n",
      "Epoch 8/25, Batch 241/2194, Loss: 0.1335\n",
      "Epoch 8/25, Batch 251/2194, Loss: 0.1142\n",
      "Epoch 8/25, Batch 261/2194, Loss: 0.3763\n",
      "Epoch 8/25, Batch 271/2194, Loss: 0.1453\n",
      "Epoch 8/25, Batch 281/2194, Loss: 0.2037\n",
      "Epoch 8/25, Batch 291/2194, Loss: 0.0869\n",
      "Epoch 8/25, Batch 301/2194, Loss: 0.3554\n",
      "Epoch 8/25, Batch 311/2194, Loss: 0.2042\n",
      "Epoch 8/25, Batch 321/2194, Loss: 0.0702\n",
      "Epoch 8/25, Batch 331/2194, Loss: 0.0911\n",
      "Epoch 8/25, Batch 341/2194, Loss: 0.1307\n",
      "Epoch 8/25, Batch 351/2194, Loss: 0.1332\n",
      "Epoch 8/25, Batch 361/2194, Loss: 0.0655\n",
      "Epoch 8/25, Batch 371/2194, Loss: 0.1442\n",
      "Epoch 8/25, Batch 381/2194, Loss: 0.4028\n",
      "Epoch 8/25, Batch 391/2194, Loss: 0.0896\n",
      "Epoch 8/25, Batch 401/2194, Loss: 0.1892\n",
      "Epoch 8/25, Batch 411/2194, Loss: 0.1167\n",
      "Epoch 8/25, Batch 421/2194, Loss: 0.0987\n",
      "Epoch 8/25, Batch 431/2194, Loss: 0.1214\n",
      "Epoch 8/25, Batch 441/2194, Loss: 0.1182\n",
      "Epoch 8/25, Batch 451/2194, Loss: 0.2063\n",
      "Epoch 8/25, Batch 461/2194, Loss: 0.1059\n",
      "Epoch 8/25, Batch 471/2194, Loss: 0.2658\n",
      "Epoch 8/25, Batch 481/2194, Loss: 0.0717\n",
      "Epoch 8/25, Batch 491/2194, Loss: 0.1307\n",
      "Epoch 8/25, Batch 501/2194, Loss: 0.3147\n",
      "Epoch 8/25, Batch 511/2194, Loss: 0.0766\n",
      "Epoch 8/25, Batch 521/2194, Loss: 0.2924\n",
      "Epoch 8/25, Batch 531/2194, Loss: 0.0846\n",
      "Epoch 8/25, Batch 541/2194, Loss: 0.1295\n",
      "Epoch 8/25, Batch 551/2194, Loss: 0.3436\n",
      "Epoch 8/25, Batch 561/2194, Loss: 0.1192\n",
      "Epoch 8/25, Batch 571/2194, Loss: 0.1249\n",
      "Epoch 8/25, Batch 581/2194, Loss: 0.1254\n",
      "Epoch 8/25, Batch 591/2194, Loss: 0.2217\n",
      "Epoch 8/25, Batch 601/2194, Loss: 0.0297\n",
      "Epoch 8/25, Batch 611/2194, Loss: 0.0588\n",
      "Epoch 8/25, Batch 621/2194, Loss: 0.1895\n",
      "Epoch 8/25, Batch 631/2194, Loss: 0.1588\n",
      "Epoch 8/25, Batch 641/2194, Loss: 0.1071\n",
      "Epoch 8/25, Batch 651/2194, Loss: 0.0926\n",
      "Epoch 8/25, Batch 661/2194, Loss: 0.1288\n",
      "Epoch 8/25, Batch 671/2194, Loss: 0.1723\n",
      "Epoch 8/25, Batch 681/2194, Loss: 0.1355\n",
      "Epoch 8/25, Batch 691/2194, Loss: 0.0847\n",
      "Epoch 8/25, Batch 701/2194, Loss: 0.0988\n",
      "Epoch 8/25, Batch 711/2194, Loss: 0.2037\n",
      "Epoch 8/25, Batch 721/2194, Loss: 0.0532\n",
      "Epoch 8/25, Batch 731/2194, Loss: 0.1690\n",
      "Epoch 8/25, Batch 741/2194, Loss: 0.0692\n",
      "Epoch 8/25, Batch 751/2194, Loss: 0.2956\n",
      "Epoch 8/25, Batch 761/2194, Loss: 0.1232\n",
      "Epoch 8/25, Batch 771/2194, Loss: 0.2503\n",
      "Epoch 8/25, Batch 781/2194, Loss: 0.1055\n",
      "Epoch 8/25, Batch 791/2194, Loss: 0.2234\n",
      "Epoch 8/25, Batch 801/2194, Loss: 0.2036\n",
      "Epoch 8/25, Batch 811/2194, Loss: 0.0805\n",
      "Epoch 8/25, Batch 821/2194, Loss: 0.0854\n",
      "Epoch 8/25, Batch 831/2194, Loss: 0.1010\n",
      "Epoch 8/25, Batch 841/2194, Loss: 0.1016\n",
      "Epoch 8/25, Batch 851/2194, Loss: 0.1220\n",
      "Epoch 8/25, Batch 861/2194, Loss: 0.0887\n",
      "Epoch 8/25, Batch 871/2194, Loss: 0.1093\n",
      "Epoch 8/25, Batch 881/2194, Loss: 0.1430\n",
      "Epoch 8/25, Batch 891/2194, Loss: 0.1904\n",
      "Epoch 8/25, Batch 901/2194, Loss: 0.1633\n",
      "Epoch 8/25, Batch 911/2194, Loss: 0.1625\n",
      "Epoch 8/25, Batch 921/2194, Loss: 0.2155\n",
      "Epoch 8/25, Batch 931/2194, Loss: 0.1647\n",
      "Epoch 8/25, Batch 941/2194, Loss: 0.2090\n",
      "Epoch 8/25, Batch 951/2194, Loss: 0.2265\n",
      "Epoch 8/25, Batch 961/2194, Loss: 0.2462\n",
      "Epoch 8/25, Batch 971/2194, Loss: 0.0775\n",
      "Epoch 8/25, Batch 981/2194, Loss: 0.1242\n",
      "Epoch 8/25, Batch 991/2194, Loss: 0.1940\n",
      "Epoch 8/25, Batch 1001/2194, Loss: 0.0625\n",
      "Epoch 8/25, Batch 1011/2194, Loss: 0.2224\n",
      "Epoch 8/25, Batch 1021/2194, Loss: 0.1389\n",
      "Epoch 8/25, Batch 1031/2194, Loss: 0.1557\n",
      "Epoch 8/25, Batch 1041/2194, Loss: 0.1129\n",
      "Epoch 8/25, Batch 1051/2194, Loss: 0.0748\n",
      "Epoch 8/25, Batch 1061/2194, Loss: 0.1653\n",
      "Epoch 8/25, Batch 1071/2194, Loss: 0.2955\n",
      "Epoch 8/25, Batch 1081/2194, Loss: 0.3054\n",
      "Epoch 8/25, Batch 1091/2194, Loss: 0.2173\n",
      "Epoch 8/25, Batch 1101/2194, Loss: 0.4008\n",
      "Epoch 8/25, Batch 1111/2194, Loss: 0.0904\n",
      "Epoch 8/25, Batch 1121/2194, Loss: 0.5085\n",
      "Epoch 8/25, Batch 1131/2194, Loss: 0.3093\n",
      "Epoch 8/25, Batch 1141/2194, Loss: 0.1125\n",
      "Epoch 8/25, Batch 1151/2194, Loss: 0.0998\n",
      "Epoch 8/25, Batch 1161/2194, Loss: 0.1075\n",
      "Epoch 8/25, Batch 1171/2194, Loss: 0.2179\n",
      "Epoch 8/25, Batch 1181/2194, Loss: 0.1934\n",
      "Epoch 8/25, Batch 1191/2194, Loss: 0.1085\n",
      "Epoch 8/25, Batch 1201/2194, Loss: 0.2916\n",
      "Epoch 8/25, Batch 1211/2194, Loss: 0.0687\n",
      "Epoch 8/25, Batch 1221/2194, Loss: 0.0868\n",
      "Epoch 8/25, Batch 1231/2194, Loss: 0.2161\n",
      "Epoch 8/25, Batch 1241/2194, Loss: 0.1923\n",
      "Epoch 8/25, Batch 1251/2194, Loss: 0.1940\n",
      "Epoch 8/25, Batch 1261/2194, Loss: 0.0850\n",
      "Epoch 8/25, Batch 1271/2194, Loss: 0.1506\n",
      "Epoch 8/25, Batch 1281/2194, Loss: 0.1193\n",
      "Epoch 8/25, Batch 1291/2194, Loss: 0.1787\n",
      "Epoch 8/25, Batch 1301/2194, Loss: 0.0901\n",
      "Epoch 8/25, Batch 1311/2194, Loss: 0.1640\n",
      "Epoch 8/25, Batch 1321/2194, Loss: 0.0607\n",
      "Epoch 8/25, Batch 1331/2194, Loss: 0.1000\n",
      "Epoch 8/25, Batch 1341/2194, Loss: 0.1474\n",
      "Epoch 8/25, Batch 1351/2194, Loss: 0.1260\n",
      "Epoch 8/25, Batch 1361/2194, Loss: 0.1183\n",
      "Epoch 8/25, Batch 1371/2194, Loss: 0.1263\n",
      "Epoch 8/25, Batch 1381/2194, Loss: 0.2554\n",
      "Epoch 8/25, Batch 1391/2194, Loss: 0.2184\n",
      "Epoch 8/25, Batch 1401/2194, Loss: 0.4917\n",
      "Epoch 8/25, Batch 1411/2194, Loss: 0.0490\n",
      "Epoch 8/25, Batch 1421/2194, Loss: 0.0997\n",
      "Epoch 8/25, Batch 1431/2194, Loss: 0.0555\n",
      "Epoch 8/25, Batch 1441/2194, Loss: 0.2257\n",
      "Epoch 8/25, Batch 1451/2194, Loss: 0.2668\n",
      "Epoch 8/25, Batch 1461/2194, Loss: 0.1316\n",
      "Epoch 8/25, Batch 1471/2194, Loss: 0.2099\n",
      "Epoch 8/25, Batch 1481/2194, Loss: 0.0201\n",
      "Epoch 8/25, Batch 1491/2194, Loss: 0.2008\n",
      "Epoch 8/25, Batch 1501/2194, Loss: 0.2547\n",
      "Epoch 8/25, Batch 1511/2194, Loss: 0.1992\n",
      "Epoch 8/25, Batch 1521/2194, Loss: 0.2518\n",
      "Epoch 8/25, Batch 1531/2194, Loss: 0.1378\n",
      "Epoch 8/25, Batch 1541/2194, Loss: 0.2676\n",
      "Epoch 8/25, Batch 1551/2194, Loss: 0.1964\n",
      "Epoch 8/25, Batch 1561/2194, Loss: 0.2266\n",
      "Epoch 8/25, Batch 1571/2194, Loss: 0.1764\n",
      "Epoch 8/25, Batch 1581/2194, Loss: 0.0508\n",
      "Epoch 8/25, Batch 1591/2194, Loss: 0.1185\n",
      "Epoch 8/25, Batch 1601/2194, Loss: 0.0452\n",
      "Epoch 8/25, Batch 1611/2194, Loss: 0.2224\n",
      "Epoch 8/25, Batch 1621/2194, Loss: 0.1076\n",
      "Epoch 8/25, Batch 1631/2194, Loss: 0.0705\n",
      "Epoch 8/25, Batch 1641/2194, Loss: 0.1108\n",
      "Epoch 8/25, Batch 1651/2194, Loss: 0.1957\n",
      "Epoch 8/25, Batch 1661/2194, Loss: 0.1661\n",
      "Epoch 8/25, Batch 1671/2194, Loss: 0.0996\n",
      "Epoch 8/25, Batch 1681/2194, Loss: 0.1523\n",
      "Epoch 8/25, Batch 1691/2194, Loss: 0.2144\n",
      "Epoch 8/25, Batch 1701/2194, Loss: 0.2021\n",
      "Epoch 8/25, Batch 1711/2194, Loss: 0.3616\n",
      "Epoch 8/25, Batch 1721/2194, Loss: 0.2370\n",
      "Epoch 8/25, Batch 1731/2194, Loss: 0.1374\n",
      "Epoch 8/25, Batch 1741/2194, Loss: 0.1314\n",
      "Epoch 8/25, Batch 1751/2194, Loss: 0.2111\n",
      "Epoch 8/25, Batch 1761/2194, Loss: 0.0426\n",
      "Epoch 8/25, Batch 1771/2194, Loss: 0.1482\n",
      "Epoch 8/25, Batch 1781/2194, Loss: 0.1750\n",
      "Epoch 8/25, Batch 1791/2194, Loss: 0.0916\n",
      "Epoch 8/25, Batch 1801/2194, Loss: 0.1938\n",
      "Epoch 8/25, Batch 1811/2194, Loss: 0.0614\n",
      "Epoch 8/25, Batch 1821/2194, Loss: 0.1534\n",
      "Epoch 8/25, Batch 1831/2194, Loss: 0.1574\n",
      "Epoch 8/25, Batch 1841/2194, Loss: 0.0897\n",
      "Epoch 8/25, Batch 1851/2194, Loss: 0.1912\n",
      "Epoch 8/25, Batch 1861/2194, Loss: 0.0750\n",
      "Epoch 8/25, Batch 1871/2194, Loss: 0.1459\n",
      "Epoch 8/25, Batch 1881/2194, Loss: 0.1035\n",
      "Epoch 8/25, Batch 1891/2194, Loss: 0.0450\n",
      "Epoch 8/25, Batch 1901/2194, Loss: 0.2518\n",
      "Epoch 8/25, Batch 1911/2194, Loss: 0.1463\n",
      "Epoch 8/25, Batch 1921/2194, Loss: 0.1749\n",
      "Epoch 8/25, Batch 1931/2194, Loss: 0.1443\n",
      "Epoch 8/25, Batch 1941/2194, Loss: 0.1347\n",
      "Epoch 8/25, Batch 1951/2194, Loss: 0.3176\n",
      "Epoch 8/25, Batch 1961/2194, Loss: 0.1532\n",
      "Epoch 8/25, Batch 1971/2194, Loss: 0.3218\n",
      "Epoch 8/25, Batch 1981/2194, Loss: 0.1410\n",
      "Epoch 8/25, Batch 1991/2194, Loss: 0.2329\n",
      "Epoch 8/25, Batch 2001/2194, Loss: 0.0893\n",
      "Epoch 8/25, Batch 2011/2194, Loss: 0.1330\n",
      "Epoch 8/25, Batch 2021/2194, Loss: 0.1682\n",
      "Epoch 8/25, Batch 2031/2194, Loss: 0.2677\n",
      "Epoch 8/25, Batch 2041/2194, Loss: 0.0814\n",
      "Epoch 8/25, Batch 2051/2194, Loss: 0.2343\n",
      "Epoch 8/25, Batch 2061/2194, Loss: 0.0884\n",
      "Epoch 8/25, Batch 2071/2194, Loss: 0.1994\n",
      "Epoch 8/25, Batch 2081/2194, Loss: 0.1743\n",
      "Epoch 8/25, Batch 2091/2194, Loss: 0.1777\n",
      "Epoch 8/25, Batch 2101/2194, Loss: 0.2840\n",
      "Epoch 8/25, Batch 2111/2194, Loss: 0.1872\n",
      "Epoch 8/25, Batch 2121/2194, Loss: 0.1227\n",
      "Epoch 8/25, Batch 2131/2194, Loss: 0.1331\n",
      "Epoch 8/25, Batch 2141/2194, Loss: 0.5959\n",
      "Epoch 8/25, Batch 2151/2194, Loss: 0.0384\n",
      "Epoch 8/25, Batch 2161/2194, Loss: 0.1008\n",
      "Epoch 8/25, Batch 2171/2194, Loss: 0.2103\n",
      "Epoch 8/25, Batch 2181/2194, Loss: 0.3060\n",
      "Epoch 8/25, Batch 2191/2194, Loss: 0.2194\n",
      "Epoch 8/25:\n",
      "Train Loss: 0.1650, Train Acc: 92.85%\n",
      "Val Loss: 0.1589, Val Acc: 93.11%\n",
      "------------------------------------------------------------\n",
      "Starting epoch 9/25...\n",
      "Starting training loop with 2194 batches...\n",
      "Successfully loaded first batch: images shape torch.Size([32, 3, 224, 224]), labels shape torch.Size([32])\n",
      "Epoch 9/25, Batch 1/2194, Loss: 0.0945\n",
      "Epoch 9/25, Batch 11/2194, Loss: 0.0777\n",
      "Epoch 9/25, Batch 21/2194, Loss: 0.1485\n",
      "Epoch 9/25, Batch 31/2194, Loss: 0.2106\n",
      "Epoch 9/25, Batch 41/2194, Loss: 0.1794\n",
      "Epoch 9/25, Batch 51/2194, Loss: 0.3289\n",
      "Epoch 9/25, Batch 61/2194, Loss: 0.1573\n",
      "Epoch 9/25, Batch 71/2194, Loss: 0.1228\n",
      "Epoch 9/25, Batch 81/2194, Loss: 0.1387\n",
      "Epoch 9/25, Batch 91/2194, Loss: 0.0195\n",
      "Epoch 9/25, Batch 101/2194, Loss: 0.1405\n",
      "Epoch 9/25, Batch 111/2194, Loss: 0.0692\n",
      "Epoch 9/25, Batch 121/2194, Loss: 0.0572\n",
      "Epoch 9/25, Batch 131/2194, Loss: 0.2663\n",
      "Epoch 9/25, Batch 141/2194, Loss: 0.1608\n",
      "Epoch 9/25, Batch 151/2194, Loss: 0.0718\n",
      "Epoch 9/25, Batch 161/2194, Loss: 0.1826\n",
      "Epoch 9/25, Batch 171/2194, Loss: 0.1739\n",
      "Epoch 9/25, Batch 181/2194, Loss: 0.1132\n",
      "Epoch 9/25, Batch 191/2194, Loss: 0.1500\n",
      "Epoch 9/25, Batch 201/2194, Loss: 0.1208\n",
      "Epoch 9/25, Batch 211/2194, Loss: 0.3011\n",
      "Epoch 9/25, Batch 221/2194, Loss: 0.0841\n",
      "Epoch 9/25, Batch 231/2194, Loss: 0.1343\n",
      "Epoch 9/25, Batch 241/2194, Loss: 0.0819\n",
      "Epoch 9/25, Batch 251/2194, Loss: 0.1404\n",
      "Epoch 9/25, Batch 261/2194, Loss: 0.0663\n",
      "Epoch 9/25, Batch 271/2194, Loss: 0.1206\n",
      "Epoch 9/25, Batch 281/2194, Loss: 0.1382\n",
      "Epoch 9/25, Batch 291/2194, Loss: 0.2269\n",
      "Epoch 9/25, Batch 301/2194, Loss: 0.0517\n",
      "Epoch 9/25, Batch 311/2194, Loss: 0.1593\n",
      "Epoch 9/25, Batch 321/2194, Loss: 0.3254\n",
      "Epoch 9/25, Batch 331/2194, Loss: 0.2786\n",
      "Epoch 9/25, Batch 341/2194, Loss: 0.0542\n",
      "Epoch 9/25, Batch 351/2194, Loss: 0.1636\n",
      "Epoch 9/25, Batch 361/2194, Loss: 0.1345\n",
      "Epoch 9/25, Batch 371/2194, Loss: 0.1834\n",
      "Epoch 9/25, Batch 381/2194, Loss: 0.1661\n",
      "Epoch 9/25, Batch 391/2194, Loss: 0.0951\n",
      "Epoch 9/25, Batch 401/2194, Loss: 0.2467\n",
      "Epoch 9/25, Batch 411/2194, Loss: 0.1443\n",
      "Epoch 9/25, Batch 421/2194, Loss: 0.1905\n",
      "Epoch 9/25, Batch 431/2194, Loss: 0.0562\n",
      "Epoch 9/25, Batch 441/2194, Loss: 0.1945\n",
      "Epoch 9/25, Batch 451/2194, Loss: 0.2371\n",
      "Epoch 9/25, Batch 461/2194, Loss: 0.2090\n",
      "Epoch 9/25, Batch 471/2194, Loss: 0.1358\n",
      "Epoch 9/25, Batch 481/2194, Loss: 0.1027\n",
      "Epoch 9/25, Batch 491/2194, Loss: 0.2229\n",
      "Epoch 9/25, Batch 501/2194, Loss: 0.0644\n",
      "Epoch 9/25, Batch 511/2194, Loss: 0.2983\n",
      "Epoch 9/25, Batch 521/2194, Loss: 0.1721\n",
      "Epoch 9/25, Batch 531/2194, Loss: 0.2932\n",
      "Epoch 9/25, Batch 541/2194, Loss: 0.1797\n",
      "Epoch 9/25, Batch 551/2194, Loss: 0.1087\n",
      "Epoch 9/25, Batch 561/2194, Loss: 0.1854\n",
      "Epoch 9/25, Batch 571/2194, Loss: 0.1392\n",
      "Epoch 9/25, Batch 581/2194, Loss: 0.1801\n",
      "Epoch 9/25, Batch 591/2194, Loss: 0.2545\n",
      "Epoch 9/25, Batch 601/2194, Loss: 0.1608\n",
      "Epoch 9/25, Batch 611/2194, Loss: 0.0397\n",
      "Epoch 9/25, Batch 621/2194, Loss: 0.2340\n",
      "Epoch 9/25, Batch 631/2194, Loss: 0.1626\n",
      "Epoch 9/25, Batch 641/2194, Loss: 0.1572\n",
      "Epoch 9/25, Batch 651/2194, Loss: 0.3808\n",
      "Epoch 9/25, Batch 661/2194, Loss: 0.0956\n",
      "Epoch 9/25, Batch 671/2194, Loss: 0.1678\n",
      "Epoch 9/25, Batch 681/2194, Loss: 0.0388\n",
      "Epoch 9/25, Batch 691/2194, Loss: 0.3578\n",
      "Epoch 9/25, Batch 701/2194, Loss: 0.2134\n",
      "Epoch 9/25, Batch 711/2194, Loss: 0.0908\n",
      "Epoch 9/25, Batch 721/2194, Loss: 0.0947\n",
      "Epoch 9/25, Batch 731/2194, Loss: 0.1467\n",
      "Epoch 9/25, Batch 741/2194, Loss: 0.0855\n",
      "Epoch 9/25, Batch 751/2194, Loss: 0.1854\n",
      "Epoch 9/25, Batch 761/2194, Loss: 0.1660\n",
      "Epoch 9/25, Batch 771/2194, Loss: 0.2971\n",
      "Epoch 9/25, Batch 781/2194, Loss: 0.0965\n",
      "Epoch 9/25, Batch 791/2194, Loss: 0.2095\n",
      "Epoch 9/25, Batch 801/2194, Loss: 0.1588\n",
      "Epoch 9/25, Batch 811/2194, Loss: 0.2321\n",
      "Epoch 9/25, Batch 821/2194, Loss: 0.1605\n",
      "Epoch 9/25, Batch 831/2194, Loss: 0.1886\n",
      "Epoch 9/25, Batch 841/2194, Loss: 0.1067\n",
      "Epoch 9/25, Batch 851/2194, Loss: 0.2615\n",
      "Epoch 9/25, Batch 861/2194, Loss: 0.0821\n",
      "Epoch 9/25, Batch 871/2194, Loss: 0.1807\n",
      "Epoch 9/25, Batch 881/2194, Loss: 0.1363\n",
      "Epoch 9/25, Batch 891/2194, Loss: 0.2505\n",
      "Epoch 9/25, Batch 901/2194, Loss: 0.1321\n",
      "Epoch 9/25, Batch 911/2194, Loss: 0.1099\n",
      "Epoch 9/25, Batch 921/2194, Loss: 0.4134\n",
      "Epoch 9/25, Batch 931/2194, Loss: 0.1452\n",
      "Epoch 9/25, Batch 941/2194, Loss: 0.1341\n",
      "Epoch 9/25, Batch 951/2194, Loss: 0.0818\n",
      "Epoch 9/25, Batch 961/2194, Loss: 0.3210\n",
      "Epoch 9/25, Batch 971/2194, Loss: 0.1208\n",
      "Epoch 9/25, Batch 981/2194, Loss: 0.0999\n",
      "Epoch 9/25, Batch 991/2194, Loss: 0.2917\n",
      "Epoch 9/25, Batch 1001/2194, Loss: 0.1411\n",
      "Epoch 9/25, Batch 1011/2194, Loss: 0.1248\n",
      "Epoch 9/25, Batch 1021/2194, Loss: 0.1446\n",
      "Epoch 9/25, Batch 1031/2194, Loss: 0.3055\n",
      "Epoch 9/25, Batch 1041/2194, Loss: 0.1480\n",
      "Epoch 9/25, Batch 1051/2194, Loss: 0.1080\n",
      "Epoch 9/25, Batch 1061/2194, Loss: 0.1741\n",
      "Epoch 9/25, Batch 1071/2194, Loss: 0.1735\n",
      "Epoch 9/25, Batch 1081/2194, Loss: 0.1293\n",
      "Epoch 9/25, Batch 1091/2194, Loss: 0.2153\n",
      "Epoch 9/25, Batch 1101/2194, Loss: 0.1583\n",
      "Epoch 9/25, Batch 1111/2194, Loss: 0.3200\n",
      "Epoch 9/25, Batch 1121/2194, Loss: 0.2323\n",
      "Epoch 9/25, Batch 1131/2194, Loss: 0.1105\n",
      "Epoch 9/25, Batch 1141/2194, Loss: 0.2319\n",
      "Epoch 9/25, Batch 1151/2194, Loss: 0.2031\n",
      "Epoch 9/25, Batch 1161/2194, Loss: 0.0549\n",
      "Epoch 9/25, Batch 1171/2194, Loss: 0.1172\n",
      "Epoch 9/25, Batch 1181/2194, Loss: 0.1949\n",
      "Epoch 9/25, Batch 1191/2194, Loss: 0.0859\n",
      "Epoch 9/25, Batch 1201/2194, Loss: 0.0589\n",
      "Epoch 9/25, Batch 1211/2194, Loss: 0.0626\n",
      "Epoch 9/25, Batch 1221/2194, Loss: 0.0641\n",
      "Epoch 9/25, Batch 1231/2194, Loss: 0.1530\n",
      "Epoch 9/25, Batch 1241/2194, Loss: 0.0121\n",
      "Epoch 9/25, Batch 1251/2194, Loss: 0.2035\n",
      "Epoch 9/25, Batch 1261/2194, Loss: 0.2046\n",
      "Epoch 9/25, Batch 1271/2194, Loss: 0.0995\n",
      "Epoch 9/25, Batch 1281/2194, Loss: 0.1850\n",
      "Epoch 9/25, Batch 1291/2194, Loss: 0.0811\n",
      "Epoch 9/25, Batch 1301/2194, Loss: 0.2142\n",
      "Epoch 9/25, Batch 1311/2194, Loss: 0.1069\n",
      "Epoch 9/25, Batch 1321/2194, Loss: 0.0901\n",
      "Epoch 9/25, Batch 1331/2194, Loss: 0.1470\n",
      "Epoch 9/25, Batch 1341/2194, Loss: 0.1132\n",
      "Epoch 9/25, Batch 1351/2194, Loss: 0.2186\n",
      "Epoch 9/25, Batch 1361/2194, Loss: 0.2841\n",
      "Epoch 9/25, Batch 1371/2194, Loss: 0.0744\n",
      "Epoch 9/25, Batch 1381/2194, Loss: 0.1768\n",
      "Epoch 9/25, Batch 1391/2194, Loss: 0.1196\n",
      "Epoch 9/25, Batch 1401/2194, Loss: 0.3669\n",
      "Epoch 9/25, Batch 1411/2194, Loss: 0.1895\n",
      "Epoch 9/25, Batch 1421/2194, Loss: 0.1683\n",
      "Epoch 9/25, Batch 1431/2194, Loss: 0.0801\n",
      "Epoch 9/25, Batch 1441/2194, Loss: 0.1941\n",
      "Epoch 9/25, Batch 1451/2194, Loss: 0.1991\n",
      "Epoch 9/25, Batch 1461/2194, Loss: 0.1347\n",
      "Epoch 9/25, Batch 1471/2194, Loss: 0.0522\n",
      "Epoch 9/25, Batch 1481/2194, Loss: 0.0914\n",
      "Epoch 9/25, Batch 1491/2194, Loss: 0.1132\n",
      "Epoch 9/25, Batch 1501/2194, Loss: 0.2350\n",
      "Epoch 9/25, Batch 1511/2194, Loss: 0.0944\n",
      "Epoch 9/25, Batch 1521/2194, Loss: 0.0615\n",
      "Epoch 9/25, Batch 1531/2194, Loss: 0.1468\n",
      "Epoch 9/25, Batch 1541/2194, Loss: 0.1742\n",
      "Epoch 9/25, Batch 1551/2194, Loss: 0.3434\n",
      "Epoch 9/25, Batch 1561/2194, Loss: 0.0829\n",
      "Epoch 9/25, Batch 1571/2194, Loss: 0.0947\n",
      "Epoch 9/25, Batch 1581/2194, Loss: 0.0665\n",
      "Epoch 9/25, Batch 1591/2194, Loss: 0.1017\n",
      "Epoch 9/25, Batch 1601/2194, Loss: 0.2841\n",
      "Epoch 9/25, Batch 1611/2194, Loss: 0.0554\n",
      "Epoch 9/25, Batch 1621/2194, Loss: 0.0848\n",
      "Epoch 9/25, Batch 1631/2194, Loss: 0.1109\n",
      "Epoch 9/25, Batch 1641/2194, Loss: 0.1306\n",
      "Epoch 9/25, Batch 1651/2194, Loss: 0.1475\n",
      "Epoch 9/25, Batch 1661/2194, Loss: 0.0393\n",
      "Epoch 9/25, Batch 1671/2194, Loss: 0.2186\n",
      "Epoch 9/25, Batch 1681/2194, Loss: 0.2373\n",
      "Epoch 9/25, Batch 1691/2194, Loss: 0.1504\n",
      "Epoch 9/25, Batch 1701/2194, Loss: 0.1456\n",
      "Epoch 9/25, Batch 1711/2194, Loss: 0.1246\n",
      "Epoch 9/25, Batch 1721/2194, Loss: 0.3192\n",
      "Epoch 9/25, Batch 1731/2194, Loss: 0.1575\n",
      "Epoch 9/25, Batch 1741/2194, Loss: 0.1992\n",
      "Epoch 9/25, Batch 1751/2194, Loss: 0.1541\n",
      "Epoch 9/25, Batch 1761/2194, Loss: 0.2342\n",
      "Epoch 9/25, Batch 1771/2194, Loss: 0.0748\n",
      "Epoch 9/25, Batch 1781/2194, Loss: 0.1891\n",
      "Epoch 9/25, Batch 1791/2194, Loss: 0.1886\n",
      "Epoch 9/25, Batch 1801/2194, Loss: 0.3472\n",
      "Epoch 9/25, Batch 1811/2194, Loss: 0.1551\n",
      "Epoch 9/25, Batch 1821/2194, Loss: 0.3083\n",
      "Epoch 9/25, Batch 1831/2194, Loss: 0.1214\n",
      "Epoch 9/25, Batch 1841/2194, Loss: 0.1057\n",
      "Epoch 9/25, Batch 1851/2194, Loss: 0.2077\n",
      "Epoch 9/25, Batch 1861/2194, Loss: 0.0199\n",
      "Epoch 9/25, Batch 1871/2194, Loss: 0.0824\n",
      "Epoch 9/25, Batch 1881/2194, Loss: 0.1127\n",
      "Epoch 9/25, Batch 1891/2194, Loss: 0.2128\n",
      "Epoch 9/25, Batch 1901/2194, Loss: 0.1442\n",
      "Epoch 9/25, Batch 1911/2194, Loss: 0.0786\n",
      "Epoch 9/25, Batch 1921/2194, Loss: 0.0604\n",
      "Epoch 9/25, Batch 1931/2194, Loss: 0.0473\n",
      "Epoch 9/25, Batch 1941/2194, Loss: 0.2350\n",
      "Epoch 9/25, Batch 1951/2194, Loss: 0.0745\n",
      "Epoch 9/25, Batch 1961/2194, Loss: 0.0856\n",
      "Epoch 9/25, Batch 1971/2194, Loss: 0.1832\n",
      "Epoch 9/25, Batch 1981/2194, Loss: 0.0628\n",
      "Epoch 9/25, Batch 1991/2194, Loss: 0.1556\n",
      "Epoch 9/25, Batch 2001/2194, Loss: 0.1020\n",
      "Epoch 9/25, Batch 2011/2194, Loss: 0.2317\n",
      "Epoch 9/25, Batch 2021/2194, Loss: 0.2596\n",
      "Epoch 9/25, Batch 2031/2194, Loss: 0.0539\n",
      "Epoch 9/25, Batch 2041/2194, Loss: 0.2978\n",
      "Epoch 9/25, Batch 2051/2194, Loss: 0.0798\n",
      "Epoch 9/25, Batch 2061/2194, Loss: 0.2901\n",
      "Epoch 9/25, Batch 2071/2194, Loss: 0.1987\n",
      "Epoch 9/25, Batch 2081/2194, Loss: 0.1729\n",
      "Epoch 9/25, Batch 2091/2194, Loss: 0.1842\n",
      "Epoch 9/25, Batch 2101/2194, Loss: 0.0653\n",
      "Epoch 9/25, Batch 2111/2194, Loss: 0.1027\n",
      "Epoch 9/25, Batch 2121/2194, Loss: 0.3732\n",
      "Epoch 9/25, Batch 2131/2194, Loss: 0.0179\n",
      "Epoch 9/25, Batch 2141/2194, Loss: 0.1367\n",
      "Epoch 9/25, Batch 2151/2194, Loss: 0.2191\n",
      "Epoch 9/25, Batch 2161/2194, Loss: 0.0824\n",
      "Epoch 9/25, Batch 2171/2194, Loss: 0.1024\n",
      "Epoch 9/25, Batch 2181/2194, Loss: 0.0971\n",
      "Epoch 9/25, Batch 2191/2194, Loss: 0.0890\n",
      "Epoch 9/25:\n",
      "Train Loss: 0.1573, Train Acc: 93.18%\n",
      "Val Loss: 0.1601, Val Acc: 92.87%\n",
      "------------------------------------------------------------\n",
      "Starting epoch 10/25...\n",
      "Starting training loop with 2194 batches...\n",
      "Successfully loaded first batch: images shape torch.Size([32, 3, 224, 224]), labels shape torch.Size([32])\n",
      "Epoch 10/25, Batch 1/2194, Loss: 0.0334\n",
      "Epoch 10/25, Batch 11/2194, Loss: 0.0422\n",
      "Epoch 10/25, Batch 21/2194, Loss: 0.0568\n",
      "Epoch 10/25, Batch 31/2194, Loss: 0.0899\n",
      "Epoch 10/25, Batch 41/2194, Loss: 0.0791\n",
      "Epoch 10/25, Batch 51/2194, Loss: 0.1968\n",
      "Epoch 10/25, Batch 61/2194, Loss: 0.2015\n",
      "Epoch 10/25, Batch 71/2194, Loss: 0.1346\n",
      "Epoch 10/25, Batch 81/2194, Loss: 0.0458\n",
      "Epoch 10/25, Batch 91/2194, Loss: 0.1340\n",
      "Epoch 10/25, Batch 101/2194, Loss: 0.1492\n",
      "Epoch 10/25, Batch 111/2194, Loss: 0.0561\n",
      "Epoch 10/25, Batch 121/2194, Loss: 0.1414\n",
      "Epoch 10/25, Batch 131/2194, Loss: 0.1375\n",
      "Epoch 10/25, Batch 141/2194, Loss: 0.0587\n",
      "Epoch 10/25, Batch 151/2194, Loss: 0.2122\n",
      "Epoch 10/25, Batch 161/2194, Loss: 0.0867\n",
      "Epoch 10/25, Batch 171/2194, Loss: 0.1190\n",
      "Epoch 10/25, Batch 181/2194, Loss: 0.1409\n",
      "Epoch 10/25, Batch 191/2194, Loss: 0.1257\n",
      "Epoch 10/25, Batch 201/2194, Loss: 0.3691\n",
      "Epoch 10/25, Batch 211/2194, Loss: 0.1853\n",
      "Epoch 10/25, Batch 221/2194, Loss: 0.1201\n",
      "Epoch 10/25, Batch 231/2194, Loss: 0.1382\n",
      "Epoch 10/25, Batch 241/2194, Loss: 0.2007\n",
      "Epoch 10/25, Batch 251/2194, Loss: 0.1603\n",
      "Epoch 10/25, Batch 261/2194, Loss: 0.1883\n",
      "Epoch 10/25, Batch 271/2194, Loss: 0.2398\n",
      "Epoch 10/25, Batch 281/2194, Loss: 0.1395\n",
      "Epoch 10/25, Batch 291/2194, Loss: 0.1749\n",
      "Epoch 10/25, Batch 301/2194, Loss: 0.0825\n",
      "Epoch 10/25, Batch 311/2194, Loss: 0.1472\n",
      "Epoch 10/25, Batch 321/2194, Loss: 0.2166\n",
      "Epoch 10/25, Batch 331/2194, Loss: 0.0802\n",
      "Epoch 10/25, Batch 341/2194, Loss: 0.0709\n",
      "Epoch 10/25, Batch 351/2194, Loss: 0.0536\n",
      "Epoch 10/25, Batch 361/2194, Loss: 0.0634\n",
      "Epoch 10/25, Batch 371/2194, Loss: 0.1296\n",
      "Epoch 10/25, Batch 381/2194, Loss: 0.1987\n",
      "Epoch 10/25, Batch 391/2194, Loss: 0.1013\n",
      "Epoch 10/25, Batch 401/2194, Loss: 0.0732\n",
      "Epoch 10/25, Batch 411/2194, Loss: 0.1270\n",
      "Epoch 10/25, Batch 421/2194, Loss: 0.1382\n",
      "Epoch 10/25, Batch 431/2194, Loss: 0.1890\n",
      "Epoch 10/25, Batch 441/2194, Loss: 0.2067\n",
      "Epoch 10/25, Batch 451/2194, Loss: 0.1351\n",
      "Epoch 10/25, Batch 461/2194, Loss: 0.0392\n",
      "Epoch 10/25, Batch 471/2194, Loss: 0.1126\n",
      "Epoch 10/25, Batch 481/2194, Loss: 0.2707\n",
      "Epoch 10/25, Batch 491/2194, Loss: 0.4668\n",
      "Epoch 10/25, Batch 501/2194, Loss: 0.1776\n",
      "Epoch 10/25, Batch 511/2194, Loss: 0.0345\n",
      "Epoch 10/25, Batch 521/2194, Loss: 0.1536\n",
      "Epoch 10/25, Batch 531/2194, Loss: 0.1421\n",
      "Epoch 10/25, Batch 541/2194, Loss: 0.2231\n",
      "Epoch 10/25, Batch 551/2194, Loss: 0.0763\n",
      "Epoch 10/25, Batch 561/2194, Loss: 0.1655\n",
      "Epoch 10/25, Batch 571/2194, Loss: 0.1878\n",
      "Epoch 10/25, Batch 581/2194, Loss: 0.1499\n",
      "Epoch 10/25, Batch 591/2194, Loss: 0.1742\n",
      "Epoch 10/25, Batch 601/2194, Loss: 0.1357\n",
      "Epoch 10/25, Batch 611/2194, Loss: 0.1386\n",
      "Epoch 10/25, Batch 621/2194, Loss: 0.1665\n",
      "Epoch 10/25, Batch 631/2194, Loss: 0.0040\n",
      "Epoch 10/25, Batch 641/2194, Loss: 0.1315\n",
      "Epoch 10/25, Batch 651/2194, Loss: 0.2726\n",
      "Epoch 10/25, Batch 661/2194, Loss: 0.1071\n",
      "Epoch 10/25, Batch 671/2194, Loss: 0.1729\n",
      "Epoch 10/25, Batch 681/2194, Loss: 0.0868\n",
      "Epoch 10/25, Batch 691/2194, Loss: 0.1212\n",
      "Epoch 10/25, Batch 701/2194, Loss: 0.1779\n",
      "Epoch 10/25, Batch 711/2194, Loss: 0.0640\n",
      "Epoch 10/25, Batch 721/2194, Loss: 0.1012\n",
      "Epoch 10/25, Batch 731/2194, Loss: 0.0971\n",
      "Epoch 10/25, Batch 741/2194, Loss: 0.0784\n",
      "Epoch 10/25, Batch 751/2194, Loss: 0.2452\n",
      "Epoch 10/25, Batch 761/2194, Loss: 0.1194\n",
      "Epoch 10/25, Batch 771/2194, Loss: 0.2882\n",
      "Epoch 10/25, Batch 781/2194, Loss: 0.2627\n",
      "Epoch 10/25, Batch 791/2194, Loss: 0.1963\n",
      "Epoch 10/25, Batch 801/2194, Loss: 0.3343\n",
      "Epoch 10/25, Batch 811/2194, Loss: 0.0921\n",
      "Epoch 10/25, Batch 821/2194, Loss: 0.1324\n",
      "Epoch 10/25, Batch 831/2194, Loss: 0.0444\n",
      "Epoch 10/25, Batch 841/2194, Loss: 0.1107\n",
      "Epoch 10/25, Batch 851/2194, Loss: 0.1842\n",
      "Epoch 10/25, Batch 861/2194, Loss: 0.1230\n",
      "Epoch 10/25, Batch 871/2194, Loss: 0.1163\n",
      "Epoch 10/25, Batch 881/2194, Loss: 0.2127\n",
      "Epoch 10/25, Batch 891/2194, Loss: 0.1069\n",
      "Epoch 10/25, Batch 901/2194, Loss: 0.1401\n",
      "Epoch 10/25, Batch 911/2194, Loss: 0.3254\n",
      "Epoch 10/25, Batch 921/2194, Loss: 0.1827\n",
      "Epoch 10/25, Batch 931/2194, Loss: 0.0907\n",
      "Epoch 10/25, Batch 941/2194, Loss: 0.0611\n",
      "Epoch 10/25, Batch 951/2194, Loss: 0.0978\n",
      "Epoch 10/25, Batch 961/2194, Loss: 0.0821\n",
      "Epoch 10/25, Batch 971/2194, Loss: 0.0767\n",
      "Epoch 10/25, Batch 981/2194, Loss: 0.1429\n",
      "Epoch 10/25, Batch 991/2194, Loss: 0.1067\n",
      "Epoch 10/25, Batch 1001/2194, Loss: 0.1501\n",
      "Epoch 10/25, Batch 1011/2194, Loss: 0.2490\n",
      "Epoch 10/25, Batch 1021/2194, Loss: 0.0800\n",
      "Epoch 10/25, Batch 1031/2194, Loss: 0.1086\n",
      "Epoch 10/25, Batch 1041/2194, Loss: 0.0173\n",
      "Epoch 10/25, Batch 1051/2194, Loss: 0.0926\n",
      "Epoch 10/25, Batch 1061/2194, Loss: 0.0651\n",
      "Epoch 10/25, Batch 1071/2194, Loss: 0.0693\n",
      "Epoch 10/25, Batch 1081/2194, Loss: 0.1262\n",
      "Epoch 10/25, Batch 1091/2194, Loss: 0.2264\n",
      "Epoch 10/25, Batch 1101/2194, Loss: 0.1295\n",
      "Epoch 10/25, Batch 1111/2194, Loss: 0.1195\n",
      "Epoch 10/25, Batch 1121/2194, Loss: 0.1879\n",
      "Epoch 10/25, Batch 1131/2194, Loss: 0.2473\n",
      "Epoch 10/25, Batch 1141/2194, Loss: 0.0416\n",
      "Epoch 10/25, Batch 1151/2194, Loss: 0.2351\n",
      "Epoch 10/25, Batch 1161/2194, Loss: 0.2634\n",
      "Epoch 10/25, Batch 1171/2194, Loss: 0.0887\n",
      "Epoch 10/25, Batch 1181/2194, Loss: 0.0842\n",
      "Epoch 10/25, Batch 1191/2194, Loss: 0.0772\n",
      "Epoch 10/25, Batch 1201/2194, Loss: 0.0583\n",
      "Epoch 10/25, Batch 1211/2194, Loss: 0.1332\n",
      "Epoch 10/25, Batch 1221/2194, Loss: 0.1580\n",
      "Epoch 10/25, Batch 1231/2194, Loss: 0.2263\n",
      "Epoch 10/25, Batch 1241/2194, Loss: 0.1516\n",
      "Epoch 10/25, Batch 1251/2194, Loss: 0.0682\n",
      "Epoch 10/25, Batch 1261/2194, Loss: 0.1441\n",
      "Epoch 10/25, Batch 1271/2194, Loss: 0.1347\n",
      "Epoch 10/25, Batch 1281/2194, Loss: 0.3070\n",
      "Epoch 10/25, Batch 1291/2194, Loss: 0.1064\n",
      "Epoch 10/25, Batch 1301/2194, Loss: 0.2249\n",
      "Epoch 10/25, Batch 1311/2194, Loss: 0.1532\n",
      "Epoch 10/25, Batch 1321/2194, Loss: 0.3203\n",
      "Epoch 10/25, Batch 1331/2194, Loss: 0.2398\n",
      "Epoch 10/25, Batch 1341/2194, Loss: 0.1314\n",
      "Epoch 10/25, Batch 1351/2194, Loss: 0.0955\n",
      "Epoch 10/25, Batch 1361/2194, Loss: 0.1359\n",
      "Epoch 10/25, Batch 1371/2194, Loss: 0.1365\n",
      "Epoch 10/25, Batch 1381/2194, Loss: 0.1268\n",
      "Epoch 10/25, Batch 1391/2194, Loss: 0.1780\n",
      "Epoch 10/25, Batch 1401/2194, Loss: 0.0576\n",
      "Epoch 10/25, Batch 1411/2194, Loss: 0.2159\n",
      "Epoch 10/25, Batch 1421/2194, Loss: 0.2894\n",
      "Epoch 10/25, Batch 1431/2194, Loss: 0.0707\n",
      "Epoch 10/25, Batch 1441/2194, Loss: 0.1659\n",
      "Epoch 10/25, Batch 1451/2194, Loss: 0.1454\n",
      "Epoch 10/25, Batch 1461/2194, Loss: 0.2028\n",
      "Epoch 10/25, Batch 1471/2194, Loss: 0.1695\n",
      "Epoch 10/25, Batch 1481/2194, Loss: 0.0395\n",
      "Epoch 10/25, Batch 1491/2194, Loss: 0.1210\n",
      "Epoch 10/25, Batch 1501/2194, Loss: 0.2482\n",
      "Epoch 10/25, Batch 1511/2194, Loss: 0.0548\n",
      "Epoch 10/25, Batch 1521/2194, Loss: 0.2712\n",
      "Epoch 10/25, Batch 1531/2194, Loss: 0.2376\n",
      "Epoch 10/25, Batch 1541/2194, Loss: 0.0562\n",
      "Epoch 10/25, Batch 1551/2194, Loss: 0.0970\n",
      "Epoch 10/25, Batch 1561/2194, Loss: 0.1391\n",
      "Epoch 10/25, Batch 1571/2194, Loss: 0.1099\n",
      "Epoch 10/25, Batch 1581/2194, Loss: 0.2387\n",
      "Epoch 10/25, Batch 1591/2194, Loss: 0.0950\n",
      "Epoch 10/25, Batch 1601/2194, Loss: 0.2899\n",
      "Epoch 10/25, Batch 1611/2194, Loss: 0.0974\n",
      "Epoch 10/25, Batch 1621/2194, Loss: 0.0831\n",
      "Epoch 10/25, Batch 1631/2194, Loss: 0.2016\n",
      "Epoch 10/25, Batch 1641/2194, Loss: 0.0490\n",
      "Epoch 10/25, Batch 1651/2194, Loss: 0.0834\n",
      "Epoch 10/25, Batch 1661/2194, Loss: 0.2689\n",
      "Epoch 10/25, Batch 1671/2194, Loss: 0.1182\n",
      "Epoch 10/25, Batch 1681/2194, Loss: 0.1497\n",
      "Epoch 10/25, Batch 1691/2194, Loss: 0.2940\n",
      "Epoch 10/25, Batch 1701/2194, Loss: 0.0958\n",
      "Epoch 10/25, Batch 1711/2194, Loss: 0.2660\n",
      "Epoch 10/25, Batch 1721/2194, Loss: 0.0473\n",
      "Epoch 10/25, Batch 1731/2194, Loss: 0.1615\n",
      "Epoch 10/25, Batch 1741/2194, Loss: 0.1386\n",
      "Epoch 10/25, Batch 1751/2194, Loss: 0.0485\n",
      "Epoch 10/25, Batch 1761/2194, Loss: 0.1792\n",
      "Epoch 10/25, Batch 1771/2194, Loss: 0.0911\n",
      "Epoch 10/25, Batch 1781/2194, Loss: 0.0950\n",
      "Epoch 10/25, Batch 1791/2194, Loss: 0.2335\n",
      "Epoch 10/25, Batch 1801/2194, Loss: 0.1202\n",
      "Epoch 10/25, Batch 1811/2194, Loss: 0.1086\n",
      "Epoch 10/25, Batch 1821/2194, Loss: 0.0795\n",
      "Epoch 10/25, Batch 1831/2194, Loss: 0.2957\n",
      "Epoch 10/25, Batch 1841/2194, Loss: 0.1323\n",
      "Epoch 10/25, Batch 1851/2194, Loss: 0.2289\n",
      "Epoch 10/25, Batch 1861/2194, Loss: 0.0924\n",
      "Epoch 10/25, Batch 1871/2194, Loss: 0.2441\n",
      "Epoch 10/25, Batch 1881/2194, Loss: 0.3263\n",
      "Epoch 10/25, Batch 1891/2194, Loss: 0.1408\n",
      "Epoch 10/25, Batch 1901/2194, Loss: 0.3064\n",
      "Epoch 10/25, Batch 1911/2194, Loss: 0.0955\n",
      "Epoch 10/25, Batch 1921/2194, Loss: 0.2959\n",
      "Epoch 10/25, Batch 1931/2194, Loss: 0.1511\n",
      "Epoch 10/25, Batch 1941/2194, Loss: 0.1032\n",
      "Epoch 10/25, Batch 1951/2194, Loss: 0.1640\n",
      "Epoch 10/25, Batch 1961/2194, Loss: 0.1358\n",
      "Epoch 10/25, Batch 1971/2194, Loss: 0.0705\n",
      "Epoch 10/25, Batch 1981/2194, Loss: 0.0751\n",
      "Epoch 10/25, Batch 1991/2194, Loss: 0.0846\n",
      "Epoch 10/25, Batch 2001/2194, Loss: 0.1505\n",
      "Epoch 10/25, Batch 2011/2194, Loss: 0.0931\n",
      "Epoch 10/25, Batch 2021/2194, Loss: 0.0600\n",
      "Epoch 10/25, Batch 2031/2194, Loss: 0.0607\n",
      "Epoch 10/25, Batch 2041/2194, Loss: 0.0294\n",
      "Epoch 10/25, Batch 2051/2194, Loss: 0.0486\n",
      "Epoch 10/25, Batch 2061/2194, Loss: 0.1164\n",
      "Epoch 10/25, Batch 2071/2194, Loss: 0.2213\n",
      "Epoch 10/25, Batch 2081/2194, Loss: 0.0517\n",
      "Epoch 10/25, Batch 2091/2194, Loss: 0.1420\n",
      "Epoch 10/25, Batch 2101/2194, Loss: 0.1462\n",
      "Epoch 10/25, Batch 2111/2194, Loss: 0.1524\n",
      "Epoch 10/25, Batch 2121/2194, Loss: 0.3833\n",
      "Epoch 10/25, Batch 2131/2194, Loss: 0.1666\n",
      "Epoch 10/25, Batch 2141/2194, Loss: 0.1767\n",
      "Epoch 10/25, Batch 2151/2194, Loss: 0.1261\n",
      "Epoch 10/25, Batch 2161/2194, Loss: 0.0435\n",
      "Epoch 10/25, Batch 2171/2194, Loss: 0.1640\n",
      "Epoch 10/25, Batch 2181/2194, Loss: 0.2486\n",
      "Epoch 10/25, Batch 2191/2194, Loss: 0.2131\n",
      "Epoch 10/25:\n",
      "Train Loss: 0.1508, Train Acc: 93.56%\n",
      "Val Loss: 0.2038, Val Acc: 90.95%\n",
      "------------------------------------------------------------\n",
      "Starting epoch 11/25...\n",
      "Starting training loop with 2194 batches...\n",
      "Successfully loaded first batch: images shape torch.Size([32, 3, 224, 224]), labels shape torch.Size([32])\n",
      "Epoch 11/25, Batch 1/2194, Loss: 0.1446\n",
      "Epoch 11/25, Batch 11/2194, Loss: 0.2506\n",
      "Epoch 11/25, Batch 21/2194, Loss: 0.3654\n",
      "Epoch 11/25, Batch 31/2194, Loss: 0.1034\n",
      "Epoch 11/25, Batch 41/2194, Loss: 0.0749\n",
      "Epoch 11/25, Batch 51/2194, Loss: 0.2257\n",
      "Epoch 11/25, Batch 61/2194, Loss: 0.1443\n",
      "Epoch 11/25, Batch 71/2194, Loss: 0.2078\n",
      "Epoch 11/25, Batch 81/2194, Loss: 0.2255\n",
      "Epoch 11/25, Batch 91/2194, Loss: 0.0845\n",
      "Epoch 11/25, Batch 101/2194, Loss: 0.1181\n",
      "Epoch 11/25, Batch 111/2194, Loss: 0.1395\n",
      "Epoch 11/25, Batch 121/2194, Loss: 0.2402\n",
      "Epoch 11/25, Batch 131/2194, Loss: 0.0912\n",
      "Epoch 11/25, Batch 141/2194, Loss: 0.1937\n",
      "Epoch 11/25, Batch 151/2194, Loss: 0.1334\n",
      "Epoch 11/25, Batch 161/2194, Loss: 0.0727\n",
      "Epoch 11/25, Batch 171/2194, Loss: 0.1370\n",
      "Epoch 11/25, Batch 181/2194, Loss: 0.1438\n",
      "Epoch 11/25, Batch 191/2194, Loss: 0.2112\n",
      "Epoch 11/25, Batch 201/2194, Loss: 0.2072\n",
      "Epoch 11/25, Batch 211/2194, Loss: 0.2007\n",
      "Epoch 11/25, Batch 221/2194, Loss: 0.1064\n",
      "Epoch 11/25, Batch 231/2194, Loss: 0.2177\n",
      "Epoch 11/25, Batch 241/2194, Loss: 0.1216\n",
      "Epoch 11/25, Batch 251/2194, Loss: 0.1558\n",
      "Epoch 11/25, Batch 261/2194, Loss: 0.1010\n",
      "Epoch 11/25, Batch 271/2194, Loss: 0.1101\n",
      "Epoch 11/25, Batch 281/2194, Loss: 0.2168\n",
      "Epoch 11/25, Batch 291/2194, Loss: 0.2143\n",
      "Epoch 11/25, Batch 301/2194, Loss: 0.0653\n",
      "Epoch 11/25, Batch 311/2194, Loss: 0.1624\n",
      "Epoch 11/25, Batch 321/2194, Loss: 0.0750\n",
      "Epoch 11/25, Batch 331/2194, Loss: 0.1175\n",
      "Epoch 11/25, Batch 341/2194, Loss: 0.0310\n",
      "Epoch 11/25, Batch 351/2194, Loss: 0.2447\n",
      "Epoch 11/25, Batch 361/2194, Loss: 0.0954\n",
      "Epoch 11/25, Batch 371/2194, Loss: 0.0803\n",
      "Epoch 11/25, Batch 381/2194, Loss: 0.1005\n",
      "Epoch 11/25, Batch 391/2194, Loss: 0.1208\n",
      "Epoch 11/25, Batch 401/2194, Loss: 0.0838\n",
      "Epoch 11/25, Batch 411/2194, Loss: 0.1505\n",
      "Epoch 11/25, Batch 421/2194, Loss: 0.1293\n",
      "Epoch 11/25, Batch 431/2194, Loss: 0.1019\n",
      "Epoch 11/25, Batch 441/2194, Loss: 0.1573\n",
      "Epoch 11/25, Batch 451/2194, Loss: 0.1919\n",
      "Epoch 11/25, Batch 461/2194, Loss: 0.0567\n",
      "Epoch 11/25, Batch 471/2194, Loss: 0.0669\n",
      "Epoch 11/25, Batch 481/2194, Loss: 0.3235\n",
      "Epoch 11/25, Batch 491/2194, Loss: 0.2670\n",
      "Epoch 11/25, Batch 501/2194, Loss: 0.1386\n",
      "Epoch 11/25, Batch 511/2194, Loss: 0.0987\n",
      "Epoch 11/25, Batch 521/2194, Loss: 0.0970\n",
      "Epoch 11/25, Batch 531/2194, Loss: 0.1493\n",
      "Epoch 11/25, Batch 541/2194, Loss: 0.1566\n",
      "Epoch 11/25, Batch 551/2194, Loss: 0.1344\n",
      "Epoch 11/25, Batch 561/2194, Loss: 0.3042\n",
      "Epoch 11/25, Batch 571/2194, Loss: 0.1266\n",
      "Epoch 11/25, Batch 581/2194, Loss: 0.1768\n",
      "Epoch 11/25, Batch 591/2194, Loss: 0.4510\n",
      "Epoch 11/25, Batch 601/2194, Loss: 0.1688\n",
      "Epoch 11/25, Batch 611/2194, Loss: 0.1376\n",
      "Epoch 11/25, Batch 621/2194, Loss: 0.0943\n",
      "Epoch 11/25, Batch 631/2194, Loss: 0.2845\n",
      "Epoch 11/25, Batch 641/2194, Loss: 0.1042\n",
      "Epoch 11/25, Batch 651/2194, Loss: 0.0814\n",
      "Epoch 11/25, Batch 661/2194, Loss: 0.0971\n",
      "Epoch 11/25, Batch 671/2194, Loss: 0.1338\n",
      "Epoch 11/25, Batch 681/2194, Loss: 0.2029\n",
      "Epoch 11/25, Batch 691/2194, Loss: 0.2165\n",
      "Epoch 11/25, Batch 701/2194, Loss: 0.0538\n",
      "Epoch 11/25, Batch 711/2194, Loss: 0.1830\n",
      "Epoch 11/25, Batch 721/2194, Loss: 0.1277\n",
      "Epoch 11/25, Batch 731/2194, Loss: 0.1331\n",
      "Epoch 11/25, Batch 741/2194, Loss: 0.2560\n",
      "Epoch 11/25, Batch 751/2194, Loss: 0.2617\n",
      "Epoch 11/25, Batch 761/2194, Loss: 0.0830\n",
      "Epoch 11/25, Batch 771/2194, Loss: 0.0324\n",
      "Epoch 11/25, Batch 781/2194, Loss: 0.1564\n",
      "Epoch 11/25, Batch 791/2194, Loss: 0.1766\n",
      "Epoch 11/25, Batch 801/2194, Loss: 0.0894\n",
      "Epoch 11/25, Batch 811/2194, Loss: 0.0506\n",
      "Epoch 11/25, Batch 821/2194, Loss: 0.1650\n",
      "Epoch 11/25, Batch 831/2194, Loss: 0.0754\n",
      "Epoch 11/25, Batch 841/2194, Loss: 0.2284\n",
      "Epoch 11/25, Batch 851/2194, Loss: 0.2675\n",
      "Epoch 11/25, Batch 861/2194, Loss: 0.1862\n",
      "Epoch 11/25, Batch 871/2194, Loss: 0.0579\n",
      "Epoch 11/25, Batch 881/2194, Loss: 0.1070\n",
      "Epoch 11/25, Batch 891/2194, Loss: 0.0799\n",
      "Epoch 11/25, Batch 901/2194, Loss: 0.0338\n",
      "Epoch 11/25, Batch 911/2194, Loss: 0.0480\n",
      "Epoch 11/25, Batch 921/2194, Loss: 0.0662\n",
      "Epoch 11/25, Batch 931/2194, Loss: 0.3132\n",
      "Epoch 11/25, Batch 941/2194, Loss: 0.0362\n",
      "Epoch 11/25, Batch 951/2194, Loss: 0.4443\n",
      "Epoch 11/25, Batch 961/2194, Loss: 0.1169\n",
      "Epoch 11/25, Batch 971/2194, Loss: 0.1321\n",
      "Epoch 11/25, Batch 981/2194, Loss: 0.1338\n",
      "Epoch 11/25, Batch 991/2194, Loss: 0.1661\n",
      "Epoch 11/25, Batch 1001/2194, Loss: 0.1290\n",
      "Epoch 11/25, Batch 1011/2194, Loss: 0.2310\n",
      "Epoch 11/25, Batch 1021/2194, Loss: 0.1862\n",
      "Epoch 11/25, Batch 1031/2194, Loss: 0.1246\n",
      "Epoch 11/25, Batch 1041/2194, Loss: 0.0523\n",
      "Epoch 11/25, Batch 1051/2194, Loss: 0.2960\n",
      "Epoch 11/25, Batch 1061/2194, Loss: 0.3043\n",
      "Epoch 11/25, Batch 1071/2194, Loss: 0.1177\n",
      "Epoch 11/25, Batch 1081/2194, Loss: 0.2375\n",
      "Epoch 11/25, Batch 1091/2194, Loss: 0.1722\n",
      "Epoch 11/25, Batch 1101/2194, Loss: 0.0791\n",
      "Epoch 11/25, Batch 1111/2194, Loss: 0.0462\n",
      "Epoch 11/25, Batch 1121/2194, Loss: 0.1933\n",
      "Epoch 11/25, Batch 1131/2194, Loss: 0.2395\n",
      "Epoch 11/25, Batch 1141/2194, Loss: 0.1989\n",
      "Epoch 11/25, Batch 1151/2194, Loss: 0.2013\n",
      "Epoch 11/25, Batch 1161/2194, Loss: 0.2845\n",
      "Epoch 11/25, Batch 1171/2194, Loss: 0.0619\n",
      "Epoch 11/25, Batch 1181/2194, Loss: 0.0424\n",
      "Epoch 11/25, Batch 1191/2194, Loss: 0.0737\n",
      "Epoch 11/25, Batch 1201/2194, Loss: 0.1302\n",
      "Epoch 11/25, Batch 1211/2194, Loss: 0.1472\n",
      "Epoch 11/25, Batch 1221/2194, Loss: 0.0599\n",
      "Epoch 11/25, Batch 1231/2194, Loss: 0.0948\n",
      "Epoch 11/25, Batch 1241/2194, Loss: 0.1741\n",
      "Epoch 11/25, Batch 1251/2194, Loss: 0.1634\n",
      "Epoch 11/25, Batch 1261/2194, Loss: 0.0890\n",
      "Epoch 11/25, Batch 1271/2194, Loss: 0.0330\n",
      "Epoch 11/25, Batch 1281/2194, Loss: 0.1879\n",
      "Epoch 11/25, Batch 1291/2194, Loss: 0.0344\n",
      "Epoch 11/25, Batch 1301/2194, Loss: 0.0412\n",
      "Epoch 11/25, Batch 1311/2194, Loss: 0.1685\n",
      "Epoch 11/25, Batch 1321/2194, Loss: 0.0363\n",
      "Epoch 11/25, Batch 1331/2194, Loss: 0.1094\n",
      "Epoch 11/25, Batch 1341/2194, Loss: 0.2037\n",
      "Epoch 11/25, Batch 1351/2194, Loss: 0.1182\n",
      "Epoch 11/25, Batch 1361/2194, Loss: 0.1441\n",
      "Epoch 11/25, Batch 1371/2194, Loss: 0.1383\n",
      "Epoch 11/25, Batch 1381/2194, Loss: 0.0523\n",
      "Epoch 11/25, Batch 1391/2194, Loss: 0.1095\n",
      "Epoch 11/25, Batch 1401/2194, Loss: 0.0940\n",
      "Epoch 11/25, Batch 1411/2194, Loss: 0.1375\n",
      "Epoch 11/25, Batch 1421/2194, Loss: 0.0891\n",
      "Epoch 11/25, Batch 1431/2194, Loss: 0.1014\n",
      "Epoch 11/25, Batch 1441/2194, Loss: 0.1125\n",
      "Epoch 11/25, Batch 1451/2194, Loss: 0.1473\n",
      "Epoch 11/25, Batch 1461/2194, Loss: 0.0702\n",
      "Epoch 11/25, Batch 1471/2194, Loss: 0.0579\n",
      "Epoch 11/25, Batch 1481/2194, Loss: 0.1740\n",
      "Epoch 11/25, Batch 1491/2194, Loss: 0.1518\n",
      "Epoch 11/25, Batch 1501/2194, Loss: 0.0924\n",
      "Epoch 11/25, Batch 1511/2194, Loss: 0.0391\n",
      "Epoch 11/25, Batch 1521/2194, Loss: 0.2845\n",
      "Epoch 11/25, Batch 1531/2194, Loss: 0.1491\n",
      "Epoch 11/25, Batch 1541/2194, Loss: 0.0384\n",
      "Epoch 11/25, Batch 1551/2194, Loss: 0.1622\n",
      "Epoch 11/25, Batch 1561/2194, Loss: 0.0760\n",
      "Epoch 11/25, Batch 1571/2194, Loss: 0.0789\n",
      "Epoch 11/25, Batch 1581/2194, Loss: 0.0550\n",
      "Epoch 11/25, Batch 1591/2194, Loss: 0.1972\n",
      "Epoch 11/25, Batch 1601/2194, Loss: 0.2404\n",
      "Epoch 11/25, Batch 1611/2194, Loss: 0.0542\n",
      "Epoch 11/25, Batch 1621/2194, Loss: 0.1355\n",
      "Epoch 11/25, Batch 1631/2194, Loss: 0.1692\n",
      "Epoch 11/25, Batch 1641/2194, Loss: 0.2035\n",
      "Epoch 11/25, Batch 1651/2194, Loss: 0.1677\n",
      "Epoch 11/25, Batch 1661/2194, Loss: 0.1283\n",
      "Epoch 11/25, Batch 1671/2194, Loss: 0.0826\n",
      "Epoch 11/25, Batch 1681/2194, Loss: 0.1679\n",
      "Epoch 11/25, Batch 1691/2194, Loss: 0.1204\n",
      "Epoch 11/25, Batch 1701/2194, Loss: 0.2347\n",
      "Epoch 11/25, Batch 1711/2194, Loss: 0.3105\n",
      "Epoch 11/25, Batch 1721/2194, Loss: 0.1029\n",
      "Epoch 11/25, Batch 1731/2194, Loss: 0.1750\n",
      "Epoch 11/25, Batch 1741/2194, Loss: 0.2260\n",
      "Epoch 11/25, Batch 1751/2194, Loss: 0.1247\n",
      "Epoch 11/25, Batch 1761/2194, Loss: 0.1893\n",
      "Epoch 11/25, Batch 1771/2194, Loss: 0.0906\n",
      "Epoch 11/25, Batch 1781/2194, Loss: 0.1114\n",
      "Epoch 11/25, Batch 1791/2194, Loss: 0.3127\n",
      "Epoch 11/25, Batch 1801/2194, Loss: 0.0414\n",
      "Epoch 11/25, Batch 1811/2194, Loss: 0.1390\n",
      "Epoch 11/25, Batch 1821/2194, Loss: 0.1059\n",
      "Epoch 11/25, Batch 1831/2194, Loss: 0.0524\n",
      "Epoch 11/25, Batch 1841/2194, Loss: 0.1710\n",
      "Epoch 11/25, Batch 1851/2194, Loss: 0.0894\n",
      "Epoch 11/25, Batch 1861/2194, Loss: 0.1989\n",
      "Epoch 11/25, Batch 1871/2194, Loss: 0.0801\n",
      "Epoch 11/25, Batch 1881/2194, Loss: 0.0992\n",
      "Epoch 11/25, Batch 1891/2194, Loss: 0.2709\n",
      "Epoch 11/25, Batch 1901/2194, Loss: 0.1554\n",
      "Epoch 11/25, Batch 1911/2194, Loss: 0.2616\n",
      "Epoch 11/25, Batch 1921/2194, Loss: 0.1557\n",
      "Epoch 11/25, Batch 1931/2194, Loss: 0.1484\n",
      "Epoch 11/25, Batch 1941/2194, Loss: 0.1540\n",
      "Epoch 11/25, Batch 1951/2194, Loss: 0.2086\n",
      "Epoch 11/25, Batch 1961/2194, Loss: 0.3390\n",
      "Epoch 11/25, Batch 1971/2194, Loss: 0.1410\n",
      "Epoch 11/25, Batch 1981/2194, Loss: 0.2208\n",
      "Epoch 11/25, Batch 1991/2194, Loss: 0.0930\n",
      "Epoch 11/25, Batch 2001/2194, Loss: 0.0756\n",
      "Epoch 11/25, Batch 2011/2194, Loss: 0.1308\n",
      "Epoch 11/25, Batch 2021/2194, Loss: 0.1407\n",
      "Epoch 11/25, Batch 2031/2194, Loss: 0.1603\n",
      "Epoch 11/25, Batch 2041/2194, Loss: 0.0821\n",
      "Epoch 11/25, Batch 2051/2194, Loss: 0.0682\n",
      "Epoch 11/25, Batch 2061/2194, Loss: 0.1442\n",
      "Epoch 11/25, Batch 2071/2194, Loss: 0.2334\n",
      "Epoch 11/25, Batch 2081/2194, Loss: 0.2008\n",
      "Epoch 11/25, Batch 2091/2194, Loss: 0.1472\n",
      "Epoch 11/25, Batch 2101/2194, Loss: 0.0696\n",
      "Epoch 11/25, Batch 2111/2194, Loss: 0.0849\n",
      "Epoch 11/25, Batch 2121/2194, Loss: 0.1626\n",
      "Epoch 11/25, Batch 2131/2194, Loss: 0.1154\n",
      "Epoch 11/25, Batch 2141/2194, Loss: 0.1652\n",
      "Epoch 11/25, Batch 2151/2194, Loss: 0.1163\n",
      "Epoch 11/25, Batch 2161/2194, Loss: 0.0428\n",
      "Epoch 11/25, Batch 2171/2194, Loss: 0.0550\n",
      "Epoch 11/25, Batch 2181/2194, Loss: 0.1436\n",
      "Epoch 11/25, Batch 2191/2194, Loss: 0.1167\n",
      "Epoch 11/25:\n",
      "Train Loss: 0.1421, Train Acc: 93.91%\n",
      "Val Loss: 0.1685, Val Acc: 92.79%\n",
      "------------------------------------------------------------\n",
      "Starting epoch 12/25...\n",
      "Starting training loop with 2194 batches...\n",
      "Successfully loaded first batch: images shape torch.Size([32, 3, 224, 224]), labels shape torch.Size([32])\n",
      "Epoch 12/25, Batch 1/2194, Loss: 0.0673\n",
      "Epoch 12/25, Batch 11/2194, Loss: 0.0992\n",
      "Epoch 12/25, Batch 21/2194, Loss: 0.1000\n",
      "Epoch 12/25, Batch 31/2194, Loss: 0.0731\n",
      "Epoch 12/25, Batch 41/2194, Loss: 0.0869\n",
      "Epoch 12/25, Batch 51/2194, Loss: 0.1745\n",
      "Epoch 12/25, Batch 61/2194, Loss: 0.0566\n",
      "Epoch 12/25, Batch 71/2194, Loss: 0.1459\n",
      "Epoch 12/25, Batch 81/2194, Loss: 0.1033\n",
      "Epoch 12/25, Batch 91/2194, Loss: 0.0801\n",
      "Epoch 12/25, Batch 101/2194, Loss: 0.1556\n",
      "Epoch 12/25, Batch 111/2194, Loss: 0.0878\n",
      "Epoch 12/25, Batch 121/2194, Loss: 0.0841\n",
      "Epoch 12/25, Batch 131/2194, Loss: 0.1503\n",
      "Epoch 12/25, Batch 141/2194, Loss: 0.0928\n",
      "Epoch 12/25, Batch 151/2194, Loss: 0.2629\n",
      "Epoch 12/25, Batch 161/2194, Loss: 0.0812\n",
      "Epoch 12/25, Batch 171/2194, Loss: 0.0498\n",
      "Epoch 12/25, Batch 181/2194, Loss: 0.2870\n",
      "Epoch 12/25, Batch 191/2194, Loss: 0.2145\n",
      "Epoch 12/25, Batch 201/2194, Loss: 0.0376\n",
      "Epoch 12/25, Batch 211/2194, Loss: 0.2379\n",
      "Epoch 12/25, Batch 221/2194, Loss: 0.2193\n",
      "Epoch 12/25, Batch 231/2194, Loss: 0.0949\n",
      "Epoch 12/25, Batch 241/2194, Loss: 0.0919\n",
      "Epoch 12/25, Batch 251/2194, Loss: 0.3596\n",
      "Epoch 12/25, Batch 261/2194, Loss: 0.1677\n",
      "Epoch 12/25, Batch 271/2194, Loss: 0.1161\n",
      "Epoch 12/25, Batch 281/2194, Loss: 0.0893\n",
      "Epoch 12/25, Batch 291/2194, Loss: 0.1019\n",
      "Epoch 12/25, Batch 301/2194, Loss: 0.0673\n",
      "Epoch 12/25, Batch 311/2194, Loss: 0.1026\n",
      "Epoch 12/25, Batch 321/2194, Loss: 0.1217\n",
      "Epoch 12/25, Batch 331/2194, Loss: 0.1551\n",
      "Epoch 12/25, Batch 341/2194, Loss: 0.1696\n",
      "Epoch 12/25, Batch 351/2194, Loss: 0.0755\n",
      "Epoch 12/25, Batch 361/2194, Loss: 0.0815\n",
      "Epoch 12/25, Batch 371/2194, Loss: 0.1881\n",
      "Epoch 12/25, Batch 381/2194, Loss: 0.1181\n",
      "Epoch 12/25, Batch 391/2194, Loss: 0.1014\n",
      "Epoch 12/25, Batch 401/2194, Loss: 0.1667\n",
      "Epoch 12/25, Batch 411/2194, Loss: 0.0853\n",
      "Epoch 12/25, Batch 421/2194, Loss: 0.0754\n",
      "Epoch 12/25, Batch 431/2194, Loss: 0.0230\n",
      "Epoch 12/25, Batch 441/2194, Loss: 0.1402\n",
      "Epoch 12/25, Batch 451/2194, Loss: 0.1628\n",
      "Epoch 12/25, Batch 461/2194, Loss: 0.1416\n",
      "Epoch 12/25, Batch 471/2194, Loss: 0.0354\n",
      "Epoch 12/25, Batch 481/2194, Loss: 0.0970\n",
      "Epoch 12/25, Batch 491/2194, Loss: 0.0903\n",
      "Epoch 12/25, Batch 501/2194, Loss: 0.1683\n",
      "Epoch 12/25, Batch 511/2194, Loss: 0.2100\n",
      "Epoch 12/25, Batch 521/2194, Loss: 0.0727\n",
      "Epoch 12/25, Batch 531/2194, Loss: 0.0385\n",
      "Epoch 12/25, Batch 541/2194, Loss: 0.0321\n",
      "Epoch 12/25, Batch 551/2194, Loss: 0.1392\n",
      "Epoch 12/25, Batch 561/2194, Loss: 0.1310\n",
      "Epoch 12/25, Batch 571/2194, Loss: 0.1843\n",
      "Epoch 12/25, Batch 581/2194, Loss: 0.2493\n",
      "Epoch 12/25, Batch 591/2194, Loss: 0.2186\n",
      "Epoch 12/25, Batch 601/2194, Loss: 0.0840\n",
      "Epoch 12/25, Batch 611/2194, Loss: 0.0675\n",
      "Epoch 12/25, Batch 621/2194, Loss: 0.1814\n",
      "Epoch 12/25, Batch 631/2194, Loss: 0.2345\n",
      "Epoch 12/25, Batch 641/2194, Loss: 0.0482\n",
      "Epoch 12/25, Batch 651/2194, Loss: 0.1694\n",
      "Epoch 12/25, Batch 661/2194, Loss: 0.1577\n",
      "Epoch 12/25, Batch 671/2194, Loss: 0.1612\n",
      "Epoch 12/25, Batch 681/2194, Loss: 0.1114\n",
      "Epoch 12/25, Batch 691/2194, Loss: 0.1190\n",
      "Epoch 12/25, Batch 701/2194, Loss: 0.0511\n",
      "Epoch 12/25, Batch 711/2194, Loss: 0.2819\n",
      "Epoch 12/25, Batch 721/2194, Loss: 0.1705\n",
      "Epoch 12/25, Batch 731/2194, Loss: 0.0977\n",
      "Epoch 12/25, Batch 741/2194, Loss: 0.2144\n",
      "Epoch 12/25, Batch 751/2194, Loss: 0.0850\n",
      "Epoch 12/25, Batch 761/2194, Loss: 0.1190\n",
      "Epoch 12/25, Batch 771/2194, Loss: 0.0820\n",
      "Epoch 12/25, Batch 781/2194, Loss: 0.0991\n",
      "Epoch 12/25, Batch 791/2194, Loss: 0.2656\n",
      "Epoch 12/25, Batch 801/2194, Loss: 0.0571\n",
      "Epoch 12/25, Batch 811/2194, Loss: 0.0758\n",
      "Epoch 12/25, Batch 821/2194, Loss: 0.0851\n",
      "Epoch 12/25, Batch 831/2194, Loss: 0.1012\n",
      "Epoch 12/25, Batch 841/2194, Loss: 0.0143\n",
      "Epoch 12/25, Batch 851/2194, Loss: 0.0834\n",
      "Epoch 12/25, Batch 861/2194, Loss: 0.1484\n",
      "Epoch 12/25, Batch 871/2194, Loss: 0.0904\n",
      "Epoch 12/25, Batch 881/2194, Loss: 0.1275\n",
      "Epoch 12/25, Batch 891/2194, Loss: 0.0420\n",
      "Epoch 12/25, Batch 901/2194, Loss: 0.0456\n",
      "Epoch 12/25, Batch 911/2194, Loss: 0.1337\n",
      "Epoch 12/25, Batch 921/2194, Loss: 0.0859\n",
      "Epoch 12/25, Batch 931/2194, Loss: 0.1530\n",
      "Epoch 12/25, Batch 941/2194, Loss: 0.0240\n",
      "Epoch 12/25, Batch 951/2194, Loss: 0.0281\n",
      "Epoch 12/25, Batch 961/2194, Loss: 0.1857\n",
      "Epoch 12/25, Batch 971/2194, Loss: 0.1638\n",
      "Epoch 12/25, Batch 981/2194, Loss: 0.1088\n",
      "Epoch 12/25, Batch 991/2194, Loss: 0.1431\n",
      "Epoch 12/25, Batch 1001/2194, Loss: 0.1325\n",
      "Epoch 12/25, Batch 1011/2194, Loss: 0.0529\n",
      "Epoch 12/25, Batch 1021/2194, Loss: 0.2547\n",
      "Epoch 12/25, Batch 1031/2194, Loss: 0.1160\n",
      "Epoch 12/25, Batch 1041/2194, Loss: 0.1862\n",
      "Epoch 12/25, Batch 1051/2194, Loss: 0.0716\n",
      "Epoch 12/25, Batch 1061/2194, Loss: 0.1251\n",
      "Epoch 12/25, Batch 1071/2194, Loss: 0.0879\n",
      "Epoch 12/25, Batch 1081/2194, Loss: 0.2339\n",
      "Epoch 12/25, Batch 1091/2194, Loss: 0.0473\n",
      "Epoch 12/25, Batch 1101/2194, Loss: 0.1657\n",
      "Epoch 12/25, Batch 1111/2194, Loss: 0.0972\n",
      "Epoch 12/25, Batch 1121/2194, Loss: 0.1855\n",
      "Epoch 12/25, Batch 1131/2194, Loss: 0.1051\n",
      "Epoch 12/25, Batch 1141/2194, Loss: 0.1293\n",
      "Epoch 12/25, Batch 1151/2194, Loss: 0.0585\n",
      "Epoch 12/25, Batch 1161/2194, Loss: 0.2868\n",
      "Epoch 12/25, Batch 1171/2194, Loss: 0.1351\n",
      "Epoch 12/25, Batch 1181/2194, Loss: 0.1140\n",
      "Epoch 12/25, Batch 1191/2194, Loss: 0.1643\n",
      "Epoch 12/25, Batch 1201/2194, Loss: 0.2212\n",
      "Epoch 12/25, Batch 1211/2194, Loss: 0.1604\n",
      "Epoch 12/25, Batch 1221/2194, Loss: 0.0204\n",
      "Epoch 12/25, Batch 1231/2194, Loss: 0.0380\n",
      "Epoch 12/25, Batch 1241/2194, Loss: 0.4014\n",
      "Epoch 12/25, Batch 1251/2194, Loss: 0.1156\n",
      "Epoch 12/25, Batch 1261/2194, Loss: 0.0993\n",
      "Epoch 12/25, Batch 1271/2194, Loss: 0.1112\n",
      "Epoch 12/25, Batch 1281/2194, Loss: 0.1125\n",
      "Epoch 12/25, Batch 1291/2194, Loss: 0.0506\n",
      "Epoch 12/25, Batch 1301/2194, Loss: 0.0675\n",
      "Epoch 12/25, Batch 1311/2194, Loss: 0.1204\n",
      "Epoch 12/25, Batch 1321/2194, Loss: 0.1828\n",
      "Epoch 12/25, Batch 1331/2194, Loss: 0.1321\n",
      "Epoch 12/25, Batch 1341/2194, Loss: 0.0751\n",
      "Epoch 12/25, Batch 1351/2194, Loss: 0.2645\n",
      "Epoch 12/25, Batch 1361/2194, Loss: 0.0806\n",
      "Epoch 12/25, Batch 1371/2194, Loss: 0.2765\n",
      "Epoch 12/25, Batch 1381/2194, Loss: 0.1338\n",
      "Epoch 12/25, Batch 1391/2194, Loss: 0.2343\n",
      "Epoch 12/25, Batch 1401/2194, Loss: 0.1109\n",
      "Epoch 12/25, Batch 1411/2194, Loss: 0.1093\n",
      "Epoch 12/25, Batch 1421/2194, Loss: 0.2379\n",
      "Epoch 12/25, Batch 1431/2194, Loss: 0.1069\n",
      "Epoch 12/25, Batch 1441/2194, Loss: 0.1932\n",
      "Epoch 12/25, Batch 1451/2194, Loss: 0.0675\n",
      "Epoch 12/25, Batch 1461/2194, Loss: 0.1714\n",
      "Epoch 12/25, Batch 1471/2194, Loss: 0.0456\n",
      "Epoch 12/25, Batch 1481/2194, Loss: 0.1421\n",
      "Epoch 12/25, Batch 1491/2194, Loss: 0.1069\n",
      "Epoch 12/25, Batch 1501/2194, Loss: 0.0935\n",
      "Epoch 12/25, Batch 1511/2194, Loss: 0.1724\n",
      "Epoch 12/25, Batch 1521/2194, Loss: 0.1172\n",
      "Epoch 12/25, Batch 1531/2194, Loss: 0.0827\n",
      "Epoch 12/25, Batch 1541/2194, Loss: 0.1829\n",
      "Epoch 12/25, Batch 1551/2194, Loss: 0.4832\n",
      "Epoch 12/25, Batch 1561/2194, Loss: 0.0729\n",
      "Epoch 12/25, Batch 1571/2194, Loss: 0.1013\n",
      "Epoch 12/25, Batch 1581/2194, Loss: 0.1033\n",
      "Epoch 12/25, Batch 1591/2194, Loss: 0.0854\n",
      "Epoch 12/25, Batch 1601/2194, Loss: 0.0518\n",
      "Epoch 12/25, Batch 1611/2194, Loss: 0.1088\n",
      "Epoch 12/25, Batch 1621/2194, Loss: 0.0538\n",
      "Epoch 12/25, Batch 1631/2194, Loss: 0.1455\n",
      "Epoch 12/25, Batch 1641/2194, Loss: 0.1598\n",
      "Epoch 12/25, Batch 1651/2194, Loss: 0.0429\n",
      "Epoch 12/25, Batch 1661/2194, Loss: 0.1055\n",
      "Epoch 12/25, Batch 1671/2194, Loss: 0.0852\n",
      "Epoch 12/25, Batch 1681/2194, Loss: 0.1418\n",
      "Epoch 12/25, Batch 1691/2194, Loss: 0.1186\n",
      "Epoch 12/25, Batch 1701/2194, Loss: 0.0651\n",
      "Epoch 12/25, Batch 1711/2194, Loss: 0.0872\n",
      "Epoch 12/25, Batch 1721/2194, Loss: 0.1076\n",
      "Epoch 12/25, Batch 1731/2194, Loss: 0.2967\n",
      "Epoch 12/25, Batch 1741/2194, Loss: 0.2971\n",
      "Epoch 12/25, Batch 1751/2194, Loss: 0.0825\n",
      "Epoch 12/25, Batch 1761/2194, Loss: 0.2256\n",
      "Epoch 12/25, Batch 1771/2194, Loss: 0.1995\n",
      "Epoch 12/25, Batch 1781/2194, Loss: 0.0430\n",
      "Epoch 12/25, Batch 1791/2194, Loss: 0.1542\n",
      "Epoch 12/25, Batch 1801/2194, Loss: 0.1107\n",
      "Epoch 12/25, Batch 1811/2194, Loss: 0.0580\n",
      "Epoch 12/25, Batch 1821/2194, Loss: 0.0616\n",
      "Epoch 12/25, Batch 1831/2194, Loss: 0.1263\n",
      "Epoch 12/25, Batch 1841/2194, Loss: 0.0735\n",
      "Epoch 12/25, Batch 1851/2194, Loss: 0.1745\n",
      "Epoch 12/25, Batch 1861/2194, Loss: 0.1318\n",
      "Epoch 12/25, Batch 1871/2194, Loss: 0.0512\n",
      "Epoch 12/25, Batch 1881/2194, Loss: 0.0468\n",
      "Epoch 12/25, Batch 1891/2194, Loss: 0.2101\n",
      "Epoch 12/25, Batch 1901/2194, Loss: 0.1105\n",
      "Epoch 12/25, Batch 1911/2194, Loss: 0.2044\n",
      "Epoch 12/25, Batch 1921/2194, Loss: 0.2033\n",
      "Epoch 12/25, Batch 1931/2194, Loss: 0.1069\n",
      "Epoch 12/25, Batch 1941/2194, Loss: 0.1060\n",
      "Epoch 12/25, Batch 1951/2194, Loss: 0.1239\n",
      "Epoch 12/25, Batch 1961/2194, Loss: 0.1662\n",
      "Epoch 12/25, Batch 1971/2194, Loss: 0.1305\n",
      "Epoch 12/25, Batch 1981/2194, Loss: 0.0572\n",
      "Epoch 12/25, Batch 1991/2194, Loss: 0.1883\n",
      "Epoch 12/25, Batch 2001/2194, Loss: 0.1230\n",
      "Epoch 12/25, Batch 2011/2194, Loss: 0.1466\n",
      "Epoch 12/25, Batch 2021/2194, Loss: 0.1102\n",
      "Epoch 12/25, Batch 2031/2194, Loss: 0.2551\n",
      "Epoch 12/25, Batch 2041/2194, Loss: 0.1153\n",
      "Epoch 12/25, Batch 2051/2194, Loss: 0.2159\n",
      "Epoch 12/25, Batch 2061/2194, Loss: 0.1234\n",
      "Epoch 12/25, Batch 2071/2194, Loss: 0.0615\n",
      "Epoch 12/25, Batch 2081/2194, Loss: 0.1006\n",
      "Epoch 12/25, Batch 2091/2194, Loss: 0.1720\n",
      "Epoch 12/25, Batch 2101/2194, Loss: 0.1899\n",
      "Epoch 12/25, Batch 2111/2194, Loss: 0.1630\n",
      "Epoch 12/25, Batch 2121/2194, Loss: 0.1001\n",
      "Epoch 12/25, Batch 2131/2194, Loss: 0.0225\n",
      "Epoch 12/25, Batch 2141/2194, Loss: 0.0697\n",
      "Epoch 12/25, Batch 2151/2194, Loss: 0.3518\n",
      "Epoch 12/25, Batch 2161/2194, Loss: 0.1237\n",
      "Epoch 12/25, Batch 2171/2194, Loss: 0.1688\n",
      "Epoch 12/25, Batch 2181/2194, Loss: 0.1914\n",
      "Epoch 12/25, Batch 2191/2194, Loss: 0.0895\n",
      "Epoch 12/25:\n",
      "Train Loss: 0.1378, Train Acc: 94.03%\n",
      "Val Loss: 0.1514, Val Acc: 93.57%\n",
      "------------------------------------------------------------\n",
      "Starting epoch 13/25...\n",
      "Starting training loop with 2194 batches...\n",
      "Successfully loaded first batch: images shape torch.Size([32, 3, 224, 224]), labels shape torch.Size([32])\n",
      "Epoch 13/25, Batch 1/2194, Loss: 0.0678\n",
      "Epoch 13/25, Batch 11/2194, Loss: 0.1228\n",
      "Epoch 13/25, Batch 21/2194, Loss: 0.1136\n",
      "Epoch 13/25, Batch 31/2194, Loss: 0.0176\n",
      "Epoch 13/25, Batch 41/2194, Loss: 0.0778\n",
      "Epoch 13/25, Batch 51/2194, Loss: 0.0806\n",
      "Epoch 13/25, Batch 61/2194, Loss: 0.0769\n",
      "Epoch 13/25, Batch 71/2194, Loss: 0.0413\n",
      "Epoch 13/25, Batch 81/2194, Loss: 0.0956\n",
      "Epoch 13/25, Batch 91/2194, Loss: 0.1467\n",
      "Epoch 13/25, Batch 101/2194, Loss: 0.0558\n",
      "Epoch 13/25, Batch 111/2194, Loss: 0.2319\n",
      "Epoch 13/25, Batch 121/2194, Loss: 0.0595\n",
      "Epoch 13/25, Batch 131/2194, Loss: 0.0575\n",
      "Epoch 13/25, Batch 141/2194, Loss: 0.1042\n",
      "Epoch 13/25, Batch 151/2194, Loss: 0.2117\n",
      "Epoch 13/25, Batch 161/2194, Loss: 0.1279\n",
      "Epoch 13/25, Batch 171/2194, Loss: 0.1124\n",
      "Epoch 13/25, Batch 181/2194, Loss: 0.2264\n",
      "Epoch 13/25, Batch 191/2194, Loss: 0.1905\n",
      "Epoch 13/25, Batch 201/2194, Loss: 0.1282\n",
      "Epoch 13/25, Batch 211/2194, Loss: 0.1013\n",
      "Epoch 13/25, Batch 221/2194, Loss: 0.1247\n",
      "Epoch 13/25, Batch 231/2194, Loss: 0.1231\n",
      "Epoch 13/25, Batch 241/2194, Loss: 0.1138\n",
      "Epoch 13/25, Batch 251/2194, Loss: 0.1261\n",
      "Epoch 13/25, Batch 261/2194, Loss: 0.0475\n",
      "Epoch 13/25, Batch 271/2194, Loss: 0.1003\n",
      "Epoch 13/25, Batch 281/2194, Loss: 0.1356\n",
      "Epoch 13/25, Batch 291/2194, Loss: 0.1554\n",
      "Epoch 13/25, Batch 301/2194, Loss: 0.2392\n",
      "Epoch 13/25, Batch 311/2194, Loss: 0.2967\n",
      "Epoch 13/25, Batch 321/2194, Loss: 0.0529\n",
      "Epoch 13/25, Batch 331/2194, Loss: 0.1450\n",
      "Epoch 13/25, Batch 341/2194, Loss: 0.1219\n",
      "Epoch 13/25, Batch 351/2194, Loss: 0.1461\n",
      "Epoch 13/25, Batch 361/2194, Loss: 0.1064\n",
      "Epoch 13/25, Batch 371/2194, Loss: 0.0571\n",
      "Epoch 13/25, Batch 381/2194, Loss: 0.0888\n",
      "Epoch 13/25, Batch 391/2194, Loss: 0.1671\n",
      "Epoch 13/25, Batch 401/2194, Loss: 0.1372\n",
      "Epoch 13/25, Batch 411/2194, Loss: 0.1349\n",
      "Epoch 13/25, Batch 421/2194, Loss: 0.1005\n",
      "Epoch 13/25, Batch 431/2194, Loss: 0.0367\n",
      "Epoch 13/25, Batch 441/2194, Loss: 0.1046\n",
      "Epoch 13/25, Batch 451/2194, Loss: 0.1047\n",
      "Epoch 13/25, Batch 461/2194, Loss: 0.0940\n",
      "Epoch 13/25, Batch 471/2194, Loss: 0.0307\n",
      "Epoch 13/25, Batch 481/2194, Loss: 0.0758\n",
      "Epoch 13/25, Batch 491/2194, Loss: 0.1066\n",
      "Epoch 13/25, Batch 501/2194, Loss: 0.0794\n",
      "Epoch 13/25, Batch 511/2194, Loss: 0.0665\n",
      "Epoch 13/25, Batch 521/2194, Loss: 0.0945\n",
      "Epoch 13/25, Batch 531/2194, Loss: 0.2562\n",
      "Epoch 13/25, Batch 541/2194, Loss: 0.0416\n",
      "Epoch 13/25, Batch 551/2194, Loss: 0.0138\n",
      "Epoch 13/25, Batch 561/2194, Loss: 0.1805\n",
      "Epoch 13/25, Batch 571/2194, Loss: 0.0865\n",
      "Epoch 13/25, Batch 581/2194, Loss: 0.1937\n",
      "Epoch 13/25, Batch 591/2194, Loss: 0.1227\n",
      "Epoch 13/25, Batch 601/2194, Loss: 0.0752\n",
      "Epoch 13/25, Batch 611/2194, Loss: 0.1514\n",
      "Epoch 13/25, Batch 621/2194, Loss: 0.1276\n",
      "Epoch 13/25, Batch 631/2194, Loss: 0.0926\n",
      "Epoch 13/25, Batch 641/2194, Loss: 0.0741\n",
      "Epoch 13/25, Batch 651/2194, Loss: 0.0771\n",
      "Epoch 13/25, Batch 661/2194, Loss: 0.0806\n",
      "Epoch 13/25, Batch 671/2194, Loss: 0.0730\n",
      "Epoch 13/25, Batch 681/2194, Loss: 0.0502\n",
      "Epoch 13/25, Batch 691/2194, Loss: 0.2247\n",
      "Epoch 13/25, Batch 701/2194, Loss: 0.0993\n",
      "Epoch 13/25, Batch 711/2194, Loss: 0.1671\n",
      "Epoch 13/25, Batch 721/2194, Loss: 0.1574\n",
      "Epoch 13/25, Batch 731/2194, Loss: 0.1210\n",
      "Epoch 13/25, Batch 741/2194, Loss: 0.2045\n",
      "Epoch 13/25, Batch 751/2194, Loss: 0.1185\n",
      "Epoch 13/25, Batch 761/2194, Loss: 0.0252\n",
      "Epoch 13/25, Batch 771/2194, Loss: 0.2382\n",
      "Epoch 13/25, Batch 781/2194, Loss: 0.3841\n",
      "Epoch 13/25, Batch 791/2194, Loss: 0.1994\n",
      "Epoch 13/25, Batch 801/2194, Loss: 0.1224\n",
      "Epoch 13/25, Batch 811/2194, Loss: 0.0782\n",
      "Epoch 13/25, Batch 821/2194, Loss: 0.0750\n",
      "Epoch 13/25, Batch 831/2194, Loss: 0.1331\n",
      "Epoch 13/25, Batch 841/2194, Loss: 0.1033\n",
      "Epoch 13/25, Batch 851/2194, Loss: 0.0910\n",
      "Epoch 13/25, Batch 861/2194, Loss: 0.0861\n",
      "Epoch 13/25, Batch 871/2194, Loss: 0.1258\n",
      "Epoch 13/25, Batch 881/2194, Loss: 0.0891\n",
      "Epoch 13/25, Batch 891/2194, Loss: 0.0774\n",
      "Epoch 13/25, Batch 901/2194, Loss: 0.1312\n",
      "Epoch 13/25, Batch 911/2194, Loss: 0.1658\n",
      "Epoch 13/25, Batch 921/2194, Loss: 0.0630\n",
      "Epoch 13/25, Batch 931/2194, Loss: 0.1860\n",
      "Epoch 13/25, Batch 941/2194, Loss: 0.2015\n",
      "Epoch 13/25, Batch 951/2194, Loss: 0.0930\n",
      "Epoch 13/25, Batch 961/2194, Loss: 0.0873\n",
      "Epoch 13/25, Batch 971/2194, Loss: 0.1487\n",
      "Epoch 13/25, Batch 981/2194, Loss: 0.1237\n",
      "Epoch 13/25, Batch 991/2194, Loss: 0.2429\n",
      "Epoch 13/25, Batch 1001/2194, Loss: 0.0982\n",
      "Epoch 13/25, Batch 1011/2194, Loss: 0.2568\n",
      "Epoch 13/25, Batch 1021/2194, Loss: 0.1857\n",
      "Epoch 13/25, Batch 1031/2194, Loss: 0.0854\n",
      "Epoch 13/25, Batch 1041/2194, Loss: 0.0780\n",
      "Epoch 13/25, Batch 1051/2194, Loss: 0.2548\n",
      "Epoch 13/25, Batch 1061/2194, Loss: 0.3008\n",
      "Epoch 13/25, Batch 1071/2194, Loss: 0.0863\n",
      "Epoch 13/25, Batch 1081/2194, Loss: 0.0798\n",
      "Epoch 13/25, Batch 1091/2194, Loss: 0.1013\n",
      "Epoch 13/25, Batch 1101/2194, Loss: 0.1570\n",
      "Epoch 13/25, Batch 1111/2194, Loss: 0.0539\n",
      "Epoch 13/25, Batch 1121/2194, Loss: 0.2307\n",
      "Epoch 13/25, Batch 1131/2194, Loss: 0.1734\n",
      "Epoch 13/25, Batch 1141/2194, Loss: 0.0520\n",
      "Epoch 13/25, Batch 1151/2194, Loss: 0.0379\n",
      "Epoch 13/25, Batch 1161/2194, Loss: 0.0985\n",
      "Epoch 13/25, Batch 1171/2194, Loss: 0.0726\n",
      "Epoch 13/25, Batch 1181/2194, Loss: 0.2764\n",
      "Epoch 13/25, Batch 1191/2194, Loss: 0.2612\n",
      "Epoch 13/25, Batch 1201/2194, Loss: 0.2004\n",
      "Epoch 13/25, Batch 1211/2194, Loss: 0.1622\n",
      "Epoch 13/25, Batch 1221/2194, Loss: 0.1066\n",
      "Epoch 13/25, Batch 1231/2194, Loss: 0.1189\n",
      "Epoch 13/25, Batch 1241/2194, Loss: 0.1553\n",
      "Epoch 13/25, Batch 1251/2194, Loss: 0.2679\n",
      "Epoch 13/25, Batch 1261/2194, Loss: 0.0902\n",
      "Epoch 13/25, Batch 1271/2194, Loss: 0.1742\n",
      "Epoch 13/25, Batch 1281/2194, Loss: 0.0349\n",
      "Epoch 13/25, Batch 1291/2194, Loss: 0.1788\n",
      "Epoch 13/25, Batch 1301/2194, Loss: 0.0893\n",
      "Epoch 13/25, Batch 1311/2194, Loss: 0.1164\n",
      "Epoch 13/25, Batch 1321/2194, Loss: 0.0739\n",
      "Epoch 13/25, Batch 1331/2194, Loss: 0.0364\n",
      "Epoch 13/25, Batch 1341/2194, Loss: 0.2054\n",
      "Epoch 13/25, Batch 1351/2194, Loss: 0.0606\n",
      "Epoch 13/25, Batch 1361/2194, Loss: 0.0729\n",
      "Epoch 13/25, Batch 1371/2194, Loss: 0.0447\n",
      "Epoch 13/25, Batch 1381/2194, Loss: 0.3530\n",
      "Epoch 13/25, Batch 1391/2194, Loss: 0.1357\n",
      "Epoch 13/25, Batch 1401/2194, Loss: 0.1838\n",
      "Epoch 13/25, Batch 1411/2194, Loss: 0.1621\n",
      "Epoch 13/25, Batch 1421/2194, Loss: 0.2982\n",
      "Epoch 13/25, Batch 1431/2194, Loss: 0.2551\n",
      "Epoch 13/25, Batch 1441/2194, Loss: 0.1523\n",
      "Epoch 13/25, Batch 1451/2194, Loss: 0.1173\n",
      "Epoch 13/25, Batch 1461/2194, Loss: 0.1600\n",
      "Epoch 13/25, Batch 1471/2194, Loss: 0.0725\n",
      "Epoch 13/25, Batch 1481/2194, Loss: 0.1599\n",
      "Epoch 13/25, Batch 1491/2194, Loss: 0.0912\n",
      "Epoch 13/25, Batch 1501/2194, Loss: 0.1289\n",
      "Epoch 13/25, Batch 1511/2194, Loss: 0.1759\n",
      "Epoch 13/25, Batch 1521/2194, Loss: 0.0571\n",
      "Epoch 13/25, Batch 1531/2194, Loss: 0.0045\n",
      "Epoch 13/25, Batch 1541/2194, Loss: 0.1633\n",
      "Epoch 13/25, Batch 1551/2194, Loss: 0.2562\n",
      "Epoch 13/25, Batch 1561/2194, Loss: 0.1463\n",
      "Epoch 13/25, Batch 1571/2194, Loss: 0.0760\n",
      "Epoch 13/25, Batch 1581/2194, Loss: 0.0379\n",
      "Epoch 13/25, Batch 1591/2194, Loss: 0.1567\n",
      "Epoch 13/25, Batch 1601/2194, Loss: 0.0547\n",
      "Epoch 13/25, Batch 1611/2194, Loss: 0.0905\n",
      "Epoch 13/25, Batch 1621/2194, Loss: 0.1684\n",
      "Epoch 13/25, Batch 1631/2194, Loss: 0.1073\n",
      "Epoch 13/25, Batch 1641/2194, Loss: 0.0963\n",
      "Epoch 13/25, Batch 1651/2194, Loss: 0.0784\n",
      "Epoch 13/25, Batch 1661/2194, Loss: 0.1049\n",
      "Epoch 13/25, Batch 1671/2194, Loss: 0.1915\n",
      "Epoch 13/25, Batch 1681/2194, Loss: 0.1253\n",
      "Epoch 13/25, Batch 1691/2194, Loss: 0.0895\n",
      "Epoch 13/25, Batch 1701/2194, Loss: 0.1312\n",
      "Epoch 13/25, Batch 1711/2194, Loss: 0.0705\n",
      "Epoch 13/25, Batch 1721/2194, Loss: 0.2310\n",
      "Epoch 13/25, Batch 1731/2194, Loss: 0.0605\n",
      "Epoch 13/25, Batch 1741/2194, Loss: 0.1307\n",
      "Epoch 13/25, Batch 1751/2194, Loss: 0.2000\n",
      "Epoch 13/25, Batch 1761/2194, Loss: 0.1063\n",
      "Epoch 13/25, Batch 1771/2194, Loss: 0.1203\n",
      "Epoch 13/25, Batch 1781/2194, Loss: 0.0584\n",
      "Epoch 13/25, Batch 1791/2194, Loss: 0.3042\n",
      "Epoch 13/25, Batch 1801/2194, Loss: 0.1563\n",
      "Epoch 13/25, Batch 1811/2194, Loss: 0.0577\n",
      "Epoch 13/25, Batch 1821/2194, Loss: 0.4166\n",
      "Epoch 13/25, Batch 1831/2194, Loss: 0.1227\n",
      "Epoch 13/25, Batch 1841/2194, Loss: 0.0511\n",
      "Epoch 13/25, Batch 1851/2194, Loss: 0.1210\n",
      "Epoch 13/25, Batch 1861/2194, Loss: 0.1097\n",
      "Epoch 13/25, Batch 1871/2194, Loss: 0.0785\n",
      "Epoch 13/25, Batch 1881/2194, Loss: 0.1109\n",
      "Epoch 13/25, Batch 1891/2194, Loss: 0.1659\n",
      "Epoch 13/25, Batch 1901/2194, Loss: 0.0686\n",
      "Epoch 13/25, Batch 1911/2194, Loss: 0.2211\n",
      "Epoch 13/25, Batch 1921/2194, Loss: 0.1614\n",
      "Epoch 13/25, Batch 1931/2194, Loss: 0.0759\n",
      "Epoch 13/25, Batch 1941/2194, Loss: 0.4038\n",
      "Epoch 13/25, Batch 1951/2194, Loss: 0.1498\n",
      "Epoch 13/25, Batch 1961/2194, Loss: 0.0119\n",
      "Epoch 13/25, Batch 1971/2194, Loss: 0.1469\n",
      "Epoch 13/25, Batch 1981/2194, Loss: 0.1598\n",
      "Epoch 13/25, Batch 1991/2194, Loss: 0.1185\n",
      "Epoch 13/25, Batch 2001/2194, Loss: 0.1799\n",
      "Epoch 13/25, Batch 2011/2194, Loss: 0.1748\n",
      "Epoch 13/25, Batch 2021/2194, Loss: 0.0286\n",
      "Epoch 13/25, Batch 2031/2194, Loss: 0.2553\n",
      "Epoch 13/25, Batch 2041/2194, Loss: 0.0861\n",
      "Epoch 13/25, Batch 2051/2194, Loss: 0.1674\n",
      "Epoch 13/25, Batch 2061/2194, Loss: 0.1260\n",
      "Epoch 13/25, Batch 2071/2194, Loss: 0.1392\n",
      "Epoch 13/25, Batch 2081/2194, Loss: 0.0989\n",
      "Epoch 13/25, Batch 2091/2194, Loss: 0.2056\n",
      "Epoch 13/25, Batch 2101/2194, Loss: 0.1678\n",
      "Epoch 13/25, Batch 2111/2194, Loss: 0.2903\n",
      "Epoch 13/25, Batch 2121/2194, Loss: 0.1295\n",
      "Epoch 13/25, Batch 2131/2194, Loss: 0.0892\n",
      "Epoch 13/25, Batch 2141/2194, Loss: 0.0468\n",
      "Epoch 13/25, Batch 2151/2194, Loss: 0.1063\n",
      "Epoch 13/25, Batch 2161/2194, Loss: 0.1697\n",
      "Epoch 13/25, Batch 2171/2194, Loss: 0.2907\n",
      "Epoch 13/25, Batch 2181/2194, Loss: 0.0781\n",
      "Epoch 13/25, Batch 2191/2194, Loss: 0.0658\n",
      "Epoch 13/25:\n",
      "Train Loss: 0.1333, Train Acc: 94.36%\n",
      "Val Loss: 0.1587, Val Acc: 93.22%\n",
      "------------------------------------------------------------\n",
      "Starting epoch 14/25...\n",
      "Starting training loop with 2194 batches...\n",
      "Successfully loaded first batch: images shape torch.Size([32, 3, 224, 224]), labels shape torch.Size([32])\n",
      "Epoch 14/25, Batch 1/2194, Loss: 0.0432\n",
      "Epoch 14/25, Batch 11/2194, Loss: 0.0919\n",
      "Epoch 14/25, Batch 21/2194, Loss: 0.0997\n",
      "Epoch 14/25, Batch 31/2194, Loss: 0.3852\n",
      "Epoch 14/25, Batch 41/2194, Loss: 0.0891\n",
      "Epoch 14/25, Batch 51/2194, Loss: 0.1545\n",
      "Epoch 14/25, Batch 61/2194, Loss: 0.0470\n",
      "Epoch 14/25, Batch 71/2194, Loss: 0.0771\n",
      "Epoch 14/25, Batch 81/2194, Loss: 0.1794\n",
      "Epoch 14/25, Batch 91/2194, Loss: 0.1614\n",
      "Epoch 14/25, Batch 101/2194, Loss: 0.1301\n",
      "Epoch 14/25, Batch 111/2194, Loss: 0.1523\n",
      "Epoch 14/25, Batch 121/2194, Loss: 0.0432\n",
      "Epoch 14/25, Batch 131/2194, Loss: 0.1396\n",
      "Epoch 14/25, Batch 141/2194, Loss: 0.1288\n",
      "Epoch 14/25, Batch 151/2194, Loss: 0.1915\n",
      "Epoch 14/25, Batch 161/2194, Loss: 0.1554\n",
      "Epoch 14/25, Batch 171/2194, Loss: 0.1553\n",
      "Epoch 14/25, Batch 181/2194, Loss: 0.1599\n",
      "Epoch 14/25, Batch 191/2194, Loss: 0.0665\n",
      "Epoch 14/25, Batch 201/2194, Loss: 0.0992\n",
      "Epoch 14/25, Batch 211/2194, Loss: 0.1173\n",
      "Epoch 14/25, Batch 221/2194, Loss: 0.0694\n",
      "Epoch 14/25, Batch 231/2194, Loss: 0.1660\n",
      "Epoch 14/25, Batch 241/2194, Loss: 0.0549\n",
      "Epoch 14/25, Batch 251/2194, Loss: 0.2032\n",
      "Epoch 14/25, Batch 261/2194, Loss: 0.1264\n",
      "Epoch 14/25, Batch 271/2194, Loss: 0.2119\n",
      "Epoch 14/25, Batch 281/2194, Loss: 0.0997\n",
      "Epoch 14/25, Batch 291/2194, Loss: 0.1048\n",
      "Epoch 14/25, Batch 301/2194, Loss: 0.2327\n",
      "Epoch 14/25, Batch 311/2194, Loss: 0.2130\n",
      "Epoch 14/25, Batch 321/2194, Loss: 0.1156\n",
      "Epoch 14/25, Batch 331/2194, Loss: 0.0968\n",
      "Epoch 14/25, Batch 341/2194, Loss: 0.1016\n",
      "Epoch 14/25, Batch 351/2194, Loss: 0.2543\n",
      "Epoch 14/25, Batch 361/2194, Loss: 0.1124\n",
      "Epoch 14/25, Batch 371/2194, Loss: 0.1028\n",
      "Epoch 14/25, Batch 381/2194, Loss: 0.1755\n",
      "Epoch 14/25, Batch 391/2194, Loss: 0.1608\n",
      "Epoch 14/25, Batch 401/2194, Loss: 0.1194\n",
      "Epoch 14/25, Batch 411/2194, Loss: 0.1512\n",
      "Epoch 14/25, Batch 421/2194, Loss: 0.1627\n",
      "Epoch 14/25, Batch 431/2194, Loss: 0.1271\n",
      "Epoch 14/25, Batch 441/2194, Loss: 0.1700\n",
      "Epoch 14/25, Batch 451/2194, Loss: 0.1482\n",
      "Epoch 14/25, Batch 461/2194, Loss: 0.1098\n",
      "Epoch 14/25, Batch 471/2194, Loss: 0.2170\n",
      "Epoch 14/25, Batch 481/2194, Loss: 0.1304\n",
      "Epoch 14/25, Batch 491/2194, Loss: 0.2030\n",
      "Epoch 14/25, Batch 501/2194, Loss: 0.0816\n",
      "Epoch 14/25, Batch 511/2194, Loss: 0.0829\n",
      "Epoch 14/25, Batch 521/2194, Loss: 0.0509\n",
      "Epoch 14/25, Batch 531/2194, Loss: 0.1188\n",
      "Epoch 14/25, Batch 541/2194, Loss: 0.0936\n",
      "Epoch 14/25, Batch 551/2194, Loss: 0.2378\n",
      "Epoch 14/25, Batch 561/2194, Loss: 0.1340\n",
      "Epoch 14/25, Batch 571/2194, Loss: 0.0509\n",
      "Epoch 14/25, Batch 581/2194, Loss: 0.0469\n",
      "Epoch 14/25, Batch 591/2194, Loss: 0.1787\n",
      "Epoch 14/25, Batch 601/2194, Loss: 0.0928\n",
      "Epoch 14/25, Batch 611/2194, Loss: 0.0868\n",
      "Epoch 14/25, Batch 621/2194, Loss: 0.1367\n",
      "Epoch 14/25, Batch 631/2194, Loss: 0.1379\n",
      "Epoch 14/25, Batch 641/2194, Loss: 0.2001\n",
      "Epoch 14/25, Batch 651/2194, Loss: 0.2240\n",
      "Epoch 14/25, Batch 661/2194, Loss: 0.0550\n",
      "Epoch 14/25, Batch 671/2194, Loss: 0.1851\n",
      "Epoch 14/25, Batch 681/2194, Loss: 0.0855\n",
      "Epoch 14/25, Batch 691/2194, Loss: 0.0735\n",
      "Epoch 14/25, Batch 701/2194, Loss: 0.0884\n",
      "Epoch 14/25, Batch 711/2194, Loss: 0.2414\n",
      "Epoch 14/25, Batch 721/2194, Loss: 0.1541\n",
      "Epoch 14/25, Batch 731/2194, Loss: 0.1296\n",
      "Epoch 14/25, Batch 741/2194, Loss: 0.0993\n",
      "Epoch 14/25, Batch 751/2194, Loss: 0.1055\n",
      "Epoch 14/25, Batch 761/2194, Loss: 0.0444\n",
      "Epoch 14/25, Batch 771/2194, Loss: 0.0964\n",
      "Epoch 14/25, Batch 781/2194, Loss: 0.2407\n",
      "Epoch 14/25, Batch 791/2194, Loss: 0.0337\n",
      "Epoch 14/25, Batch 801/2194, Loss: 0.0944\n",
      "Epoch 14/25, Batch 811/2194, Loss: 0.3617\n",
      "Epoch 14/25, Batch 821/2194, Loss: 0.2257\n",
      "Epoch 14/25, Batch 831/2194, Loss: 0.2138\n",
      "Epoch 14/25, Batch 841/2194, Loss: 0.1371\n",
      "Epoch 14/25, Batch 851/2194, Loss: 0.2941\n",
      "Epoch 14/25, Batch 861/2194, Loss: 0.0808\n",
      "Epoch 14/25, Batch 871/2194, Loss: 0.1659\n",
      "Epoch 14/25, Batch 881/2194, Loss: 0.1902\n",
      "Epoch 14/25, Batch 891/2194, Loss: 0.1899\n",
      "Epoch 14/25, Batch 901/2194, Loss: 0.1274\n",
      "Epoch 14/25, Batch 911/2194, Loss: 0.0149\n",
      "Epoch 14/25, Batch 921/2194, Loss: 0.2028\n",
      "Epoch 14/25, Batch 931/2194, Loss: 0.2093\n",
      "Epoch 14/25, Batch 941/2194, Loss: 0.1791\n",
      "Epoch 14/25, Batch 951/2194, Loss: 0.0363\n",
      "Epoch 14/25, Batch 961/2194, Loss: 0.2722\n",
      "Epoch 14/25, Batch 971/2194, Loss: 0.0359\n",
      "Epoch 14/25, Batch 981/2194, Loss: 0.1177\n",
      "Epoch 14/25, Batch 991/2194, Loss: 0.0567\n",
      "Epoch 14/25, Batch 1001/2194, Loss: 0.1073\n",
      "Epoch 14/25, Batch 1011/2194, Loss: 0.1148\n",
      "Epoch 14/25, Batch 1021/2194, Loss: 0.1082\n",
      "Epoch 14/25, Batch 1031/2194, Loss: 0.2509\n",
      "Epoch 14/25, Batch 1041/2194, Loss: 0.0973\n",
      "Epoch 14/25, Batch 1051/2194, Loss: 0.1738\n",
      "Epoch 14/25, Batch 1061/2194, Loss: 0.0092\n",
      "Epoch 14/25, Batch 1071/2194, Loss: 0.1766\n",
      "Epoch 14/25, Batch 1081/2194, Loss: 0.1162\n",
      "Epoch 14/25, Batch 1091/2194, Loss: 0.1513\n",
      "Epoch 14/25, Batch 1101/2194, Loss: 0.1139\n",
      "Epoch 14/25, Batch 1111/2194, Loss: 0.0232\n",
      "Epoch 14/25, Batch 1121/2194, Loss: 0.0699\n",
      "Epoch 14/25, Batch 1131/2194, Loss: 0.1095\n",
      "Epoch 14/25, Batch 1141/2194, Loss: 0.3804\n",
      "Epoch 14/25, Batch 1151/2194, Loss: 0.2799\n",
      "Epoch 14/25, Batch 1161/2194, Loss: 0.1158\n",
      "Epoch 14/25, Batch 1171/2194, Loss: 0.2159\n",
      "Epoch 14/25, Batch 1181/2194, Loss: 0.0624\n",
      "Epoch 14/25, Batch 1191/2194, Loss: 0.0653\n",
      "Epoch 14/25, Batch 1201/2194, Loss: 0.1138\n",
      "Epoch 14/25, Batch 1211/2194, Loss: 0.0856\n",
      "Epoch 14/25, Batch 1221/2194, Loss: 0.1561\n",
      "Epoch 14/25, Batch 1231/2194, Loss: 0.0934\n",
      "Epoch 14/25, Batch 1241/2194, Loss: 0.3200\n",
      "Epoch 14/25, Batch 1251/2194, Loss: 0.0806\n",
      "Epoch 14/25, Batch 1261/2194, Loss: 0.1027\n",
      "Epoch 14/25, Batch 1271/2194, Loss: 0.0349\n",
      "Epoch 14/25, Batch 1281/2194, Loss: 0.2860\n",
      "Epoch 14/25, Batch 1291/2194, Loss: 0.2172\n",
      "Epoch 14/25, Batch 1301/2194, Loss: 0.2766\n",
      "Epoch 14/25, Batch 1311/2194, Loss: 0.1846\n",
      "Epoch 14/25, Batch 1321/2194, Loss: 0.1607\n",
      "Epoch 14/25, Batch 1331/2194, Loss: 0.1872\n",
      "Epoch 14/25, Batch 1341/2194, Loss: 0.2017\n",
      "Epoch 14/25, Batch 1351/2194, Loss: 0.0239\n",
      "Epoch 14/25, Batch 1361/2194, Loss: 0.0508\n",
      "Epoch 14/25, Batch 1371/2194, Loss: 0.1870\n",
      "Epoch 14/25, Batch 1381/2194, Loss: 0.0190\n",
      "Epoch 14/25, Batch 1391/2194, Loss: 0.1962\n",
      "Epoch 14/25, Batch 1401/2194, Loss: 0.1485\n",
      "Epoch 14/25, Batch 1411/2194, Loss: 0.0707\n",
      "Epoch 14/25, Batch 1421/2194, Loss: 0.0813\n",
      "Epoch 14/25, Batch 1431/2194, Loss: 0.2766\n",
      "Epoch 14/25, Batch 1441/2194, Loss: 0.1416\n",
      "Epoch 14/25, Batch 1451/2194, Loss: 0.0741\n",
      "Epoch 14/25, Batch 1461/2194, Loss: 0.1607\n",
      "Epoch 14/25, Batch 1471/2194, Loss: 0.1323\n",
      "Epoch 14/25, Batch 1481/2194, Loss: 0.0939\n",
      "Epoch 14/25, Batch 1491/2194, Loss: 0.2216\n",
      "Epoch 14/25, Batch 1501/2194, Loss: 0.2308\n",
      "Epoch 14/25, Batch 1511/2194, Loss: 0.0832\n",
      "Epoch 14/25, Batch 1521/2194, Loss: 0.0928\n",
      "Epoch 14/25, Batch 1531/2194, Loss: 0.2270\n",
      "Epoch 14/25, Batch 1541/2194, Loss: 0.1962\n",
      "Epoch 14/25, Batch 1551/2194, Loss: 0.0809\n",
      "Epoch 14/25, Batch 1561/2194, Loss: 0.0601\n",
      "Epoch 14/25, Batch 1571/2194, Loss: 0.0298\n",
      "Epoch 14/25, Batch 1581/2194, Loss: 0.0857\n",
      "Epoch 14/25, Batch 1591/2194, Loss: 0.1266\n",
      "Epoch 14/25, Batch 1601/2194, Loss: 0.1771\n",
      "Epoch 14/25, Batch 1611/2194, Loss: 0.1408\n",
      "Epoch 14/25, Batch 1621/2194, Loss: 0.1608\n",
      "Epoch 14/25, Batch 1631/2194, Loss: 0.1045\n",
      "Epoch 14/25, Batch 1641/2194, Loss: 0.0848\n",
      "Epoch 14/25, Batch 1651/2194, Loss: 0.0395\n",
      "Epoch 14/25, Batch 1661/2194, Loss: 0.1494\n",
      "Epoch 14/25, Batch 1671/2194, Loss: 0.0375\n",
      "Epoch 14/25, Batch 1681/2194, Loss: 0.0607\n",
      "Epoch 14/25, Batch 1691/2194, Loss: 0.3402\n",
      "Epoch 14/25, Batch 1701/2194, Loss: 0.0761\n",
      "Epoch 14/25, Batch 1711/2194, Loss: 0.1431\n",
      "Epoch 14/25, Batch 1721/2194, Loss: 0.0806\n",
      "Epoch 14/25, Batch 1731/2194, Loss: 0.2376\n",
      "Epoch 14/25, Batch 1741/2194, Loss: 0.0937\n",
      "Epoch 14/25, Batch 1751/2194, Loss: 0.1308\n",
      "Epoch 14/25, Batch 1761/2194, Loss: 0.1861\n",
      "Epoch 14/25, Batch 1771/2194, Loss: 0.1021\n",
      "Epoch 14/25, Batch 1781/2194, Loss: 0.1937\n",
      "Epoch 14/25, Batch 1791/2194, Loss: 0.0460\n",
      "Epoch 14/25, Batch 1801/2194, Loss: 0.0302\n",
      "Epoch 14/25, Batch 1811/2194, Loss: 0.0862\n",
      "Epoch 14/25, Batch 1821/2194, Loss: 0.0803\n",
      "Epoch 14/25, Batch 1831/2194, Loss: 0.4026\n",
      "Epoch 14/25, Batch 1841/2194, Loss: 0.1682\n",
      "Epoch 14/25, Batch 1851/2194, Loss: 0.0829\n",
      "Epoch 14/25, Batch 1861/2194, Loss: 0.0626\n",
      "Epoch 14/25, Batch 1871/2194, Loss: 0.0299\n",
      "Epoch 14/25, Batch 1881/2194, Loss: 0.0941\n",
      "Epoch 14/25, Batch 1891/2194, Loss: 0.0551\n",
      "Epoch 14/25, Batch 1901/2194, Loss: 0.0948\n",
      "Epoch 14/25, Batch 1911/2194, Loss: 0.1101\n",
      "Epoch 14/25, Batch 1921/2194, Loss: 0.0990\n",
      "Epoch 14/25, Batch 1931/2194, Loss: 0.2285\n",
      "Epoch 14/25, Batch 1941/2194, Loss: 0.1921\n",
      "Epoch 14/25, Batch 1951/2194, Loss: 0.2570\n",
      "Epoch 14/25, Batch 1961/2194, Loss: 0.1280\n",
      "Epoch 14/25, Batch 1971/2194, Loss: 0.2273\n",
      "Epoch 14/25, Batch 1981/2194, Loss: 0.0443\n",
      "Epoch 14/25, Batch 1991/2194, Loss: 0.1113\n",
      "Epoch 14/25, Batch 2001/2194, Loss: 0.0783\n",
      "Epoch 14/25, Batch 2011/2194, Loss: 0.0906\n",
      "Epoch 14/25, Batch 2021/2194, Loss: 0.1906\n",
      "Epoch 14/25, Batch 2031/2194, Loss: 0.1164\n",
      "Epoch 14/25, Batch 2041/2194, Loss: 0.0996\n",
      "Epoch 14/25, Batch 2051/2194, Loss: 0.1082\n",
      "Epoch 14/25, Batch 2061/2194, Loss: 0.1486\n",
      "Epoch 14/25, Batch 2071/2194, Loss: 0.0070\n",
      "Epoch 14/25, Batch 2081/2194, Loss: 0.1199\n",
      "Epoch 14/25, Batch 2091/2194, Loss: 0.1159\n",
      "Epoch 14/25, Batch 2101/2194, Loss: 0.1968\n",
      "Epoch 14/25, Batch 2111/2194, Loss: 0.1888\n",
      "Epoch 14/25, Batch 2121/2194, Loss: 0.0724\n",
      "Epoch 14/25, Batch 2131/2194, Loss: 0.1162\n",
      "Epoch 14/25, Batch 2141/2194, Loss: 0.0737\n",
      "Epoch 14/25, Batch 2151/2194, Loss: 0.1109\n",
      "Epoch 14/25, Batch 2161/2194, Loss: 0.1002\n",
      "Epoch 14/25, Batch 2171/2194, Loss: 0.1260\n",
      "Epoch 14/25, Batch 2181/2194, Loss: 0.1912\n",
      "Epoch 14/25, Batch 2191/2194, Loss: 0.1723\n",
      "Epoch 14/25:\n",
      "Train Loss: 0.1285, Train Acc: 94.42%\n",
      "Val Loss: 0.1637, Val Acc: 93.01%\n",
      "------------------------------------------------------------\n",
      "Starting epoch 15/25...\n",
      "Starting training loop with 2194 batches...\n",
      "Successfully loaded first batch: images shape torch.Size([32, 3, 224, 224]), labels shape torch.Size([32])\n",
      "Epoch 15/25, Batch 1/2194, Loss: 0.1193\n",
      "Epoch 15/25, Batch 11/2194, Loss: 0.1274\n",
      "Epoch 15/25, Batch 21/2194, Loss: 0.2112\n",
      "Epoch 15/25, Batch 31/2194, Loss: 0.0883\n",
      "Epoch 15/25, Batch 41/2194, Loss: 0.0735\n",
      "Epoch 15/25, Batch 51/2194, Loss: 0.1270\n",
      "Epoch 15/25, Batch 61/2194, Loss: 0.1285\n",
      "Epoch 15/25, Batch 71/2194, Loss: 0.2694\n",
      "Epoch 15/25, Batch 81/2194, Loss: 0.2154\n",
      "Epoch 15/25, Batch 91/2194, Loss: 0.1436\n",
      "Epoch 15/25, Batch 101/2194, Loss: 0.1644\n",
      "Epoch 15/25, Batch 111/2194, Loss: 0.2082\n",
      "Epoch 15/25, Batch 121/2194, Loss: 0.0520\n",
      "Epoch 15/25, Batch 131/2194, Loss: 0.1296\n",
      "Epoch 15/25, Batch 141/2194, Loss: 0.0492\n",
      "Epoch 15/25, Batch 151/2194, Loss: 0.0280\n",
      "Epoch 15/25, Batch 161/2194, Loss: 0.2184\n",
      "Epoch 15/25, Batch 171/2194, Loss: 0.1058\n",
      "Epoch 15/25, Batch 181/2194, Loss: 0.1864\n",
      "Epoch 15/25, Batch 191/2194, Loss: 0.0464\n",
      "Epoch 15/25, Batch 201/2194, Loss: 0.1369\n",
      "Epoch 15/25, Batch 211/2194, Loss: 0.0439\n",
      "Epoch 15/25, Batch 221/2194, Loss: 0.0570\n",
      "Epoch 15/25, Batch 231/2194, Loss: 0.1642\n",
      "Epoch 15/25, Batch 241/2194, Loss: 0.1470\n",
      "Epoch 15/25, Batch 251/2194, Loss: 0.1373\n",
      "Epoch 15/25, Batch 261/2194, Loss: 0.1481\n",
      "Epoch 15/25, Batch 271/2194, Loss: 0.0690\n",
      "Epoch 15/25, Batch 281/2194, Loss: 0.2357\n",
      "Epoch 15/25, Batch 291/2194, Loss: 0.1036\n",
      "Epoch 15/25, Batch 301/2194, Loss: 0.0780\n",
      "Epoch 15/25, Batch 311/2194, Loss: 0.2118\n",
      "Epoch 15/25, Batch 321/2194, Loss: 0.1760\n",
      "Epoch 15/25, Batch 331/2194, Loss: 0.1489\n",
      "Epoch 15/25, Batch 341/2194, Loss: 0.1263\n",
      "Epoch 15/25, Batch 351/2194, Loss: 0.0622\n",
      "Epoch 15/25, Batch 361/2194, Loss: 0.1329\n",
      "Epoch 15/25, Batch 371/2194, Loss: 0.0541\n",
      "Epoch 15/25, Batch 381/2194, Loss: 0.1676\n",
      "Epoch 15/25, Batch 391/2194, Loss: 0.0394\n",
      "Epoch 15/25, Batch 401/2194, Loss: 0.0852\n",
      "Epoch 15/25, Batch 411/2194, Loss: 0.1430\n",
      "Epoch 15/25, Batch 421/2194, Loss: 0.0806\n",
      "Epoch 15/25, Batch 431/2194, Loss: 0.0735\n",
      "Epoch 15/25, Batch 441/2194, Loss: 0.2087\n",
      "Epoch 15/25, Batch 451/2194, Loss: 0.0698\n",
      "Epoch 15/25, Batch 461/2194, Loss: 0.0993\n",
      "Epoch 15/25, Batch 471/2194, Loss: 0.1654\n",
      "Epoch 15/25, Batch 481/2194, Loss: 0.0502\n",
      "Epoch 15/25, Batch 491/2194, Loss: 0.2066\n",
      "Epoch 15/25, Batch 501/2194, Loss: 0.0806\n",
      "Epoch 15/25, Batch 511/2194, Loss: 0.1224\n",
      "Epoch 15/25, Batch 521/2194, Loss: 0.1855\n",
      "Epoch 15/25, Batch 531/2194, Loss: 0.1758\n",
      "Epoch 15/25, Batch 541/2194, Loss: 0.0486\n",
      "Epoch 15/25, Batch 551/2194, Loss: 0.2891\n",
      "Epoch 15/25, Batch 561/2194, Loss: 0.0286\n",
      "Epoch 15/25, Batch 571/2194, Loss: 0.0825\n",
      "Epoch 15/25, Batch 581/2194, Loss: 0.0648\n",
      "Epoch 15/25, Batch 591/2194, Loss: 0.0608\n",
      "Epoch 15/25, Batch 601/2194, Loss: 0.0895\n",
      "Epoch 15/25, Batch 611/2194, Loss: 0.1316\n",
      "Epoch 15/25, Batch 621/2194, Loss: 0.0555\n",
      "Epoch 15/25, Batch 631/2194, Loss: 0.1782\n",
      "Epoch 15/25, Batch 641/2194, Loss: 0.0890\n",
      "Epoch 15/25, Batch 651/2194, Loss: 0.1202\n",
      "Epoch 15/25, Batch 661/2194, Loss: 0.1427\n",
      "Epoch 15/25, Batch 671/2194, Loss: 0.0943\n",
      "Epoch 15/25, Batch 681/2194, Loss: 0.1316\n",
      "Epoch 15/25, Batch 691/2194, Loss: 0.1742\n",
      "Epoch 15/25, Batch 701/2194, Loss: 0.2129\n",
      "Epoch 15/25, Batch 711/2194, Loss: 0.0966\n",
      "Epoch 15/25, Batch 721/2194, Loss: 0.1947\n",
      "Epoch 15/25, Batch 731/2194, Loss: 0.0975\n",
      "Epoch 15/25, Batch 741/2194, Loss: 0.0345\n",
      "Epoch 15/25, Batch 751/2194, Loss: 0.1679\n",
      "Epoch 15/25, Batch 761/2194, Loss: 0.0670\n",
      "Epoch 15/25, Batch 771/2194, Loss: 0.0937\n",
      "Epoch 15/25, Batch 781/2194, Loss: 0.0479\n",
      "Epoch 15/25, Batch 791/2194, Loss: 0.2174\n",
      "Epoch 15/25, Batch 801/2194, Loss: 0.0948\n",
      "Epoch 15/25, Batch 811/2194, Loss: 0.1843\n",
      "Epoch 15/25, Batch 821/2194, Loss: 0.0179\n",
      "Epoch 15/25, Batch 831/2194, Loss: 0.0645\n",
      "Epoch 15/25, Batch 841/2194, Loss: 0.1797\n",
      "Epoch 15/25, Batch 851/2194, Loss: 0.2501\n",
      "Epoch 15/25, Batch 861/2194, Loss: 0.0538\n",
      "Epoch 15/25, Batch 871/2194, Loss: 0.0303\n",
      "Epoch 15/25, Batch 881/2194, Loss: 0.0416\n",
      "Epoch 15/25, Batch 891/2194, Loss: 0.1496\n",
      "Epoch 15/25, Batch 901/2194, Loss: 0.0892\n",
      "Epoch 15/25, Batch 911/2194, Loss: 0.2406\n",
      "Epoch 15/25, Batch 921/2194, Loss: 0.4311\n",
      "Epoch 15/25, Batch 931/2194, Loss: 0.0650\n",
      "Epoch 15/25, Batch 941/2194, Loss: 0.0696\n",
      "Epoch 15/25, Batch 951/2194, Loss: 0.2010\n",
      "Epoch 15/25, Batch 961/2194, Loss: 0.0761\n",
      "Epoch 15/25, Batch 971/2194, Loss: 0.0446\n",
      "Epoch 15/25, Batch 981/2194, Loss: 0.0911\n",
      "Epoch 15/25, Batch 991/2194, Loss: 0.1807\n",
      "Epoch 15/25, Batch 1001/2194, Loss: 0.1907\n",
      "Epoch 15/25, Batch 1011/2194, Loss: 0.1734\n",
      "Epoch 15/25, Batch 1021/2194, Loss: 0.1374\n",
      "Epoch 15/25, Batch 1031/2194, Loss: 0.1713\n",
      "Epoch 15/25, Batch 1041/2194, Loss: 0.1866\n",
      "Epoch 15/25, Batch 1051/2194, Loss: 0.0538\n",
      "Epoch 15/25, Batch 1061/2194, Loss: 0.1297\n",
      "Epoch 15/25, Batch 1071/2194, Loss: 0.1522\n",
      "Epoch 15/25, Batch 1081/2194, Loss: 0.0979\n",
      "Epoch 15/25, Batch 1091/2194, Loss: 0.0880\n",
      "Epoch 15/25, Batch 1101/2194, Loss: 0.0254\n",
      "Epoch 15/25, Batch 1111/2194, Loss: 0.0795\n",
      "Epoch 15/25, Batch 1121/2194, Loss: 0.1809\n",
      "Epoch 15/25, Batch 1131/2194, Loss: 0.0825\n",
      "Epoch 15/25, Batch 1141/2194, Loss: 0.1129\n",
      "Epoch 15/25, Batch 1151/2194, Loss: 0.0256\n",
      "Epoch 15/25, Batch 1161/2194, Loss: 0.1800\n",
      "Epoch 15/25, Batch 1171/2194, Loss: 0.0916\n",
      "Epoch 15/25, Batch 1181/2194, Loss: 0.1186\n",
      "Epoch 15/25, Batch 1191/2194, Loss: 0.1526\n",
      "Epoch 15/25, Batch 1201/2194, Loss: 0.0515\n",
      "Epoch 15/25, Batch 1211/2194, Loss: 0.1433\n",
      "Epoch 15/25, Batch 1221/2194, Loss: 0.1271\n",
      "Epoch 15/25, Batch 1231/2194, Loss: 0.0686\n",
      "Epoch 15/25, Batch 1241/2194, Loss: 0.1329\n",
      "Epoch 15/25, Batch 1251/2194, Loss: 0.0384\n",
      "Epoch 15/25, Batch 1261/2194, Loss: 0.0450\n",
      "Epoch 15/25, Batch 1271/2194, Loss: 0.2939\n",
      "Epoch 15/25, Batch 1281/2194, Loss: 0.1686\n",
      "Epoch 15/25, Batch 1291/2194, Loss: 0.1070\n",
      "Epoch 15/25, Batch 1301/2194, Loss: 0.3140\n",
      "Epoch 15/25, Batch 1311/2194, Loss: 0.1745\n",
      "Epoch 15/25, Batch 1321/2194, Loss: 0.1156\n",
      "Epoch 15/25, Batch 1331/2194, Loss: 0.1960\n",
      "Epoch 15/25, Batch 1341/2194, Loss: 0.2511\n",
      "Epoch 15/25, Batch 1351/2194, Loss: 0.0500\n",
      "Epoch 15/25, Batch 1361/2194, Loss: 0.1023\n",
      "Epoch 15/25, Batch 1371/2194, Loss: 0.1984\n",
      "Epoch 15/25, Batch 1381/2194, Loss: 0.1810\n",
      "Epoch 15/25, Batch 1391/2194, Loss: 0.1092\n",
      "Epoch 15/25, Batch 1401/2194, Loss: 0.1965\n",
      "Epoch 15/25, Batch 1411/2194, Loss: 0.1397\n",
      "Epoch 15/25, Batch 1421/2194, Loss: 0.1104\n",
      "Epoch 15/25, Batch 1431/2194, Loss: 0.0603\n",
      "Epoch 15/25, Batch 1441/2194, Loss: 0.1273\n",
      "Epoch 15/25, Batch 1451/2194, Loss: 0.1170\n",
      "Epoch 15/25, Batch 1461/2194, Loss: 0.1469\n",
      "Epoch 15/25, Batch 1471/2194, Loss: 0.1826\n",
      "Epoch 15/25, Batch 1481/2194, Loss: 0.2556\n",
      "Epoch 15/25, Batch 1491/2194, Loss: 0.1573\n",
      "Epoch 15/25, Batch 1501/2194, Loss: 0.1759\n",
      "Epoch 15/25, Batch 1511/2194, Loss: 0.1604\n",
      "Epoch 15/25, Batch 1521/2194, Loss: 0.1728\n",
      "Epoch 15/25, Batch 1531/2194, Loss: 0.1190\n",
      "Epoch 15/25, Batch 1541/2194, Loss: 0.1674\n",
      "Epoch 15/25, Batch 1551/2194, Loss: 0.1554\n",
      "Epoch 15/25, Batch 1561/2194, Loss: 0.2571\n",
      "Epoch 15/25, Batch 1571/2194, Loss: 0.1207\n",
      "Epoch 15/25, Batch 1581/2194, Loss: 0.1583\n",
      "Epoch 15/25, Batch 1591/2194, Loss: 0.1096\n",
      "Epoch 15/25, Batch 1601/2194, Loss: 0.0848\n",
      "Epoch 15/25, Batch 1611/2194, Loss: 0.0705\n",
      "Epoch 15/25, Batch 1621/2194, Loss: 0.3531\n",
      "Epoch 15/25, Batch 1631/2194, Loss: 0.2041\n",
      "Epoch 15/25, Batch 1641/2194, Loss: 0.1585\n",
      "Epoch 15/25, Batch 1651/2194, Loss: 0.0535\n",
      "Epoch 15/25, Batch 1661/2194, Loss: 0.0249\n",
      "Epoch 15/25, Batch 1671/2194, Loss: 0.1718\n",
      "Epoch 15/25, Batch 1681/2194, Loss: 0.0489\n",
      "Epoch 15/25, Batch 1691/2194, Loss: 0.0673\n",
      "Epoch 15/25, Batch 1701/2194, Loss: 0.1216\n",
      "Epoch 15/25, Batch 1711/2194, Loss: 0.1272\n",
      "Epoch 15/25, Batch 1721/2194, Loss: 0.0603\n",
      "Epoch 15/25, Batch 1731/2194, Loss: 0.2089\n",
      "Epoch 15/25, Batch 1741/2194, Loss: 0.0775\n",
      "Epoch 15/25, Batch 1751/2194, Loss: 0.2814\n",
      "Epoch 15/25, Batch 1761/2194, Loss: 0.0671\n",
      "Epoch 15/25, Batch 1771/2194, Loss: 0.1106\n",
      "Epoch 15/25, Batch 1781/2194, Loss: 0.0206\n",
      "Epoch 15/25, Batch 1791/2194, Loss: 0.0454\n",
      "Epoch 15/25, Batch 1801/2194, Loss: 0.0276\n",
      "Epoch 15/25, Batch 1811/2194, Loss: 0.0742\n",
      "Epoch 15/25, Batch 1821/2194, Loss: 0.0174\n",
      "Epoch 15/25, Batch 1831/2194, Loss: 0.0569\n",
      "Epoch 15/25, Batch 1841/2194, Loss: 0.2929\n",
      "Epoch 15/25, Batch 1851/2194, Loss: 0.1152\n",
      "Epoch 15/25, Batch 1861/2194, Loss: 0.0696\n",
      "Epoch 15/25, Batch 1871/2194, Loss: 0.1491\n",
      "Epoch 15/25, Batch 1881/2194, Loss: 0.1406\n",
      "Epoch 15/25, Batch 1891/2194, Loss: 0.2094\n",
      "Epoch 15/25, Batch 1901/2194, Loss: 0.1903\n",
      "Epoch 15/25, Batch 1911/2194, Loss: 0.1474\n",
      "Epoch 15/25, Batch 1921/2194, Loss: 0.0782\n",
      "Epoch 15/25, Batch 1931/2194, Loss: 0.1250\n",
      "Epoch 15/25, Batch 1941/2194, Loss: 0.1011\n",
      "Epoch 15/25, Batch 1951/2194, Loss: 0.1070\n",
      "Epoch 15/25, Batch 1961/2194, Loss: 0.1371\n",
      "Epoch 15/25, Batch 1971/2194, Loss: 0.1180\n",
      "Epoch 15/25, Batch 1981/2194, Loss: 0.2358\n",
      "Epoch 15/25, Batch 1991/2194, Loss: 0.0667\n",
      "Epoch 15/25, Batch 2001/2194, Loss: 0.0656\n",
      "Epoch 15/25, Batch 2011/2194, Loss: 0.0245\n",
      "Epoch 15/25, Batch 2021/2194, Loss: 0.0676\n",
      "Epoch 15/25, Batch 2031/2194, Loss: 0.0993\n",
      "Epoch 15/25, Batch 2041/2194, Loss: 0.0901\n",
      "Epoch 15/25, Batch 2051/2194, Loss: 0.1176\n",
      "Epoch 15/25, Batch 2061/2194, Loss: 0.2612\n",
      "Epoch 15/25, Batch 2071/2194, Loss: 0.1371\n",
      "Epoch 15/25, Batch 2081/2194, Loss: 0.1103\n",
      "Epoch 15/25, Batch 2091/2194, Loss: 0.0847\n",
      "Epoch 15/25, Batch 2101/2194, Loss: 0.0148\n",
      "Epoch 15/25, Batch 2111/2194, Loss: 0.1810\n",
      "Epoch 15/25, Batch 2121/2194, Loss: 0.1517\n",
      "Epoch 15/25, Batch 2131/2194, Loss: 0.1157\n",
      "Epoch 15/25, Batch 2141/2194, Loss: 0.1353\n",
      "Epoch 15/25, Batch 2151/2194, Loss: 0.2151\n",
      "Epoch 15/25, Batch 2161/2194, Loss: 0.1638\n",
      "Epoch 15/25, Batch 2171/2194, Loss: 0.1084\n",
      "Epoch 15/25, Batch 2181/2194, Loss: 0.0643\n",
      "Epoch 15/25, Batch 2191/2194, Loss: 0.1540\n",
      "Epoch 15/25:\n",
      "Train Loss: 0.1248, Train Acc: 94.63%\n",
      "Val Loss: 0.1505, Val Acc: 93.74%\n",
      "------------------------------------------------------------\n",
      "Starting epoch 16/25...\n",
      "Starting training loop with 2194 batches...\n",
      "Successfully loaded first batch: images shape torch.Size([32, 3, 224, 224]), labels shape torch.Size([32])\n",
      "Epoch 16/25, Batch 1/2194, Loss: 0.0504\n",
      "Epoch 16/25, Batch 11/2194, Loss: 0.1114\n",
      "Epoch 16/25, Batch 21/2194, Loss: 0.1803\n",
      "Epoch 16/25, Batch 31/2194, Loss: 0.0518\n",
      "Epoch 16/25, Batch 41/2194, Loss: 0.0869\n",
      "Epoch 16/25, Batch 51/2194, Loss: 0.0282\n",
      "Epoch 16/25, Batch 61/2194, Loss: 0.0591\n",
      "Epoch 16/25, Batch 71/2194, Loss: 0.0930\n",
      "Epoch 16/25, Batch 81/2194, Loss: 0.0998\n",
      "Epoch 16/25, Batch 91/2194, Loss: 0.1037\n",
      "Epoch 16/25, Batch 101/2194, Loss: 0.1914\n",
      "Epoch 16/25, Batch 111/2194, Loss: 0.0271\n",
      "Epoch 16/25, Batch 121/2194, Loss: 0.0966\n",
      "Epoch 16/25, Batch 131/2194, Loss: 0.1513\n",
      "Epoch 16/25, Batch 141/2194, Loss: 0.0702\n",
      "Epoch 16/25, Batch 151/2194, Loss: 0.0516\n",
      "Epoch 16/25, Batch 161/2194, Loss: 0.2026\n",
      "Epoch 16/25, Batch 171/2194, Loss: 0.2672\n",
      "Epoch 16/25, Batch 181/2194, Loss: 0.0438\n",
      "Epoch 16/25, Batch 191/2194, Loss: 0.1755\n",
      "Epoch 16/25, Batch 201/2194, Loss: 0.0439\n",
      "Epoch 16/25, Batch 211/2194, Loss: 0.0428\n",
      "Epoch 16/25, Batch 221/2194, Loss: 0.2844\n",
      "Epoch 16/25, Batch 231/2194, Loss: 0.1088\n",
      "Epoch 16/25, Batch 241/2194, Loss: 0.0264\n",
      "Epoch 16/25, Batch 251/2194, Loss: 0.2421\n",
      "Epoch 16/25, Batch 261/2194, Loss: 0.0620\n",
      "Epoch 16/25, Batch 271/2194, Loss: 0.1527\n",
      "Epoch 16/25, Batch 281/2194, Loss: 0.0471\n",
      "Epoch 16/25, Batch 291/2194, Loss: 0.1026\n",
      "Epoch 16/25, Batch 301/2194, Loss: 0.1596\n",
      "Epoch 16/25, Batch 311/2194, Loss: 0.1173\n",
      "Epoch 16/25, Batch 321/2194, Loss: 0.0805\n",
      "Epoch 16/25, Batch 331/2194, Loss: 0.1226\n",
      "Epoch 16/25, Batch 341/2194, Loss: 0.0495\n",
      "Epoch 16/25, Batch 351/2194, Loss: 0.0928\n",
      "Epoch 16/25, Batch 361/2194, Loss: 0.1919\n",
      "Epoch 16/25, Batch 371/2194, Loss: 0.1414\n",
      "Epoch 16/25, Batch 381/2194, Loss: 0.1583\n",
      "Epoch 16/25, Batch 391/2194, Loss: 0.1480\n",
      "Epoch 16/25, Batch 401/2194, Loss: 0.0872\n",
      "Epoch 16/25, Batch 411/2194, Loss: 0.1263\n",
      "Epoch 16/25, Batch 421/2194, Loss: 0.1265\n",
      "Epoch 16/25, Batch 431/2194, Loss: 0.0936\n",
      "Epoch 16/25, Batch 441/2194, Loss: 0.1232\n",
      "Epoch 16/25, Batch 451/2194, Loss: 0.1893\n",
      "Epoch 16/25, Batch 461/2194, Loss: 0.0755\n",
      "Epoch 16/25, Batch 471/2194, Loss: 0.2930\n",
      "Epoch 16/25, Batch 481/2194, Loss: 0.1283\n",
      "Epoch 16/25, Batch 491/2194, Loss: 0.1128\n",
      "Epoch 16/25, Batch 501/2194, Loss: 0.1742\n",
      "Epoch 16/25, Batch 511/2194, Loss: 0.1334\n",
      "Epoch 16/25, Batch 521/2194, Loss: 0.0851\n",
      "Epoch 16/25, Batch 531/2194, Loss: 0.0482\n",
      "Epoch 16/25, Batch 541/2194, Loss: 0.1638\n",
      "Epoch 16/25, Batch 551/2194, Loss: 0.0244\n",
      "Epoch 16/25, Batch 561/2194, Loss: 0.0858\n",
      "Epoch 16/25, Batch 571/2194, Loss: 0.1166\n",
      "Epoch 16/25, Batch 581/2194, Loss: 0.0214\n",
      "Epoch 16/25, Batch 591/2194, Loss: 0.0882\n",
      "Epoch 16/25, Batch 601/2194, Loss: 0.2602\n",
      "Epoch 16/25, Batch 611/2194, Loss: 0.0514\n",
      "Epoch 16/25, Batch 621/2194, Loss: 0.0607\n",
      "Epoch 16/25, Batch 631/2194, Loss: 0.1362\n",
      "Epoch 16/25, Batch 641/2194, Loss: 0.0951\n",
      "Epoch 16/25, Batch 651/2194, Loss: 0.0101\n",
      "Epoch 16/25, Batch 661/2194, Loss: 0.0790\n",
      "Epoch 16/25, Batch 671/2194, Loss: 0.2248\n",
      "Epoch 16/25, Batch 681/2194, Loss: 0.1442\n",
      "Epoch 16/25, Batch 691/2194, Loss: 0.2925\n",
      "Epoch 16/25, Batch 701/2194, Loss: 0.1298\n",
      "Epoch 16/25, Batch 711/2194, Loss: 0.0430\n",
      "Epoch 16/25, Batch 721/2194, Loss: 0.0589\n",
      "Epoch 16/25, Batch 731/2194, Loss: 0.2501\n",
      "Epoch 16/25, Batch 741/2194, Loss: 0.0908\n",
      "Epoch 16/25, Batch 751/2194, Loss: 0.0651\n",
      "Epoch 16/25, Batch 761/2194, Loss: 0.0748\n",
      "Epoch 16/25, Batch 771/2194, Loss: 0.2319\n",
      "Epoch 16/25, Batch 781/2194, Loss: 0.1144\n",
      "Epoch 16/25, Batch 791/2194, Loss: 0.1081\n",
      "Epoch 16/25, Batch 801/2194, Loss: 0.1681\n",
      "Epoch 16/25, Batch 811/2194, Loss: 0.0711\n",
      "Epoch 16/25, Batch 821/2194, Loss: 0.0807\n",
      "Epoch 16/25, Batch 831/2194, Loss: 0.2212\n",
      "Epoch 16/25, Batch 841/2194, Loss: 0.1248\n",
      "Epoch 16/25, Batch 851/2194, Loss: 0.0642\n",
      "Epoch 16/25, Batch 861/2194, Loss: 0.0812\n",
      "Epoch 16/25, Batch 871/2194, Loss: 0.0766\n",
      "Epoch 16/25, Batch 881/2194, Loss: 0.1204\n",
      "Epoch 16/25, Batch 891/2194, Loss: 0.1461\n",
      "Epoch 16/25, Batch 901/2194, Loss: 0.1259\n",
      "Epoch 16/25, Batch 911/2194, Loss: 0.0433\n",
      "Epoch 16/25, Batch 921/2194, Loss: 0.1364\n",
      "Epoch 16/25, Batch 931/2194, Loss: 0.0544\n",
      "Epoch 16/25, Batch 941/2194, Loss: 0.2781\n",
      "Epoch 16/25, Batch 951/2194, Loss: 0.0967\n",
      "Epoch 16/25, Batch 961/2194, Loss: 0.0782\n",
      "Epoch 16/25, Batch 971/2194, Loss: 0.1456\n",
      "Epoch 16/25, Batch 981/2194, Loss: 0.0290\n",
      "Epoch 16/25, Batch 991/2194, Loss: 0.0968\n",
      "Epoch 16/25, Batch 1001/2194, Loss: 0.0552\n",
      "Epoch 16/25, Batch 1011/2194, Loss: 0.0714\n",
      "Epoch 16/25, Batch 1021/2194, Loss: 0.2705\n",
      "Epoch 16/25, Batch 1031/2194, Loss: 0.0840\n",
      "Epoch 16/25, Batch 1041/2194, Loss: 0.1510\n",
      "Epoch 16/25, Batch 1051/2194, Loss: 0.1685\n",
      "Epoch 16/25, Batch 1061/2194, Loss: 0.0120\n",
      "Epoch 16/25, Batch 1071/2194, Loss: 0.0271\n",
      "Epoch 16/25, Batch 1081/2194, Loss: 0.1244\n",
      "Epoch 16/25, Batch 1091/2194, Loss: 0.1282\n",
      "Epoch 16/25, Batch 1101/2194, Loss: 0.0563\n",
      "Epoch 16/25, Batch 1111/2194, Loss: 0.0900\n",
      "Epoch 16/25, Batch 1121/2194, Loss: 0.1307\n",
      "Epoch 16/25, Batch 1131/2194, Loss: 0.1528\n",
      "Epoch 16/25, Batch 1141/2194, Loss: 0.0715\n",
      "Epoch 16/25, Batch 1151/2194, Loss: 0.2271\n",
      "Epoch 16/25, Batch 1161/2194, Loss: 0.0959\n",
      "Epoch 16/25, Batch 1171/2194, Loss: 0.2824\n",
      "Epoch 16/25, Batch 1181/2194, Loss: 0.2607\n",
      "Epoch 16/25, Batch 1191/2194, Loss: 0.1164\n",
      "Epoch 16/25, Batch 1201/2194, Loss: 0.1494\n",
      "Epoch 16/25, Batch 1211/2194, Loss: 0.1957\n",
      "Epoch 16/25, Batch 1221/2194, Loss: 0.1055\n",
      "Epoch 16/25, Batch 1231/2194, Loss: 0.1719\n",
      "Epoch 16/25, Batch 1241/2194, Loss: 0.1642\n",
      "Epoch 16/25, Batch 1251/2194, Loss: 0.0987\n",
      "Epoch 16/25, Batch 1261/2194, Loss: 0.2158\n",
      "Epoch 16/25, Batch 1271/2194, Loss: 0.0836\n",
      "Epoch 16/25, Batch 1281/2194, Loss: 0.1089\n",
      "Epoch 16/25, Batch 1291/2194, Loss: 0.0192\n",
      "Epoch 16/25, Batch 1301/2194, Loss: 0.0295\n",
      "Epoch 16/25, Batch 1311/2194, Loss: 0.1075\n",
      "Epoch 16/25, Batch 1321/2194, Loss: 0.0372\n",
      "Epoch 16/25, Batch 1331/2194, Loss: 0.0636\n",
      "Epoch 16/25, Batch 1341/2194, Loss: 0.0924\n",
      "Epoch 16/25, Batch 1351/2194, Loss: 0.0752\n",
      "Epoch 16/25, Batch 1361/2194, Loss: 0.0509\n",
      "Epoch 16/25, Batch 1371/2194, Loss: 0.0934\n",
      "Epoch 16/25, Batch 1381/2194, Loss: 0.0410\n",
      "Epoch 16/25, Batch 1391/2194, Loss: 0.0839\n",
      "Epoch 16/25, Batch 1401/2194, Loss: 0.3396\n",
      "Epoch 16/25, Batch 1411/2194, Loss: 0.1237\n",
      "Epoch 16/25, Batch 1421/2194, Loss: 0.0606\n",
      "Epoch 16/25, Batch 1431/2194, Loss: 0.1368\n",
      "Epoch 16/25, Batch 1441/2194, Loss: 0.0132\n",
      "Epoch 16/25, Batch 1451/2194, Loss: 0.1531\n",
      "Epoch 16/25, Batch 1461/2194, Loss: 0.1795\n",
      "Epoch 16/25, Batch 1471/2194, Loss: 0.1593\n",
      "Epoch 16/25, Batch 1481/2194, Loss: 0.2226\n",
      "Epoch 16/25, Batch 1491/2194, Loss: 0.1805\n",
      "Epoch 16/25, Batch 1501/2194, Loss: 0.1541\n",
      "Epoch 16/25, Batch 1511/2194, Loss: 0.1923\n",
      "Epoch 16/25, Batch 1521/2194, Loss: 0.1036\n",
      "Epoch 16/25, Batch 1531/2194, Loss: 0.2364\n",
      "Epoch 16/25, Batch 1541/2194, Loss: 0.0624\n",
      "Epoch 16/25, Batch 1551/2194, Loss: 0.0501\n",
      "Epoch 16/25, Batch 1561/2194, Loss: 0.0640\n",
      "Epoch 16/25, Batch 1571/2194, Loss: 0.0837\n",
      "Epoch 16/25, Batch 1581/2194, Loss: 0.0438\n",
      "Epoch 16/25, Batch 1591/2194, Loss: 0.0891\n",
      "Epoch 16/25, Batch 1601/2194, Loss: 0.0295\n",
      "Epoch 16/25, Batch 1611/2194, Loss: 0.0447\n",
      "Epoch 16/25, Batch 1621/2194, Loss: 0.0290\n",
      "Epoch 16/25, Batch 1631/2194, Loss: 0.0065\n",
      "Epoch 16/25, Batch 1641/2194, Loss: 0.0325\n",
      "Epoch 16/25, Batch 1651/2194, Loss: 0.1104\n",
      "Epoch 16/25, Batch 1661/2194, Loss: 0.2298\n",
      "Epoch 16/25, Batch 1671/2194, Loss: 0.0410\n",
      "Epoch 16/25, Batch 1681/2194, Loss: 0.0741\n",
      "Epoch 16/25, Batch 1691/2194, Loss: 0.0579\n",
      "Epoch 16/25, Batch 1701/2194, Loss: 0.0647\n",
      "Epoch 16/25, Batch 1711/2194, Loss: 0.0392\n",
      "Epoch 16/25, Batch 1721/2194, Loss: 0.0887\n",
      "Epoch 16/25, Batch 1731/2194, Loss: 0.2057\n",
      "Epoch 16/25, Batch 1741/2194, Loss: 0.1127\n",
      "Epoch 16/25, Batch 1751/2194, Loss: 0.1735\n",
      "Epoch 16/25, Batch 1761/2194, Loss: 0.1146\n",
      "Epoch 16/25, Batch 1771/2194, Loss: 0.0716\n",
      "Epoch 16/25, Batch 1781/2194, Loss: 0.2214\n",
      "Epoch 16/25, Batch 1791/2194, Loss: 0.1668\n",
      "Epoch 16/25, Batch 1801/2194, Loss: 0.0424\n",
      "Epoch 16/25, Batch 1811/2194, Loss: 0.1014\n",
      "Epoch 16/25, Batch 1821/2194, Loss: 0.0913\n",
      "Epoch 16/25, Batch 1831/2194, Loss: 0.2091\n",
      "Epoch 16/25, Batch 1841/2194, Loss: 0.2240\n",
      "Epoch 16/25, Batch 1851/2194, Loss: 0.0879\n",
      "Epoch 16/25, Batch 1861/2194, Loss: 0.1859\n",
      "Epoch 16/25, Batch 1871/2194, Loss: 0.2636\n",
      "Epoch 16/25, Batch 1881/2194, Loss: 0.0900\n",
      "Epoch 16/25, Batch 1891/2194, Loss: 0.2016\n",
      "Epoch 16/25, Batch 1901/2194, Loss: 0.1585\n",
      "Epoch 16/25, Batch 1911/2194, Loss: 0.0511\n",
      "Epoch 16/25, Batch 1921/2194, Loss: 0.2298\n",
      "Epoch 16/25, Batch 1931/2194, Loss: 0.2119\n",
      "Epoch 16/25, Batch 1941/2194, Loss: 0.0697\n",
      "Epoch 16/25, Batch 1951/2194, Loss: 0.3162\n",
      "Epoch 16/25, Batch 1961/2194, Loss: 0.0866\n",
      "Epoch 16/25, Batch 1971/2194, Loss: 0.0999\n",
      "Epoch 16/25, Batch 1981/2194, Loss: 0.1159\n",
      "Epoch 16/25, Batch 1991/2194, Loss: 0.0532\n",
      "Epoch 16/25, Batch 2001/2194, Loss: 0.0975\n",
      "Epoch 16/25, Batch 2011/2194, Loss: 0.0290\n",
      "Epoch 16/25, Batch 2021/2194, Loss: 0.0811\n",
      "Epoch 16/25, Batch 2031/2194, Loss: 0.1242\n",
      "Epoch 16/25, Batch 2041/2194, Loss: 0.0991\n",
      "Epoch 16/25, Batch 2051/2194, Loss: 0.1452\n",
      "Epoch 16/25, Batch 2061/2194, Loss: 0.1138\n",
      "Epoch 16/25, Batch 2071/2194, Loss: 0.0355\n",
      "Epoch 16/25, Batch 2081/2194, Loss: 0.0365\n",
      "Epoch 16/25, Batch 2091/2194, Loss: 0.0775\n",
      "Epoch 16/25, Batch 2101/2194, Loss: 0.1438\n",
      "Epoch 16/25, Batch 2111/2194, Loss: 0.1017\n",
      "Epoch 16/25, Batch 2121/2194, Loss: 0.1147\n",
      "Epoch 16/25, Batch 2131/2194, Loss: 0.0714\n",
      "Epoch 16/25, Batch 2141/2194, Loss: 0.1123\n",
      "Epoch 16/25, Batch 2151/2194, Loss: 0.0195\n",
      "Epoch 16/25, Batch 2161/2194, Loss: 0.0558\n",
      "Epoch 16/25, Batch 2171/2194, Loss: 0.2295\n",
      "Epoch 16/25, Batch 2181/2194, Loss: 0.0619\n",
      "Epoch 16/25, Batch 2191/2194, Loss: 0.0263\n",
      "Epoch 16/25:\n",
      "Train Loss: 0.1175, Train Acc: 94.86%\n",
      "Val Loss: 0.1467, Val Acc: 93.64%\n",
      "------------------------------------------------------------\n",
      "Starting epoch 17/25...\n",
      "Starting training loop with 2194 batches...\n",
      "Successfully loaded first batch: images shape torch.Size([32, 3, 224, 224]), labels shape torch.Size([32])\n",
      "Epoch 17/25, Batch 1/2194, Loss: 0.0499\n",
      "Epoch 17/25, Batch 11/2194, Loss: 0.1443\n",
      "Epoch 17/25, Batch 21/2194, Loss: 0.1076\n",
      "Epoch 17/25, Batch 31/2194, Loss: 0.0853\n",
      "Epoch 17/25, Batch 41/2194, Loss: 0.1344\n",
      "Epoch 17/25, Batch 51/2194, Loss: 0.0624\n",
      "Epoch 17/25, Batch 61/2194, Loss: 0.0925\n",
      "Epoch 17/25, Batch 71/2194, Loss: 0.0867\n",
      "Epoch 17/25, Batch 81/2194, Loss: 0.0272\n",
      "Epoch 17/25, Batch 91/2194, Loss: 0.0113\n",
      "Epoch 17/25, Batch 101/2194, Loss: 0.0727\n",
      "Epoch 17/25, Batch 111/2194, Loss: 0.0158\n",
      "Epoch 17/25, Batch 121/2194, Loss: 0.0842\n",
      "Epoch 17/25, Batch 131/2194, Loss: 0.1158\n",
      "Epoch 17/25, Batch 141/2194, Loss: 0.1824\n",
      "Epoch 17/25, Batch 151/2194, Loss: 0.0636\n",
      "Epoch 17/25, Batch 161/2194, Loss: 0.0822\n",
      "Epoch 17/25, Batch 171/2194, Loss: 0.0664\n",
      "Epoch 17/25, Batch 181/2194, Loss: 0.1913\n",
      "Epoch 17/25, Batch 191/2194, Loss: 0.0864\n",
      "Epoch 17/25, Batch 201/2194, Loss: 0.1548\n",
      "Epoch 17/25, Batch 211/2194, Loss: 0.1614\n",
      "Epoch 17/25, Batch 221/2194, Loss: 0.1190\n",
      "Epoch 17/25, Batch 231/2194, Loss: 0.0902\n",
      "Epoch 17/25, Batch 241/2194, Loss: 0.0112\n",
      "Epoch 17/25, Batch 251/2194, Loss: 0.0584\n",
      "Epoch 17/25, Batch 261/2194, Loss: 0.0336\n",
      "Epoch 17/25, Batch 271/2194, Loss: 0.0614\n",
      "Epoch 17/25, Batch 281/2194, Loss: 0.1739\n",
      "Epoch 17/25, Batch 291/2194, Loss: 0.1764\n",
      "Epoch 17/25, Batch 301/2194, Loss: 0.2060\n",
      "Epoch 17/25, Batch 311/2194, Loss: 0.1248\n",
      "Epoch 17/25, Batch 321/2194, Loss: 0.0611\n",
      "Epoch 17/25, Batch 331/2194, Loss: 0.0253\n",
      "Epoch 17/25, Batch 341/2194, Loss: 0.0922\n",
      "Epoch 17/25, Batch 351/2194, Loss: 0.0640\n",
      "Epoch 17/25, Batch 361/2194, Loss: 0.0731\n",
      "Epoch 17/25, Batch 371/2194, Loss: 0.2280\n",
      "Epoch 17/25, Batch 381/2194, Loss: 0.1010\n",
      "Epoch 17/25, Batch 391/2194, Loss: 0.1700\n",
      "Epoch 17/25, Batch 401/2194, Loss: 0.2024\n",
      "Epoch 17/25, Batch 411/2194, Loss: 0.1171\n",
      "Epoch 17/25, Batch 421/2194, Loss: 0.1045\n",
      "Epoch 17/25, Batch 431/2194, Loss: 0.1556\n",
      "Epoch 17/25, Batch 441/2194, Loss: 0.0338\n",
      "Epoch 17/25, Batch 451/2194, Loss: 0.0527\n",
      "Epoch 17/25, Batch 461/2194, Loss: 0.2042\n",
      "Epoch 17/25, Batch 471/2194, Loss: 0.2979\n",
      "Epoch 17/25, Batch 481/2194, Loss: 0.1337\n",
      "Epoch 17/25, Batch 491/2194, Loss: 0.2547\n",
      "Epoch 17/25, Batch 501/2194, Loss: 0.0476\n",
      "Epoch 17/25, Batch 511/2194, Loss: 0.1262\n",
      "Epoch 17/25, Batch 521/2194, Loss: 0.0923\n",
      "Epoch 17/25, Batch 531/2194, Loss: 0.1036\n",
      "Epoch 17/25, Batch 541/2194, Loss: 0.0832\n",
      "Epoch 17/25, Batch 551/2194, Loss: 0.2074\n",
      "Epoch 17/25, Batch 561/2194, Loss: 0.0714\n",
      "Epoch 17/25, Batch 571/2194, Loss: 0.0219\n",
      "Epoch 17/25, Batch 581/2194, Loss: 0.1498\n",
      "Epoch 17/25, Batch 591/2194, Loss: 0.0295\n",
      "Epoch 17/25, Batch 601/2194, Loss: 0.1487\n",
      "Epoch 17/25, Batch 611/2194, Loss: 0.0267\n",
      "Epoch 17/25, Batch 621/2194, Loss: 0.0076\n",
      "Epoch 17/25, Batch 631/2194, Loss: 0.3404\n",
      "Epoch 17/25, Batch 641/2194, Loss: 0.0783\n",
      "Epoch 17/25, Batch 651/2194, Loss: 0.0623\n",
      "Epoch 17/25, Batch 661/2194, Loss: 0.1325\n",
      "Epoch 17/25, Batch 671/2194, Loss: 0.1296\n",
      "Epoch 17/25, Batch 681/2194, Loss: 0.1219\n",
      "Epoch 17/25, Batch 691/2194, Loss: 0.0904\n",
      "Epoch 17/25, Batch 701/2194, Loss: 0.1472\n",
      "Epoch 17/25, Batch 711/2194, Loss: 0.0759\n",
      "Epoch 17/25, Batch 721/2194, Loss: 0.0435\n",
      "Epoch 17/25, Batch 731/2194, Loss: 0.1510\n",
      "Epoch 17/25, Batch 741/2194, Loss: 0.1012\n",
      "Epoch 17/25, Batch 751/2194, Loss: 0.1122\n",
      "Epoch 17/25, Batch 761/2194, Loss: 0.0333\n",
      "Epoch 17/25, Batch 771/2194, Loss: 0.0714\n",
      "Epoch 17/25, Batch 781/2194, Loss: 0.0871\n",
      "Epoch 17/25, Batch 791/2194, Loss: 0.0353\n",
      "Epoch 17/25, Batch 801/2194, Loss: 0.1168\n",
      "Epoch 17/25, Batch 811/2194, Loss: 0.0617\n",
      "Epoch 17/25, Batch 821/2194, Loss: 0.0520\n",
      "Epoch 17/25, Batch 831/2194, Loss: 0.0915\n",
      "Epoch 17/25, Batch 841/2194, Loss: 0.1096\n",
      "Epoch 17/25, Batch 851/2194, Loss: 0.0941\n",
      "Epoch 17/25, Batch 861/2194, Loss: 0.0676\n",
      "Epoch 17/25, Batch 871/2194, Loss: 0.2006\n",
      "Epoch 17/25, Batch 881/2194, Loss: 0.0143\n",
      "Epoch 17/25, Batch 891/2194, Loss: 0.2426\n",
      "Epoch 17/25, Batch 901/2194, Loss: 0.0721\n",
      "Epoch 17/25, Batch 911/2194, Loss: 0.1793\n",
      "Epoch 17/25, Batch 921/2194, Loss: 0.0918\n",
      "Epoch 17/25, Batch 931/2194, Loss: 0.2929\n",
      "Epoch 17/25, Batch 941/2194, Loss: 0.0807\n",
      "Epoch 17/25, Batch 951/2194, Loss: 0.0405\n",
      "Epoch 17/25, Batch 961/2194, Loss: 0.1702\n",
      "Epoch 17/25, Batch 971/2194, Loss: 0.0913\n",
      "Epoch 17/25, Batch 981/2194, Loss: 0.0593\n",
      "Epoch 17/25, Batch 991/2194, Loss: 0.1105\n",
      "Epoch 17/25, Batch 1001/2194, Loss: 0.1062\n",
      "Epoch 17/25, Batch 1011/2194, Loss: 0.0902\n",
      "Epoch 17/25, Batch 1021/2194, Loss: 0.0803\n",
      "Epoch 17/25, Batch 1031/2194, Loss: 0.0477\n",
      "Epoch 17/25, Batch 1041/2194, Loss: 0.0574\n",
      "Epoch 17/25, Batch 1051/2194, Loss: 0.0531\n",
      "Epoch 17/25, Batch 1061/2194, Loss: 0.1015\n",
      "Epoch 17/25, Batch 1071/2194, Loss: 0.2393\n",
      "Epoch 17/25, Batch 1081/2194, Loss: 0.1265\n",
      "Epoch 17/25, Batch 1091/2194, Loss: 0.1745\n",
      "Epoch 17/25, Batch 1101/2194, Loss: 0.0811\n",
      "Epoch 17/25, Batch 1111/2194, Loss: 0.1672\n",
      "Epoch 17/25, Batch 1121/2194, Loss: 0.0542\n",
      "Epoch 17/25, Batch 1131/2194, Loss: 0.1493\n",
      "Epoch 17/25, Batch 1141/2194, Loss: 0.2388\n",
      "Epoch 17/25, Batch 1151/2194, Loss: 0.3389\n",
      "Epoch 17/25, Batch 1161/2194, Loss: 0.1453\n",
      "Epoch 17/25, Batch 1171/2194, Loss: 0.0541\n",
      "Epoch 17/25, Batch 1181/2194, Loss: 0.0424\n",
      "Epoch 17/25, Batch 1191/2194, Loss: 0.0816\n",
      "Epoch 17/25, Batch 1201/2194, Loss: 0.1216\n",
      "Epoch 17/25, Batch 1211/2194, Loss: 0.1437\n",
      "Epoch 17/25, Batch 1221/2194, Loss: 0.1687\n",
      "Epoch 17/25, Batch 1231/2194, Loss: 0.1324\n",
      "Epoch 17/25, Batch 1241/2194, Loss: 0.0445\n",
      "Epoch 17/25, Batch 1251/2194, Loss: 0.0608\n",
      "Epoch 17/25, Batch 1261/2194, Loss: 0.0234\n",
      "Epoch 17/25, Batch 1271/2194, Loss: 0.0911\n",
      "Epoch 17/25, Batch 1281/2194, Loss: 0.0304\n",
      "Epoch 17/25, Batch 1291/2194, Loss: 0.0638\n",
      "Epoch 17/25, Batch 1301/2194, Loss: 0.1086\n",
      "Epoch 17/25, Batch 1311/2194, Loss: 0.2194\n",
      "Epoch 17/25, Batch 1321/2194, Loss: 0.1022\n",
      "Epoch 17/25, Batch 1331/2194, Loss: 0.0448\n",
      "Epoch 17/25, Batch 1341/2194, Loss: 0.0132\n",
      "Epoch 17/25, Batch 1351/2194, Loss: 0.2052\n",
      "Epoch 17/25, Batch 1361/2194, Loss: 0.0504\n",
      "Epoch 17/25, Batch 1371/2194, Loss: 0.0415\n",
      "Epoch 17/25, Batch 1381/2194, Loss: 0.2546\n",
      "Epoch 17/25, Batch 1391/2194, Loss: 0.0924\n",
      "Epoch 17/25, Batch 1401/2194, Loss: 0.0970\n",
      "Epoch 17/25, Batch 1411/2194, Loss: 0.1706\n",
      "Epoch 17/25, Batch 1421/2194, Loss: 0.0333\n",
      "Epoch 17/25, Batch 1431/2194, Loss: 0.0594\n",
      "Epoch 17/25, Batch 1441/2194, Loss: 0.0729\n",
      "Epoch 17/25, Batch 1451/2194, Loss: 0.1033\n",
      "Epoch 17/25, Batch 1461/2194, Loss: 0.0755\n",
      "Epoch 17/25, Batch 1471/2194, Loss: 0.1922\n",
      "Epoch 17/25, Batch 1481/2194, Loss: 0.1254\n",
      "Epoch 17/25, Batch 1491/2194, Loss: 0.1154\n",
      "Epoch 17/25, Batch 1501/2194, Loss: 0.0536\n",
      "Epoch 17/25, Batch 1511/2194, Loss: 0.2402\n",
      "Epoch 17/25, Batch 1521/2194, Loss: 0.0769\n",
      "Epoch 17/25, Batch 1531/2194, Loss: 0.1357\n",
      "Epoch 17/25, Batch 1541/2194, Loss: 0.0273\n",
      "Epoch 17/25, Batch 1551/2194, Loss: 0.1017\n",
      "Epoch 17/25, Batch 1561/2194, Loss: 0.1206\n",
      "Epoch 17/25, Batch 1571/2194, Loss: 0.1875\n",
      "Epoch 17/25, Batch 1581/2194, Loss: 0.0920\n",
      "Epoch 17/25, Batch 1591/2194, Loss: 0.1694\n",
      "Epoch 17/25, Batch 1601/2194, Loss: 0.2543\n",
      "Epoch 17/25, Batch 1611/2194, Loss: 0.1512\n",
      "Epoch 17/25, Batch 1621/2194, Loss: 0.1063\n",
      "Epoch 17/25, Batch 1631/2194, Loss: 0.3377\n",
      "Epoch 17/25, Batch 1641/2194, Loss: 0.2149\n",
      "Epoch 17/25, Batch 1651/2194, Loss: 0.1126\n",
      "Epoch 17/25, Batch 1661/2194, Loss: 0.1560\n",
      "Epoch 17/25, Batch 1671/2194, Loss: 0.0654\n",
      "Epoch 17/25, Batch 1681/2194, Loss: 0.0788\n",
      "Epoch 17/25, Batch 1691/2194, Loss: 0.1042\n",
      "Epoch 17/25, Batch 1701/2194, Loss: 0.1827\n",
      "Epoch 17/25, Batch 1711/2194, Loss: 0.0611\n",
      "Epoch 17/25, Batch 1721/2194, Loss: 0.0177\n",
      "Epoch 17/25, Batch 1731/2194, Loss: 0.0267\n",
      "Epoch 17/25, Batch 1741/2194, Loss: 0.2113\n",
      "Epoch 17/25, Batch 1751/2194, Loss: 0.0872\n",
      "Epoch 17/25, Batch 1761/2194, Loss: 0.0469\n",
      "Epoch 17/25, Batch 1771/2194, Loss: 0.1068\n",
      "Epoch 17/25, Batch 1781/2194, Loss: 0.0970\n",
      "Epoch 17/25, Batch 1791/2194, Loss: 0.1280\n",
      "Epoch 17/25, Batch 1801/2194, Loss: 0.0495\n",
      "Epoch 17/25, Batch 1811/2194, Loss: 0.2374\n",
      "Epoch 17/25, Batch 1821/2194, Loss: 0.0925\n",
      "Epoch 17/25, Batch 1831/2194, Loss: 0.2630\n",
      "Epoch 17/25, Batch 1841/2194, Loss: 0.1643\n",
      "Epoch 17/25, Batch 1851/2194, Loss: 0.0803\n",
      "Epoch 17/25, Batch 1861/2194, Loss: 0.0416\n",
      "Epoch 17/25, Batch 1871/2194, Loss: 0.2350\n",
      "Epoch 17/25, Batch 1881/2194, Loss: 0.0600\n",
      "Epoch 17/25, Batch 1891/2194, Loss: 0.1207\n",
      "Epoch 17/25, Batch 1901/2194, Loss: 0.0871\n",
      "Epoch 17/25, Batch 1911/2194, Loss: 0.1584\n",
      "Epoch 17/25, Batch 1921/2194, Loss: 0.0483\n",
      "Epoch 17/25, Batch 1931/2194, Loss: 0.0662\n",
      "Epoch 17/25, Batch 1941/2194, Loss: 0.0955\n",
      "Epoch 17/25, Batch 1951/2194, Loss: 0.1039\n",
      "Epoch 17/25, Batch 1961/2194, Loss: 0.0495\n",
      "Epoch 17/25, Batch 1971/2194, Loss: 0.0766\n",
      "Epoch 17/25, Batch 1981/2194, Loss: 0.0381\n",
      "Epoch 17/25, Batch 1991/2194, Loss: 0.0428\n",
      "Epoch 17/25, Batch 2001/2194, Loss: 0.0267\n",
      "Epoch 17/25, Batch 2011/2194, Loss: 0.1650\n",
      "Epoch 17/25, Batch 2021/2194, Loss: 0.0561\n",
      "Epoch 17/25, Batch 2031/2194, Loss: 0.1238\n",
      "Epoch 17/25, Batch 2041/2194, Loss: 0.0581\n",
      "Epoch 17/25, Batch 2051/2194, Loss: 0.0780\n",
      "Epoch 17/25, Batch 2061/2194, Loss: 0.1923\n",
      "Epoch 17/25, Batch 2071/2194, Loss: 0.1495\n",
      "Epoch 17/25, Batch 2081/2194, Loss: 0.0330\n",
      "Epoch 17/25, Batch 2091/2194, Loss: 0.1177\n",
      "Epoch 17/25, Batch 2101/2194, Loss: 0.0721\n",
      "Epoch 17/25, Batch 2111/2194, Loss: 0.0390\n",
      "Epoch 17/25, Batch 2121/2194, Loss: 0.2497\n",
      "Epoch 17/25, Batch 2131/2194, Loss: 0.0908\n",
      "Epoch 17/25, Batch 2141/2194, Loss: 0.0913\n",
      "Epoch 17/25, Batch 2151/2194, Loss: 0.0974\n",
      "Epoch 17/25, Batch 2161/2194, Loss: 0.0764\n",
      "Epoch 17/25, Batch 2171/2194, Loss: 0.0507\n",
      "Epoch 17/25, Batch 2181/2194, Loss: 0.0730\n",
      "Epoch 17/25, Batch 2191/2194, Loss: 0.1290\n",
      "Epoch 17/25:\n",
      "Train Loss: 0.1135, Train Acc: 95.23%\n",
      "Val Loss: 0.1451, Val Acc: 93.74%\n",
      "------------------------------------------------------------\n",
      "Starting epoch 18/25...\n",
      "Starting training loop with 2194 batches...\n",
      "Successfully loaded first batch: images shape torch.Size([32, 3, 224, 224]), labels shape torch.Size([32])\n",
      "Epoch 18/25, Batch 1/2194, Loss: 0.0726\n",
      "Epoch 18/25, Batch 11/2194, Loss: 0.0802\n",
      "Epoch 18/25, Batch 21/2194, Loss: 0.0525\n",
      "Epoch 18/25, Batch 31/2194, Loss: 0.0229\n",
      "Epoch 18/25, Batch 41/2194, Loss: 0.1319\n",
      "Epoch 18/25, Batch 51/2194, Loss: 0.0888\n",
      "Epoch 18/25, Batch 61/2194, Loss: 0.0776\n",
      "Epoch 18/25, Batch 71/2194, Loss: 0.0669\n",
      "Epoch 18/25, Batch 81/2194, Loss: 0.1839\n",
      "Epoch 18/25, Batch 91/2194, Loss: 0.0700\n",
      "Epoch 18/25, Batch 101/2194, Loss: 0.1288\n",
      "Epoch 18/25, Batch 111/2194, Loss: 0.0831\n",
      "Epoch 18/25, Batch 121/2194, Loss: 0.0753\n",
      "Epoch 18/25, Batch 131/2194, Loss: 0.0752\n",
      "Epoch 18/25, Batch 141/2194, Loss: 0.0538\n",
      "Epoch 18/25, Batch 151/2194, Loss: 0.1360\n",
      "Epoch 18/25, Batch 161/2194, Loss: 0.0348\n",
      "Epoch 18/25, Batch 171/2194, Loss: 0.1994\n",
      "Epoch 18/25, Batch 181/2194, Loss: 0.0514\n",
      "Epoch 18/25, Batch 191/2194, Loss: 0.0158\n",
      "Epoch 18/25, Batch 201/2194, Loss: 0.1749\n",
      "Epoch 18/25, Batch 211/2194, Loss: 0.0618\n",
      "Epoch 18/25, Batch 221/2194, Loss: 0.0569\n",
      "Epoch 18/25, Batch 231/2194, Loss: 0.1391\n",
      "Epoch 18/25, Batch 241/2194, Loss: 0.1370\n",
      "Epoch 18/25, Batch 251/2194, Loss: 0.1180\n",
      "Epoch 18/25, Batch 261/2194, Loss: 0.1064\n",
      "Epoch 18/25, Batch 271/2194, Loss: 0.2578\n",
      "Epoch 18/25, Batch 281/2194, Loss: 0.0864\n",
      "Epoch 18/25, Batch 291/2194, Loss: 0.0205\n",
      "Epoch 18/25, Batch 301/2194, Loss: 0.0159\n",
      "Epoch 18/25, Batch 311/2194, Loss: 0.1066\n",
      "Epoch 18/25, Batch 321/2194, Loss: 0.0665\n",
      "Epoch 18/25, Batch 331/2194, Loss: 0.1164\n",
      "Epoch 18/25, Batch 341/2194, Loss: 0.0522\n",
      "Epoch 18/25, Batch 351/2194, Loss: 0.0302\n",
      "Epoch 18/25, Batch 361/2194, Loss: 0.0808\n",
      "Epoch 18/25, Batch 371/2194, Loss: 0.1465\n",
      "Epoch 18/25, Batch 381/2194, Loss: 0.0600\n",
      "Epoch 18/25, Batch 391/2194, Loss: 0.1304\n",
      "Epoch 18/25, Batch 401/2194, Loss: 0.1095\n",
      "Epoch 18/25, Batch 411/2194, Loss: 0.0841\n",
      "Epoch 18/25, Batch 421/2194, Loss: 0.1077\n",
      "Epoch 18/25, Batch 431/2194, Loss: 0.0753\n",
      "Epoch 18/25, Batch 441/2194, Loss: 0.1518\n",
      "Epoch 18/25, Batch 451/2194, Loss: 0.0167\n",
      "Epoch 18/25, Batch 461/2194, Loss: 0.2033\n",
      "Epoch 18/25, Batch 471/2194, Loss: 0.0576\n",
      "Epoch 18/25, Batch 481/2194, Loss: 0.1064\n",
      "Epoch 18/25, Batch 491/2194, Loss: 0.0397\n",
      "Epoch 18/25, Batch 501/2194, Loss: 0.0127\n",
      "Epoch 18/25, Batch 511/2194, Loss: 0.0887\n",
      "Epoch 18/25, Batch 521/2194, Loss: 0.1640\n",
      "Epoch 18/25, Batch 531/2194, Loss: 0.0819\n",
      "Epoch 18/25, Batch 541/2194, Loss: 0.0170\n",
      "Epoch 18/25, Batch 551/2194, Loss: 0.1229\n",
      "Epoch 18/25, Batch 561/2194, Loss: 0.0511\n",
      "Epoch 18/25, Batch 571/2194, Loss: 0.0874\n",
      "Epoch 18/25, Batch 581/2194, Loss: 0.0163\n",
      "Epoch 18/25, Batch 591/2194, Loss: 0.1053\n",
      "Epoch 18/25, Batch 601/2194, Loss: 0.1956\n",
      "Epoch 18/25, Batch 611/2194, Loss: 0.0773\n",
      "Epoch 18/25, Batch 621/2194, Loss: 0.1104\n",
      "Epoch 18/25, Batch 631/2194, Loss: 0.0524\n",
      "Epoch 18/25, Batch 641/2194, Loss: 0.1901\n",
      "Epoch 18/25, Batch 651/2194, Loss: 0.0865\n",
      "Epoch 18/25, Batch 661/2194, Loss: 0.1759\n",
      "Epoch 18/25, Batch 671/2194, Loss: 0.0204\n",
      "Epoch 18/25, Batch 681/2194, Loss: 0.0738\n",
      "Epoch 18/25, Batch 691/2194, Loss: 0.0461\n",
      "Epoch 18/25, Batch 701/2194, Loss: 0.1699\n",
      "Epoch 18/25, Batch 711/2194, Loss: 0.0179\n",
      "Epoch 18/25, Batch 721/2194, Loss: 0.0912\n",
      "Epoch 18/25, Batch 731/2194, Loss: 0.1246\n",
      "Epoch 18/25, Batch 741/2194, Loss: 0.0956\n",
      "Epoch 18/25, Batch 751/2194, Loss: 0.1498\n",
      "Epoch 18/25, Batch 761/2194, Loss: 0.0870\n",
      "Epoch 18/25, Batch 771/2194, Loss: 0.0635\n",
      "Epoch 18/25, Batch 781/2194, Loss: 0.0319\n",
      "Epoch 18/25, Batch 791/2194, Loss: 0.1610\n",
      "Epoch 18/25, Batch 801/2194, Loss: 0.0958\n",
      "Epoch 18/25, Batch 811/2194, Loss: 0.0847\n",
      "Epoch 18/25, Batch 821/2194, Loss: 0.0843\n",
      "Epoch 18/25, Batch 831/2194, Loss: 0.0595\n",
      "Epoch 18/25, Batch 841/2194, Loss: 0.1275\n",
      "Epoch 18/25, Batch 851/2194, Loss: 0.1543\n",
      "Epoch 18/25, Batch 861/2194, Loss: 0.1967\n",
      "Epoch 18/25, Batch 871/2194, Loss: 0.1088\n",
      "Epoch 18/25, Batch 881/2194, Loss: 0.0434\n",
      "Epoch 18/25, Batch 891/2194, Loss: 0.1573\n",
      "Epoch 18/25, Batch 901/2194, Loss: 0.0614\n",
      "Epoch 18/25, Batch 911/2194, Loss: 0.2238\n",
      "Epoch 18/25, Batch 921/2194, Loss: 0.0438\n",
      "Epoch 18/25, Batch 931/2194, Loss: 0.0704\n",
      "Epoch 18/25, Batch 941/2194, Loss: 0.2748\n",
      "Epoch 18/25, Batch 951/2194, Loss: 0.1495\n",
      "Epoch 18/25, Batch 961/2194, Loss: 0.0743\n",
      "Epoch 18/25, Batch 971/2194, Loss: 0.0747\n",
      "Epoch 18/25, Batch 981/2194, Loss: 0.0115\n",
      "Epoch 18/25, Batch 991/2194, Loss: 0.1075\n",
      "Epoch 18/25, Batch 1001/2194, Loss: 0.1988\n",
      "Epoch 18/25, Batch 1011/2194, Loss: 0.0949\n",
      "Epoch 18/25, Batch 1021/2194, Loss: 0.0743\n",
      "Epoch 18/25, Batch 1031/2194, Loss: 0.0753\n",
      "Epoch 18/25, Batch 1041/2194, Loss: 0.1078\n",
      "Epoch 18/25, Batch 1051/2194, Loss: 0.1771\n",
      "Epoch 18/25, Batch 1061/2194, Loss: 0.0458\n",
      "Epoch 18/25, Batch 1071/2194, Loss: 0.1032\n",
      "Epoch 18/25, Batch 1081/2194, Loss: 0.0208\n",
      "Epoch 18/25, Batch 1091/2194, Loss: 0.2264\n",
      "Epoch 18/25, Batch 1101/2194, Loss: 0.0831\n",
      "Epoch 18/25, Batch 1111/2194, Loss: 0.1258\n",
      "Epoch 18/25, Batch 1121/2194, Loss: 0.0899\n",
      "Epoch 18/25, Batch 1131/2194, Loss: 0.1900\n",
      "Epoch 18/25, Batch 1141/2194, Loss: 0.0104\n",
      "Epoch 18/25, Batch 1151/2194, Loss: 0.0534\n",
      "Epoch 18/25, Batch 1161/2194, Loss: 0.1760\n",
      "Epoch 18/25, Batch 1171/2194, Loss: 0.2807\n",
      "Epoch 18/25, Batch 1181/2194, Loss: 0.1663\n",
      "Epoch 18/25, Batch 1191/2194, Loss: 0.3359\n",
      "Epoch 18/25, Batch 1201/2194, Loss: 0.2636\n",
      "Epoch 18/25, Batch 1211/2194, Loss: 0.1102\n",
      "Epoch 18/25, Batch 1221/2194, Loss: 0.1614\n",
      "Epoch 18/25, Batch 1231/2194, Loss: 0.0372\n",
      "Epoch 18/25, Batch 1241/2194, Loss: 0.2020\n",
      "Epoch 18/25, Batch 1251/2194, Loss: 0.0130\n",
      "Epoch 18/25, Batch 1261/2194, Loss: 0.2004\n",
      "Epoch 18/25, Batch 1271/2194, Loss: 0.0223\n",
      "Epoch 18/25, Batch 1281/2194, Loss: 0.3663\n",
      "Epoch 18/25, Batch 1291/2194, Loss: 0.0916\n",
      "Epoch 18/25, Batch 1301/2194, Loss: 0.1309\n",
      "Epoch 18/25, Batch 1311/2194, Loss: 0.0348\n",
      "Epoch 18/25, Batch 1321/2194, Loss: 0.0831\n",
      "Epoch 18/25, Batch 1331/2194, Loss: 0.0373\n",
      "Epoch 18/25, Batch 1341/2194, Loss: 0.1629\n",
      "Epoch 18/25, Batch 1351/2194, Loss: 0.0316\n",
      "Epoch 18/25, Batch 1361/2194, Loss: 0.1216\n",
      "Epoch 18/25, Batch 1371/2194, Loss: 0.0648\n",
      "Epoch 18/25, Batch 1381/2194, Loss: 0.1420\n",
      "Epoch 18/25, Batch 1391/2194, Loss: 0.0513\n",
      "Epoch 18/25, Batch 1401/2194, Loss: 0.1133\n",
      "Epoch 18/25, Batch 1411/2194, Loss: 0.1122\n",
      "Epoch 18/25, Batch 1421/2194, Loss: 0.1603\n",
      "Epoch 18/25, Batch 1431/2194, Loss: 0.0821\n",
      "Epoch 18/25, Batch 1441/2194, Loss: 0.0129\n",
      "Epoch 18/25, Batch 1451/2194, Loss: 0.0103\n",
      "Epoch 18/25, Batch 1461/2194, Loss: 0.1266\n",
      "Epoch 18/25, Batch 1471/2194, Loss: 0.4216\n",
      "Epoch 18/25, Batch 1481/2194, Loss: 0.2143\n",
      "Epoch 18/25, Batch 1491/2194, Loss: 0.1895\n",
      "Epoch 18/25, Batch 1501/2194, Loss: 0.0568\n",
      "Epoch 18/25, Batch 1511/2194, Loss: 0.1168\n",
      "Epoch 18/25, Batch 1521/2194, Loss: 0.1250\n",
      "Epoch 18/25, Batch 1531/2194, Loss: 0.0224\n",
      "Epoch 18/25, Batch 1541/2194, Loss: 0.1018\n",
      "Epoch 18/25, Batch 1551/2194, Loss: 0.0693\n",
      "Epoch 18/25, Batch 1561/2194, Loss: 0.0593\n",
      "Epoch 18/25, Batch 1571/2194, Loss: 0.0940\n",
      "Epoch 18/25, Batch 1581/2194, Loss: 0.0540\n",
      "Epoch 18/25, Batch 1591/2194, Loss: 0.0657\n",
      "Epoch 18/25, Batch 1601/2194, Loss: 0.0845\n",
      "Epoch 18/25, Batch 1611/2194, Loss: 0.0824\n",
      "Epoch 18/25, Batch 1621/2194, Loss: 0.0936\n",
      "Epoch 18/25, Batch 1631/2194, Loss: 0.1223\n",
      "Epoch 18/25, Batch 1641/2194, Loss: 0.0293\n",
      "Epoch 18/25, Batch 1651/2194, Loss: 0.1060\n",
      "Epoch 18/25, Batch 1661/2194, Loss: 0.0997\n",
      "Epoch 18/25, Batch 1671/2194, Loss: 0.0889\n",
      "Epoch 18/25, Batch 1681/2194, Loss: 0.0867\n",
      "Epoch 18/25, Batch 1691/2194, Loss: 0.0946\n",
      "Epoch 18/25, Batch 1701/2194, Loss: 0.0982\n",
      "Epoch 18/25, Batch 1711/2194, Loss: 0.0552\n",
      "Epoch 18/25, Batch 1721/2194, Loss: 0.0779\n",
      "Epoch 18/25, Batch 1731/2194, Loss: 0.0744\n",
      "Epoch 18/25, Batch 1741/2194, Loss: 0.1392\n",
      "Epoch 18/25, Batch 1751/2194, Loss: 0.0337\n",
      "Epoch 18/25, Batch 1761/2194, Loss: 0.1398\n",
      "Epoch 18/25, Batch 1771/2194, Loss: 0.0565\n",
      "Epoch 18/25, Batch 1781/2194, Loss: 0.0162\n",
      "Epoch 18/25, Batch 1791/2194, Loss: 0.0774\n",
      "Epoch 18/25, Batch 1801/2194, Loss: 0.0326\n",
      "Epoch 18/25, Batch 1811/2194, Loss: 0.1506\n",
      "Epoch 18/25, Batch 1821/2194, Loss: 0.1720\n",
      "Epoch 18/25, Batch 1831/2194, Loss: 0.0542\n",
      "Epoch 18/25, Batch 1841/2194, Loss: 0.1542\n",
      "Epoch 18/25, Batch 1851/2194, Loss: 0.1958\n",
      "Epoch 18/25, Batch 1861/2194, Loss: 0.0638\n",
      "Epoch 18/25, Batch 1871/2194, Loss: 0.0919\n",
      "Epoch 18/25, Batch 1881/2194, Loss: 0.0867\n",
      "Epoch 18/25, Batch 1891/2194, Loss: 0.0805\n",
      "Epoch 18/25, Batch 1901/2194, Loss: 0.0430\n",
      "Epoch 18/25, Batch 1911/2194, Loss: 0.0371\n",
      "Epoch 18/25, Batch 1921/2194, Loss: 0.1438\n",
      "Epoch 18/25, Batch 1931/2194, Loss: 0.0489\n",
      "Epoch 18/25, Batch 1941/2194, Loss: 0.1164\n",
      "Epoch 18/25, Batch 1951/2194, Loss: 0.0495\n",
      "Epoch 18/25, Batch 1961/2194, Loss: 0.0495\n",
      "Epoch 18/25, Batch 1971/2194, Loss: 0.0754\n",
      "Epoch 18/25, Batch 1981/2194, Loss: 0.0291\n",
      "Epoch 18/25, Batch 1991/2194, Loss: 0.0322\n",
      "Epoch 18/25, Batch 2001/2194, Loss: 0.0523\n",
      "Epoch 18/25, Batch 2011/2194, Loss: 0.0385\n",
      "Epoch 18/25, Batch 2021/2194, Loss: 0.1266\n",
      "Epoch 18/25, Batch 2031/2194, Loss: 0.0316\n",
      "Epoch 18/25, Batch 2041/2194, Loss: 0.0982\n",
      "Epoch 18/25, Batch 2051/2194, Loss: 0.0774\n",
      "Epoch 18/25, Batch 2061/2194, Loss: 0.1625\n",
      "Epoch 18/25, Batch 2071/2194, Loss: 0.1610\n",
      "Epoch 18/25, Batch 2081/2194, Loss: 0.0626\n",
      "Epoch 18/25, Batch 2091/2194, Loss: 0.0505\n",
      "Epoch 18/25, Batch 2101/2194, Loss: 0.0151\n",
      "Epoch 18/25, Batch 2111/2194, Loss: 0.1876\n",
      "Epoch 18/25, Batch 2121/2194, Loss: 0.1892\n",
      "Epoch 18/25, Batch 2131/2194, Loss: 0.1075\n",
      "Epoch 18/25, Batch 2141/2194, Loss: 0.1284\n",
      "Epoch 18/25, Batch 2151/2194, Loss: 0.1995\n",
      "Epoch 18/25, Batch 2161/2194, Loss: 0.0717\n",
      "Epoch 18/25, Batch 2171/2194, Loss: 0.1699\n",
      "Epoch 18/25, Batch 2181/2194, Loss: 0.2203\n",
      "Epoch 18/25, Batch 2191/2194, Loss: 0.0248\n",
      "Epoch 18/25:\n",
      "Train Loss: 0.1087, Train Acc: 95.44%\n",
      "Val Loss: 0.1420, Val Acc: 93.82%\n",
      "------------------------------------------------------------\n",
      "Starting epoch 19/25...\n",
      "Starting training loop with 2194 batches...\n",
      "Successfully loaded first batch: images shape torch.Size([32, 3, 224, 224]), labels shape torch.Size([32])\n",
      "Epoch 19/25, Batch 1/2194, Loss: 0.1224\n",
      "Epoch 19/25, Batch 11/2194, Loss: 0.2183\n",
      "Epoch 19/25, Batch 21/2194, Loss: 0.0837\n",
      "Epoch 19/25, Batch 31/2194, Loss: 0.1110\n",
      "Epoch 19/25, Batch 41/2194, Loss: 0.0288\n",
      "Epoch 19/25, Batch 51/2194, Loss: 0.0294\n",
      "Epoch 19/25, Batch 61/2194, Loss: 0.0151\n",
      "Epoch 19/25, Batch 71/2194, Loss: 0.1298\n",
      "Epoch 19/25, Batch 81/2194, Loss: 0.1046\n",
      "Epoch 19/25, Batch 91/2194, Loss: 0.0588\n",
      "Epoch 19/25, Batch 101/2194, Loss: 0.0158\n",
      "Epoch 19/25, Batch 111/2194, Loss: 0.1044\n",
      "Epoch 19/25, Batch 121/2194, Loss: 0.1891\n",
      "Epoch 19/25, Batch 131/2194, Loss: 0.1186\n",
      "Epoch 19/25, Batch 141/2194, Loss: 0.0815\n",
      "Epoch 19/25, Batch 151/2194, Loss: 0.0152\n",
      "Epoch 19/25, Batch 161/2194, Loss: 0.1305\n",
      "Epoch 19/25, Batch 171/2194, Loss: 0.1117\n",
      "Epoch 19/25, Batch 181/2194, Loss: 0.0997\n",
      "Epoch 19/25, Batch 191/2194, Loss: 0.0334\n",
      "Epoch 19/25, Batch 201/2194, Loss: 0.1230\n",
      "Epoch 19/25, Batch 211/2194, Loss: 0.1014\n",
      "Epoch 19/25, Batch 221/2194, Loss: 0.0889\n",
      "Epoch 19/25, Batch 231/2194, Loss: 0.1122\n",
      "Epoch 19/25, Batch 241/2194, Loss: 0.2442\n",
      "Epoch 19/25, Batch 251/2194, Loss: 0.0709\n",
      "Epoch 19/25, Batch 261/2194, Loss: 0.2254\n",
      "Epoch 19/25, Batch 271/2194, Loss: 0.0817\n",
      "Epoch 19/25, Batch 281/2194, Loss: 0.0492\n",
      "Epoch 19/25, Batch 291/2194, Loss: 0.1329\n",
      "Epoch 19/25, Batch 301/2194, Loss: 0.0195\n",
      "Epoch 19/25, Batch 311/2194, Loss: 0.1327\n",
      "Epoch 19/25, Batch 321/2194, Loss: 0.0508\n",
      "Epoch 19/25, Batch 331/2194, Loss: 0.0376\n",
      "Epoch 19/25, Batch 341/2194, Loss: 0.1104\n",
      "Epoch 19/25, Batch 351/2194, Loss: 0.1838\n",
      "Epoch 19/25, Batch 361/2194, Loss: 0.0225\n",
      "Epoch 19/25, Batch 371/2194, Loss: 0.0842\n",
      "Epoch 19/25, Batch 381/2194, Loss: 0.0559\n",
      "Epoch 19/25, Batch 391/2194, Loss: 0.1828\n",
      "Epoch 19/25, Batch 401/2194, Loss: 0.0394\n",
      "Epoch 19/25, Batch 411/2194, Loss: 0.0780\n",
      "Epoch 19/25, Batch 421/2194, Loss: 0.0998\n",
      "Epoch 19/25, Batch 431/2194, Loss: 0.1088\n",
      "Epoch 19/25, Batch 441/2194, Loss: 0.1398\n",
      "Epoch 19/25, Batch 451/2194, Loss: 0.0607\n",
      "Epoch 19/25, Batch 461/2194, Loss: 0.1132\n",
      "Epoch 19/25, Batch 471/2194, Loss: 0.0165\n",
      "Epoch 19/25, Batch 481/2194, Loss: 0.2297\n",
      "Epoch 19/25, Batch 491/2194, Loss: 0.0694\n",
      "Epoch 19/25, Batch 501/2194, Loss: 0.1289\n",
      "Epoch 19/25, Batch 511/2194, Loss: 0.0955\n",
      "Epoch 19/25, Batch 521/2194, Loss: 0.1736\n",
      "Epoch 19/25, Batch 531/2194, Loss: 0.1842\n",
      "Epoch 19/25, Batch 541/2194, Loss: 0.0898\n",
      "Epoch 19/25, Batch 551/2194, Loss: 0.0408\n",
      "Epoch 19/25, Batch 561/2194, Loss: 0.0262\n",
      "Epoch 19/25, Batch 571/2194, Loss: 0.0288\n",
      "Epoch 19/25, Batch 581/2194, Loss: 0.1461\n",
      "Epoch 19/25, Batch 591/2194, Loss: 0.0617\n",
      "Epoch 19/25, Batch 601/2194, Loss: 0.0676\n",
      "Epoch 19/25, Batch 611/2194, Loss: 0.0928\n",
      "Epoch 19/25, Batch 621/2194, Loss: 0.0451\n",
      "Epoch 19/25, Batch 631/2194, Loss: 0.1762\n",
      "Epoch 19/25, Batch 641/2194, Loss: 0.0753\n",
      "Epoch 19/25, Batch 651/2194, Loss: 0.0950\n",
      "Epoch 19/25, Batch 661/2194, Loss: 0.0418\n",
      "Epoch 19/25, Batch 671/2194, Loss: 0.0883\n",
      "Epoch 19/25, Batch 681/2194, Loss: 0.0505\n",
      "Epoch 19/25, Batch 691/2194, Loss: 0.1232\n",
      "Epoch 19/25, Batch 701/2194, Loss: 0.0203\n",
      "Epoch 19/25, Batch 711/2194, Loss: 0.0793\n",
      "Epoch 19/25, Batch 721/2194, Loss: 0.0303\n",
      "Epoch 19/25, Batch 731/2194, Loss: 0.0772\n",
      "Epoch 19/25, Batch 741/2194, Loss: 0.1635\n",
      "Epoch 19/25, Batch 751/2194, Loss: 0.0111\n",
      "Epoch 19/25, Batch 761/2194, Loss: 0.2596\n",
      "Epoch 19/25, Batch 771/2194, Loss: 0.1320\n",
      "Epoch 19/25, Batch 781/2194, Loss: 0.0698\n",
      "Epoch 19/25, Batch 791/2194, Loss: 0.0999\n",
      "Epoch 19/25, Batch 801/2194, Loss: 0.0986\n",
      "Epoch 19/25, Batch 811/2194, Loss: 0.1124\n",
      "Epoch 19/25, Batch 821/2194, Loss: 0.0843\n",
      "Epoch 19/25, Batch 831/2194, Loss: 0.2213\n",
      "Epoch 19/25, Batch 841/2194, Loss: 0.0779\n",
      "Epoch 19/25, Batch 851/2194, Loss: 0.2226\n",
      "Epoch 19/25, Batch 861/2194, Loss: 0.0156\n",
      "Epoch 19/25, Batch 871/2194, Loss: 0.0039\n",
      "Epoch 19/25, Batch 881/2194, Loss: 0.0908\n",
      "Epoch 19/25, Batch 891/2194, Loss: 0.2785\n",
      "Epoch 19/25, Batch 901/2194, Loss: 0.1707\n",
      "Epoch 19/25, Batch 911/2194, Loss: 0.0384\n",
      "Epoch 19/25, Batch 921/2194, Loss: 0.2474\n",
      "Epoch 19/25, Batch 931/2194, Loss: 0.0168\n",
      "Epoch 19/25, Batch 941/2194, Loss: 0.0445\n",
      "Epoch 19/25, Batch 951/2194, Loss: 0.0893\n",
      "Epoch 19/25, Batch 961/2194, Loss: 0.1591\n",
      "Epoch 19/25, Batch 971/2194, Loss: 0.1094\n",
      "Epoch 19/25, Batch 981/2194, Loss: 0.0540\n",
      "Epoch 19/25, Batch 991/2194, Loss: 0.0654\n",
      "Epoch 19/25, Batch 1001/2194, Loss: 0.1018\n",
      "Epoch 19/25, Batch 1011/2194, Loss: 0.1372\n",
      "Epoch 19/25, Batch 1021/2194, Loss: 0.0533\n",
      "Epoch 19/25, Batch 1031/2194, Loss: 0.0323\n",
      "Epoch 19/25, Batch 1041/2194, Loss: 0.2156\n",
      "Epoch 19/25, Batch 1051/2194, Loss: 0.1420\n",
      "Epoch 19/25, Batch 1061/2194, Loss: 0.0860\n",
      "Epoch 19/25, Batch 1071/2194, Loss: 0.2473\n",
      "Epoch 19/25, Batch 1081/2194, Loss: 0.1327\n",
      "Epoch 19/25, Batch 1091/2194, Loss: 0.1450\n",
      "Epoch 19/25, Batch 1101/2194, Loss: 0.1459\n",
      "Epoch 19/25, Batch 1111/2194, Loss: 0.0534\n",
      "Epoch 19/25, Batch 1121/2194, Loss: 0.0609\n",
      "Epoch 19/25, Batch 1131/2194, Loss: 0.0271\n",
      "Epoch 19/25, Batch 1141/2194, Loss: 0.0545\n",
      "Epoch 19/25, Batch 1151/2194, Loss: 0.0865\n",
      "Epoch 19/25, Batch 1161/2194, Loss: 0.1070\n",
      "Epoch 19/25, Batch 1171/2194, Loss: 0.0111\n",
      "Epoch 19/25, Batch 1181/2194, Loss: 0.0960\n",
      "Epoch 19/25, Batch 1191/2194, Loss: 0.2258\n",
      "Epoch 19/25, Batch 1201/2194, Loss: 0.0357\n",
      "Epoch 19/25, Batch 1211/2194, Loss: 0.1639\n",
      "Epoch 19/25, Batch 1221/2194, Loss: 0.0925\n",
      "Epoch 19/25, Batch 1231/2194, Loss: 0.1662\n",
      "Epoch 19/25, Batch 1241/2194, Loss: 0.0625\n",
      "Epoch 19/25, Batch 1251/2194, Loss: 0.1150\n",
      "Epoch 19/25, Batch 1261/2194, Loss: 0.1188\n",
      "Epoch 19/25, Batch 1271/2194, Loss: 0.2683\n",
      "Epoch 19/25, Batch 1281/2194, Loss: 0.0634\n",
      "Epoch 19/25, Batch 1291/2194, Loss: 0.2294\n",
      "Epoch 19/25, Batch 1301/2194, Loss: 0.3374\n",
      "Epoch 19/25, Batch 1311/2194, Loss: 0.0983\n",
      "Epoch 19/25, Batch 1321/2194, Loss: 0.1187\n",
      "Epoch 19/25, Batch 1331/2194, Loss: 0.2744\n",
      "Epoch 19/25, Batch 1341/2194, Loss: 0.0665\n",
      "Epoch 19/25, Batch 1351/2194, Loss: 0.1570\n",
      "Epoch 19/25, Batch 1361/2194, Loss: 0.0622\n",
      "Epoch 19/25, Batch 1371/2194, Loss: 0.0189\n",
      "Epoch 19/25, Batch 1381/2194, Loss: 0.0562\n",
      "Epoch 19/25, Batch 1391/2194, Loss: 0.0336\n",
      "Epoch 19/25, Batch 1401/2194, Loss: 0.0949\n",
      "Epoch 19/25, Batch 1411/2194, Loss: 0.1276\n",
      "Epoch 19/25, Batch 1421/2194, Loss: 0.1398\n",
      "Epoch 19/25, Batch 1431/2194, Loss: 0.0291\n",
      "Epoch 19/25, Batch 1441/2194, Loss: 0.0704\n",
      "Epoch 19/25, Batch 1451/2194, Loss: 0.0796\n",
      "Epoch 19/25, Batch 1461/2194, Loss: 0.1031\n",
      "Epoch 19/25, Batch 1471/2194, Loss: 0.1485\n",
      "Epoch 19/25, Batch 1481/2194, Loss: 0.0572\n",
      "Epoch 19/25, Batch 1491/2194, Loss: 0.0665\n",
      "Epoch 19/25, Batch 1501/2194, Loss: 0.0707\n",
      "Epoch 19/25, Batch 1511/2194, Loss: 0.1566\n",
      "Epoch 19/25, Batch 1521/2194, Loss: 0.2873\n",
      "Epoch 19/25, Batch 1531/2194, Loss: 0.0569\n",
      "Epoch 19/25, Batch 1541/2194, Loss: 0.0531\n",
      "Epoch 19/25, Batch 1551/2194, Loss: 0.1211\n",
      "Epoch 19/25, Batch 1561/2194, Loss: 0.2637\n",
      "Epoch 19/25, Batch 1571/2194, Loss: 0.1877\n",
      "Epoch 19/25, Batch 1581/2194, Loss: 0.0983\n",
      "Epoch 19/25, Batch 1591/2194, Loss: 0.1177\n",
      "Epoch 19/25, Batch 1601/2194, Loss: 0.1179\n",
      "Epoch 19/25, Batch 1611/2194, Loss: 0.0179\n",
      "Epoch 19/25, Batch 1621/2194, Loss: 0.0693\n",
      "Epoch 19/25, Batch 1631/2194, Loss: 0.0547\n",
      "Epoch 19/25, Batch 1641/2194, Loss: 0.1728\n",
      "Epoch 19/25, Batch 1651/2194, Loss: 0.0538\n",
      "Epoch 19/25, Batch 1661/2194, Loss: 0.0835\n",
      "Epoch 19/25, Batch 1671/2194, Loss: 0.0506\n",
      "Epoch 19/25, Batch 1681/2194, Loss: 0.0973\n",
      "Epoch 19/25, Batch 1691/2194, Loss: 0.0884\n",
      "Epoch 19/25, Batch 1701/2194, Loss: 0.1247\n",
      "Epoch 19/25, Batch 1711/2194, Loss: 0.1240\n",
      "Epoch 19/25, Batch 1721/2194, Loss: 0.0823\n",
      "Epoch 19/25, Batch 1731/2194, Loss: 0.0412\n",
      "Epoch 19/25, Batch 1741/2194, Loss: 0.1308\n",
      "Epoch 19/25, Batch 1751/2194, Loss: 0.0411\n",
      "Epoch 19/25, Batch 1761/2194, Loss: 0.0645\n",
      "Epoch 19/25, Batch 1771/2194, Loss: 0.1753\n",
      "Epoch 19/25, Batch 1781/2194, Loss: 0.0404\n",
      "Epoch 19/25, Batch 1791/2194, Loss: 0.1034\n",
      "Epoch 19/25, Batch 1801/2194, Loss: 0.0111\n",
      "Epoch 19/25, Batch 1811/2194, Loss: 0.0654\n",
      "Epoch 19/25, Batch 1821/2194, Loss: 0.1907\n",
      "Epoch 19/25, Batch 1831/2194, Loss: 0.0188\n",
      "Epoch 19/25, Batch 1841/2194, Loss: 0.0521\n",
      "Epoch 19/25, Batch 1851/2194, Loss: 0.0778\n",
      "Epoch 19/25, Batch 1861/2194, Loss: 0.1590\n",
      "Epoch 19/25, Batch 1871/2194, Loss: 0.0974\n",
      "Epoch 19/25, Batch 1881/2194, Loss: 0.0954\n",
      "Epoch 19/25, Batch 1891/2194, Loss: 0.2522\n",
      "Epoch 19/25, Batch 1901/2194, Loss: 0.1154\n",
      "Epoch 19/25, Batch 1911/2194, Loss: 0.2051\n",
      "Epoch 19/25, Batch 1921/2194, Loss: 0.0987\n",
      "Epoch 19/25, Batch 1931/2194, Loss: 0.1149\n",
      "Epoch 19/25, Batch 1941/2194, Loss: 0.2888\n",
      "Epoch 19/25, Batch 1951/2194, Loss: 0.0451\n",
      "Epoch 19/25, Batch 1961/2194, Loss: 0.1536\n",
      "Epoch 19/25, Batch 1971/2194, Loss: 0.0208\n",
      "Epoch 19/25, Batch 1981/2194, Loss: 0.0174\n",
      "Epoch 19/25, Batch 1991/2194, Loss: 0.0353\n",
      "Epoch 19/25, Batch 2001/2194, Loss: 0.1196\n",
      "Epoch 19/25, Batch 2011/2194, Loss: 0.1068\n",
      "Epoch 19/25, Batch 2021/2194, Loss: 0.0383\n",
      "Epoch 19/25, Batch 2031/2194, Loss: 0.1112\n",
      "Epoch 19/25, Batch 2041/2194, Loss: 0.0742\n",
      "Epoch 19/25, Batch 2051/2194, Loss: 0.0221\n",
      "Epoch 19/25, Batch 2061/2194, Loss: 0.1256\n",
      "Epoch 19/25, Batch 2071/2194, Loss: 0.0768\n",
      "Epoch 19/25, Batch 2081/2194, Loss: 0.0630\n",
      "Epoch 19/25, Batch 2091/2194, Loss: 0.0090\n",
      "Epoch 19/25, Batch 2101/2194, Loss: 0.1103\n",
      "Epoch 19/25, Batch 2111/2194, Loss: 0.1233\n",
      "Epoch 19/25, Batch 2121/2194, Loss: 0.1442\n",
      "Epoch 19/25, Batch 2131/2194, Loss: 0.1365\n",
      "Epoch 19/25, Batch 2141/2194, Loss: 0.1227\n",
      "Epoch 19/25, Batch 2151/2194, Loss: 0.0408\n",
      "Epoch 19/25, Batch 2161/2194, Loss: 0.1872\n",
      "Epoch 19/25, Batch 2171/2194, Loss: 0.0965\n",
      "Epoch 19/25, Batch 2181/2194, Loss: 0.1229\n",
      "Epoch 19/25, Batch 2191/2194, Loss: 0.1610\n",
      "Epoch 19/25:\n",
      "Train Loss: 0.1039, Train Acc: 95.68%\n",
      "Val Loss: 0.1616, Val Acc: 93.32%\n",
      "------------------------------------------------------------\n",
      "Starting epoch 20/25...\n",
      "Starting training loop with 2194 batches...\n",
      "Successfully loaded first batch: images shape torch.Size([32, 3, 224, 224]), labels shape torch.Size([32])\n",
      "Epoch 20/25, Batch 1/2194, Loss: 0.0351\n",
      "Epoch 20/25, Batch 11/2194, Loss: 0.0549\n",
      "Epoch 20/25, Batch 21/2194, Loss: 0.1929\n",
      "Epoch 20/25, Batch 31/2194, Loss: 0.0804\n",
      "Epoch 20/25, Batch 41/2194, Loss: 0.0971\n",
      "Epoch 20/25, Batch 51/2194, Loss: 0.0656\n",
      "Epoch 20/25, Batch 61/2194, Loss: 0.1689\n",
      "Epoch 20/25, Batch 71/2194, Loss: 0.1049\n",
      "Epoch 20/25, Batch 81/2194, Loss: 0.0436\n",
      "Epoch 20/25, Batch 91/2194, Loss: 0.1204\n",
      "Epoch 20/25, Batch 101/2194, Loss: 0.0763\n",
      "Epoch 20/25, Batch 111/2194, Loss: 0.0673\n",
      "Epoch 20/25, Batch 121/2194, Loss: 0.0718\n",
      "Epoch 20/25, Batch 131/2194, Loss: 0.2202\n",
      "Epoch 20/25, Batch 141/2194, Loss: 0.1212\n",
      "Epoch 20/25, Batch 151/2194, Loss: 0.0858\n",
      "Epoch 20/25, Batch 161/2194, Loss: 0.0188\n",
      "Epoch 20/25, Batch 171/2194, Loss: 0.0435\n",
      "Epoch 20/25, Batch 181/2194, Loss: 0.0542\n",
      "Epoch 20/25, Batch 191/2194, Loss: 0.0209\n",
      "Epoch 20/25, Batch 201/2194, Loss: 0.0103\n",
      "Epoch 20/25, Batch 211/2194, Loss: 0.0865\n",
      "Epoch 20/25, Batch 221/2194, Loss: 0.1389\n",
      "Epoch 20/25, Batch 231/2194, Loss: 0.1840\n",
      "Epoch 20/25, Batch 241/2194, Loss: 0.0433\n",
      "Epoch 20/25, Batch 251/2194, Loss: 0.1060\n",
      "Epoch 20/25, Batch 261/2194, Loss: 0.1224\n",
      "Epoch 20/25, Batch 271/2194, Loss: 0.0705\n",
      "Epoch 20/25, Batch 281/2194, Loss: 0.0514\n",
      "Epoch 20/25, Batch 291/2194, Loss: 0.0769\n",
      "Epoch 20/25, Batch 301/2194, Loss: 0.0958\n",
      "Epoch 20/25, Batch 311/2194, Loss: 0.0398\n",
      "Epoch 20/25, Batch 321/2194, Loss: 0.0791\n",
      "Epoch 20/25, Batch 331/2194, Loss: 0.1147\n",
      "Epoch 20/25, Batch 341/2194, Loss: 0.2971\n",
      "Epoch 20/25, Batch 351/2194, Loss: 0.0464\n",
      "Epoch 20/25, Batch 361/2194, Loss: 0.0850\n",
      "Epoch 20/25, Batch 371/2194, Loss: 0.1487\n",
      "Epoch 20/25, Batch 381/2194, Loss: 0.0566\n",
      "Epoch 20/25, Batch 391/2194, Loss: 0.0327\n",
      "Epoch 20/25, Batch 401/2194, Loss: 0.0899\n",
      "Epoch 20/25, Batch 411/2194, Loss: 0.0679\n",
      "Epoch 20/25, Batch 421/2194, Loss: 0.1211\n",
      "Epoch 20/25, Batch 431/2194, Loss: 0.0215\n",
      "Epoch 20/25, Batch 441/2194, Loss: 0.1138\n",
      "Epoch 20/25, Batch 451/2194, Loss: 0.1839\n",
      "Epoch 20/25, Batch 461/2194, Loss: 0.1721\n",
      "Epoch 20/25, Batch 471/2194, Loss: 0.0249\n",
      "Epoch 20/25, Batch 481/2194, Loss: 0.0974\n",
      "Epoch 20/25, Batch 491/2194, Loss: 0.1173\n",
      "Epoch 20/25, Batch 501/2194, Loss: 0.0645\n",
      "Epoch 20/25, Batch 511/2194, Loss: 0.1209\n",
      "Epoch 20/25, Batch 521/2194, Loss: 0.0687\n",
      "Epoch 20/25, Batch 531/2194, Loss: 0.1050\n",
      "Epoch 20/25, Batch 541/2194, Loss: 0.1031\n",
      "Epoch 20/25, Batch 551/2194, Loss: 0.0960\n",
      "Epoch 20/25, Batch 561/2194, Loss: 0.1363\n",
      "Epoch 20/25, Batch 571/2194, Loss: 0.0501\n",
      "Epoch 20/25, Batch 581/2194, Loss: 0.1436\n",
      "Epoch 20/25, Batch 591/2194, Loss: 0.0946\n",
      "Epoch 20/25, Batch 601/2194, Loss: 0.0578\n",
      "Epoch 20/25, Batch 611/2194, Loss: 0.1960\n",
      "Epoch 20/25, Batch 621/2194, Loss: 0.0285\n",
      "Epoch 20/25, Batch 631/2194, Loss: 0.0889\n",
      "Epoch 20/25, Batch 641/2194, Loss: 0.0384\n",
      "Epoch 20/25, Batch 651/2194, Loss: 0.0901\n",
      "Epoch 20/25, Batch 661/2194, Loss: 0.3413\n",
      "Epoch 20/25, Batch 671/2194, Loss: 0.0692\n",
      "Epoch 20/25, Batch 681/2194, Loss: 0.0251\n",
      "Epoch 20/25, Batch 691/2194, Loss: 0.2043\n",
      "Epoch 20/25, Batch 701/2194, Loss: 0.1661\n",
      "Epoch 20/25, Batch 711/2194, Loss: 0.0954\n",
      "Epoch 20/25, Batch 721/2194, Loss: 0.1011\n",
      "Epoch 20/25, Batch 731/2194, Loss: 0.0894\n",
      "Epoch 20/25, Batch 741/2194, Loss: 0.0399\n",
      "Epoch 20/25, Batch 751/2194, Loss: 0.0477\n",
      "Epoch 20/25, Batch 761/2194, Loss: 0.1629\n",
      "Epoch 20/25, Batch 771/2194, Loss: 0.0542\n",
      "Epoch 20/25, Batch 781/2194, Loss: 0.0848\n",
      "Epoch 20/25, Batch 791/2194, Loss: 0.1488\n",
      "Epoch 20/25, Batch 801/2194, Loss: 0.1539\n",
      "Epoch 20/25, Batch 811/2194, Loss: 0.1038\n",
      "Epoch 20/25, Batch 821/2194, Loss: 0.0986\n",
      "Epoch 20/25, Batch 831/2194, Loss: 0.0947\n",
      "Epoch 20/25, Batch 841/2194, Loss: 0.0792\n",
      "Epoch 20/25, Batch 851/2194, Loss: 0.0371\n",
      "Epoch 20/25, Batch 861/2194, Loss: 0.0948\n",
      "Epoch 20/25, Batch 871/2194, Loss: 0.0389\n",
      "Epoch 20/25, Batch 881/2194, Loss: 0.0487\n",
      "Epoch 20/25, Batch 891/2194, Loss: 0.2025\n",
      "Epoch 20/25, Batch 901/2194, Loss: 0.0521\n",
      "Epoch 20/25, Batch 911/2194, Loss: 0.0590\n",
      "Epoch 20/25, Batch 921/2194, Loss: 0.0622\n",
      "Epoch 20/25, Batch 931/2194, Loss: 0.0816\n",
      "Epoch 20/25, Batch 941/2194, Loss: 0.0031\n",
      "Epoch 20/25, Batch 951/2194, Loss: 0.1412\n",
      "Epoch 20/25, Batch 961/2194, Loss: 0.0648\n",
      "Epoch 20/25, Batch 971/2194, Loss: 0.0329\n",
      "Epoch 20/25, Batch 981/2194, Loss: 0.0310\n",
      "Epoch 20/25, Batch 991/2194, Loss: 0.1972\n",
      "Epoch 20/25, Batch 1001/2194, Loss: 0.1238\n",
      "Epoch 20/25, Batch 1011/2194, Loss: 0.3121\n",
      "Epoch 20/25, Batch 1021/2194, Loss: 0.1174\n",
      "Epoch 20/25, Batch 1031/2194, Loss: 0.0866\n",
      "Epoch 20/25, Batch 1041/2194, Loss: 0.0128\n",
      "Epoch 20/25, Batch 1051/2194, Loss: 0.0869\n",
      "Epoch 20/25, Batch 1061/2194, Loss: 0.0942\n",
      "Epoch 20/25, Batch 1071/2194, Loss: 0.0449\n",
      "Epoch 20/25, Batch 1081/2194, Loss: 0.0623\n",
      "Epoch 20/25, Batch 1091/2194, Loss: 0.1941\n",
      "Epoch 20/25, Batch 1101/2194, Loss: 0.1049\n",
      "Epoch 20/25, Batch 1111/2194, Loss: 0.0737\n",
      "Epoch 20/25, Batch 1121/2194, Loss: 0.1662\n",
      "Epoch 20/25, Batch 1131/2194, Loss: 0.0838\n",
      "Epoch 20/25, Batch 1141/2194, Loss: 0.2006\n",
      "Epoch 20/25, Batch 1151/2194, Loss: 0.0475\n",
      "Epoch 20/25, Batch 1161/2194, Loss: 0.0590\n",
      "Epoch 20/25, Batch 1171/2194, Loss: 0.0271\n",
      "Epoch 20/25, Batch 1181/2194, Loss: 0.1421\n",
      "Epoch 20/25, Batch 1191/2194, Loss: 0.1227\n",
      "Epoch 20/25, Batch 1201/2194, Loss: 0.0229\n",
      "Epoch 20/25, Batch 1211/2194, Loss: 0.0989\n",
      "Epoch 20/25, Batch 1221/2194, Loss: 0.0633\n",
      "Epoch 20/25, Batch 1231/2194, Loss: 0.1258\n",
      "Epoch 20/25, Batch 1241/2194, Loss: 0.0831\n",
      "Epoch 20/25, Batch 1251/2194, Loss: 0.1275\n",
      "Epoch 20/25, Batch 1261/2194, Loss: 0.2220\n",
      "Epoch 20/25, Batch 1271/2194, Loss: 0.2243\n",
      "Epoch 20/25, Batch 1281/2194, Loss: 0.1090\n",
      "Epoch 20/25, Batch 1291/2194, Loss: 0.0601\n",
      "Epoch 20/25, Batch 1301/2194, Loss: 0.0497\n",
      "Epoch 20/25, Batch 1311/2194, Loss: 0.0689\n",
      "Epoch 20/25, Batch 1321/2194, Loss: 0.1380\n",
      "Epoch 20/25, Batch 1331/2194, Loss: 0.2447\n",
      "Epoch 20/25, Batch 1341/2194, Loss: 0.0111\n",
      "Epoch 20/25, Batch 1351/2194, Loss: 0.1495\n",
      "Epoch 20/25, Batch 1361/2194, Loss: 0.1363\n",
      "Epoch 20/25, Batch 1371/2194, Loss: 0.1358\n",
      "Epoch 20/25, Batch 1381/2194, Loss: 0.0519\n",
      "Epoch 20/25, Batch 1391/2194, Loss: 0.0555\n",
      "Epoch 20/25, Batch 1401/2194, Loss: 0.0747\n",
      "Epoch 20/25, Batch 1411/2194, Loss: 0.0666\n",
      "Epoch 20/25, Batch 1421/2194, Loss: 0.0499\n",
      "Epoch 20/25, Batch 1431/2194, Loss: 0.1268\n",
      "Epoch 20/25, Batch 1441/2194, Loss: 0.0326\n",
      "Epoch 20/25, Batch 1451/2194, Loss: 0.0304\n",
      "Epoch 20/25, Batch 1461/2194, Loss: 0.2346\n",
      "Epoch 20/25, Batch 1471/2194, Loss: 0.0105\n",
      "Epoch 20/25, Batch 1481/2194, Loss: 0.0578\n",
      "Epoch 20/25, Batch 1491/2194, Loss: 0.0611\n",
      "Epoch 20/25, Batch 1501/2194, Loss: 0.0898\n",
      "Epoch 20/25, Batch 1511/2194, Loss: 0.0117\n",
      "Epoch 20/25, Batch 1521/2194, Loss: 0.0574\n",
      "Epoch 20/25, Batch 1531/2194, Loss: 0.0490\n",
      "Epoch 20/25, Batch 1541/2194, Loss: 0.0218\n",
      "Epoch 20/25, Batch 1551/2194, Loss: 0.0759\n",
      "Epoch 20/25, Batch 1561/2194, Loss: 0.0678\n",
      "Epoch 20/25, Batch 1571/2194, Loss: 0.0606\n",
      "Epoch 20/25, Batch 1581/2194, Loss: 0.2629\n",
      "Epoch 20/25, Batch 1591/2194, Loss: 0.1346\n",
      "Epoch 20/25, Batch 1601/2194, Loss: 0.0300\n",
      "Epoch 20/25, Batch 1611/2194, Loss: 0.0645\n",
      "Epoch 20/25, Batch 1621/2194, Loss: 0.0788\n",
      "Epoch 20/25, Batch 1631/2194, Loss: 0.1538\n",
      "Epoch 20/25, Batch 1641/2194, Loss: 0.0168\n",
      "Epoch 20/25, Batch 1651/2194, Loss: 0.0207\n",
      "Epoch 20/25, Batch 1661/2194, Loss: 0.0119\n",
      "Epoch 20/25, Batch 1671/2194, Loss: 0.0565\n",
      "Epoch 20/25, Batch 1681/2194, Loss: 0.0988\n",
      "Epoch 20/25, Batch 1691/2194, Loss: 0.0775\n",
      "Epoch 20/25, Batch 1701/2194, Loss: 0.0195\n",
      "Epoch 20/25, Batch 1711/2194, Loss: 0.1031\n",
      "Epoch 20/25, Batch 1721/2194, Loss: 0.0567\n",
      "Epoch 20/25, Batch 1731/2194, Loss: 0.1391\n",
      "Epoch 20/25, Batch 1741/2194, Loss: 0.0526\n",
      "Epoch 20/25, Batch 1751/2194, Loss: 0.0703\n",
      "Epoch 20/25, Batch 1761/2194, Loss: 0.0208\n",
      "Epoch 20/25, Batch 1771/2194, Loss: 0.1071\n",
      "Epoch 20/25, Batch 1781/2194, Loss: 0.0084\n",
      "Epoch 20/25, Batch 1791/2194, Loss: 0.0214\n",
      "Epoch 20/25, Batch 1801/2194, Loss: 0.1236\n",
      "Epoch 20/25, Batch 1811/2194, Loss: 0.0559\n",
      "Epoch 20/25, Batch 1821/2194, Loss: 0.0763\n",
      "Epoch 20/25, Batch 1831/2194, Loss: 0.1404\n",
      "Epoch 20/25, Batch 1841/2194, Loss: 0.1268\n",
      "Epoch 20/25, Batch 1851/2194, Loss: 0.1057\n",
      "Epoch 20/25, Batch 1861/2194, Loss: 0.1172\n",
      "Epoch 20/25, Batch 1871/2194, Loss: 0.1569\n",
      "Epoch 20/25, Batch 1881/2194, Loss: 0.0909\n",
      "Epoch 20/25, Batch 1891/2194, Loss: 0.0525\n",
      "Epoch 20/25, Batch 1901/2194, Loss: 0.1400\n",
      "Epoch 20/25, Batch 1911/2194, Loss: 0.0668\n",
      "Epoch 20/25, Batch 1921/2194, Loss: 0.1311\n",
      "Epoch 20/25, Batch 1931/2194, Loss: 0.0350\n",
      "Epoch 20/25, Batch 1941/2194, Loss: 0.0866\n",
      "Epoch 20/25, Batch 1951/2194, Loss: 0.1185\n",
      "Epoch 20/25, Batch 1961/2194, Loss: 0.2616\n",
      "Epoch 20/25, Batch 1971/2194, Loss: 0.1266\n",
      "Epoch 20/25, Batch 1981/2194, Loss: 0.1155\n",
      "Epoch 20/25, Batch 1991/2194, Loss: 0.0738\n",
      "Epoch 20/25, Batch 2001/2194, Loss: 0.1206\n",
      "Epoch 20/25, Batch 2011/2194, Loss: 0.0579\n",
      "Epoch 20/25, Batch 2021/2194, Loss: 0.0638\n",
      "Epoch 20/25, Batch 2031/2194, Loss: 0.0467\n",
      "Epoch 20/25, Batch 2041/2194, Loss: 0.0171\n",
      "Epoch 20/25, Batch 2051/2194, Loss: 0.1152\n",
      "Epoch 20/25, Batch 2061/2194, Loss: 0.3897\n",
      "Epoch 20/25, Batch 2071/2194, Loss: 0.0401\n",
      "Epoch 20/25, Batch 2081/2194, Loss: 0.0809\n",
      "Epoch 20/25, Batch 2091/2194, Loss: 0.0717\n",
      "Epoch 20/25, Batch 2101/2194, Loss: 0.1471\n",
      "Epoch 20/25, Batch 2111/2194, Loss: 0.0909\n",
      "Epoch 20/25, Batch 2121/2194, Loss: 0.0330\n",
      "Epoch 20/25, Batch 2131/2194, Loss: 0.0769\n",
      "Epoch 20/25, Batch 2141/2194, Loss: 0.0747\n",
      "Epoch 20/25, Batch 2151/2194, Loss: 0.0536\n",
      "Epoch 20/25, Batch 2161/2194, Loss: 0.0728\n",
      "Epoch 20/25, Batch 2171/2194, Loss: 0.0303\n",
      "Epoch 20/25, Batch 2181/2194, Loss: 0.1243\n",
      "Epoch 20/25, Batch 2191/2194, Loss: 0.0313\n",
      "Epoch 20/25:\n",
      "Train Loss: 0.1009, Train Acc: 95.83%\n",
      "Val Loss: 0.1607, Val Acc: 93.61%\n",
      "------------------------------------------------------------\n",
      "Starting epoch 21/25...\n",
      "Starting training loop with 2194 batches...\n",
      "Successfully loaded first batch: images shape torch.Size([32, 3, 224, 224]), labels shape torch.Size([32])\n",
      "Epoch 21/25, Batch 1/2194, Loss: 0.0453\n",
      "Epoch 21/25, Batch 11/2194, Loss: 0.0897\n",
      "Epoch 21/25, Batch 21/2194, Loss: 0.1133\n",
      "Epoch 21/25, Batch 31/2194, Loss: 0.2162\n",
      "Epoch 21/25, Batch 41/2194, Loss: 0.0535\n",
      "Epoch 21/25, Batch 51/2194, Loss: 0.1693\n",
      "Epoch 21/25, Batch 61/2194, Loss: 0.1084\n",
      "Epoch 21/25, Batch 71/2194, Loss: 0.1881\n",
      "Epoch 21/25, Batch 81/2194, Loss: 0.0745\n",
      "Epoch 21/25, Batch 91/2194, Loss: 0.1067\n",
      "Epoch 21/25, Batch 101/2194, Loss: 0.0958\n",
      "Epoch 21/25, Batch 111/2194, Loss: 0.0322\n",
      "Epoch 21/25, Batch 121/2194, Loss: 0.1308\n",
      "Epoch 21/25, Batch 131/2194, Loss: 0.1133\n",
      "Epoch 21/25, Batch 141/2194, Loss: 0.2113\n",
      "Epoch 21/25, Batch 151/2194, Loss: 0.0852\n",
      "Epoch 21/25, Batch 161/2194, Loss: 0.1552\n",
      "Epoch 21/25, Batch 171/2194, Loss: 0.0291\n",
      "Epoch 21/25, Batch 181/2194, Loss: 0.1336\n",
      "Epoch 21/25, Batch 191/2194, Loss: 0.0325\n",
      "Epoch 21/25, Batch 201/2194, Loss: 0.0584\n",
      "Epoch 21/25, Batch 211/2194, Loss: 0.0967\n",
      "Epoch 21/25, Batch 221/2194, Loss: 0.0719\n",
      "Epoch 21/25, Batch 231/2194, Loss: 0.1944\n",
      "Epoch 21/25, Batch 241/2194, Loss: 0.0061\n",
      "Epoch 21/25, Batch 251/2194, Loss: 0.1224\n",
      "Epoch 21/25, Batch 261/2194, Loss: 0.1049\n",
      "Epoch 21/25, Batch 271/2194, Loss: 0.1545\n",
      "Epoch 21/25, Batch 281/2194, Loss: 0.0710\n",
      "Epoch 21/25, Batch 291/2194, Loss: 0.0793\n",
      "Epoch 21/25, Batch 301/2194, Loss: 0.0577\n",
      "Epoch 21/25, Batch 311/2194, Loss: 0.0358\n",
      "Epoch 21/25, Batch 321/2194, Loss: 0.1547\n",
      "Epoch 21/25, Batch 331/2194, Loss: 0.0757\n",
      "Epoch 21/25, Batch 341/2194, Loss: 0.0928\n",
      "Epoch 21/25, Batch 351/2194, Loss: 0.0709\n",
      "Epoch 21/25, Batch 361/2194, Loss: 0.0162\n",
      "Epoch 21/25, Batch 371/2194, Loss: 0.0712\n",
      "Epoch 21/25, Batch 381/2194, Loss: 0.0886\n",
      "Epoch 21/25, Batch 391/2194, Loss: 0.1472\n",
      "Epoch 21/25, Batch 401/2194, Loss: 0.1808\n",
      "Epoch 21/25, Batch 411/2194, Loss: 0.2149\n",
      "Epoch 21/25, Batch 421/2194, Loss: 0.0350\n",
      "Epoch 21/25, Batch 431/2194, Loss: 0.2406\n",
      "Epoch 21/25, Batch 441/2194, Loss: 0.0266\n",
      "Epoch 21/25, Batch 451/2194, Loss: 0.1047\n",
      "Epoch 21/25, Batch 461/2194, Loss: 0.0657\n",
      "Epoch 21/25, Batch 471/2194, Loss: 0.1650\n",
      "Epoch 21/25, Batch 481/2194, Loss: 0.1273\n",
      "Epoch 21/25, Batch 491/2194, Loss: 0.0235\n",
      "Epoch 21/25, Batch 501/2194, Loss: 0.1392\n",
      "Epoch 21/25, Batch 511/2194, Loss: 0.0300\n",
      "Epoch 21/25, Batch 521/2194, Loss: 0.0988\n",
      "Epoch 21/25, Batch 531/2194, Loss: 0.0943\n",
      "Epoch 21/25, Batch 541/2194, Loss: 0.1489\n",
      "Epoch 21/25, Batch 551/2194, Loss: 0.0865\n",
      "Epoch 21/25, Batch 561/2194, Loss: 0.1195\n",
      "Epoch 21/25, Batch 571/2194, Loss: 0.0300\n",
      "Epoch 21/25, Batch 581/2194, Loss: 0.2014\n",
      "Epoch 21/25, Batch 591/2194, Loss: 0.2081\n",
      "Epoch 21/25, Batch 601/2194, Loss: 0.0997\n",
      "Epoch 21/25, Batch 611/2194, Loss: 0.0965\n",
      "Epoch 21/25, Batch 621/2194, Loss: 0.0498\n",
      "Epoch 21/25, Batch 631/2194, Loss: 0.1629\n",
      "Epoch 21/25, Batch 641/2194, Loss: 0.0952\n",
      "Epoch 21/25, Batch 651/2194, Loss: 0.2132\n",
      "Epoch 21/25, Batch 661/2194, Loss: 0.1278\n",
      "Epoch 21/25, Batch 671/2194, Loss: 0.0955\n",
      "Epoch 21/25, Batch 681/2194, Loss: 0.0536\n",
      "Epoch 21/25, Batch 691/2194, Loss: 0.0584\n",
      "Epoch 21/25, Batch 701/2194, Loss: 0.1260\n",
      "Epoch 21/25, Batch 711/2194, Loss: 0.2721\n",
      "Epoch 21/25, Batch 721/2194, Loss: 0.1472\n",
      "Epoch 21/25, Batch 731/2194, Loss: 0.1000\n",
      "Epoch 21/25, Batch 741/2194, Loss: 0.0428\n",
      "Epoch 21/25, Batch 751/2194, Loss: 0.1287\n",
      "Epoch 21/25, Batch 761/2194, Loss: 0.0678\n",
      "Epoch 21/25, Batch 771/2194, Loss: 0.1777\n",
      "Epoch 21/25, Batch 781/2194, Loss: 0.0166\n",
      "Epoch 21/25, Batch 791/2194, Loss: 0.0121\n",
      "Epoch 21/25, Batch 801/2194, Loss: 0.0595\n",
      "Epoch 21/25, Batch 811/2194, Loss: 0.0556\n",
      "Epoch 21/25, Batch 821/2194, Loss: 0.1375\n",
      "Epoch 21/25, Batch 831/2194, Loss: 0.0627\n",
      "Epoch 21/25, Batch 841/2194, Loss: 0.0723\n",
      "Epoch 21/25, Batch 851/2194, Loss: 0.0487\n",
      "Epoch 21/25, Batch 861/2194, Loss: 0.0293\n",
      "Epoch 21/25, Batch 871/2194, Loss: 0.0392\n",
      "Epoch 21/25, Batch 881/2194, Loss: 0.1806\n",
      "Epoch 21/25, Batch 891/2194, Loss: 0.0380\n",
      "Epoch 21/25, Batch 901/2194, Loss: 0.0902\n",
      "Epoch 21/25, Batch 911/2194, Loss: 0.0466\n",
      "Epoch 21/25, Batch 921/2194, Loss: 0.2938\n",
      "Epoch 21/25, Batch 931/2194, Loss: 0.0327\n",
      "Epoch 21/25, Batch 941/2194, Loss: 0.0260\n",
      "Epoch 21/25, Batch 951/2194, Loss: 0.1337\n",
      "Epoch 21/25, Batch 961/2194, Loss: 0.0441\n",
      "Epoch 21/25, Batch 971/2194, Loss: 0.1126\n",
      "Epoch 21/25, Batch 981/2194, Loss: 0.1056\n",
      "Epoch 21/25, Batch 991/2194, Loss: 0.0803\n",
      "Epoch 21/25, Batch 1001/2194, Loss: 0.0629\n",
      "Epoch 21/25, Batch 1011/2194, Loss: 0.0303\n",
      "Epoch 21/25, Batch 1021/2194, Loss: 0.0577\n",
      "Epoch 21/25, Batch 1031/2194, Loss: 0.0513\n",
      "Epoch 21/25, Batch 1041/2194, Loss: 0.1123\n",
      "Epoch 21/25, Batch 1051/2194, Loss: 0.1766\n",
      "Epoch 21/25, Batch 1061/2194, Loss: 0.1295\n",
      "Epoch 21/25, Batch 1071/2194, Loss: 0.0739\n",
      "Epoch 21/25, Batch 1081/2194, Loss: 0.1140\n",
      "Epoch 21/25, Batch 1091/2194, Loss: 0.1576\n",
      "Epoch 21/25, Batch 1101/2194, Loss: 0.0592\n",
      "Epoch 21/25, Batch 1111/2194, Loss: 0.1064\n",
      "Epoch 21/25, Batch 1121/2194, Loss: 0.1779\n",
      "Epoch 21/25, Batch 1131/2194, Loss: 0.2039\n",
      "Epoch 21/25, Batch 1141/2194, Loss: 0.0279\n",
      "Epoch 21/25, Batch 1151/2194, Loss: 0.1183\n",
      "Epoch 21/25, Batch 1161/2194, Loss: 0.1516\n",
      "Epoch 21/25, Batch 1171/2194, Loss: 0.0401\n",
      "Epoch 21/25, Batch 1181/2194, Loss: 0.1290\n",
      "Epoch 21/25, Batch 1191/2194, Loss: 0.0800\n",
      "Epoch 21/25, Batch 1201/2194, Loss: 0.0596\n",
      "Epoch 21/25, Batch 1211/2194, Loss: 0.0593\n",
      "Epoch 21/25, Batch 1221/2194, Loss: 0.0536\n",
      "Epoch 21/25, Batch 1231/2194, Loss: 0.0261\n",
      "Epoch 21/25, Batch 1241/2194, Loss: 0.0375\n",
      "Epoch 21/25, Batch 1251/2194, Loss: 0.2190\n",
      "Epoch 21/25, Batch 1261/2194, Loss: 0.2519\n",
      "Epoch 21/25, Batch 1271/2194, Loss: 0.1000\n",
      "Epoch 21/25, Batch 1281/2194, Loss: 0.0230\n",
      "Epoch 21/25, Batch 1291/2194, Loss: 0.2292\n",
      "Epoch 21/25, Batch 1301/2194, Loss: 0.0998\n",
      "Epoch 21/25, Batch 1311/2194, Loss: 0.0794\n",
      "Epoch 21/25, Batch 1321/2194, Loss: 0.1037\n",
      "Epoch 21/25, Batch 1331/2194, Loss: 0.0532\n",
      "Epoch 21/25, Batch 1341/2194, Loss: 0.0772\n",
      "Epoch 21/25, Batch 1351/2194, Loss: 0.1227\n",
      "Epoch 21/25, Batch 1361/2194, Loss: 0.0953\n",
      "Epoch 21/25, Batch 1371/2194, Loss: 0.1389\n",
      "Epoch 21/25, Batch 1381/2194, Loss: 0.0366\n",
      "Epoch 21/25, Batch 1391/2194, Loss: 0.0122\n",
      "Epoch 21/25, Batch 1401/2194, Loss: 0.1613\n",
      "Epoch 21/25, Batch 1411/2194, Loss: 0.0950\n",
      "Epoch 21/25, Batch 1421/2194, Loss: 0.1270\n",
      "Epoch 21/25, Batch 1431/2194, Loss: 0.0278\n",
      "Epoch 21/25, Batch 1441/2194, Loss: 0.0297\n",
      "Epoch 21/25, Batch 1451/2194, Loss: 0.1451\n",
      "Epoch 21/25, Batch 1461/2194, Loss: 0.0569\n",
      "Epoch 21/25, Batch 1471/2194, Loss: 0.0828\n",
      "Epoch 21/25, Batch 1481/2194, Loss: 0.2312\n",
      "Epoch 21/25, Batch 1491/2194, Loss: 0.1002\n",
      "Epoch 21/25, Batch 1501/2194, Loss: 0.0259\n",
      "Epoch 21/25, Batch 1511/2194, Loss: 0.2396\n",
      "Epoch 21/25, Batch 1521/2194, Loss: 0.1056\n",
      "Epoch 21/25, Batch 1531/2194, Loss: 0.0640\n",
      "Epoch 21/25, Batch 1541/2194, Loss: 0.0135\n",
      "Epoch 21/25, Batch 1551/2194, Loss: 0.1322\n",
      "Epoch 21/25, Batch 1561/2194, Loss: 0.1046\n",
      "Epoch 21/25, Batch 1571/2194, Loss: 0.1346\n",
      "Epoch 21/25, Batch 1581/2194, Loss: 0.0749\n",
      "Epoch 21/25, Batch 1591/2194, Loss: 0.0359\n",
      "Epoch 21/25, Batch 1601/2194, Loss: 0.1702\n",
      "Epoch 21/25, Batch 1611/2194, Loss: 0.0528\n",
      "Epoch 21/25, Batch 1621/2194, Loss: 0.0546\n",
      "Epoch 21/25, Batch 1631/2194, Loss: 0.1182\n",
      "Epoch 21/25, Batch 1641/2194, Loss: 0.2299\n",
      "Epoch 21/25, Batch 1651/2194, Loss: 0.2098\n",
      "Epoch 21/25, Batch 1661/2194, Loss: 0.0652\n",
      "Epoch 21/25, Batch 1671/2194, Loss: 0.1032\n",
      "Epoch 21/25, Batch 1681/2194, Loss: 0.0678\n",
      "Epoch 21/25, Batch 1691/2194, Loss: 0.0570\n",
      "Epoch 21/25, Batch 1701/2194, Loss: 0.0503\n",
      "Epoch 21/25, Batch 1711/2194, Loss: 0.0802\n",
      "Epoch 21/25, Batch 1721/2194, Loss: 0.1424\n",
      "Epoch 21/25, Batch 1731/2194, Loss: 0.1766\n",
      "Epoch 21/25, Batch 1741/2194, Loss: 0.0215\n",
      "Epoch 21/25, Batch 1751/2194, Loss: 0.2135\n",
      "Epoch 21/25, Batch 1761/2194, Loss: 0.1198\n",
      "Epoch 21/25, Batch 1771/2194, Loss: 0.1001\n",
      "Epoch 21/25, Batch 1781/2194, Loss: 0.1908\n",
      "Epoch 21/25, Batch 1791/2194, Loss: 0.0282\n",
      "Epoch 21/25, Batch 1801/2194, Loss: 0.1549\n",
      "Epoch 21/25, Batch 1811/2194, Loss: 0.0545\n",
      "Epoch 21/25, Batch 1821/2194, Loss: 0.0627\n",
      "Epoch 21/25, Batch 1831/2194, Loss: 0.0317\n",
      "Epoch 21/25, Batch 1841/2194, Loss: 0.1683\n",
      "Epoch 21/25, Batch 1851/2194, Loss: 0.1033\n",
      "Epoch 21/25, Batch 1861/2194, Loss: 0.1063\n",
      "Epoch 21/25, Batch 1871/2194, Loss: 0.1338\n",
      "Epoch 21/25, Batch 1881/2194, Loss: 0.1522\n",
      "Epoch 21/25, Batch 1891/2194, Loss: 0.0570\n",
      "Epoch 21/25, Batch 1901/2194, Loss: 0.0770\n",
      "Epoch 21/25, Batch 1911/2194, Loss: 0.1060\n",
      "Epoch 21/25, Batch 1921/2194, Loss: 0.1809\n",
      "Epoch 21/25, Batch 1931/2194, Loss: 0.0056\n",
      "Epoch 21/25, Batch 1941/2194, Loss: 0.1182\n",
      "Epoch 21/25, Batch 1951/2194, Loss: 0.0081\n",
      "Epoch 21/25, Batch 1961/2194, Loss: 0.0781\n",
      "Epoch 21/25, Batch 1971/2194, Loss: 0.1359\n",
      "Epoch 21/25, Batch 1981/2194, Loss: 0.1031\n",
      "Epoch 21/25, Batch 1991/2194, Loss: 0.0220\n",
      "Epoch 21/25, Batch 2001/2194, Loss: 0.0583\n",
      "Epoch 21/25, Batch 2011/2194, Loss: 0.1816\n",
      "Epoch 21/25, Batch 2021/2194, Loss: 0.0769\n",
      "Epoch 21/25, Batch 2031/2194, Loss: 0.0623\n",
      "Epoch 21/25, Batch 2041/2194, Loss: 0.0453\n",
      "Epoch 21/25, Batch 2051/2194, Loss: 0.1211\n",
      "Epoch 21/25, Batch 2061/2194, Loss: 0.0948\n",
      "Epoch 21/25, Batch 2071/2194, Loss: 0.1476\n",
      "Epoch 21/25, Batch 2081/2194, Loss: 0.0910\n",
      "Epoch 21/25, Batch 2091/2194, Loss: 0.0951\n",
      "Epoch 21/25, Batch 2101/2194, Loss: 0.1033\n",
      "Epoch 21/25, Batch 2111/2194, Loss: 0.0374\n",
      "Epoch 21/25, Batch 2121/2194, Loss: 0.1002\n",
      "Epoch 21/25, Batch 2131/2194, Loss: 0.0709\n",
      "Epoch 21/25, Batch 2141/2194, Loss: 0.0145\n",
      "Epoch 21/25, Batch 2151/2194, Loss: 0.3508\n",
      "Epoch 21/25, Batch 2161/2194, Loss: 0.0722\n",
      "Epoch 21/25, Batch 2171/2194, Loss: 0.1505\n",
      "Epoch 21/25, Batch 2181/2194, Loss: 0.1200\n",
      "Epoch 21/25, Batch 2191/2194, Loss: 0.0314\n",
      "Epoch 21/25:\n",
      "Train Loss: 0.0968, Train Acc: 95.90%\n",
      "Val Loss: 0.1715, Val Acc: 93.16%\n",
      "------------------------------------------------------------\n",
      "Starting epoch 22/25...\n",
      "Starting training loop with 2194 batches...\n",
      "Successfully loaded first batch: images shape torch.Size([32, 3, 224, 224]), labels shape torch.Size([32])\n",
      "Epoch 22/25, Batch 1/2194, Loss: 0.0886\n",
      "Epoch 22/25, Batch 11/2194, Loss: 0.0586\n",
      "Epoch 22/25, Batch 21/2194, Loss: 0.0665\n",
      "Epoch 22/25, Batch 31/2194, Loss: 0.0188\n",
      "Epoch 22/25, Batch 41/2194, Loss: 0.0534\n",
      "Epoch 22/25, Batch 51/2194, Loss: 0.0115\n",
      "Epoch 22/25, Batch 61/2194, Loss: 0.1374\n",
      "Epoch 22/25, Batch 71/2194, Loss: 0.0190\n",
      "Epoch 22/25, Batch 81/2194, Loss: 0.0705\n",
      "Epoch 22/25, Batch 91/2194, Loss: 0.0806\n",
      "Epoch 22/25, Batch 101/2194, Loss: 0.0558\n",
      "Epoch 22/25, Batch 111/2194, Loss: 0.0292\n",
      "Epoch 22/25, Batch 121/2194, Loss: 0.0855\n",
      "Epoch 22/25, Batch 131/2194, Loss: 0.0018\n",
      "Epoch 22/25, Batch 141/2194, Loss: 0.0679\n",
      "Epoch 22/25, Batch 151/2194, Loss: 0.1441\n",
      "Epoch 22/25, Batch 161/2194, Loss: 0.1308\n",
      "Epoch 22/25, Batch 171/2194, Loss: 0.3037\n",
      "Epoch 22/25, Batch 181/2194, Loss: 0.1019\n",
      "Epoch 22/25, Batch 191/2194, Loss: 0.1164\n",
      "Epoch 22/25, Batch 201/2194, Loss: 0.1545\n",
      "Epoch 22/25, Batch 211/2194, Loss: 0.1992\n",
      "Epoch 22/25, Batch 221/2194, Loss: 0.0233\n",
      "Epoch 22/25, Batch 231/2194, Loss: 0.0815\n",
      "Epoch 22/25, Batch 241/2194, Loss: 0.0846\n",
      "Epoch 22/25, Batch 251/2194, Loss: 0.1508\n",
      "Epoch 22/25, Batch 261/2194, Loss: 0.2157\n",
      "Epoch 22/25, Batch 271/2194, Loss: 0.0566\n",
      "Epoch 22/25, Batch 281/2194, Loss: 0.0801\n",
      "Epoch 22/25, Batch 291/2194, Loss: 0.1827\n",
      "Epoch 22/25, Batch 301/2194, Loss: 0.0902\n",
      "Epoch 22/25, Batch 311/2194, Loss: 0.0455\n",
      "Epoch 22/25, Batch 321/2194, Loss: 0.0719\n",
      "Epoch 22/25, Batch 331/2194, Loss: 0.0187\n",
      "Epoch 22/25, Batch 341/2194, Loss: 0.0678\n",
      "Epoch 22/25, Batch 351/2194, Loss: 0.2275\n",
      "Epoch 22/25, Batch 361/2194, Loss: 0.0318\n",
      "Epoch 22/25, Batch 371/2194, Loss: 0.1097\n",
      "Epoch 22/25, Batch 381/2194, Loss: 0.0448\n",
      "Epoch 22/25, Batch 391/2194, Loss: 0.1027\n",
      "Epoch 22/25, Batch 401/2194, Loss: 0.1447\n",
      "Epoch 22/25, Batch 411/2194, Loss: 0.0494\n",
      "Epoch 22/25, Batch 421/2194, Loss: 0.0540\n",
      "Epoch 22/25, Batch 431/2194, Loss: 0.0854\n",
      "Epoch 22/25, Batch 441/2194, Loss: 0.0489\n",
      "Epoch 22/25, Batch 451/2194, Loss: 0.2471\n",
      "Epoch 22/25, Batch 461/2194, Loss: 0.0554\n",
      "Epoch 22/25, Batch 471/2194, Loss: 0.0756\n",
      "Epoch 22/25, Batch 481/2194, Loss: 0.0195\n",
      "Epoch 22/25, Batch 491/2194, Loss: 0.0391\n",
      "Epoch 22/25, Batch 501/2194, Loss: 0.0914\n",
      "Epoch 22/25, Batch 511/2194, Loss: 0.0774\n",
      "Epoch 22/25, Batch 521/2194, Loss: 0.0383\n",
      "Epoch 22/25, Batch 531/2194, Loss: 0.2514\n",
      "Epoch 22/25, Batch 541/2194, Loss: 0.0787\n",
      "Epoch 22/25, Batch 551/2194, Loss: 0.1017\n",
      "Epoch 22/25, Batch 561/2194, Loss: 0.0699\n",
      "Epoch 22/25, Batch 571/2194, Loss: 0.0764\n",
      "Epoch 22/25, Batch 581/2194, Loss: 0.0798\n",
      "Epoch 22/25, Batch 591/2194, Loss: 0.0365\n",
      "Epoch 22/25, Batch 601/2194, Loss: 0.0962\n",
      "Epoch 22/25, Batch 611/2194, Loss: 0.1403\n",
      "Epoch 22/25, Batch 621/2194, Loss: 0.0578\n",
      "Epoch 22/25, Batch 631/2194, Loss: 0.1774\n",
      "Epoch 22/25, Batch 641/2194, Loss: 0.0357\n",
      "Epoch 22/25, Batch 651/2194, Loss: 0.0720\n",
      "Epoch 22/25, Batch 661/2194, Loss: 0.1659\n",
      "Epoch 22/25, Batch 671/2194, Loss: 0.0781\n",
      "Epoch 22/25, Batch 681/2194, Loss: 0.1015\n",
      "Epoch 22/25, Batch 691/2194, Loss: 0.0902\n",
      "Epoch 22/25, Batch 701/2194, Loss: 0.0867\n",
      "Epoch 22/25, Batch 711/2194, Loss: 0.0556\n",
      "Epoch 22/25, Batch 721/2194, Loss: 0.1586\n",
      "Epoch 22/25, Batch 731/2194, Loss: 0.0488\n",
      "Epoch 22/25, Batch 741/2194, Loss: 0.0629\n",
      "Epoch 22/25, Batch 751/2194, Loss: 0.0129\n",
      "Epoch 22/25, Batch 761/2194, Loss: 0.0201\n",
      "Epoch 22/25, Batch 771/2194, Loss: 0.0919\n",
      "Epoch 22/25, Batch 781/2194, Loss: 0.0769\n",
      "Epoch 22/25, Batch 791/2194, Loss: 0.0723\n",
      "Epoch 22/25, Batch 801/2194, Loss: 0.0346\n",
      "Epoch 22/25, Batch 811/2194, Loss: 0.0751\n",
      "Epoch 22/25, Batch 821/2194, Loss: 0.0207\n",
      "Epoch 22/25, Batch 831/2194, Loss: 0.0275\n",
      "Epoch 22/25, Batch 841/2194, Loss: 0.2150\n",
      "Epoch 22/25, Batch 851/2194, Loss: 0.1911\n",
      "Epoch 22/25, Batch 861/2194, Loss: 0.0141\n",
      "Epoch 22/25, Batch 871/2194, Loss: 0.0232\n",
      "Epoch 22/25, Batch 881/2194, Loss: 0.1145\n",
      "Epoch 22/25, Batch 891/2194, Loss: 0.1053\n",
      "Epoch 22/25, Batch 901/2194, Loss: 0.0047\n",
      "Epoch 22/25, Batch 911/2194, Loss: 0.0249\n",
      "Epoch 22/25, Batch 921/2194, Loss: 0.0721\n",
      "Epoch 22/25, Batch 931/2194, Loss: 0.1017\n",
      "Epoch 22/25, Batch 941/2194, Loss: 0.0866\n",
      "Epoch 22/25, Batch 951/2194, Loss: 0.0848\n",
      "Epoch 22/25, Batch 961/2194, Loss: 0.0436\n",
      "Epoch 22/25, Batch 971/2194, Loss: 0.0995\n",
      "Epoch 22/25, Batch 981/2194, Loss: 0.0604\n",
      "Epoch 22/25, Batch 991/2194, Loss: 0.0401\n",
      "Epoch 22/25, Batch 1001/2194, Loss: 0.1361\n",
      "Epoch 22/25, Batch 1011/2194, Loss: 0.2286\n",
      "Epoch 22/25, Batch 1021/2194, Loss: 0.0569\n",
      "Epoch 22/25, Batch 1031/2194, Loss: 0.1052\n",
      "Epoch 22/25, Batch 1041/2194, Loss: 0.0408\n",
      "Epoch 22/25, Batch 1051/2194, Loss: 0.0109\n",
      "Epoch 22/25, Batch 1061/2194, Loss: 0.0780\n",
      "Epoch 22/25, Batch 1071/2194, Loss: 0.1311\n",
      "Epoch 22/25, Batch 1081/2194, Loss: 0.0149\n",
      "Epoch 22/25, Batch 1091/2194, Loss: 0.0532\n",
      "Epoch 22/25, Batch 1101/2194, Loss: 0.0752\n",
      "Epoch 22/25, Batch 1111/2194, Loss: 0.0073\n",
      "Epoch 22/25, Batch 1121/2194, Loss: 0.0562\n",
      "Epoch 22/25, Batch 1131/2194, Loss: 0.0762\n",
      "Epoch 22/25, Batch 1141/2194, Loss: 0.1071\n",
      "Epoch 22/25, Batch 1151/2194, Loss: 0.0763\n",
      "Epoch 22/25, Batch 1161/2194, Loss: 0.1359\n",
      "Epoch 22/25, Batch 1171/2194, Loss: 0.0202\n",
      "Epoch 22/25, Batch 1181/2194, Loss: 0.0961\n",
      "Epoch 22/25, Batch 1191/2194, Loss: 0.1106\n",
      "Epoch 22/25, Batch 1201/2194, Loss: 0.0449\n",
      "Epoch 22/25, Batch 1211/2194, Loss: 0.1709\n",
      "Epoch 22/25, Batch 1221/2194, Loss: 0.0157\n",
      "Epoch 22/25, Batch 1231/2194, Loss: 0.1494\n",
      "Epoch 22/25, Batch 1241/2194, Loss: 0.0750\n",
      "Epoch 22/25, Batch 1251/2194, Loss: 0.0915\n",
      "Epoch 22/25, Batch 1261/2194, Loss: 0.0416\n",
      "Epoch 22/25, Batch 1271/2194, Loss: 0.0980\n",
      "Epoch 22/25, Batch 1281/2194, Loss: 0.0244\n",
      "Epoch 22/25, Batch 1291/2194, Loss: 0.2487\n",
      "Epoch 22/25, Batch 1301/2194, Loss: 0.0532\n",
      "Epoch 22/25, Batch 1311/2194, Loss: 0.1869\n",
      "Epoch 22/25, Batch 1321/2194, Loss: 0.0571\n",
      "Epoch 22/25, Batch 1331/2194, Loss: 0.0823\n",
      "Epoch 22/25, Batch 1341/2194, Loss: 0.0078\n",
      "Epoch 22/25, Batch 1351/2194, Loss: 0.0684\n",
      "Epoch 22/25, Batch 1361/2194, Loss: 0.1090\n",
      "Epoch 22/25, Batch 1371/2194, Loss: 0.0748\n",
      "Epoch 22/25, Batch 1381/2194, Loss: 0.1896\n",
      "Epoch 22/25, Batch 1391/2194, Loss: 0.1899\n",
      "Epoch 22/25, Batch 1401/2194, Loss: 0.1378\n",
      "Epoch 22/25, Batch 1411/2194, Loss: 0.0735\n",
      "Epoch 22/25, Batch 1421/2194, Loss: 0.0664\n",
      "Epoch 22/25, Batch 1431/2194, Loss: 0.0827\n",
      "Epoch 22/25, Batch 1441/2194, Loss: 0.0594\n",
      "Epoch 22/25, Batch 1451/2194, Loss: 0.2134\n",
      "Epoch 22/25, Batch 1461/2194, Loss: 0.0130\n",
      "Epoch 22/25, Batch 1471/2194, Loss: 0.0806\n",
      "Epoch 22/25, Batch 1481/2194, Loss: 0.0174\n",
      "Epoch 22/25, Batch 1491/2194, Loss: 0.0432\n",
      "Epoch 22/25, Batch 1501/2194, Loss: 0.1361\n",
      "Epoch 22/25, Batch 1511/2194, Loss: 0.0707\n",
      "Epoch 22/25, Batch 1521/2194, Loss: 0.1028\n",
      "Epoch 22/25, Batch 1531/2194, Loss: 0.0189\n",
      "Epoch 22/25, Batch 1541/2194, Loss: 0.1262\n",
      "Epoch 22/25, Batch 1551/2194, Loss: 0.1263\n",
      "Epoch 22/25, Batch 1561/2194, Loss: 0.1339\n",
      "Epoch 22/25, Batch 1571/2194, Loss: 0.0321\n",
      "Epoch 22/25, Batch 1581/2194, Loss: 0.2358\n",
      "Epoch 22/25, Batch 1591/2194, Loss: 0.0936\n",
      "Epoch 22/25, Batch 1601/2194, Loss: 0.0876\n",
      "Epoch 22/25, Batch 1611/2194, Loss: 0.0396\n",
      "Epoch 22/25, Batch 1621/2194, Loss: 0.0742\n",
      "Epoch 22/25, Batch 1631/2194, Loss: 0.0789\n",
      "Epoch 22/25, Batch 1641/2194, Loss: 0.0336\n",
      "Epoch 22/25, Batch 1651/2194, Loss: 0.0932\n",
      "Epoch 22/25, Batch 1661/2194, Loss: 0.0475\n",
      "Epoch 22/25, Batch 1671/2194, Loss: 0.1502\n",
      "Epoch 22/25, Batch 1681/2194, Loss: 0.1463\n",
      "Epoch 22/25, Batch 1691/2194, Loss: 0.2265\n",
      "Epoch 22/25, Batch 1701/2194, Loss: 0.1169\n",
      "Epoch 22/25, Batch 1711/2194, Loss: 0.0237\n",
      "Epoch 22/25, Batch 1721/2194, Loss: 0.0273\n",
      "Epoch 22/25, Batch 1731/2194, Loss: 0.0747\n",
      "Epoch 22/25, Batch 1741/2194, Loss: 0.0537\n",
      "Epoch 22/25, Batch 1751/2194, Loss: 0.0921\n",
      "Epoch 22/25, Batch 1761/2194, Loss: 0.1589\n",
      "Epoch 22/25, Batch 1771/2194, Loss: 0.0479\n",
      "Epoch 22/25, Batch 1781/2194, Loss: 0.1046\n",
      "Epoch 22/25, Batch 1791/2194, Loss: 0.2403\n",
      "Epoch 22/25, Batch 1801/2194, Loss: 0.0454\n",
      "Epoch 22/25, Batch 1811/2194, Loss: 0.1327\n",
      "Epoch 22/25, Batch 1821/2194, Loss: 0.2299\n",
      "Epoch 22/25, Batch 1831/2194, Loss: 0.0523\n",
      "Epoch 22/25, Batch 1841/2194, Loss: 0.1107\n",
      "Epoch 22/25, Batch 1851/2194, Loss: 0.0850\n",
      "Epoch 22/25, Batch 1861/2194, Loss: 0.3262\n",
      "Epoch 22/25, Batch 1871/2194, Loss: 0.0662\n",
      "Epoch 22/25, Batch 1881/2194, Loss: 0.0318\n",
      "Epoch 22/25, Batch 1891/2194, Loss: 0.0762\n",
      "Epoch 22/25, Batch 1901/2194, Loss: 0.0949\n",
      "Epoch 22/25, Batch 1911/2194, Loss: 0.0993\n",
      "Epoch 22/25, Batch 1921/2194, Loss: 0.1589\n",
      "Epoch 22/25, Batch 1931/2194, Loss: 0.0899\n",
      "Epoch 22/25, Batch 1941/2194, Loss: 0.1145\n",
      "Epoch 22/25, Batch 1951/2194, Loss: 0.0606\n",
      "Epoch 22/25, Batch 1961/2194, Loss: 0.0336\n",
      "Epoch 22/25, Batch 1971/2194, Loss: 0.0723\n",
      "Epoch 22/25, Batch 1981/2194, Loss: 0.0627\n",
      "Epoch 22/25, Batch 1991/2194, Loss: 0.1476\n",
      "Epoch 22/25, Batch 2001/2194, Loss: 0.0829\n",
      "Epoch 22/25, Batch 2011/2194, Loss: 0.0088\n",
      "Epoch 22/25, Batch 2021/2194, Loss: 0.1713\n",
      "Epoch 22/25, Batch 2031/2194, Loss: 0.0454\n",
      "Epoch 22/25, Batch 2041/2194, Loss: 0.0105\n",
      "Epoch 22/25, Batch 2051/2194, Loss: 0.1086\n",
      "Epoch 22/25, Batch 2061/2194, Loss: 0.1638\n",
      "Epoch 22/25, Batch 2071/2194, Loss: 0.1431\n",
      "Epoch 22/25, Batch 2081/2194, Loss: 0.2320\n",
      "Epoch 22/25, Batch 2091/2194, Loss: 0.0968\n",
      "Epoch 22/25, Batch 2101/2194, Loss: 0.1027\n",
      "Epoch 22/25, Batch 2111/2194, Loss: 0.1536\n",
      "Epoch 22/25, Batch 2121/2194, Loss: 0.1738\n",
      "Epoch 22/25, Batch 2131/2194, Loss: 0.2872\n",
      "Epoch 22/25, Batch 2141/2194, Loss: 0.0760\n",
      "Epoch 22/25, Batch 2151/2194, Loss: 0.1108\n",
      "Epoch 22/25, Batch 2161/2194, Loss: 0.0725\n",
      "Epoch 22/25, Batch 2171/2194, Loss: 0.0693\n",
      "Epoch 22/25, Batch 2181/2194, Loss: 0.1617\n",
      "Epoch 22/25, Batch 2191/2194, Loss: 0.0913\n",
      "Epoch 22/25:\n",
      "Train Loss: 0.0918, Train Acc: 96.26%\n",
      "Val Loss: 0.1622, Val Acc: 93.53%\n",
      "------------------------------------------------------------\n",
      "Starting epoch 23/25...\n",
      "Starting training loop with 2194 batches...\n",
      "Successfully loaded first batch: images shape torch.Size([32, 3, 224, 224]), labels shape torch.Size([32])\n",
      "Epoch 23/25, Batch 1/2194, Loss: 0.0247\n",
      "Epoch 23/25, Batch 11/2194, Loss: 0.0219\n",
      "Epoch 23/25, Batch 21/2194, Loss: 0.0901\n",
      "Epoch 23/25, Batch 31/2194, Loss: 0.1435\n",
      "Epoch 23/25, Batch 41/2194, Loss: 0.0717\n",
      "Epoch 23/25, Batch 51/2194, Loss: 0.0342\n",
      "Epoch 23/25, Batch 61/2194, Loss: 0.0837\n",
      "Epoch 23/25, Batch 71/2194, Loss: 0.0694\n",
      "Epoch 23/25, Batch 81/2194, Loss: 0.0800\n",
      "Epoch 23/25, Batch 91/2194, Loss: 0.0491\n",
      "Epoch 23/25, Batch 101/2194, Loss: 0.1903\n",
      "Epoch 23/25, Batch 111/2194, Loss: 0.0659\n",
      "Epoch 23/25, Batch 121/2194, Loss: 0.0891\n",
      "Epoch 23/25, Batch 131/2194, Loss: 0.1085\n",
      "Epoch 23/25, Batch 141/2194, Loss: 0.0273\n",
      "Epoch 23/25, Batch 151/2194, Loss: 0.0597\n",
      "Epoch 23/25, Batch 161/2194, Loss: 0.0678\n",
      "Epoch 23/25, Batch 171/2194, Loss: 0.0669\n",
      "Epoch 23/25, Batch 181/2194, Loss: 0.0374\n",
      "Epoch 23/25, Batch 191/2194, Loss: 0.0125\n",
      "Epoch 23/25, Batch 201/2194, Loss: 0.0058\n",
      "Epoch 23/25, Batch 211/2194, Loss: 0.0571\n",
      "Epoch 23/25, Batch 221/2194, Loss: 0.0161\n",
      "Epoch 23/25, Batch 231/2194, Loss: 0.1444\n",
      "Epoch 23/25, Batch 241/2194, Loss: 0.0703\n",
      "Epoch 23/25, Batch 251/2194, Loss: 0.0580\n",
      "Epoch 23/25, Batch 261/2194, Loss: 0.0241\n",
      "Epoch 23/25, Batch 271/2194, Loss: 0.0503\n",
      "Epoch 23/25, Batch 281/2194, Loss: 0.2211\n",
      "Epoch 23/25, Batch 291/2194, Loss: 0.1180\n",
      "Epoch 23/25, Batch 301/2194, Loss: 0.0727\n",
      "Epoch 23/25, Batch 311/2194, Loss: 0.1064\n",
      "Epoch 23/25, Batch 321/2194, Loss: 0.0738\n",
      "Epoch 23/25, Batch 331/2194, Loss: 0.0905\n",
      "Epoch 23/25, Batch 341/2194, Loss: 0.0272\n",
      "Epoch 23/25, Batch 351/2194, Loss: 0.0394\n",
      "Epoch 23/25, Batch 361/2194, Loss: 0.2945\n",
      "Epoch 23/25, Batch 371/2194, Loss: 0.0411\n",
      "Epoch 23/25, Batch 381/2194, Loss: 0.0416\n",
      "Epoch 23/25, Batch 391/2194, Loss: 0.0727\n",
      "Epoch 23/25, Batch 401/2194, Loss: 0.0253\n",
      "Epoch 23/25, Batch 411/2194, Loss: 0.0771\n",
      "Epoch 23/25, Batch 421/2194, Loss: 0.1404\n",
      "Epoch 23/25, Batch 431/2194, Loss: 0.0621\n",
      "Epoch 23/25, Batch 441/2194, Loss: 0.1697\n",
      "Epoch 23/25, Batch 451/2194, Loss: 0.1216\n",
      "Epoch 23/25, Batch 461/2194, Loss: 0.0322\n",
      "Epoch 23/25, Batch 471/2194, Loss: 0.0454\n",
      "Epoch 23/25, Batch 481/2194, Loss: 0.0532\n",
      "Epoch 23/25, Batch 491/2194, Loss: 0.0644\n",
      "Epoch 23/25, Batch 501/2194, Loss: 0.0115\n",
      "Epoch 23/25, Batch 511/2194, Loss: 0.0986\n",
      "Epoch 23/25, Batch 521/2194, Loss: 0.0590\n",
      "Epoch 23/25, Batch 531/2194, Loss: 0.0816\n",
      "Epoch 23/25, Batch 541/2194, Loss: 0.0985\n",
      "Epoch 23/25, Batch 551/2194, Loss: 0.1043\n",
      "Epoch 23/25, Batch 561/2194, Loss: 0.0279\n",
      "Epoch 23/25, Batch 571/2194, Loss: 0.0411\n",
      "Epoch 23/25, Batch 581/2194, Loss: 0.0240\n",
      "Epoch 23/25, Batch 591/2194, Loss: 0.1644\n",
      "Epoch 23/25, Batch 601/2194, Loss: 0.1016\n",
      "Epoch 23/25, Batch 611/2194, Loss: 0.0293\n",
      "Epoch 23/25, Batch 621/2194, Loss: 0.1599\n",
      "Epoch 23/25, Batch 631/2194, Loss: 0.0776\n",
      "Epoch 23/25, Batch 641/2194, Loss: 0.0441\n",
      "Epoch 23/25, Batch 651/2194, Loss: 0.0724\n",
      "Epoch 23/25, Batch 661/2194, Loss: 0.0565\n",
      "Epoch 23/25, Batch 671/2194, Loss: 0.0359\n",
      "Epoch 23/25, Batch 681/2194, Loss: 0.0049\n",
      "Epoch 23/25, Batch 691/2194, Loss: 0.0884\n",
      "Epoch 23/25, Batch 701/2194, Loss: 0.1503\n",
      "Epoch 23/25, Batch 711/2194, Loss: 0.1188\n",
      "Epoch 23/25, Batch 721/2194, Loss: 0.0508\n",
      "Epoch 23/25, Batch 731/2194, Loss: 0.1024\n",
      "Epoch 23/25, Batch 741/2194, Loss: 0.0271\n",
      "Epoch 23/25, Batch 751/2194, Loss: 0.0582\n",
      "Epoch 23/25, Batch 761/2194, Loss: 0.0217\n",
      "Epoch 23/25, Batch 771/2194, Loss: 0.1058\n",
      "Epoch 23/25, Batch 781/2194, Loss: 0.0739\n",
      "Epoch 23/25, Batch 791/2194, Loss: 0.0877\n",
      "Epoch 23/25, Batch 801/2194, Loss: 0.0416\n",
      "Epoch 23/25, Batch 811/2194, Loss: 0.0943\n",
      "Epoch 23/25, Batch 821/2194, Loss: 0.0668\n",
      "Epoch 23/25, Batch 831/2194, Loss: 0.2214\n",
      "Epoch 23/25, Batch 841/2194, Loss: 0.0905\n",
      "Epoch 23/25, Batch 851/2194, Loss: 0.0609\n",
      "Epoch 23/25, Batch 861/2194, Loss: 0.0687\n",
      "Epoch 23/25, Batch 871/2194, Loss: 0.1415\n",
      "Epoch 23/25, Batch 881/2194, Loss: 0.0061\n",
      "Epoch 23/25, Batch 891/2194, Loss: 0.0268\n",
      "Epoch 23/25, Batch 901/2194, Loss: 0.0634\n",
      "Epoch 23/25, Batch 911/2194, Loss: 0.0570\n",
      "Epoch 23/25, Batch 921/2194, Loss: 0.2198\n",
      "Epoch 23/25, Batch 931/2194, Loss: 0.0794\n",
      "Epoch 23/25, Batch 941/2194, Loss: 0.1117\n",
      "Epoch 23/25, Batch 951/2194, Loss: 0.0220\n",
      "Epoch 23/25, Batch 961/2194, Loss: 0.0077\n",
      "Epoch 23/25, Batch 971/2194, Loss: 0.1164\n",
      "Epoch 23/25, Batch 981/2194, Loss: 0.1558\n",
      "Epoch 23/25, Batch 991/2194, Loss: 0.0977\n",
      "Epoch 23/25, Batch 1001/2194, Loss: 0.0445\n",
      "Epoch 23/25, Batch 1011/2194, Loss: 0.1275\n",
      "Epoch 23/25, Batch 1021/2194, Loss: 0.0787\n",
      "Epoch 23/25, Batch 1031/2194, Loss: 0.0571\n",
      "Epoch 23/25, Batch 1041/2194, Loss: 0.1991\n",
      "Epoch 23/25, Batch 1051/2194, Loss: 0.0259\n",
      "Epoch 23/25, Batch 1061/2194, Loss: 0.0532\n",
      "Epoch 23/25, Batch 1071/2194, Loss: 0.1119\n",
      "Epoch 23/25, Batch 1081/2194, Loss: 0.0167\n",
      "Epoch 23/25, Batch 1091/2194, Loss: 0.0439\n",
      "Epoch 23/25, Batch 1101/2194, Loss: 0.0477\n",
      "Epoch 23/25, Batch 1111/2194, Loss: 0.0256\n",
      "Epoch 23/25, Batch 1121/2194, Loss: 0.0546\n",
      "Epoch 23/25, Batch 1131/2194, Loss: 0.0302\n",
      "Epoch 23/25, Batch 1141/2194, Loss: 0.0721\n",
      "Epoch 23/25, Batch 1151/2194, Loss: 0.0559\n",
      "Epoch 23/25, Batch 1161/2194, Loss: 0.1767\n",
      "Epoch 23/25, Batch 1171/2194, Loss: 0.1099\n",
      "Epoch 23/25, Batch 1181/2194, Loss: 0.0580\n",
      "Epoch 23/25, Batch 1191/2194, Loss: 0.0449\n",
      "Epoch 23/25, Batch 1201/2194, Loss: 0.0615\n",
      "Epoch 23/25, Batch 1211/2194, Loss: 0.0191\n",
      "Epoch 23/25, Batch 1221/2194, Loss: 0.0947\n",
      "Epoch 23/25, Batch 1231/2194, Loss: 0.1220\n",
      "Epoch 23/25, Batch 1241/2194, Loss: 0.0193\n",
      "Epoch 23/25, Batch 1251/2194, Loss: 0.0271\n",
      "Epoch 23/25, Batch 1261/2194, Loss: 0.0290\n",
      "Epoch 23/25, Batch 1271/2194, Loss: 0.1534\n",
      "Epoch 23/25, Batch 1281/2194, Loss: 0.0363\n",
      "Epoch 23/25, Batch 1291/2194, Loss: 0.1197\n",
      "Epoch 23/25, Batch 1301/2194, Loss: 0.0493\n",
      "Epoch 23/25, Batch 1311/2194, Loss: 0.1075\n",
      "Epoch 23/25, Batch 1321/2194, Loss: 0.0496\n",
      "Epoch 23/25, Batch 1331/2194, Loss: 0.1073\n",
      "Epoch 23/25, Batch 1341/2194, Loss: 0.0317\n",
      "Epoch 23/25, Batch 1351/2194, Loss: 0.0245\n",
      "Epoch 23/25, Batch 1361/2194, Loss: 0.1341\n",
      "Epoch 23/25, Batch 1371/2194, Loss: 0.2695\n",
      "Epoch 23/25, Batch 1381/2194, Loss: 0.0211\n",
      "Epoch 23/25, Batch 1391/2194, Loss: 0.0585\n",
      "Epoch 23/25, Batch 1401/2194, Loss: 0.0116\n",
      "Epoch 23/25, Batch 1411/2194, Loss: 0.0180\n",
      "Epoch 23/25, Batch 1421/2194, Loss: 0.1481\n",
      "Epoch 23/25, Batch 1431/2194, Loss: 0.1207\n",
      "Epoch 23/25, Batch 1441/2194, Loss: 0.1868\n",
      "Epoch 23/25, Batch 1451/2194, Loss: 0.0428\n",
      "Epoch 23/25, Batch 1461/2194, Loss: 0.0568\n",
      "Epoch 23/25, Batch 1471/2194, Loss: 0.1657\n",
      "Epoch 23/25, Batch 1481/2194, Loss: 0.0386\n",
      "Epoch 23/25, Batch 1491/2194, Loss: 0.1846\n",
      "Epoch 23/25, Batch 1501/2194, Loss: 0.0168\n",
      "Epoch 23/25, Batch 1511/2194, Loss: 0.1332\n",
      "Epoch 23/25, Batch 1521/2194, Loss: 0.1957\n",
      "Epoch 23/25, Batch 1531/2194, Loss: 0.0912\n",
      "Epoch 23/25, Batch 1541/2194, Loss: 0.0091\n",
      "Epoch 23/25, Batch 1551/2194, Loss: 0.0473\n",
      "Epoch 23/25, Batch 1561/2194, Loss: 0.0130\n",
      "Epoch 23/25, Batch 1571/2194, Loss: 0.0624\n",
      "Epoch 23/25, Batch 1581/2194, Loss: 0.1745\n",
      "Epoch 23/25, Batch 1591/2194, Loss: 0.0462\n",
      "Epoch 23/25, Batch 1601/2194, Loss: 0.0180\n",
      "Epoch 23/25, Batch 1611/2194, Loss: 0.0734\n",
      "Epoch 23/25, Batch 1621/2194, Loss: 0.1724\n",
      "Epoch 23/25, Batch 1631/2194, Loss: 0.1325\n",
      "Epoch 23/25, Batch 1641/2194, Loss: 0.0440\n",
      "Epoch 23/25, Batch 1651/2194, Loss: 0.1689\n",
      "Epoch 23/25, Batch 1661/2194, Loss: 0.1395\n",
      "Epoch 23/25, Batch 1671/2194, Loss: 0.0706\n",
      "Epoch 23/25, Batch 1681/2194, Loss: 0.0148\n",
      "Epoch 23/25, Batch 1691/2194, Loss: 0.2033\n",
      "Epoch 23/25, Batch 1701/2194, Loss: 0.0862\n",
      "Epoch 23/25, Batch 1711/2194, Loss: 0.0433\n",
      "Epoch 23/25, Batch 1721/2194, Loss: 0.0872\n",
      "Epoch 23/25, Batch 1731/2194, Loss: 0.0878\n",
      "Epoch 23/25, Batch 1741/2194, Loss: 0.1294\n",
      "Epoch 23/25, Batch 1751/2194, Loss: 0.0213\n",
      "Epoch 23/25, Batch 1761/2194, Loss: 0.0583\n",
      "Epoch 23/25, Batch 1771/2194, Loss: 0.1683\n",
      "Epoch 23/25, Batch 1781/2194, Loss: 0.3210\n",
      "Epoch 23/25, Batch 1791/2194, Loss: 0.0375\n",
      "Epoch 23/25, Batch 1801/2194, Loss: 0.0581\n",
      "Epoch 23/25, Batch 1811/2194, Loss: 0.0148\n",
      "Epoch 23/25, Batch 1821/2194, Loss: 0.1437\n",
      "Epoch 23/25, Batch 1831/2194, Loss: 0.1781\n",
      "Epoch 23/25, Batch 1841/2194, Loss: 0.1530\n",
      "Epoch 23/25, Batch 1851/2194, Loss: 0.1102\n",
      "Epoch 23/25, Batch 1861/2194, Loss: 0.0375\n",
      "Epoch 23/25, Batch 1871/2194, Loss: 0.1134\n",
      "Epoch 23/25, Batch 1881/2194, Loss: 0.1144\n",
      "Epoch 23/25, Batch 1891/2194, Loss: 0.1095\n",
      "Epoch 23/25, Batch 1901/2194, Loss: 0.0732\n",
      "Epoch 23/25, Batch 1911/2194, Loss: 0.0903\n",
      "Epoch 23/25, Batch 1921/2194, Loss: 0.1039\n",
      "Epoch 23/25, Batch 1931/2194, Loss: 0.0955\n",
      "Epoch 23/25, Batch 1941/2194, Loss: 0.0309\n",
      "Epoch 23/25, Batch 1951/2194, Loss: 0.1239\n",
      "Epoch 23/25, Batch 1961/2194, Loss: 0.0972\n",
      "Epoch 23/25, Batch 1971/2194, Loss: 0.0990\n",
      "Epoch 23/25, Batch 1981/2194, Loss: 0.1089\n",
      "Epoch 23/25, Batch 1991/2194, Loss: 0.0251\n",
      "Epoch 23/25, Batch 2001/2194, Loss: 0.0894\n",
      "Epoch 23/25, Batch 2011/2194, Loss: 0.2466\n",
      "Epoch 23/25, Batch 2021/2194, Loss: 0.0615\n",
      "Epoch 23/25, Batch 2031/2194, Loss: 0.0250\n",
      "Epoch 23/25, Batch 2041/2194, Loss: 0.0191\n",
      "Epoch 23/25, Batch 2051/2194, Loss: 0.1585\n",
      "Epoch 23/25, Batch 2061/2194, Loss: 0.0706\n",
      "Epoch 23/25, Batch 2071/2194, Loss: 0.0215\n",
      "Epoch 23/25, Batch 2081/2194, Loss: 0.0762\n",
      "Epoch 23/25, Batch 2091/2194, Loss: 0.0424\n",
      "Epoch 23/25, Batch 2101/2194, Loss: 0.0499\n",
      "Epoch 23/25, Batch 2111/2194, Loss: 0.0348\n",
      "Epoch 23/25, Batch 2121/2194, Loss: 0.0309\n",
      "Epoch 23/25, Batch 2131/2194, Loss: 0.0298\n",
      "Epoch 23/25, Batch 2141/2194, Loss: 0.0645\n",
      "Epoch 23/25, Batch 2151/2194, Loss: 0.0205\n",
      "Epoch 23/25, Batch 2161/2194, Loss: 0.0255\n",
      "Epoch 23/25, Batch 2171/2194, Loss: 0.0553\n",
      "Epoch 23/25, Batch 2181/2194, Loss: 0.0042\n",
      "Epoch 23/25, Batch 2191/2194, Loss: 0.0977\n",
      "Epoch 23/25:\n",
      "Train Loss: 0.0876, Train Acc: 96.40%\n",
      "Val Loss: 0.1633, Val Acc: 93.61%\n",
      "------------------------------------------------------------\n",
      "Starting epoch 24/25...\n",
      "Starting training loop with 2194 batches...\n",
      "Successfully loaded first batch: images shape torch.Size([32, 3, 224, 224]), labels shape torch.Size([32])\n",
      "Epoch 24/25, Batch 1/2194, Loss: 0.0112\n",
      "Epoch 24/25, Batch 11/2194, Loss: 0.1027\n",
      "Epoch 24/25, Batch 21/2194, Loss: 0.1868\n",
      "Epoch 24/25, Batch 31/2194, Loss: 0.1018\n",
      "Epoch 24/25, Batch 41/2194, Loss: 0.0465\n",
      "Epoch 24/25, Batch 51/2194, Loss: 0.1276\n",
      "Epoch 24/25, Batch 61/2194, Loss: 0.0839\n",
      "Epoch 24/25, Batch 71/2194, Loss: 0.2345\n",
      "Epoch 24/25, Batch 81/2194, Loss: 0.0103\n",
      "Epoch 24/25, Batch 91/2194, Loss: 0.0109\n",
      "Epoch 24/25, Batch 101/2194, Loss: 0.0604\n",
      "Epoch 24/25, Batch 111/2194, Loss: 0.2810\n",
      "Epoch 24/25, Batch 121/2194, Loss: 0.0058\n",
      "Epoch 24/25, Batch 131/2194, Loss: 0.1427\n",
      "Epoch 24/25, Batch 141/2194, Loss: 0.2170\n",
      "Epoch 24/25, Batch 151/2194, Loss: 0.1875\n",
      "Epoch 24/25, Batch 161/2194, Loss: 0.0417\n",
      "Epoch 24/25, Batch 171/2194, Loss: 0.0747\n",
      "Epoch 24/25, Batch 181/2194, Loss: 0.0310\n",
      "Epoch 24/25, Batch 191/2194, Loss: 0.1632\n",
      "Epoch 24/25, Batch 201/2194, Loss: 0.0850\n",
      "Epoch 24/25, Batch 211/2194, Loss: 0.0997\n",
      "Epoch 24/25, Batch 221/2194, Loss: 0.0709\n",
      "Epoch 24/25, Batch 231/2194, Loss: 0.0343\n",
      "Epoch 24/25, Batch 241/2194, Loss: 0.0599\n",
      "Epoch 24/25, Batch 251/2194, Loss: 0.0346\n",
      "Epoch 24/25, Batch 261/2194, Loss: 0.1250\n",
      "Epoch 24/25, Batch 271/2194, Loss: 0.0284\n",
      "Epoch 24/25, Batch 281/2194, Loss: 0.2321\n",
      "Epoch 24/25, Batch 291/2194, Loss: 0.0471\n",
      "Epoch 24/25, Batch 301/2194, Loss: 0.0436\n",
      "Epoch 24/25, Batch 311/2194, Loss: 0.0983\n",
      "Epoch 24/25, Batch 321/2194, Loss: 0.0268\n",
      "Epoch 24/25, Batch 331/2194, Loss: 0.1145\n",
      "Epoch 24/25, Batch 341/2194, Loss: 0.0997\n",
      "Epoch 24/25, Batch 351/2194, Loss: 0.2846\n",
      "Epoch 24/25, Batch 361/2194, Loss: 0.0711\n",
      "Epoch 24/25, Batch 371/2194, Loss: 0.0396\n",
      "Epoch 24/25, Batch 381/2194, Loss: 0.0220\n",
      "Epoch 24/25, Batch 391/2194, Loss: 0.0138\n",
      "Epoch 24/25, Batch 401/2194, Loss: 0.0318\n",
      "Epoch 24/25, Batch 411/2194, Loss: 0.0975\n",
      "Epoch 24/25, Batch 421/2194, Loss: 0.1259\n",
      "Epoch 24/25, Batch 431/2194, Loss: 0.0163\n",
      "Epoch 24/25, Batch 441/2194, Loss: 0.1276\n",
      "Epoch 24/25, Batch 451/2194, Loss: 0.1797\n",
      "Epoch 24/25, Batch 461/2194, Loss: 0.0046\n",
      "Epoch 24/25, Batch 471/2194, Loss: 0.0314\n",
      "Epoch 24/25, Batch 481/2194, Loss: 0.1104\n",
      "Epoch 24/25, Batch 491/2194, Loss: 0.0880\n",
      "Epoch 24/25, Batch 501/2194, Loss: 0.0503\n",
      "Epoch 24/25, Batch 511/2194, Loss: 0.0781\n",
      "Epoch 24/25, Batch 521/2194, Loss: 0.0295\n",
      "Epoch 24/25, Batch 531/2194, Loss: 0.1378\n",
      "Epoch 24/25, Batch 541/2194, Loss: 0.0700\n",
      "Epoch 24/25, Batch 551/2194, Loss: 0.1964\n",
      "Epoch 24/25, Batch 561/2194, Loss: 0.1167\n",
      "Epoch 24/25, Batch 571/2194, Loss: 0.1300\n",
      "Epoch 24/25, Batch 581/2194, Loss: 0.2479\n",
      "Epoch 24/25, Batch 591/2194, Loss: 0.0347\n",
      "Epoch 24/25, Batch 601/2194, Loss: 0.0742\n",
      "Epoch 24/25, Batch 611/2194, Loss: 0.0460\n",
      "Epoch 24/25, Batch 621/2194, Loss: 0.1104\n",
      "Epoch 24/25, Batch 631/2194, Loss: 0.0454\n",
      "Epoch 24/25, Batch 641/2194, Loss: 0.1381\n",
      "Epoch 24/25, Batch 651/2194, Loss: 0.1492\n",
      "Epoch 24/25, Batch 661/2194, Loss: 0.2128\n",
      "Epoch 24/25, Batch 671/2194, Loss: 0.1219\n",
      "Epoch 24/25, Batch 681/2194, Loss: 0.0317\n",
      "Epoch 24/25, Batch 691/2194, Loss: 0.3210\n",
      "Epoch 24/25, Batch 701/2194, Loss: 0.1291\n",
      "Epoch 24/25, Batch 711/2194, Loss: 0.1174\n",
      "Epoch 24/25, Batch 721/2194, Loss: 0.0698\n",
      "Epoch 24/25, Batch 731/2194, Loss: 0.0602\n",
      "Epoch 24/25, Batch 741/2194, Loss: 0.0282\n",
      "Epoch 24/25, Batch 751/2194, Loss: 0.0944\n",
      "Epoch 24/25, Batch 761/2194, Loss: 0.2397\n",
      "Epoch 24/25, Batch 771/2194, Loss: 0.1033\n",
      "Epoch 24/25, Batch 781/2194, Loss: 0.0280\n",
      "Epoch 24/25, Batch 791/2194, Loss: 0.0940\n",
      "Epoch 24/25, Batch 801/2194, Loss: 0.1018\n",
      "Epoch 24/25, Batch 811/2194, Loss: 0.0619\n",
      "Epoch 24/25, Batch 821/2194, Loss: 0.1538\n",
      "Epoch 24/25, Batch 831/2194, Loss: 0.0279\n",
      "Epoch 24/25, Batch 841/2194, Loss: 0.0172\n",
      "Epoch 24/25, Batch 851/2194, Loss: 0.0969\n",
      "Epoch 24/25, Batch 861/2194, Loss: 0.0738\n",
      "Epoch 24/25, Batch 871/2194, Loss: 0.0607\n",
      "Epoch 24/25, Batch 881/2194, Loss: 0.0298\n",
      "Epoch 24/25, Batch 891/2194, Loss: 0.0282\n",
      "Epoch 24/25, Batch 901/2194, Loss: 0.1320\n",
      "Epoch 24/25, Batch 911/2194, Loss: 0.0750\n",
      "Epoch 24/25, Batch 921/2194, Loss: 0.0099\n",
      "Epoch 24/25, Batch 931/2194, Loss: 0.1982\n",
      "Epoch 24/25, Batch 941/2194, Loss: 0.0211\n",
      "Epoch 24/25, Batch 951/2194, Loss: 0.0743\n",
      "Epoch 24/25, Batch 961/2194, Loss: 0.1429\n",
      "Epoch 24/25, Batch 971/2194, Loss: 0.2299\n",
      "Epoch 24/25, Batch 981/2194, Loss: 0.0769\n",
      "Epoch 24/25, Batch 991/2194, Loss: 0.1353\n",
      "Epoch 24/25, Batch 1001/2194, Loss: 0.0998\n",
      "Epoch 24/25, Batch 1011/2194, Loss: 0.0999\n",
      "Epoch 24/25, Batch 1021/2194, Loss: 0.0900\n",
      "Epoch 24/25, Batch 1031/2194, Loss: 0.0294\n",
      "Epoch 24/25, Batch 1041/2194, Loss: 0.0307\n",
      "Epoch 24/25, Batch 1051/2194, Loss: 0.0451\n",
      "Epoch 24/25, Batch 1061/2194, Loss: 0.0078\n",
      "Epoch 24/25, Batch 1071/2194, Loss: 0.0195\n",
      "Epoch 24/25, Batch 1081/2194, Loss: 0.0746\n",
      "Epoch 24/25, Batch 1091/2194, Loss: 0.0924\n",
      "Epoch 24/25, Batch 1101/2194, Loss: 0.0674\n",
      "Epoch 24/25, Batch 1111/2194, Loss: 0.0440\n",
      "Epoch 24/25, Batch 1121/2194, Loss: 0.0751\n",
      "Epoch 24/25, Batch 1131/2194, Loss: 0.0776\n",
      "Epoch 24/25, Batch 1141/2194, Loss: 0.0557\n",
      "Epoch 24/25, Batch 1151/2194, Loss: 0.0928\n",
      "Epoch 24/25, Batch 1161/2194, Loss: 0.0336\n",
      "Epoch 24/25, Batch 1171/2194, Loss: 0.0657\n",
      "Epoch 24/25, Batch 1181/2194, Loss: 0.0299\n",
      "Epoch 24/25, Batch 1191/2194, Loss: 0.0479\n",
      "Epoch 24/25, Batch 1201/2194, Loss: 0.3057\n",
      "Epoch 24/25, Batch 1211/2194, Loss: 0.1725\n",
      "Epoch 24/25, Batch 1221/2194, Loss: 0.0391\n",
      "Epoch 24/25, Batch 1231/2194, Loss: 0.0777\n",
      "Epoch 24/25, Batch 1241/2194, Loss: 0.0233\n",
      "Epoch 24/25, Batch 1251/2194, Loss: 0.0940\n",
      "Epoch 24/25, Batch 1261/2194, Loss: 0.0382\n",
      "Epoch 24/25, Batch 1271/2194, Loss: 0.0132\n",
      "Epoch 24/25, Batch 1281/2194, Loss: 0.0956\n",
      "Epoch 24/25, Batch 1291/2194, Loss: 0.0909\n",
      "Epoch 24/25, Batch 1301/2194, Loss: 0.0719\n",
      "Epoch 24/25, Batch 1311/2194, Loss: 0.0642\n",
      "Epoch 24/25, Batch 1321/2194, Loss: 0.1565\n",
      "Epoch 24/25, Batch 1331/2194, Loss: 0.0192\n",
      "Epoch 24/25, Batch 1341/2194, Loss: 0.1018\n",
      "Epoch 24/25, Batch 1351/2194, Loss: 0.2096\n",
      "Epoch 24/25, Batch 1361/2194, Loss: 0.0453\n",
      "Epoch 24/25, Batch 1371/2194, Loss: 0.1077\n",
      "Epoch 24/25, Batch 1381/2194, Loss: 0.1102\n",
      "Epoch 24/25, Batch 1391/2194, Loss: 0.0669\n",
      "Epoch 24/25, Batch 1401/2194, Loss: 0.0490\n",
      "Epoch 24/25, Batch 1411/2194, Loss: 0.1057\n",
      "Epoch 24/25, Batch 1421/2194, Loss: 0.0688\n",
      "Epoch 24/25, Batch 1431/2194, Loss: 0.1770\n",
      "Epoch 24/25, Batch 1441/2194, Loss: 0.0192\n",
      "Epoch 24/25, Batch 1451/2194, Loss: 0.0981\n",
      "Epoch 24/25, Batch 1461/2194, Loss: 0.1058\n",
      "Epoch 24/25, Batch 1471/2194, Loss: 0.1574\n",
      "Epoch 24/25, Batch 1481/2194, Loss: 0.1536\n",
      "Epoch 24/25, Batch 1491/2194, Loss: 0.0427\n",
      "Epoch 24/25, Batch 1501/2194, Loss: 0.0591\n",
      "Epoch 24/25, Batch 1511/2194, Loss: 0.0325\n",
      "Epoch 24/25, Batch 1521/2194, Loss: 0.0801\n",
      "Epoch 24/25, Batch 1531/2194, Loss: 0.1987\n",
      "Epoch 24/25, Batch 1541/2194, Loss: 0.1045\n",
      "Epoch 24/25, Batch 1551/2194, Loss: 0.0416\n",
      "Epoch 24/25, Batch 1561/2194, Loss: 0.0749\n",
      "Epoch 24/25, Batch 1571/2194, Loss: 0.0471\n",
      "Epoch 24/25, Batch 1581/2194, Loss: 0.1141\n",
      "Epoch 24/25, Batch 1591/2194, Loss: 0.1153\n",
      "Epoch 24/25, Batch 1601/2194, Loss: 0.2503\n",
      "Epoch 24/25, Batch 1611/2194, Loss: 0.0387\n",
      "Epoch 24/25, Batch 1621/2194, Loss: 0.0144\n",
      "Epoch 24/25, Batch 1631/2194, Loss: 0.0416\n",
      "Epoch 24/25, Batch 1641/2194, Loss: 0.1941\n",
      "Epoch 24/25, Batch 1651/2194, Loss: 0.0205\n",
      "Epoch 24/25, Batch 1661/2194, Loss: 0.0718\n",
      "Epoch 24/25, Batch 1671/2194, Loss: 0.0163\n",
      "Epoch 24/25, Batch 1681/2194, Loss: 0.0425\n",
      "Epoch 24/25, Batch 1691/2194, Loss: 0.0501\n",
      "Epoch 24/25, Batch 1701/2194, Loss: 0.0210\n",
      "Epoch 24/25, Batch 1711/2194, Loss: 0.0416\n",
      "Epoch 24/25, Batch 1721/2194, Loss: 0.3419\n",
      "Epoch 24/25, Batch 1731/2194, Loss: 0.0702\n",
      "Epoch 24/25, Batch 1741/2194, Loss: 0.0344\n",
      "Epoch 24/25, Batch 1751/2194, Loss: 0.0708\n",
      "Epoch 24/25, Batch 1761/2194, Loss: 0.0196\n",
      "Epoch 24/25, Batch 1771/2194, Loss: 0.0760\n",
      "Epoch 24/25, Batch 1781/2194, Loss: 0.1527\n",
      "Epoch 24/25, Batch 1791/2194, Loss: 0.0214\n",
      "Epoch 24/25, Batch 1801/2194, Loss: 0.0750\n",
      "Epoch 24/25, Batch 1811/2194, Loss: 0.1046\n",
      "Epoch 24/25, Batch 1821/2194, Loss: 0.0181\n",
      "Epoch 24/25, Batch 1831/2194, Loss: 0.0568\n",
      "Epoch 24/25, Batch 1841/2194, Loss: 0.1740\n",
      "Epoch 24/25, Batch 1851/2194, Loss: 0.1593\n",
      "Epoch 24/25, Batch 1861/2194, Loss: 0.1209\n",
      "Epoch 24/25, Batch 1871/2194, Loss: 0.0617\n",
      "Epoch 24/25, Batch 1881/2194, Loss: 0.0323\n",
      "Epoch 24/25, Batch 1891/2194, Loss: 0.0582\n",
      "Epoch 24/25, Batch 1901/2194, Loss: 0.1269\n",
      "Epoch 24/25, Batch 1911/2194, Loss: 0.0766\n",
      "Epoch 24/25, Batch 1921/2194, Loss: 0.0290\n",
      "Epoch 24/25, Batch 1931/2194, Loss: 0.0364\n",
      "Epoch 24/25, Batch 1941/2194, Loss: 0.1833\n",
      "Epoch 24/25, Batch 1951/2194, Loss: 0.0417\n",
      "Epoch 24/25, Batch 1961/2194, Loss: 0.0055\n",
      "Epoch 24/25, Batch 1971/2194, Loss: 0.2101\n",
      "Epoch 24/25, Batch 1981/2194, Loss: 0.0172\n",
      "Epoch 24/25, Batch 1991/2194, Loss: 0.0390\n",
      "Epoch 24/25, Batch 2001/2194, Loss: 0.0252\n",
      "Epoch 24/25, Batch 2011/2194, Loss: 0.1526\n",
      "Epoch 24/25, Batch 2021/2194, Loss: 0.0552\n",
      "Epoch 24/25, Batch 2031/2194, Loss: 0.0570\n",
      "Epoch 24/25, Batch 2041/2194, Loss: 0.0579\n",
      "Epoch 24/25, Batch 2051/2194, Loss: 0.0558\n",
      "Epoch 24/25, Batch 2061/2194, Loss: 0.0649\n",
      "Epoch 24/25, Batch 2071/2194, Loss: 0.1185\n",
      "Epoch 24/25, Batch 2081/2194, Loss: 0.1071\n",
      "Epoch 24/25, Batch 2091/2194, Loss: 0.1420\n",
      "Epoch 24/25, Batch 2101/2194, Loss: 0.0823\n",
      "Epoch 24/25, Batch 2111/2194, Loss: 0.0510\n",
      "Epoch 24/25, Batch 2121/2194, Loss: 0.0322\n",
      "Epoch 24/25, Batch 2131/2194, Loss: 0.1497\n",
      "Epoch 24/25, Batch 2141/2194, Loss: 0.1888\n",
      "Epoch 24/25, Batch 2151/2194, Loss: 0.0836\n",
      "Epoch 24/25, Batch 2161/2194, Loss: 0.0817\n",
      "Epoch 24/25, Batch 2171/2194, Loss: 0.0136\n",
      "Epoch 24/25, Batch 2181/2194, Loss: 0.0059\n",
      "Epoch 24/25, Batch 2191/2194, Loss: 0.2045\n",
      "Epoch 24/25:\n",
      "Train Loss: 0.0861, Train Acc: 96.45%\n",
      "Val Loss: 0.1659, Val Acc: 93.77%\n",
      "------------------------------------------------------------\n",
      "Starting epoch 25/25...\n",
      "Starting training loop with 2194 batches...\n",
      "Successfully loaded first batch: images shape torch.Size([32, 3, 224, 224]), labels shape torch.Size([32])\n",
      "Epoch 25/25, Batch 1/2194, Loss: 0.1581\n",
      "Epoch 25/25, Batch 11/2194, Loss: 0.2120\n",
      "Epoch 25/25, Batch 21/2194, Loss: 0.0464\n",
      "Epoch 25/25, Batch 31/2194, Loss: 0.1786\n",
      "Epoch 25/25, Batch 41/2194, Loss: 0.1311\n",
      "Epoch 25/25, Batch 51/2194, Loss: 0.0592\n",
      "Epoch 25/25, Batch 61/2194, Loss: 0.2159\n",
      "Epoch 25/25, Batch 71/2194, Loss: 0.0537\n",
      "Epoch 25/25, Batch 81/2194, Loss: 0.0504\n",
      "Epoch 25/25, Batch 91/2194, Loss: 0.0249\n",
      "Epoch 25/25, Batch 101/2194, Loss: 0.1173\n",
      "Epoch 25/25, Batch 111/2194, Loss: 0.0137\n",
      "Epoch 25/25, Batch 121/2194, Loss: 0.0543\n",
      "Epoch 25/25, Batch 131/2194, Loss: 0.0136\n",
      "Epoch 25/25, Batch 141/2194, Loss: 0.0078\n",
      "Epoch 25/25, Batch 151/2194, Loss: 0.0195\n",
      "Epoch 25/25, Batch 161/2194, Loss: 0.1057\n",
      "Epoch 25/25, Batch 171/2194, Loss: 0.0776\n",
      "Epoch 25/25, Batch 181/2194, Loss: 0.0518\n",
      "Epoch 25/25, Batch 191/2194, Loss: 0.0706\n",
      "Epoch 25/25, Batch 201/2194, Loss: 0.1063\n",
      "Epoch 25/25, Batch 211/2194, Loss: 0.1193\n",
      "Epoch 25/25, Batch 221/2194, Loss: 0.0294\n",
      "Epoch 25/25, Batch 231/2194, Loss: 0.0248\n",
      "Epoch 25/25, Batch 241/2194, Loss: 0.0119\n",
      "Epoch 25/25, Batch 251/2194, Loss: 0.0624\n",
      "Epoch 25/25, Batch 261/2194, Loss: 0.0120\n",
      "Epoch 25/25, Batch 271/2194, Loss: 0.1200\n",
      "Epoch 25/25, Batch 281/2194, Loss: 0.0464\n",
      "Epoch 25/25, Batch 291/2194, Loss: 0.0372\n",
      "Epoch 25/25, Batch 301/2194, Loss: 0.0969\n",
      "Epoch 25/25, Batch 311/2194, Loss: 0.0051\n",
      "Epoch 25/25, Batch 321/2194, Loss: 0.0534\n",
      "Epoch 25/25, Batch 331/2194, Loss: 0.1337\n",
      "Epoch 25/25, Batch 341/2194, Loss: 0.0279\n",
      "Epoch 25/25, Batch 351/2194, Loss: 0.0824\n",
      "Epoch 25/25, Batch 361/2194, Loss: 0.0729\n",
      "Epoch 25/25, Batch 371/2194, Loss: 0.0896\n",
      "Epoch 25/25, Batch 381/2194, Loss: 0.0676\n",
      "Epoch 25/25, Batch 391/2194, Loss: 0.1074\n",
      "Epoch 25/25, Batch 401/2194, Loss: 0.0207\n",
      "Epoch 25/25, Batch 411/2194, Loss: 0.0367\n",
      "Epoch 25/25, Batch 421/2194, Loss: 0.0502\n",
      "Epoch 25/25, Batch 431/2194, Loss: 0.1176\n",
      "Epoch 25/25, Batch 441/2194, Loss: 0.0860\n",
      "Epoch 25/25, Batch 451/2194, Loss: 0.0343\n",
      "Epoch 25/25, Batch 461/2194, Loss: 0.0691\n",
      "Epoch 25/25, Batch 471/2194, Loss: 0.0971\n",
      "Epoch 25/25, Batch 481/2194, Loss: 0.1708\n",
      "Epoch 25/25, Batch 491/2194, Loss: 0.1274\n",
      "Epoch 25/25, Batch 501/2194, Loss: 0.1241\n",
      "Epoch 25/25, Batch 511/2194, Loss: 0.0236\n",
      "Epoch 25/25, Batch 521/2194, Loss: 0.0595\n",
      "Epoch 25/25, Batch 531/2194, Loss: 0.0800\n",
      "Epoch 25/25, Batch 541/2194, Loss: 0.0221\n",
      "Epoch 25/25, Batch 551/2194, Loss: 0.0019\n",
      "Epoch 25/25, Batch 561/2194, Loss: 0.1159\n",
      "Epoch 25/25, Batch 571/2194, Loss: 0.0371\n",
      "Epoch 25/25, Batch 581/2194, Loss: 0.0212\n",
      "Epoch 25/25, Batch 591/2194, Loss: 0.0026\n",
      "Epoch 25/25, Batch 601/2194, Loss: 0.0185\n",
      "Epoch 25/25, Batch 611/2194, Loss: 0.0959\n",
      "Epoch 25/25, Batch 621/2194, Loss: 0.0341\n",
      "Epoch 25/25, Batch 631/2194, Loss: 0.0940\n",
      "Epoch 25/25, Batch 641/2194, Loss: 0.0109\n",
      "Epoch 25/25, Batch 651/2194, Loss: 0.1144\n",
      "Epoch 25/25, Batch 661/2194, Loss: 0.0034\n",
      "Epoch 25/25, Batch 671/2194, Loss: 0.1646\n",
      "Epoch 25/25, Batch 681/2194, Loss: 0.0465\n",
      "Epoch 25/25, Batch 691/2194, Loss: 0.0620\n",
      "Epoch 25/25, Batch 701/2194, Loss: 0.0328\n",
      "Epoch 25/25, Batch 711/2194, Loss: 0.0394\n",
      "Epoch 25/25, Batch 721/2194, Loss: 0.1358\n",
      "Epoch 25/25, Batch 731/2194, Loss: 0.0430\n",
      "Epoch 25/25, Batch 741/2194, Loss: 0.0102\n",
      "Epoch 25/25, Batch 751/2194, Loss: 0.1074\n",
      "Epoch 25/25, Batch 761/2194, Loss: 0.1193\n",
      "Epoch 25/25, Batch 771/2194, Loss: 0.0060\n",
      "Epoch 25/25, Batch 781/2194, Loss: 0.0188\n",
      "Epoch 25/25, Batch 791/2194, Loss: 0.0228\n",
      "Epoch 25/25, Batch 801/2194, Loss: 0.0220\n",
      "Epoch 25/25, Batch 811/2194, Loss: 0.0047\n",
      "Epoch 25/25, Batch 821/2194, Loss: 0.0615\n",
      "Epoch 25/25, Batch 831/2194, Loss: 0.0911\n",
      "Epoch 25/25, Batch 841/2194, Loss: 0.0039\n",
      "Epoch 25/25, Batch 851/2194, Loss: 0.0083\n",
      "Epoch 25/25, Batch 861/2194, Loss: 0.0756\n",
      "Epoch 25/25, Batch 871/2194, Loss: 0.0250\n",
      "Epoch 25/25, Batch 881/2194, Loss: 0.0680\n",
      "Epoch 25/25, Batch 891/2194, Loss: 0.1201\n",
      "Epoch 25/25, Batch 901/2194, Loss: 0.0632\n",
      "Epoch 25/25, Batch 911/2194, Loss: 0.0091\n",
      "Epoch 25/25, Batch 921/2194, Loss: 0.1368\n",
      "Epoch 25/25, Batch 931/2194, Loss: 0.0327\n",
      "Epoch 25/25, Batch 941/2194, Loss: 0.1986\n",
      "Epoch 25/25, Batch 951/2194, Loss: 0.0482\n",
      "Epoch 25/25, Batch 961/2194, Loss: 0.0182\n",
      "Epoch 25/25, Batch 971/2194, Loss: 0.0441\n",
      "Epoch 25/25, Batch 981/2194, Loss: 0.0194\n",
      "Epoch 25/25, Batch 991/2194, Loss: 0.0089\n",
      "Epoch 25/25, Batch 1001/2194, Loss: 0.0373\n",
      "Epoch 25/25, Batch 1011/2194, Loss: 0.0329\n",
      "Epoch 25/25, Batch 1021/2194, Loss: 0.0082\n",
      "Epoch 25/25, Batch 1031/2194, Loss: 0.0353\n",
      "Epoch 25/25, Batch 1041/2194, Loss: 0.0035\n",
      "Epoch 25/25, Batch 1051/2194, Loss: 0.0006\n",
      "Epoch 25/25, Batch 1061/2194, Loss: 0.0854\n",
      "Epoch 25/25, Batch 1071/2194, Loss: 0.0881\n",
      "Epoch 25/25, Batch 1081/2194, Loss: 0.0265\n",
      "Epoch 25/25, Batch 1091/2194, Loss: 0.0213\n",
      "Epoch 25/25, Batch 1101/2194, Loss: 0.0360\n",
      "Epoch 25/25, Batch 1111/2194, Loss: 0.0735\n",
      "Epoch 25/25, Batch 1121/2194, Loss: 0.0500\n",
      "Epoch 25/25, Batch 1131/2194, Loss: 0.0286\n",
      "Epoch 25/25, Batch 1141/2194, Loss: 0.1039\n",
      "Epoch 25/25, Batch 1151/2194, Loss: 0.0333\n",
      "Epoch 25/25, Batch 1161/2194, Loss: 0.1472\n",
      "Epoch 25/25, Batch 1171/2194, Loss: 0.0158\n",
      "Epoch 25/25, Batch 1181/2194, Loss: 0.0249\n",
      "Epoch 25/25, Batch 1191/2194, Loss: 0.1098\n",
      "Epoch 25/25, Batch 1201/2194, Loss: 0.1302\n",
      "Epoch 25/25, Batch 1211/2194, Loss: 0.0025\n",
      "Epoch 25/25, Batch 1221/2194, Loss: 0.0084\n",
      "Epoch 25/25, Batch 1231/2194, Loss: 0.0138\n",
      "Epoch 25/25, Batch 1241/2194, Loss: 0.0297\n",
      "Epoch 25/25, Batch 1251/2194, Loss: 0.0662\n",
      "Epoch 25/25, Batch 1261/2194, Loss: 0.0751\n",
      "Epoch 25/25, Batch 1271/2194, Loss: 0.0397\n",
      "Epoch 25/25, Batch 1281/2194, Loss: 0.0219\n",
      "Epoch 25/25, Batch 1291/2194, Loss: 0.0128\n",
      "Epoch 25/25, Batch 1301/2194, Loss: 0.0711\n",
      "Epoch 25/25, Batch 1311/2194, Loss: 0.0103\n",
      "Epoch 25/25, Batch 1321/2194, Loss: 0.0618\n",
      "Epoch 25/25, Batch 1331/2194, Loss: 0.0814\n",
      "Epoch 25/25, Batch 1341/2194, Loss: 0.0299\n",
      "Epoch 25/25, Batch 1351/2194, Loss: 0.0172\n",
      "Epoch 25/25, Batch 1361/2194, Loss: 0.0172\n",
      "Epoch 25/25, Batch 1371/2194, Loss: 0.0034\n",
      "Epoch 25/25, Batch 1381/2194, Loss: 0.0485\n",
      "Epoch 25/25, Batch 1391/2194, Loss: 0.0087\n",
      "Epoch 25/25, Batch 1401/2194, Loss: 0.0807\n",
      "Epoch 25/25, Batch 1411/2194, Loss: 0.0389\n",
      "Epoch 25/25, Batch 1421/2194, Loss: 0.1090\n",
      "Epoch 25/25, Batch 1431/2194, Loss: 0.2468\n",
      "Epoch 25/25, Batch 1441/2194, Loss: 0.0064\n",
      "Epoch 25/25, Batch 1451/2194, Loss: 0.0380\n",
      "Epoch 25/25, Batch 1461/2194, Loss: 0.0051\n",
      "Epoch 25/25, Batch 1471/2194, Loss: 0.0626\n",
      "Epoch 25/25, Batch 1481/2194, Loss: 0.0681\n",
      "Epoch 25/25, Batch 1491/2194, Loss: 0.0326\n",
      "Epoch 25/25, Batch 1501/2194, Loss: 0.0132\n",
      "Epoch 25/25, Batch 1511/2194, Loss: 0.0569\n",
      "Epoch 25/25, Batch 1521/2194, Loss: 0.0122\n",
      "Epoch 25/25, Batch 1531/2194, Loss: 0.0367\n",
      "Epoch 25/25, Batch 1541/2194, Loss: 0.0032\n",
      "Epoch 25/25, Batch 1551/2194, Loss: 0.0134\n",
      "Epoch 25/25, Batch 1561/2194, Loss: 0.0169\n",
      "Epoch 25/25, Batch 1571/2194, Loss: 0.0096\n",
      "Epoch 25/25, Batch 1581/2194, Loss: 0.0345\n",
      "Epoch 25/25, Batch 1591/2194, Loss: 0.0005\n",
      "Epoch 25/25, Batch 1601/2194, Loss: 0.0020\n",
      "Epoch 25/25, Batch 1611/2194, Loss: 0.0046\n",
      "Epoch 25/25, Batch 1621/2194, Loss: 0.0778\n",
      "Epoch 25/25, Batch 1631/2194, Loss: 0.0971\n",
      "Epoch 25/25, Batch 1641/2194, Loss: 0.0084\n",
      "Epoch 25/25, Batch 1651/2194, Loss: 0.0678\n",
      "Epoch 25/25, Batch 1661/2194, Loss: 0.0167\n",
      "Epoch 25/25, Batch 1671/2194, Loss: 0.0603\n",
      "Epoch 25/25, Batch 1681/2194, Loss: 0.0745\n",
      "Epoch 25/25, Batch 1691/2194, Loss: 0.0021\n",
      "Epoch 25/25, Batch 1701/2194, Loss: 0.0157\n",
      "Epoch 25/25, Batch 1711/2194, Loss: 0.0091\n",
      "Epoch 25/25, Batch 1721/2194, Loss: 0.0766\n",
      "Epoch 25/25, Batch 1731/2194, Loss: 0.0128\n",
      "Epoch 25/25, Batch 1741/2194, Loss: 0.0178\n",
      "Epoch 25/25, Batch 1751/2194, Loss: 0.0935\n",
      "Epoch 25/25, Batch 1761/2194, Loss: 0.0393\n",
      "Epoch 25/25, Batch 1771/2194, Loss: 0.0457\n",
      "Epoch 25/25, Batch 1781/2194, Loss: 0.0087\n",
      "Epoch 25/25, Batch 1791/2194, Loss: 0.0083\n",
      "Epoch 25/25, Batch 1801/2194, Loss: 0.0486\n",
      "Epoch 25/25, Batch 1811/2194, Loss: 0.0009\n",
      "Epoch 25/25, Batch 1821/2194, Loss: 0.0107\n",
      "Epoch 25/25, Batch 1831/2194, Loss: 0.0522\n",
      "Epoch 25/25, Batch 1841/2194, Loss: 0.0023\n",
      "Epoch 25/25, Batch 1851/2194, Loss: 0.0027\n",
      "Epoch 25/25, Batch 1861/2194, Loss: 0.1294\n",
      "Epoch 25/25, Batch 1871/2194, Loss: 0.0406\n",
      "Epoch 25/25, Batch 1881/2194, Loss: 0.0174\n",
      "Epoch 25/25, Batch 1891/2194, Loss: 0.0125\n",
      "Epoch 25/25, Batch 1901/2194, Loss: 0.0167\n",
      "Epoch 25/25, Batch 1911/2194, Loss: 0.0251\n",
      "Epoch 25/25, Batch 1921/2194, Loss: 0.0558\n",
      "Epoch 25/25, Batch 1931/2194, Loss: 0.0201\n",
      "Epoch 25/25, Batch 1941/2194, Loss: 0.0207\n",
      "Epoch 25/25, Batch 1951/2194, Loss: 0.0379\n",
      "Epoch 25/25, Batch 1961/2194, Loss: 0.0773\n",
      "Epoch 25/25, Batch 1971/2194, Loss: 0.0174\n",
      "Epoch 25/25, Batch 1981/2194, Loss: 0.0405\n",
      "Epoch 25/25, Batch 1991/2194, Loss: 0.0342\n",
      "Epoch 25/25, Batch 2001/2194, Loss: 0.0360\n",
      "Epoch 25/25, Batch 2011/2194, Loss: 0.0015\n",
      "Epoch 25/25, Batch 2021/2194, Loss: 0.0215\n",
      "Epoch 25/25, Batch 2031/2194, Loss: 0.0280\n",
      "Epoch 25/25, Batch 2041/2194, Loss: 0.0112\n",
      "Epoch 25/25, Batch 2051/2194, Loss: 0.0747\n",
      "Epoch 25/25, Batch 2061/2194, Loss: 0.0135\n",
      "Epoch 25/25, Batch 2071/2194, Loss: 0.0725\n",
      "Epoch 25/25, Batch 2081/2194, Loss: 0.0078\n",
      "Epoch 25/25, Batch 2091/2194, Loss: 0.0531\n",
      "Epoch 25/25, Batch 2101/2194, Loss: 0.0383\n",
      "Epoch 25/25, Batch 2111/2194, Loss: 0.0782\n",
      "Epoch 25/25, Batch 2121/2194, Loss: 0.1043\n",
      "Epoch 25/25, Batch 2131/2194, Loss: 0.0089\n",
      "Epoch 25/25, Batch 2141/2194, Loss: 0.0428\n",
      "Epoch 25/25, Batch 2151/2194, Loss: 0.0399\n",
      "Epoch 25/25, Batch 2161/2194, Loss: 0.0019\n",
      "Epoch 25/25, Batch 2171/2194, Loss: 0.0193\n",
      "Epoch 25/25, Batch 2181/2194, Loss: 0.0317\n",
      "Epoch 25/25, Batch 2191/2194, Loss: 0.0190\n",
      "Epoch 25/25:\n",
      "Train Loss: 0.0543, Train Acc: 97.82%\n",
      "Val Loss: 0.1726, Val Acc: 93.81%\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1500x500 with 2 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading best model for evaluation...\n",
      "Test Accuracy: 0.9396\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Normal       0.94      0.97      0.96     15600\n",
      "      Stroke       0.94      0.88      0.91      7800\n",
      "\n",
      "    accuracy                           0.94     23400\n",
      "   macro avg       0.94      0.92      0.93     23400\n",
      "weighted avg       0.94      0.94      0.94     23400\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 800x600 with 2 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Test Accuracy: 0.9396\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "BALANCED DATASET",
   "id": "8693b9f2d08d7c82"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-08T20:47:52.020904Z",
     "start_time": "2025-06-08T09:10:56.689115Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from transformers import ViTForImageClassification, ViTConfig\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "class BrainCTDataset(Dataset):\n",
    "    def __init__(self, image_paths, labels, transform=None):\n",
    "        self.image_paths = image_paths\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_path = self.image_paths[idx]\n",
    "        image = Image.open(image_path).convert('RGB')\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label\n",
    "\n",
    "def load_dataset(data_dir):\n",
    "    \"\"\"\n",
    "    Load dataset from directory structure:\n",
    "    data_dir/\n",
    "        Stroke/\n",
    "            image1.jpg\n",
    "            image2.jpg\n",
    "        Normal/\n",
    "            image1.jpg\n",
    "            image2.jpg\n",
    "    \"\"\"\n",
    "    image_paths = []\n",
    "    labels = []\n",
    "    class_names = ['Normal', 'Stroke']  # 0: Normal, 1: Stroke\n",
    "\n",
    "    for class_idx, class_name in enumerate(class_names):\n",
    "        class_dir = os.path.join(data_dir, class_name)\n",
    "        if os.path.exists(class_dir):\n",
    "            for img_name in os.listdir(class_dir):\n",
    "                if img_name.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp', '.tiff')):\n",
    "                    image_paths.append(os.path.join(class_dir, img_name))\n",
    "                    labels.append(class_idx)\n",
    "\n",
    "    return image_paths, labels, class_names\n",
    "\n",
    "def create_data_augmentation():\n",
    "    \"\"\"Create comprehensive data augmentation for medical images\"\"\"\n",
    "\n",
    "    # Training augmentation - aggressive for small datasets\n",
    "    train_transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.RandomRotation(15),  # Small rotation for medical images\n",
    "        transforms.RandomHorizontalFlip(p=0.5),\n",
    "        transforms.RandomVerticalFlip(p=0.3),  # Medical images can be flipped vertically\n",
    "        transforms.RandomAffine(degrees=0, translate=(0.1, 0.1), scale=(0.9, 1.1)),\n",
    "        transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.1, hue=0.05),\n",
    "        transforms.RandomGrayscale(p=0.1),\n",
    "        transforms.ToTensor(),  # Convert to tensor BEFORE tensor-only transforms\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        transforms.GaussianBlur(kernel_size=3, sigma=(0.1, 2.0)),  # Move after ToTensor\n",
    "        transforms.RandomErasing(p=0.2, scale=(0.02, 0.1)),  # Move after ToTensor\n",
    "    ])\n",
    "\n",
    "    # Validation/Test transform - no augmentation\n",
    "    val_transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "\n",
    "    return train_transform, val_transform\n",
    "\n",
    "def split_data(image_paths, labels, test_size=0.2, val_size=0.2, random_state=42):\n",
    "    \"\"\"Split data into train, validation, and test sets\"\"\"\n",
    "\n",
    "    # First split: train+val vs test\n",
    "    X_temp, X_test, y_temp, y_test = train_test_split(\n",
    "        image_paths, labels, test_size=test_size,\n",
    "        random_state=random_state, stratify=labels\n",
    "    )\n",
    "\n",
    "    # Second split: train vs val\n",
    "    val_ratio = val_size / (1 - test_size)\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X_temp, y_temp, test_size=val_ratio,\n",
    "        random_state=random_state, stratify=y_temp\n",
    "    )\n",
    "\n",
    "    print(f\"Dataset split:\")\n",
    "    print(f\"Train: {len(X_train)} images\")\n",
    "    print(f\"Validation: {len(X_val)} images\")\n",
    "    print(f\"Test: {len(X_test)} images\")\n",
    "\n",
    "    return X_train, X_val, X_test, y_train, y_val, y_test\n",
    "\n",
    "class ViTBrainClassifier(nn.Module):\n",
    "    def __init__(self, num_classes=2, pretrained=True):\n",
    "        super(ViTBrainClassifier, self).__init__()\n",
    "\n",
    "        if pretrained:\n",
    "            # Use pre-trained ViT-Base\n",
    "            self.vit = ViTForImageClassification.from_pretrained(\n",
    "                'google/vit-base-patch16-224',\n",
    "                num_labels=num_classes,\n",
    "                ignore_mismatched_sizes=True\n",
    "            )\n",
    "        else:\n",
    "            # Create ViT from scratch (for very small datasets)\n",
    "            config = ViTConfig(\n",
    "                image_size=224,\n",
    "                patch_size=16,\n",
    "                num_channels=3,\n",
    "                num_classes=num_classes,\n",
    "                hidden_size=768,\n",
    "                num_hidden_layers=12,\n",
    "                num_attention_heads=12,\n",
    "                intermediate_size=3072\n",
    "            )\n",
    "            self.vit = ViTForImageClassification(config)\n",
    "\n",
    "    def forward(self, x):\n",
    "        outputs = self.vit(x)\n",
    "        return outputs.logits\n",
    "\n",
    "def train_model(model, train_loader, val_loader, num_epochs=25, learning_rate=1e-4):\n",
    "    \"\"\"Train the ViT model with early stopping\"\"\"\n",
    "\n",
    "    # Loss and optimizer\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=0.01)\n",
    "\n",
    "    # Learning rate scheduler\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, mode='min', patience=5, factor=0.5\n",
    "    )\n",
    "\n",
    "    # Training history\n",
    "    train_losses = []\n",
    "    train_accuracies = []\n",
    "    val_losses = []\n",
    "    val_accuracies = []\n",
    "\n",
    "    best_val_acc = 0.0\n",
    "    patience_counter = 0\n",
    "    patience = 10\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"Starting epoch {epoch+1}/{num_epochs}...\")\n",
    "\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        train_correct = 0\n",
    "        train_total = 0\n",
    "\n",
    "        print(f\"Starting training loop with {len(train_loader)} batches...\")\n",
    "\n",
    "        for batch_idx, (images, labels) in enumerate(train_loader):\n",
    "            if batch_idx == 0:\n",
    "                print(f\"Successfully loaded first batch: images shape {images.shape}, labels shape {labels.shape}\")\n",
    "\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            train_total += labels.size(0)\n",
    "            train_correct += (predicted == labels).sum().item()\n",
    "\n",
    "            if batch_idx % 10 == 0:\n",
    "                print(f'Epoch {epoch+1}/{num_epochs}, Batch {batch_idx+1}/{len(train_loader)}, Loss: {loss.item():.4f}')\n",
    "\n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for images, labels in val_loader:\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                val_loss += loss.item()\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                val_total += labels.size(0)\n",
    "                val_correct += (predicted == labels).sum().item()\n",
    "\n",
    "        # Calculate metrics\n",
    "        train_acc = 100 * train_correct / train_total\n",
    "        val_acc = 100 * val_correct / val_total\n",
    "        avg_train_loss = train_loss / len(train_loader)\n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "\n",
    "        train_losses.append(avg_train_loss)\n",
    "        train_accuracies.append(train_acc)\n",
    "        val_losses.append(avg_val_loss)\n",
    "        val_accuracies.append(val_acc)\n",
    "\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}:')\n",
    "        print(f'Train Loss: {avg_train_loss:.4f}, Train Acc: {train_acc:.2f}%')\n",
    "        print(f'Val Loss: {avg_val_loss:.4f}, Val Acc: {val_acc:.2f}%')\n",
    "        print('-' * 60)\n",
    "\n",
    "        # Learning rate scheduling\n",
    "        scheduler.step(avg_val_loss)\n",
    "\n",
    "        # Early stopping and model saving\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            torch.save(model.state_dict(), 'Balanced_dataset_vit_stroke.pth')\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "\n",
    "        if patience_counter >= patience:\n",
    "            print(f'Early stopping at epoch {epoch+1}')\n",
    "            break\n",
    "\n",
    "    return train_losses, train_accuracies, val_losses, val_accuracies\n",
    "\n",
    "def evaluate_model(model, test_loader, class_names):\n",
    "    \"\"\"Evaluate the model on test set\"\"\"\n",
    "    model.eval()\n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "\n",
    "            all_predictions.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(all_labels, all_predictions)\n",
    "    print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(all_labels, all_predictions, target_names=class_names))\n",
    "\n",
    "    # Confusion Matrix\n",
    "    cm = confusion_matrix(all_labels, all_predictions)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                xticklabels=class_names, yticklabels=class_names)\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.show()\n",
    "\n",
    "    return accuracy\n",
    "\n",
    "def plot_training_history(train_losses, train_accuracies, val_losses, val_accuracies):\n",
    "    \"\"\"Plot training history\"\"\"\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "    # Loss plot\n",
    "    ax1.plot(train_losses, label='Training Loss')\n",
    "    ax1.plot(val_losses, label='Validation Loss')\n",
    "    ax1.set_title('Model Loss')\n",
    "    ax1.set_xlabel('Epoch')\n",
    "    ax1.set_ylabel('Loss')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True)\n",
    "\n",
    "    # Accuracy plot\n",
    "    ax2.plot(train_accuracies, label='Training Accuracy')\n",
    "    ax2.plot(val_accuracies, label='Validation Accuracy')\n",
    "    ax2.set_title('Model Accuracy')\n",
    "    ax2.set_xlabel('Epoch')\n",
    "    ax2.set_ylabel('Accuracy (%)')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def main():\n",
    "    # Configuration\n",
    "    DATA_DIR = \"Balanced_dataset\"  # Update this path\n",
    "    BATCH_SIZE = 32  # Small batch size for small datasets\n",
    "    NUM_EPOCHS = 25\n",
    "    LEARNING_RATE = 1e-4\n",
    "\n",
    "    # Load dataset\n",
    "    print(\"Loading dataset...\")\n",
    "    image_paths, labels, class_names = load_dataset(DATA_DIR)\n",
    "    print(f\"Total images: {len(image_paths)}\")\n",
    "    print(f\"Classes: {class_names}\")\n",
    "    print(f\"Class distribution: Normal: {labels.count(0)}, Stroke: {labels.count(1)}\")\n",
    "\n",
    "    # Split dataset\n",
    "    X_train, X_val, X_test, y_train, y_val, y_test = split_data(image_paths, labels)\n",
    "\n",
    "    # Create data transforms\n",
    "    train_transform, val_transform = create_data_augmentation()\n",
    "\n",
    "    # Create datasets\n",
    "    train_dataset = BrainCTDataset(X_train, y_train, transform=train_transform)\n",
    "    val_dataset = BrainCTDataset(X_val, y_val, transform=val_transform)\n",
    "    test_dataset = BrainCTDataset(X_test, y_test, transform=val_transform)\n",
    "\n",
    "    # Create data loaders (set num_workers=0 for Windows compatibility)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=0)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
    "\n",
    "    # Create model\n",
    "    print(\"Creating ViT model...\")\n",
    "    model = ViTBrainClassifier(num_classes=len(class_names), pretrained=True)\n",
    "    model = model.to(device)\n",
    "\n",
    "    # Print model info\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print(f\"Total parameters: {total_params:,}\")\n",
    "    print(f\"Trainable parameters: {trainable_params:,}\")\n",
    "\n",
    "    # Train model\n",
    "    print(\"Starting training...\")\n",
    "    train_losses, train_accs, val_losses, val_accs = train_model(\n",
    "        model, train_loader, val_loader, NUM_EPOCHS, LEARNING_RATE\n",
    "    )\n",
    "\n",
    "    # Plot training history\n",
    "    plot_training_history(train_losses, train_accs, val_losses, val_accs)\n",
    "\n",
    "    # Load best model and evaluate\n",
    "    print(\"Loading best model for evaluation...\")\n",
    "    model.load_state_dict(torch.load('Balanced_dataset_vit_stroke.pth'))\n",
    "    test_accuracy = evaluate_model(model, test_loader, class_names)\n",
    "\n",
    "    print(f\"Final Test Accuracy: {test_accuracy:.4f}\")\n",
    "    torch.save(model.state_dict(), 'Balanced_dataset_vit_stroke.pth')\n",
    "    torch.save(model, 'Balanced_dataset_vit_stroke.h5')\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n",
    "# Additional utility functions for small datasets\n",
    "\n",
    "def create_weighted_sampler(labels):\n",
    "    \"\"\"Create weighted sampler for imbalanced datasets\"\"\"\n",
    "    from torch.utils.data import WeightedRandomSampler\n",
    "\n",
    "    class_counts = np.bincount(labels)\n",
    "    class_weights = 1.0 / class_counts\n",
    "    sample_weights = [class_weights[label] for label in labels]\n",
    "\n",
    "    return WeightedRandomSampler(\n",
    "        weights=sample_weights,\n",
    "        num_samples=len(sample_weights),\n",
    "        replacement=True\n",
    "    )\n",
    "\n",
    "def apply_mixup(data, targets, alpha=0.2):\n",
    "    \"\"\"Apply MixUp augmentation for small datasets\"\"\"\n",
    "    if alpha > 0:\n",
    "        lam = np.random.beta(alpha, alpha)\n",
    "    else:\n",
    "        lam = 1\n",
    "\n",
    "    batch_size = data.size(0)\n",
    "    index = torch.randperm(batch_size)\n",
    "\n",
    "    mixed_data = lam * data + (1 - lam) * data[index, :]\n",
    "    targets_a, targets_b = targets, targets[index]\n",
    "\n",
    "    return mixed_data, targets_a, targets_b, lam\n",
    "\n",
    "def focal_loss(inputs, targets, alpha=1, gamma=2):\n",
    "    \"\"\"Focal loss for handling class imbalance\"\"\"\n",
    "    ce_loss = nn.CrossEntropyLoss(reduction='none')(inputs, targets)\n",
    "    pt = torch.exp(-ce_loss)\n",
    "    focal_loss = alpha * (1 - pt) ** gamma * ce_loss\n",
    "    return focal_loss.mean()\n",
    "\n",
    "# Example usage for very small datasets (< 100 images):\n",
    "def small_dataset_modifications():\n",
    "    \"\"\"\n",
    "    For very small datasets, consider these modifications:\n",
    "\n",
    "    1. Use stronger data augmentation\n",
    "    2. Reduce model size or use smaller ViT variants\n",
    "    3. Apply transfer learning with frozen early layers\n",
    "    4. Use techniques like MixUp, CutMix\n",
    "    5. Apply focal loss for class imbalance\n",
    "    6. Use k-fold cross-validation instead of single split\n",
    "    \"\"\"\n",
    "\n",
    "    # Example: Freeze early layers for transfer learning\n",
    "    def freeze_early_layers(model, num_layers_to_freeze=6):\n",
    "        for i, (name, param) in enumerate(model.vit.vit.encoder.layer.named_parameters()):\n",
    "            if i < num_layers_to_freeze:\n",
    "                param.requires_grad = False\n",
    "\n",
    "    # Example: Use smaller learning rate and longer training\n",
    "    optimizer_config = {\n",
    "        'lr': 5e-5,  # Smaller learning rate\n",
    "        'weight_decay': 0.01,\n",
    "        'eps': 1e-8\n",
    "    }\n",
    "\n",
    "    return optimizer_config"
   ],
   "id": "78516f74a3f6bbf5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Loading dataset...\n",
      "Total images: 120000\n",
      "Classes: ['Normal', 'Stroke']\n",
      "Class distribution: Normal: 60000, Stroke: 60000\n",
      "Dataset split:\n",
      "Train: 72000 images\n",
      "Validation: 24000 images\n",
      "Test: 24000 images\n",
      "Creating ViT model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized because the shapes did not match:\n",
      "- classifier.bias: found shape torch.Size([1000]) in the checkpoint and torch.Size([2]) in the model instantiated\n",
      "- classifier.weight: found shape torch.Size([1000, 768]) in the checkpoint and torch.Size([2, 768]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters: 85,800,194\n",
      "Trainable parameters: 85,800,194\n",
      "Starting training...\n",
      "Starting epoch 1/25...\n",
      "Starting training loop with 2250 batches...\n",
      "Successfully loaded first batch: images shape torch.Size([32, 3, 224, 224]), labels shape torch.Size([32])\n",
      "Epoch 1/25, Batch 1/2250, Loss: 0.6991\n",
      "Epoch 1/25, Batch 11/2250, Loss: 0.7237\n",
      "Epoch 1/25, Batch 21/2250, Loss: 0.6190\n",
      "Epoch 1/25, Batch 31/2250, Loss: 0.5427\n",
      "Epoch 1/25, Batch 41/2250, Loss: 0.5985\n",
      "Epoch 1/25, Batch 51/2250, Loss: 0.5578\n",
      "Epoch 1/25, Batch 61/2250, Loss: 1.1644\n",
      "Epoch 1/25, Batch 71/2250, Loss: 0.5341\n",
      "Epoch 1/25, Batch 81/2250, Loss: 0.5284\n",
      "Epoch 1/25, Batch 91/2250, Loss: 0.3605\n",
      "Epoch 1/25, Batch 101/2250, Loss: 0.4721\n",
      "Epoch 1/25, Batch 111/2250, Loss: 0.4834\n",
      "Epoch 1/25, Batch 121/2250, Loss: 0.4848\n",
      "Epoch 1/25, Batch 131/2250, Loss: 0.5313\n",
      "Epoch 1/25, Batch 141/2250, Loss: 0.4661\n",
      "Epoch 1/25, Batch 151/2250, Loss: 0.4508\n",
      "Epoch 1/25, Batch 161/2250, Loss: 0.4358\n",
      "Epoch 1/25, Batch 171/2250, Loss: 0.4370\n",
      "Epoch 1/25, Batch 181/2250, Loss: 0.4193\n",
      "Epoch 1/25, Batch 191/2250, Loss: 0.4118\n",
      "Epoch 1/25, Batch 201/2250, Loss: 0.3919\n",
      "Epoch 1/25, Batch 211/2250, Loss: 0.3324\n",
      "Epoch 1/25, Batch 221/2250, Loss: 0.3600\n",
      "Epoch 1/25, Batch 231/2250, Loss: 0.4008\n",
      "Epoch 1/25, Batch 241/2250, Loss: 0.5268\n",
      "Epoch 1/25, Batch 251/2250, Loss: 0.4762\n",
      "Epoch 1/25, Batch 261/2250, Loss: 0.3345\n",
      "Epoch 1/25, Batch 271/2250, Loss: 0.6613\n",
      "Epoch 1/25, Batch 281/2250, Loss: 0.3138\n",
      "Epoch 1/25, Batch 291/2250, Loss: 0.3908\n",
      "Epoch 1/25, Batch 301/2250, Loss: 0.3835\n",
      "Epoch 1/25, Batch 311/2250, Loss: 0.4243\n",
      "Epoch 1/25, Batch 321/2250, Loss: 0.4996\n",
      "Epoch 1/25, Batch 331/2250, Loss: 0.3457\n",
      "Epoch 1/25, Batch 341/2250, Loss: 0.2256\n",
      "Epoch 1/25, Batch 351/2250, Loss: 0.2854\n",
      "Epoch 1/25, Batch 361/2250, Loss: 0.2569\n",
      "Epoch 1/25, Batch 371/2250, Loss: 0.3937\n",
      "Epoch 1/25, Batch 381/2250, Loss: 0.3662\n",
      "Epoch 1/25, Batch 391/2250, Loss: 0.4328\n",
      "Epoch 1/25, Batch 401/2250, Loss: 0.3562\n",
      "Epoch 1/25, Batch 411/2250, Loss: 0.3301\n",
      "Epoch 1/25, Batch 421/2250, Loss: 0.2650\n",
      "Epoch 1/25, Batch 431/2250, Loss: 0.3917\n",
      "Epoch 1/25, Batch 441/2250, Loss: 0.5627\n",
      "Epoch 1/25, Batch 451/2250, Loss: 0.5441\n",
      "Epoch 1/25, Batch 461/2250, Loss: 0.1169\n",
      "Epoch 1/25, Batch 471/2250, Loss: 0.5350\n",
      "Epoch 1/25, Batch 481/2250, Loss: 0.3374\n",
      "Epoch 1/25, Batch 491/2250, Loss: 0.4363\n",
      "Epoch 1/25, Batch 501/2250, Loss: 0.2736\n",
      "Epoch 1/25, Batch 511/2250, Loss: 0.2728\n",
      "Epoch 1/25, Batch 521/2250, Loss: 0.4072\n",
      "Epoch 1/25, Batch 531/2250, Loss: 0.3966\n",
      "Epoch 1/25, Batch 541/2250, Loss: 0.3359\n",
      "Epoch 1/25, Batch 551/2250, Loss: 0.3543\n",
      "Epoch 1/25, Batch 561/2250, Loss: 0.2531\n",
      "Epoch 1/25, Batch 571/2250, Loss: 0.3204\n",
      "Epoch 1/25, Batch 581/2250, Loss: 0.3852\n",
      "Epoch 1/25, Batch 591/2250, Loss: 0.2540\n",
      "Epoch 1/25, Batch 601/2250, Loss: 0.2288\n",
      "Epoch 1/25, Batch 611/2250, Loss: 0.2413\n",
      "Epoch 1/25, Batch 621/2250, Loss: 0.5909\n",
      "Epoch 1/25, Batch 631/2250, Loss: 0.2242\n",
      "Epoch 1/25, Batch 641/2250, Loss: 0.2816\n",
      "Epoch 1/25, Batch 651/2250, Loss: 0.4171\n",
      "Epoch 1/25, Batch 661/2250, Loss: 0.3481\n",
      "Epoch 1/25, Batch 671/2250, Loss: 0.2598\n",
      "Epoch 1/25, Batch 681/2250, Loss: 0.2412\n",
      "Epoch 1/25, Batch 691/2250, Loss: 0.3574\n",
      "Epoch 1/25, Batch 701/2250, Loss: 0.4306\n",
      "Epoch 1/25, Batch 711/2250, Loss: 0.5286\n",
      "Epoch 1/25, Batch 721/2250, Loss: 0.5225\n",
      "Epoch 1/25, Batch 731/2250, Loss: 0.3412\n",
      "Epoch 1/25, Batch 741/2250, Loss: 0.5069\n",
      "Epoch 1/25, Batch 751/2250, Loss: 0.3667\n",
      "Epoch 1/25, Batch 761/2250, Loss: 0.3127\n",
      "Epoch 1/25, Batch 771/2250, Loss: 0.3084\n",
      "Epoch 1/25, Batch 781/2250, Loss: 0.2188\n",
      "Epoch 1/25, Batch 791/2250, Loss: 0.3307\n",
      "Epoch 1/25, Batch 801/2250, Loss: 0.1286\n",
      "Epoch 1/25, Batch 811/2250, Loss: 0.3832\n",
      "Epoch 1/25, Batch 821/2250, Loss: 0.2928\n",
      "Epoch 1/25, Batch 831/2250, Loss: 0.2199\n",
      "Epoch 1/25, Batch 841/2250, Loss: 0.2842\n",
      "Epoch 1/25, Batch 851/2250, Loss: 0.4411\n",
      "Epoch 1/25, Batch 861/2250, Loss: 0.4937\n",
      "Epoch 1/25, Batch 871/2250, Loss: 0.3133\n",
      "Epoch 1/25, Batch 881/2250, Loss: 0.4570\n",
      "Epoch 1/25, Batch 891/2250, Loss: 0.2530\n",
      "Epoch 1/25, Batch 901/2250, Loss: 0.3582\n",
      "Epoch 1/25, Batch 911/2250, Loss: 0.3763\n",
      "Epoch 1/25, Batch 921/2250, Loss: 0.4991\n",
      "Epoch 1/25, Batch 931/2250, Loss: 0.4931\n",
      "Epoch 1/25, Batch 941/2250, Loss: 0.2540\n",
      "Epoch 1/25, Batch 951/2250, Loss: 0.1707\n",
      "Epoch 1/25, Batch 961/2250, Loss: 0.3159\n",
      "Epoch 1/25, Batch 971/2250, Loss: 0.3210\n",
      "Epoch 1/25, Batch 981/2250, Loss: 0.3590\n",
      "Epoch 1/25, Batch 991/2250, Loss: 0.4535\n",
      "Epoch 1/25, Batch 1001/2250, Loss: 0.1304\n",
      "Epoch 1/25, Batch 1011/2250, Loss: 0.2877\n",
      "Epoch 1/25, Batch 1021/2250, Loss: 0.2840\n",
      "Epoch 1/25, Batch 1031/2250, Loss: 0.3719\n",
      "Epoch 1/25, Batch 1041/2250, Loss: 0.2583\n",
      "Epoch 1/25, Batch 1051/2250, Loss: 0.3109\n",
      "Epoch 1/25, Batch 1061/2250, Loss: 0.5495\n",
      "Epoch 1/25, Batch 1071/2250, Loss: 0.2312\n",
      "Epoch 1/25, Batch 1081/2250, Loss: 0.3445\n",
      "Epoch 1/25, Batch 1091/2250, Loss: 0.3875\n",
      "Epoch 1/25, Batch 1101/2250, Loss: 0.2953\n",
      "Epoch 1/25, Batch 1111/2250, Loss: 0.2636\n",
      "Epoch 1/25, Batch 1121/2250, Loss: 0.2819\n",
      "Epoch 1/25, Batch 1131/2250, Loss: 0.3117\n",
      "Epoch 1/25, Batch 1141/2250, Loss: 0.2319\n",
      "Epoch 1/25, Batch 1151/2250, Loss: 0.3079\n",
      "Epoch 1/25, Batch 1161/2250, Loss: 0.2729\n",
      "Epoch 1/25, Batch 1171/2250, Loss: 0.3471\n",
      "Epoch 1/25, Batch 1181/2250, Loss: 0.2056\n",
      "Epoch 1/25, Batch 1191/2250, Loss: 0.4339\n",
      "Epoch 1/25, Batch 1201/2250, Loss: 0.1830\n",
      "Epoch 1/25, Batch 1211/2250, Loss: 0.2802\n",
      "Epoch 1/25, Batch 1221/2250, Loss: 0.2738\n",
      "Epoch 1/25, Batch 1231/2250, Loss: 0.3342\n",
      "Epoch 1/25, Batch 1241/2250, Loss: 0.1817\n",
      "Epoch 1/25, Batch 1251/2250, Loss: 0.3347\n",
      "Epoch 1/25, Batch 1261/2250, Loss: 0.3185\n",
      "Epoch 1/25, Batch 1271/2250, Loss: 0.3655\n",
      "Epoch 1/25, Batch 1281/2250, Loss: 0.2133\n",
      "Epoch 1/25, Batch 1291/2250, Loss: 0.2639\n",
      "Epoch 1/25, Batch 1301/2250, Loss: 0.2734\n",
      "Epoch 1/25, Batch 1311/2250, Loss: 0.2957\n",
      "Epoch 1/25, Batch 1321/2250, Loss: 0.3770\n",
      "Epoch 1/25, Batch 1331/2250, Loss: 0.3319\n",
      "Epoch 1/25, Batch 1341/2250, Loss: 0.3267\n",
      "Epoch 1/25, Batch 1351/2250, Loss: 0.1578\n",
      "Epoch 1/25, Batch 1361/2250, Loss: 0.2497\n",
      "Epoch 1/25, Batch 1371/2250, Loss: 0.2822\n",
      "Epoch 1/25, Batch 1381/2250, Loss: 0.3584\n",
      "Epoch 1/25, Batch 1391/2250, Loss: 0.2252\n",
      "Epoch 1/25, Batch 1401/2250, Loss: 0.3908\n",
      "Epoch 1/25, Batch 1411/2250, Loss: 0.3289\n",
      "Epoch 1/25, Batch 1421/2250, Loss: 0.2449\n",
      "Epoch 1/25, Batch 1431/2250, Loss: 0.1873\n",
      "Epoch 1/25, Batch 1441/2250, Loss: 0.3807\n",
      "Epoch 1/25, Batch 1451/2250, Loss: 0.2360\n",
      "Epoch 1/25, Batch 1461/2250, Loss: 0.2183\n",
      "Epoch 1/25, Batch 1471/2250, Loss: 0.3070\n",
      "Epoch 1/25, Batch 1481/2250, Loss: 0.4950\n",
      "Epoch 1/25, Batch 1491/2250, Loss: 0.2395\n",
      "Epoch 1/25, Batch 1501/2250, Loss: 0.3682\n",
      "Epoch 1/25, Batch 1511/2250, Loss: 0.2717\n",
      "Epoch 1/25, Batch 1521/2250, Loss: 0.3038\n",
      "Epoch 1/25, Batch 1531/2250, Loss: 0.3056\n",
      "Epoch 1/25, Batch 1541/2250, Loss: 0.3086\n",
      "Epoch 1/25, Batch 1551/2250, Loss: 0.2761\n",
      "Epoch 1/25, Batch 1561/2250, Loss: 0.2241\n",
      "Epoch 1/25, Batch 1571/2250, Loss: 0.4385\n",
      "Epoch 1/25, Batch 1581/2250, Loss: 0.3371\n",
      "Epoch 1/25, Batch 1591/2250, Loss: 0.1660\n",
      "Epoch 1/25, Batch 1601/2250, Loss: 0.1319\n",
      "Epoch 1/25, Batch 1611/2250, Loss: 0.4631\n",
      "Epoch 1/25, Batch 1621/2250, Loss: 0.1265\n",
      "Epoch 1/25, Batch 1631/2250, Loss: 0.2594\n",
      "Epoch 1/25, Batch 1641/2250, Loss: 0.2571\n",
      "Epoch 1/25, Batch 1651/2250, Loss: 0.1820\n",
      "Epoch 1/25, Batch 1661/2250, Loss: 0.2865\n",
      "Epoch 1/25, Batch 1671/2250, Loss: 0.3064\n",
      "Epoch 1/25, Batch 1681/2250, Loss: 0.1806\n",
      "Epoch 1/25, Batch 1691/2250, Loss: 0.4832\n",
      "Epoch 1/25, Batch 1701/2250, Loss: 0.1390\n",
      "Epoch 1/25, Batch 1711/2250, Loss: 0.1547\n",
      "Epoch 1/25, Batch 1721/2250, Loss: 0.1656\n",
      "Epoch 1/25, Batch 1731/2250, Loss: 0.2443\n",
      "Epoch 1/25, Batch 1741/2250, Loss: 0.1097\n",
      "Epoch 1/25, Batch 1751/2250, Loss: 0.2980\n",
      "Epoch 1/25, Batch 1761/2250, Loss: 0.1975\n",
      "Epoch 1/25, Batch 1771/2250, Loss: 0.1604\n",
      "Epoch 1/25, Batch 1781/2250, Loss: 0.3653\n",
      "Epoch 1/25, Batch 1791/2250, Loss: 0.2699\n",
      "Epoch 1/25, Batch 1801/2250, Loss: 0.2618\n",
      "Epoch 1/25, Batch 1811/2250, Loss: 0.1211\n",
      "Epoch 1/25, Batch 1821/2250, Loss: 0.3107\n",
      "Epoch 1/25, Batch 1831/2250, Loss: 0.1845\n",
      "Epoch 1/25, Batch 1841/2250, Loss: 0.2139\n",
      "Epoch 1/25, Batch 1851/2250, Loss: 0.4918\n",
      "Epoch 1/25, Batch 1861/2250, Loss: 0.2703\n",
      "Epoch 1/25, Batch 1871/2250, Loss: 0.3863\n",
      "Epoch 1/25, Batch 1881/2250, Loss: 0.2105\n",
      "Epoch 1/25, Batch 1891/2250, Loss: 0.2083\n",
      "Epoch 1/25, Batch 1901/2250, Loss: 0.3278\n",
      "Epoch 1/25, Batch 1911/2250, Loss: 0.2697\n",
      "Epoch 1/25, Batch 1921/2250, Loss: 0.2449\n",
      "Epoch 1/25, Batch 1931/2250, Loss: 0.1535\n",
      "Epoch 1/25, Batch 1941/2250, Loss: 0.3121\n",
      "Epoch 1/25, Batch 1951/2250, Loss: 0.3037\n",
      "Epoch 1/25, Batch 1961/2250, Loss: 0.4272\n",
      "Epoch 1/25, Batch 1971/2250, Loss: 0.2001\n",
      "Epoch 1/25, Batch 1981/2250, Loss: 0.2148\n",
      "Epoch 1/25, Batch 1991/2250, Loss: 0.2883\n",
      "Epoch 1/25, Batch 2001/2250, Loss: 0.2779\n",
      "Epoch 1/25, Batch 2011/2250, Loss: 0.1766\n",
      "Epoch 1/25, Batch 2021/2250, Loss: 0.1744\n",
      "Epoch 1/25, Batch 2031/2250, Loss: 0.2410\n",
      "Epoch 1/25, Batch 2041/2250, Loss: 0.2386\n",
      "Epoch 1/25, Batch 2051/2250, Loss: 0.2394\n",
      "Epoch 1/25, Batch 2061/2250, Loss: 0.2461\n",
      "Epoch 1/25, Batch 2071/2250, Loss: 0.2126\n",
      "Epoch 1/25, Batch 2081/2250, Loss: 0.1588\n",
      "Epoch 1/25, Batch 2091/2250, Loss: 0.2266\n",
      "Epoch 1/25, Batch 2101/2250, Loss: 0.2166\n",
      "Epoch 1/25, Batch 2111/2250, Loss: 0.3628\n",
      "Epoch 1/25, Batch 2121/2250, Loss: 0.4395\n",
      "Epoch 1/25, Batch 2131/2250, Loss: 0.3927\n",
      "Epoch 1/25, Batch 2141/2250, Loss: 0.2166\n",
      "Epoch 1/25, Batch 2151/2250, Loss: 0.3506\n",
      "Epoch 1/25, Batch 2161/2250, Loss: 0.1609\n",
      "Epoch 1/25, Batch 2171/2250, Loss: 0.2801\n",
      "Epoch 1/25, Batch 2181/2250, Loss: 0.2852\n",
      "Epoch 1/25, Batch 2191/2250, Loss: 0.2589\n",
      "Epoch 1/25, Batch 2201/2250, Loss: 0.3838\n",
      "Epoch 1/25, Batch 2211/2250, Loss: 0.1861\n",
      "Epoch 1/25, Batch 2221/2250, Loss: 0.2506\n",
      "Epoch 1/25, Batch 2231/2250, Loss: 0.2012\n",
      "Epoch 1/25, Batch 2241/2250, Loss: 0.0970\n",
      "Epoch 1/25:\n",
      "Train Loss: 0.3350, Train Acc: 84.79%\n",
      "Val Loss: 0.2375, Val Acc: 90.31%\n",
      "------------------------------------------------------------\n",
      "Starting epoch 2/25...\n",
      "Starting training loop with 2250 batches...\n",
      "Successfully loaded first batch: images shape torch.Size([32, 3, 224, 224]), labels shape torch.Size([32])\n",
      "Epoch 2/25, Batch 1/2250, Loss: 0.2615\n",
      "Epoch 2/25, Batch 11/2250, Loss: 0.2454\n",
      "Epoch 2/25, Batch 21/2250, Loss: 0.3196\n",
      "Epoch 2/25, Batch 31/2250, Loss: 0.2621\n",
      "Epoch 2/25, Batch 41/2250, Loss: 0.1772\n",
      "Epoch 2/25, Batch 51/2250, Loss: 0.2965\n",
      "Epoch 2/25, Batch 61/2250, Loss: 0.2255\n",
      "Epoch 2/25, Batch 71/2250, Loss: 0.2953\n",
      "Epoch 2/25, Batch 81/2250, Loss: 0.2001\n",
      "Epoch 2/25, Batch 91/2250, Loss: 0.1365\n",
      "Epoch 2/25, Batch 101/2250, Loss: 0.0752\n",
      "Epoch 2/25, Batch 111/2250, Loss: 0.1552\n",
      "Epoch 2/25, Batch 121/2250, Loss: 0.3947\n",
      "Epoch 2/25, Batch 131/2250, Loss: 0.2439\n",
      "Epoch 2/25, Batch 141/2250, Loss: 0.1428\n",
      "Epoch 2/25, Batch 151/2250, Loss: 0.1660\n",
      "Epoch 2/25, Batch 161/2250, Loss: 0.0721\n",
      "Epoch 2/25, Batch 171/2250, Loss: 0.5727\n",
      "Epoch 2/25, Batch 181/2250, Loss: 0.1979\n",
      "Epoch 2/25, Batch 191/2250, Loss: 0.1931\n",
      "Epoch 2/25, Batch 201/2250, Loss: 0.1938\n",
      "Epoch 2/25, Batch 211/2250, Loss: 0.1976\n",
      "Epoch 2/25, Batch 221/2250, Loss: 0.1043\n",
      "Epoch 2/25, Batch 231/2250, Loss: 0.1520\n",
      "Epoch 2/25, Batch 241/2250, Loss: 0.2618\n",
      "Epoch 2/25, Batch 251/2250, Loss: 0.1305\n",
      "Epoch 2/25, Batch 261/2250, Loss: 0.0961\n",
      "Epoch 2/25, Batch 271/2250, Loss: 0.2993\n",
      "Epoch 2/25, Batch 281/2250, Loss: 0.2078\n",
      "Epoch 2/25, Batch 291/2250, Loss: 0.2345\n",
      "Epoch 2/25, Batch 301/2250, Loss: 0.2866\n",
      "Epoch 2/25, Batch 311/2250, Loss: 0.3461\n",
      "Epoch 2/25, Batch 321/2250, Loss: 0.4172\n",
      "Epoch 2/25, Batch 331/2250, Loss: 0.2344\n",
      "Epoch 2/25, Batch 341/2250, Loss: 0.1528\n",
      "Epoch 2/25, Batch 351/2250, Loss: 0.1942\n",
      "Epoch 2/25, Batch 361/2250, Loss: 0.3413\n",
      "Epoch 2/25, Batch 371/2250, Loss: 0.1758\n",
      "Epoch 2/25, Batch 381/2250, Loss: 0.2723\n",
      "Epoch 2/25, Batch 391/2250, Loss: 0.0967\n",
      "Epoch 2/25, Batch 401/2250, Loss: 0.2085\n",
      "Epoch 2/25, Batch 411/2250, Loss: 0.3889\n",
      "Epoch 2/25, Batch 421/2250, Loss: 0.1085\n",
      "Epoch 2/25, Batch 431/2250, Loss: 0.4656\n",
      "Epoch 2/25, Batch 441/2250, Loss: 0.2106\n",
      "Epoch 2/25, Batch 451/2250, Loss: 0.2429\n",
      "Epoch 2/25, Batch 461/2250, Loss: 0.0840\n",
      "Epoch 2/25, Batch 471/2250, Loss: 0.4953\n",
      "Epoch 2/25, Batch 481/2250, Loss: 0.2780\n",
      "Epoch 2/25, Batch 491/2250, Loss: 0.2468\n",
      "Epoch 2/25, Batch 501/2250, Loss: 0.2182\n",
      "Epoch 2/25, Batch 511/2250, Loss: 0.1922\n",
      "Epoch 2/25, Batch 521/2250, Loss: 0.2422\n",
      "Epoch 2/25, Batch 531/2250, Loss: 0.2874\n",
      "Epoch 2/25, Batch 541/2250, Loss: 0.1995\n",
      "Epoch 2/25, Batch 551/2250, Loss: 0.3078\n",
      "Epoch 2/25, Batch 561/2250, Loss: 0.2214\n",
      "Epoch 2/25, Batch 571/2250, Loss: 0.3419\n",
      "Epoch 2/25, Batch 581/2250, Loss: 0.2416\n",
      "Epoch 2/25, Batch 591/2250, Loss: 0.2676\n",
      "Epoch 2/25, Batch 601/2250, Loss: 0.1316\n",
      "Epoch 2/25, Batch 611/2250, Loss: 0.2171\n",
      "Epoch 2/25, Batch 621/2250, Loss: 0.4362\n",
      "Epoch 2/25, Batch 631/2250, Loss: 0.1430\n",
      "Epoch 2/25, Batch 641/2250, Loss: 0.1446\n",
      "Epoch 2/25, Batch 651/2250, Loss: 0.2520\n",
      "Epoch 2/25, Batch 661/2250, Loss: 0.1515\n",
      "Epoch 2/25, Batch 671/2250, Loss: 0.2358\n",
      "Epoch 2/25, Batch 681/2250, Loss: 0.2200\n",
      "Epoch 2/25, Batch 691/2250, Loss: 0.1305\n",
      "Epoch 2/25, Batch 701/2250, Loss: 0.1277\n",
      "Epoch 2/25, Batch 711/2250, Loss: 0.1816\n",
      "Epoch 2/25, Batch 721/2250, Loss: 0.2812\n",
      "Epoch 2/25, Batch 731/2250, Loss: 0.3077\n",
      "Epoch 2/25, Batch 741/2250, Loss: 0.1434\n",
      "Epoch 2/25, Batch 751/2250, Loss: 0.1793\n",
      "Epoch 2/25, Batch 761/2250, Loss: 0.2657\n",
      "Epoch 2/25, Batch 771/2250, Loss: 0.1014\n",
      "Epoch 2/25, Batch 781/2250, Loss: 0.2749\n",
      "Epoch 2/25, Batch 791/2250, Loss: 0.2120\n",
      "Epoch 2/25, Batch 801/2250, Loss: 0.1598\n",
      "Epoch 2/25, Batch 811/2250, Loss: 0.1493\n",
      "Epoch 2/25, Batch 821/2250, Loss: 0.1547\n",
      "Epoch 2/25, Batch 831/2250, Loss: 0.1452\n",
      "Epoch 2/25, Batch 841/2250, Loss: 0.2040\n",
      "Epoch 2/25, Batch 851/2250, Loss: 0.4065\n",
      "Epoch 2/25, Batch 861/2250, Loss: 0.1037\n",
      "Epoch 2/25, Batch 871/2250, Loss: 0.4453\n",
      "Epoch 2/25, Batch 881/2250, Loss: 0.1864\n",
      "Epoch 2/25, Batch 891/2250, Loss: 0.0764\n",
      "Epoch 2/25, Batch 901/2250, Loss: 0.2164\n",
      "Epoch 2/25, Batch 911/2250, Loss: 0.1687\n",
      "Epoch 2/25, Batch 921/2250, Loss: 0.4163\n",
      "Epoch 2/25, Batch 931/2250, Loss: 0.1472\n",
      "Epoch 2/25, Batch 941/2250, Loss: 0.1413\n",
      "Epoch 2/25, Batch 951/2250, Loss: 0.3840\n",
      "Epoch 2/25, Batch 961/2250, Loss: 0.2047\n",
      "Epoch 2/25, Batch 971/2250, Loss: 0.3119\n",
      "Epoch 2/25, Batch 981/2250, Loss: 0.4894\n",
      "Epoch 2/25, Batch 991/2250, Loss: 0.2117\n",
      "Epoch 2/25, Batch 1001/2250, Loss: 0.4389\n",
      "Epoch 2/25, Batch 1011/2250, Loss: 0.1994\n",
      "Epoch 2/25, Batch 1021/2250, Loss: 0.1574\n",
      "Epoch 2/25, Batch 1031/2250, Loss: 0.1696\n",
      "Epoch 2/25, Batch 1041/2250, Loss: 0.1318\n",
      "Epoch 2/25, Batch 1051/2250, Loss: 0.3682\n",
      "Epoch 2/25, Batch 1061/2250, Loss: 0.1936\n",
      "Epoch 2/25, Batch 1071/2250, Loss: 0.2870\n",
      "Epoch 2/25, Batch 1081/2250, Loss: 0.1793\n",
      "Epoch 2/25, Batch 1091/2250, Loss: 0.3533\n",
      "Epoch 2/25, Batch 1101/2250, Loss: 0.1447\n",
      "Epoch 2/25, Batch 1111/2250, Loss: 0.2029\n",
      "Epoch 2/25, Batch 1121/2250, Loss: 0.2995\n",
      "Epoch 2/25, Batch 1131/2250, Loss: 0.2263\n",
      "Epoch 2/25, Batch 1141/2250, Loss: 0.1441\n",
      "Epoch 2/25, Batch 1151/2250, Loss: 0.1919\n",
      "Epoch 2/25, Batch 1161/2250, Loss: 0.1669\n",
      "Epoch 2/25, Batch 1171/2250, Loss: 0.1846\n",
      "Epoch 2/25, Batch 1181/2250, Loss: 0.1329\n",
      "Epoch 2/25, Batch 1191/2250, Loss: 0.1833\n",
      "Epoch 2/25, Batch 1201/2250, Loss: 0.1298\n",
      "Epoch 2/25, Batch 1211/2250, Loss: 0.2486\n",
      "Epoch 2/25, Batch 1221/2250, Loss: 0.2783\n",
      "Epoch 2/25, Batch 1231/2250, Loss: 0.1545\n",
      "Epoch 2/25, Batch 1241/2250, Loss: 0.3161\n",
      "Epoch 2/25, Batch 1251/2250, Loss: 0.1698\n",
      "Epoch 2/25, Batch 1261/2250, Loss: 0.1403\n",
      "Epoch 2/25, Batch 1271/2250, Loss: 0.3315\n",
      "Epoch 2/25, Batch 1281/2250, Loss: 0.4370\n",
      "Epoch 2/25, Batch 1291/2250, Loss: 0.1693\n",
      "Epoch 2/25, Batch 1301/2250, Loss: 0.2113\n",
      "Epoch 2/25, Batch 1311/2250, Loss: 0.1818\n",
      "Epoch 2/25, Batch 1321/2250, Loss: 0.1804\n",
      "Epoch 2/25, Batch 1331/2250, Loss: 0.1906\n",
      "Epoch 2/25, Batch 1341/2250, Loss: 0.1793\n",
      "Epoch 2/25, Batch 1351/2250, Loss: 0.1586\n",
      "Epoch 2/25, Batch 1361/2250, Loss: 0.0880\n",
      "Epoch 2/25, Batch 1371/2250, Loss: 0.3885\n",
      "Epoch 2/25, Batch 1381/2250, Loss: 0.2004\n",
      "Epoch 2/25, Batch 1391/2250, Loss: 0.2621\n",
      "Epoch 2/25, Batch 1401/2250, Loss: 0.3864\n",
      "Epoch 2/25, Batch 1411/2250, Loss: 0.3178\n",
      "Epoch 2/25, Batch 1421/2250, Loss: 0.2198\n",
      "Epoch 2/25, Batch 1431/2250, Loss: 0.1585\n",
      "Epoch 2/25, Batch 1441/2250, Loss: 0.1872\n",
      "Epoch 2/25, Batch 1451/2250, Loss: 0.1555\n",
      "Epoch 2/25, Batch 1461/2250, Loss: 0.2473\n",
      "Epoch 2/25, Batch 1471/2250, Loss: 0.1527\n",
      "Epoch 2/25, Batch 1481/2250, Loss: 0.3961\n",
      "Epoch 2/25, Batch 1491/2250, Loss: 0.1721\n",
      "Epoch 2/25, Batch 1501/2250, Loss: 0.2215\n",
      "Epoch 2/25, Batch 1511/2250, Loss: 0.1267\n",
      "Epoch 2/25, Batch 1521/2250, Loss: 0.1821\n",
      "Epoch 2/25, Batch 1531/2250, Loss: 0.3164\n",
      "Epoch 2/25, Batch 1541/2250, Loss: 0.1730\n",
      "Epoch 2/25, Batch 1551/2250, Loss: 0.2309\n",
      "Epoch 2/25, Batch 1561/2250, Loss: 0.3317\n",
      "Epoch 2/25, Batch 1571/2250, Loss: 0.2378\n",
      "Epoch 2/25, Batch 1581/2250, Loss: 0.3563\n",
      "Epoch 2/25, Batch 1591/2250, Loss: 0.4350\n",
      "Epoch 2/25, Batch 1601/2250, Loss: 0.4228\n",
      "Epoch 2/25, Batch 1611/2250, Loss: 0.2148\n",
      "Epoch 2/25, Batch 1621/2250, Loss: 0.1735\n",
      "Epoch 2/25, Batch 1631/2250, Loss: 0.1111\n",
      "Epoch 2/25, Batch 1641/2250, Loss: 0.1695\n",
      "Epoch 2/25, Batch 1651/2250, Loss: 0.1047\n",
      "Epoch 2/25, Batch 1661/2250, Loss: 0.1209\n",
      "Epoch 2/25, Batch 1671/2250, Loss: 0.2815\n",
      "Epoch 2/25, Batch 1681/2250, Loss: 0.2751\n",
      "Epoch 2/25, Batch 1691/2250, Loss: 0.3258\n",
      "Epoch 2/25, Batch 1701/2250, Loss: 0.2428\n",
      "Epoch 2/25, Batch 1711/2250, Loss: 0.3415\n",
      "Epoch 2/25, Batch 1721/2250, Loss: 0.2245\n",
      "Epoch 2/25, Batch 1731/2250, Loss: 0.1672\n",
      "Epoch 2/25, Batch 1741/2250, Loss: 0.1465\n",
      "Epoch 2/25, Batch 1751/2250, Loss: 0.2769\n",
      "Epoch 2/25, Batch 1761/2250, Loss: 0.3366\n",
      "Epoch 2/25, Batch 1771/2250, Loss: 0.1176\n",
      "Epoch 2/25, Batch 1781/2250, Loss: 0.3333\n",
      "Epoch 2/25, Batch 1791/2250, Loss: 0.3433\n",
      "Epoch 2/25, Batch 1801/2250, Loss: 0.4020\n",
      "Epoch 2/25, Batch 1811/2250, Loss: 0.2873\n",
      "Epoch 2/25, Batch 1821/2250, Loss: 0.2644\n",
      "Epoch 2/25, Batch 1831/2250, Loss: 0.2969\n",
      "Epoch 2/25, Batch 1841/2250, Loss: 0.2983\n",
      "Epoch 2/25, Batch 1851/2250, Loss: 0.3465\n",
      "Epoch 2/25, Batch 1861/2250, Loss: 0.2838\n",
      "Epoch 2/25, Batch 1871/2250, Loss: 0.1466\n",
      "Epoch 2/25, Batch 1881/2250, Loss: 0.1723\n",
      "Epoch 2/25, Batch 1891/2250, Loss: 0.1614\n",
      "Epoch 2/25, Batch 1901/2250, Loss: 0.1202\n",
      "Epoch 2/25, Batch 1911/2250, Loss: 0.1451\n",
      "Epoch 2/25, Batch 1921/2250, Loss: 0.3156\n",
      "Epoch 2/25, Batch 1931/2250, Loss: 0.2066\n",
      "Epoch 2/25, Batch 1941/2250, Loss: 0.3002\n",
      "Epoch 2/25, Batch 1951/2250, Loss: 0.0926\n",
      "Epoch 2/25, Batch 1961/2250, Loss: 0.1377\n",
      "Epoch 2/25, Batch 1971/2250, Loss: 0.2711\n",
      "Epoch 2/25, Batch 1981/2250, Loss: 0.1963\n",
      "Epoch 2/25, Batch 1991/2250, Loss: 0.3410\n",
      "Epoch 2/25, Batch 2001/2250, Loss: 0.1388\n",
      "Epoch 2/25, Batch 2011/2250, Loss: 0.0920\n",
      "Epoch 2/25, Batch 2021/2250, Loss: 0.1060\n",
      "Epoch 2/25, Batch 2031/2250, Loss: 0.1173\n",
      "Epoch 2/25, Batch 2041/2250, Loss: 0.1639\n",
      "Epoch 2/25, Batch 2051/2250, Loss: 0.0977\n",
      "Epoch 2/25, Batch 2061/2250, Loss: 0.2180\n",
      "Epoch 2/25, Batch 2071/2250, Loss: 0.0820\n",
      "Epoch 2/25, Batch 2081/2250, Loss: 0.2581\n",
      "Epoch 2/25, Batch 2091/2250, Loss: 0.1955\n",
      "Epoch 2/25, Batch 2101/2250, Loss: 0.2837\n",
      "Epoch 2/25, Batch 2111/2250, Loss: 0.2841\n",
      "Epoch 2/25, Batch 2121/2250, Loss: 0.2438\n",
      "Epoch 2/25, Batch 2131/2250, Loss: 0.2416\n",
      "Epoch 2/25, Batch 2141/2250, Loss: 0.4445\n",
      "Epoch 2/25, Batch 2151/2250, Loss: 0.2267\n",
      "Epoch 2/25, Batch 2161/2250, Loss: 0.0867\n",
      "Epoch 2/25, Batch 2171/2250, Loss: 0.0661\n",
      "Epoch 2/25, Batch 2181/2250, Loss: 0.1217\n",
      "Epoch 2/25, Batch 2191/2250, Loss: 0.2079\n",
      "Epoch 2/25, Batch 2201/2250, Loss: 0.1126\n",
      "Epoch 2/25, Batch 2211/2250, Loss: 0.0948\n",
      "Epoch 2/25, Batch 2221/2250, Loss: 0.1188\n",
      "Epoch 2/25, Batch 2231/2250, Loss: 0.0661\n",
      "Epoch 2/25, Batch 2241/2250, Loss: 0.2459\n",
      "Epoch 2/25:\n",
      "Train Loss: 0.2219, Train Acc: 90.56%\n",
      "Val Loss: 0.1560, Val Acc: 93.77%\n",
      "------------------------------------------------------------\n",
      "Starting epoch 3/25...\n",
      "Starting training loop with 2250 batches...\n",
      "Successfully loaded first batch: images shape torch.Size([32, 3, 224, 224]), labels shape torch.Size([32])\n",
      "Epoch 3/25, Batch 1/2250, Loss: 0.1027\n",
      "Epoch 3/25, Batch 11/2250, Loss: 0.1938\n",
      "Epoch 3/25, Batch 21/2250, Loss: 0.1098\n",
      "Epoch 3/25, Batch 31/2250, Loss: 0.1411\n",
      "Epoch 3/25, Batch 41/2250, Loss: 0.1471\n",
      "Epoch 3/25, Batch 51/2250, Loss: 0.3291\n",
      "Epoch 3/25, Batch 61/2250, Loss: 0.3662\n",
      "Epoch 3/25, Batch 71/2250, Loss: 0.1374\n",
      "Epoch 3/25, Batch 81/2250, Loss: 0.1044\n",
      "Epoch 3/25, Batch 91/2250, Loss: 0.0862\n",
      "Epoch 3/25, Batch 101/2250, Loss: 0.1146\n",
      "Epoch 3/25, Batch 111/2250, Loss: 0.0666\n",
      "Epoch 3/25, Batch 121/2250, Loss: 0.1181\n",
      "Epoch 3/25, Batch 131/2250, Loss: 0.0225\n",
      "Epoch 3/25, Batch 141/2250, Loss: 0.3109\n",
      "Epoch 3/25, Batch 151/2250, Loss: 0.2921\n",
      "Epoch 3/25, Batch 161/2250, Loss: 0.0839\n",
      "Epoch 3/25, Batch 171/2250, Loss: 0.2284\n",
      "Epoch 3/25, Batch 181/2250, Loss: 0.0338\n",
      "Epoch 3/25, Batch 191/2250, Loss: 0.1545\n",
      "Epoch 3/25, Batch 201/2250, Loss: 0.1123\n",
      "Epoch 3/25, Batch 211/2250, Loss: 0.1819\n",
      "Epoch 3/25, Batch 221/2250, Loss: 0.1971\n",
      "Epoch 3/25, Batch 231/2250, Loss: 0.1731\n",
      "Epoch 3/25, Batch 241/2250, Loss: 0.2208\n",
      "Epoch 3/25, Batch 251/2250, Loss: 0.2185\n",
      "Epoch 3/25, Batch 261/2250, Loss: 0.2407\n",
      "Epoch 3/25, Batch 271/2250, Loss: 0.1316\n",
      "Epoch 3/25, Batch 281/2250, Loss: 0.1634\n",
      "Epoch 3/25, Batch 291/2250, Loss: 0.2486\n",
      "Epoch 3/25, Batch 301/2250, Loss: 0.2364\n",
      "Epoch 3/25, Batch 311/2250, Loss: 0.1224\n",
      "Epoch 3/25, Batch 321/2250, Loss: 0.1161\n",
      "Epoch 3/25, Batch 331/2250, Loss: 0.1494\n",
      "Epoch 3/25, Batch 341/2250, Loss: 0.1368\n",
      "Epoch 3/25, Batch 351/2250, Loss: 0.1346\n",
      "Epoch 3/25, Batch 361/2250, Loss: 0.0558\n",
      "Epoch 3/25, Batch 371/2250, Loss: 0.1192\n",
      "Epoch 3/25, Batch 381/2250, Loss: 0.1675\n",
      "Epoch 3/25, Batch 391/2250, Loss: 0.2061\n",
      "Epoch 3/25, Batch 401/2250, Loss: 0.3295\n",
      "Epoch 3/25, Batch 411/2250, Loss: 0.0796\n",
      "Epoch 3/25, Batch 421/2250, Loss: 0.1509\n",
      "Epoch 3/25, Batch 431/2250, Loss: 0.0532\n",
      "Epoch 3/25, Batch 441/2250, Loss: 0.1003\n",
      "Epoch 3/25, Batch 451/2250, Loss: 0.1092\n",
      "Epoch 3/25, Batch 461/2250, Loss: 0.1962\n",
      "Epoch 3/25, Batch 471/2250, Loss: 0.2523\n",
      "Epoch 3/25, Batch 481/2250, Loss: 0.1951\n",
      "Epoch 3/25, Batch 491/2250, Loss: 0.3082\n",
      "Epoch 3/25, Batch 501/2250, Loss: 0.2228\n",
      "Epoch 3/25, Batch 511/2250, Loss: 0.0168\n",
      "Epoch 3/25, Batch 521/2250, Loss: 0.0725\n",
      "Epoch 3/25, Batch 531/2250, Loss: 0.3066\n",
      "Epoch 3/25, Batch 541/2250, Loss: 0.2792\n",
      "Epoch 3/25, Batch 551/2250, Loss: 0.2105\n",
      "Epoch 3/25, Batch 561/2250, Loss: 0.1920\n",
      "Epoch 3/25, Batch 571/2250, Loss: 0.1616\n",
      "Epoch 3/25, Batch 581/2250, Loss: 0.0595\n",
      "Epoch 3/25, Batch 591/2250, Loss: 0.1943\n",
      "Epoch 3/25, Batch 601/2250, Loss: 0.1762\n",
      "Epoch 3/25, Batch 611/2250, Loss: 0.1562\n",
      "Epoch 3/25, Batch 621/2250, Loss: 0.0929\n",
      "Epoch 3/25, Batch 631/2250, Loss: 0.1169\n",
      "Epoch 3/25, Batch 641/2250, Loss: 0.1080\n",
      "Epoch 3/25, Batch 651/2250, Loss: 0.1289\n",
      "Epoch 3/25, Batch 661/2250, Loss: 0.0466\n",
      "Epoch 3/25, Batch 671/2250, Loss: 0.1896\n",
      "Epoch 3/25, Batch 681/2250, Loss: 0.1074\n",
      "Epoch 3/25, Batch 691/2250, Loss: 0.0358\n",
      "Epoch 3/25, Batch 701/2250, Loss: 0.1829\n",
      "Epoch 3/25, Batch 711/2250, Loss: 0.3918\n",
      "Epoch 3/25, Batch 721/2250, Loss: 0.2049\n",
      "Epoch 3/25, Batch 731/2250, Loss: 0.1822\n",
      "Epoch 3/25, Batch 741/2250, Loss: 0.0093\n",
      "Epoch 3/25, Batch 751/2250, Loss: 0.1640\n",
      "Epoch 3/25, Batch 761/2250, Loss: 0.1163\n",
      "Epoch 3/25, Batch 771/2250, Loss: 0.4021\n",
      "Epoch 3/25, Batch 781/2250, Loss: 0.2137\n",
      "Epoch 3/25, Batch 791/2250, Loss: 0.1872\n",
      "Epoch 3/25, Batch 801/2250, Loss: 0.2148\n",
      "Epoch 3/25, Batch 811/2250, Loss: 0.0765\n",
      "Epoch 3/25, Batch 821/2250, Loss: 0.1981\n",
      "Epoch 3/25, Batch 831/2250, Loss: 0.0760\n",
      "Epoch 3/25, Batch 841/2250, Loss: 0.0598\n",
      "Epoch 3/25, Batch 851/2250, Loss: 0.0384\n",
      "Epoch 3/25, Batch 861/2250, Loss: 0.2664\n",
      "Epoch 3/25, Batch 871/2250, Loss: 0.3411\n",
      "Epoch 3/25, Batch 881/2250, Loss: 0.2069\n",
      "Epoch 3/25, Batch 891/2250, Loss: 0.2155\n",
      "Epoch 3/25, Batch 901/2250, Loss: 0.1376\n",
      "Epoch 3/25, Batch 911/2250, Loss: 0.1766\n",
      "Epoch 3/25, Batch 921/2250, Loss: 0.3159\n",
      "Epoch 3/25, Batch 931/2250, Loss: 0.3254\n",
      "Epoch 3/25, Batch 941/2250, Loss: 0.1950\n",
      "Epoch 3/25, Batch 951/2250, Loss: 0.2890\n",
      "Epoch 3/25, Batch 961/2250, Loss: 0.2978\n",
      "Epoch 3/25, Batch 971/2250, Loss: 0.0815\n",
      "Epoch 3/25, Batch 981/2250, Loss: 0.0777\n",
      "Epoch 3/25, Batch 991/2250, Loss: 0.3080\n",
      "Epoch 3/25, Batch 1001/2250, Loss: 0.1760\n",
      "Epoch 3/25, Batch 1011/2250, Loss: 0.1832\n",
      "Epoch 3/25, Batch 1021/2250, Loss: 0.1147\n",
      "Epoch 3/25, Batch 1031/2250, Loss: 0.1317\n",
      "Epoch 3/25, Batch 1041/2250, Loss: 0.1183\n",
      "Epoch 3/25, Batch 1051/2250, Loss: 0.1889\n",
      "Epoch 3/25, Batch 1061/2250, Loss: 0.3578\n",
      "Epoch 3/25, Batch 1071/2250, Loss: 0.1016\n",
      "Epoch 3/25, Batch 1081/2250, Loss: 0.2931\n",
      "Epoch 3/25, Batch 1091/2250, Loss: 0.1424\n",
      "Epoch 3/25, Batch 1101/2250, Loss: 0.0827\n",
      "Epoch 3/25, Batch 1111/2250, Loss: 0.3690\n",
      "Epoch 3/25, Batch 1121/2250, Loss: 0.1834\n",
      "Epoch 3/25, Batch 1131/2250, Loss: 0.1778\n",
      "Epoch 3/25, Batch 1141/2250, Loss: 0.1853\n",
      "Epoch 3/25, Batch 1151/2250, Loss: 0.0861\n",
      "Epoch 3/25, Batch 1161/2250, Loss: 0.1346\n",
      "Epoch 3/25, Batch 1171/2250, Loss: 0.0382\n",
      "Epoch 3/25, Batch 1181/2250, Loss: 0.1243\n",
      "Epoch 3/25, Batch 1191/2250, Loss: 0.2052\n",
      "Epoch 3/25, Batch 1201/2250, Loss: 0.0686\n",
      "Epoch 3/25, Batch 1211/2250, Loss: 0.1654\n",
      "Epoch 3/25, Batch 1221/2250, Loss: 0.1023\n",
      "Epoch 3/25, Batch 1231/2250, Loss: 0.1088\n",
      "Epoch 3/25, Batch 1241/2250, Loss: 0.1813\n",
      "Epoch 3/25, Batch 1251/2250, Loss: 0.1452\n",
      "Epoch 3/25, Batch 1261/2250, Loss: 0.3284\n",
      "Epoch 3/25, Batch 1271/2250, Loss: 0.0810\n",
      "Epoch 3/25, Batch 1281/2250, Loss: 0.2231\n",
      "Epoch 3/25, Batch 1291/2250, Loss: 0.0672\n",
      "Epoch 3/25, Batch 1301/2250, Loss: 0.1380\n",
      "Epoch 3/25, Batch 1311/2250, Loss: 0.0762\n",
      "Epoch 3/25, Batch 1321/2250, Loss: 0.1493\n",
      "Epoch 3/25, Batch 1331/2250, Loss: 0.0955\n",
      "Epoch 3/25, Batch 1341/2250, Loss: 0.2560\n",
      "Epoch 3/25, Batch 1351/2250, Loss: 0.1628\n",
      "Epoch 3/25, Batch 1361/2250, Loss: 0.1325\n",
      "Epoch 3/25, Batch 1371/2250, Loss: 0.1261\n",
      "Epoch 3/25, Batch 1381/2250, Loss: 0.1258\n",
      "Epoch 3/25, Batch 1391/2250, Loss: 0.0793\n",
      "Epoch 3/25, Batch 1401/2250, Loss: 0.1713\n",
      "Epoch 3/25, Batch 1411/2250, Loss: 0.2134\n",
      "Epoch 3/25, Batch 1421/2250, Loss: 0.2442\n",
      "Epoch 3/25, Batch 1431/2250, Loss: 0.2160\n",
      "Epoch 3/25, Batch 1441/2250, Loss: 0.0881\n",
      "Epoch 3/25, Batch 1451/2250, Loss: 0.2555\n",
      "Epoch 3/25, Batch 1461/2250, Loss: 0.1499\n",
      "Epoch 3/25, Batch 1471/2250, Loss: 0.0741\n",
      "Epoch 3/25, Batch 1481/2250, Loss: 0.3581\n",
      "Epoch 3/25, Batch 1491/2250, Loss: 0.1425\n",
      "Epoch 3/25, Batch 1501/2250, Loss: 0.0997\n",
      "Epoch 3/25, Batch 1511/2250, Loss: 0.0853\n",
      "Epoch 3/25, Batch 1521/2250, Loss: 0.2371\n",
      "Epoch 3/25, Batch 1531/2250, Loss: 0.2050\n",
      "Epoch 3/25, Batch 1541/2250, Loss: 0.2046\n",
      "Epoch 3/25, Batch 1551/2250, Loss: 0.1554\n",
      "Epoch 3/25, Batch 1561/2250, Loss: 0.0911\n",
      "Epoch 3/25, Batch 1571/2250, Loss: 0.2396\n",
      "Epoch 3/25, Batch 1581/2250, Loss: 0.1118\n",
      "Epoch 3/25, Batch 1591/2250, Loss: 0.1736\n",
      "Epoch 3/25, Batch 1601/2250, Loss: 0.0856\n",
      "Epoch 3/25, Batch 1611/2250, Loss: 0.0755\n",
      "Epoch 3/25, Batch 1621/2250, Loss: 0.0553\n",
      "Epoch 3/25, Batch 1631/2250, Loss: 0.1317\n",
      "Epoch 3/25, Batch 1641/2250, Loss: 0.1386\n",
      "Epoch 3/25, Batch 1651/2250, Loss: 0.3027\n",
      "Epoch 3/25, Batch 1661/2250, Loss: 0.2456\n",
      "Epoch 3/25, Batch 1671/2250, Loss: 0.3305\n",
      "Epoch 3/25, Batch 1681/2250, Loss: 0.0606\n",
      "Epoch 3/25, Batch 1691/2250, Loss: 0.3326\n",
      "Epoch 3/25, Batch 1701/2250, Loss: 0.1300\n",
      "Epoch 3/25, Batch 1711/2250, Loss: 0.1096\n",
      "Epoch 3/25, Batch 1721/2250, Loss: 0.1224\n",
      "Epoch 3/25, Batch 1731/2250, Loss: 0.1821\n",
      "Epoch 3/25, Batch 1741/2250, Loss: 0.2013\n",
      "Epoch 3/25, Batch 1751/2250, Loss: 0.1938\n",
      "Epoch 3/25, Batch 1761/2250, Loss: 0.3143\n",
      "Epoch 3/25, Batch 1771/2250, Loss: 0.2348\n",
      "Epoch 3/25, Batch 1781/2250, Loss: 0.1291\n",
      "Epoch 3/25, Batch 1791/2250, Loss: 0.1565\n",
      "Epoch 3/25, Batch 1801/2250, Loss: 0.1210\n",
      "Epoch 3/25, Batch 1811/2250, Loss: 0.1137\n",
      "Epoch 3/25, Batch 1821/2250, Loss: 0.0660\n",
      "Epoch 3/25, Batch 1831/2250, Loss: 0.1260\n",
      "Epoch 3/25, Batch 1841/2250, Loss: 0.0612\n",
      "Epoch 3/25, Batch 1851/2250, Loss: 0.1316\n",
      "Epoch 3/25, Batch 1861/2250, Loss: 0.0933\n",
      "Epoch 3/25, Batch 1871/2250, Loss: 0.1368\n",
      "Epoch 3/25, Batch 1881/2250, Loss: 0.2591\n",
      "Epoch 3/25, Batch 1891/2250, Loss: 0.2174\n",
      "Epoch 3/25, Batch 1901/2250, Loss: 0.2285\n",
      "Epoch 3/25, Batch 1911/2250, Loss: 0.1138\n",
      "Epoch 3/25, Batch 1921/2250, Loss: 0.0462\n",
      "Epoch 3/25, Batch 1931/2250, Loss: 0.2546\n",
      "Epoch 3/25, Batch 1941/2250, Loss: 0.1270\n",
      "Epoch 3/25, Batch 1951/2250, Loss: 0.2534\n",
      "Epoch 3/25, Batch 1961/2250, Loss: 0.2288\n",
      "Epoch 3/25, Batch 1971/2250, Loss: 0.1711\n",
      "Epoch 3/25, Batch 1981/2250, Loss: 0.5509\n",
      "Epoch 3/25, Batch 1991/2250, Loss: 0.0533\n",
      "Epoch 3/25, Batch 2001/2250, Loss: 0.2322\n",
      "Epoch 3/25, Batch 2011/2250, Loss: 0.1581\n",
      "Epoch 3/25, Batch 2021/2250, Loss: 0.0836\n",
      "Epoch 3/25, Batch 2031/2250, Loss: 0.1538\n",
      "Epoch 3/25, Batch 2041/2250, Loss: 0.2281\n",
      "Epoch 3/25, Batch 2051/2250, Loss: 0.0646\n",
      "Epoch 3/25, Batch 2061/2250, Loss: 0.2027\n",
      "Epoch 3/25, Batch 2071/2250, Loss: 0.0797\n",
      "Epoch 3/25, Batch 2081/2250, Loss: 0.1563\n",
      "Epoch 3/25, Batch 2091/2250, Loss: 0.1274\n",
      "Epoch 3/25, Batch 2101/2250, Loss: 0.0920\n",
      "Epoch 3/25, Batch 2111/2250, Loss: 0.0466\n",
      "Epoch 3/25, Batch 2121/2250, Loss: 0.2200\n",
      "Epoch 3/25, Batch 2131/2250, Loss: 0.1662\n",
      "Epoch 3/25, Batch 2141/2250, Loss: 0.0650\n",
      "Epoch 3/25, Batch 2151/2250, Loss: 0.1010\n",
      "Epoch 3/25, Batch 2161/2250, Loss: 0.2266\n",
      "Epoch 3/25, Batch 2171/2250, Loss: 0.1897\n",
      "Epoch 3/25, Batch 2181/2250, Loss: 0.1558\n",
      "Epoch 3/25, Batch 2191/2250, Loss: 0.2353\n",
      "Epoch 3/25, Batch 2201/2250, Loss: 0.1600\n",
      "Epoch 3/25, Batch 2211/2250, Loss: 0.1482\n",
      "Epoch 3/25, Batch 2221/2250, Loss: 0.1454\n",
      "Epoch 3/25, Batch 2231/2250, Loss: 0.0598\n",
      "Epoch 3/25, Batch 2241/2250, Loss: 0.0806\n",
      "Epoch 3/25:\n",
      "Train Loss: 0.1705, Train Acc: 93.09%\n",
      "Val Loss: 0.1407, Val Acc: 94.39%\n",
      "------------------------------------------------------------\n",
      "Starting epoch 4/25...\n",
      "Starting training loop with 2250 batches...\n",
      "Successfully loaded first batch: images shape torch.Size([32, 3, 224, 224]), labels shape torch.Size([32])\n",
      "Epoch 4/25, Batch 1/2250, Loss: 0.1757\n",
      "Epoch 4/25, Batch 11/2250, Loss: 0.0891\n",
      "Epoch 4/25, Batch 21/2250, Loss: 0.1037\n",
      "Epoch 4/25, Batch 31/2250, Loss: 0.1569\n",
      "Epoch 4/25, Batch 41/2250, Loss: 0.2863\n",
      "Epoch 4/25, Batch 51/2250, Loss: 0.1488\n",
      "Epoch 4/25, Batch 61/2250, Loss: 0.0873\n",
      "Epoch 4/25, Batch 71/2250, Loss: 0.0727\n",
      "Epoch 4/25, Batch 81/2250, Loss: 0.2442\n",
      "Epoch 4/25, Batch 91/2250, Loss: 0.2563\n",
      "Epoch 4/25, Batch 101/2250, Loss: 0.1181\n",
      "Epoch 4/25, Batch 111/2250, Loss: 0.1957\n",
      "Epoch 4/25, Batch 121/2250, Loss: 0.0194\n",
      "Epoch 4/25, Batch 131/2250, Loss: 0.2247\n",
      "Epoch 4/25, Batch 141/2250, Loss: 0.1738\n",
      "Epoch 4/25, Batch 151/2250, Loss: 0.0906\n",
      "Epoch 4/25, Batch 161/2250, Loss: 0.0628\n",
      "Epoch 4/25, Batch 171/2250, Loss: 0.0356\n",
      "Epoch 4/25, Batch 181/2250, Loss: 0.0642\n",
      "Epoch 4/25, Batch 191/2250, Loss: 0.0542\n",
      "Epoch 4/25, Batch 201/2250, Loss: 0.0652\n",
      "Epoch 4/25, Batch 211/2250, Loss: 0.1252\n",
      "Epoch 4/25, Batch 221/2250, Loss: 0.3186\n",
      "Epoch 4/25, Batch 231/2250, Loss: 0.1903\n",
      "Epoch 4/25, Batch 241/2250, Loss: 0.0671\n",
      "Epoch 4/25, Batch 251/2250, Loss: 0.2561\n",
      "Epoch 4/25, Batch 261/2250, Loss: 0.0528\n",
      "Epoch 4/25, Batch 271/2250, Loss: 0.1383\n",
      "Epoch 4/25, Batch 281/2250, Loss: 0.2469\n",
      "Epoch 4/25, Batch 291/2250, Loss: 0.1757\n",
      "Epoch 4/25, Batch 301/2250, Loss: 0.0628\n",
      "Epoch 4/25, Batch 311/2250, Loss: 0.0932\n",
      "Epoch 4/25, Batch 321/2250, Loss: 0.0600\n",
      "Epoch 4/25, Batch 331/2250, Loss: 0.0331\n",
      "Epoch 4/25, Batch 341/2250, Loss: 0.1765\n",
      "Epoch 4/25, Batch 351/2250, Loss: 0.2366\n",
      "Epoch 4/25, Batch 361/2250, Loss: 0.0500\n",
      "Epoch 4/25, Batch 371/2250, Loss: 0.1600\n",
      "Epoch 4/25, Batch 381/2250, Loss: 0.0291\n",
      "Epoch 4/25, Batch 391/2250, Loss: 0.4598\n",
      "Epoch 4/25, Batch 401/2250, Loss: 0.0466\n",
      "Epoch 4/25, Batch 411/2250, Loss: 0.0460\n",
      "Epoch 4/25, Batch 421/2250, Loss: 0.2596\n",
      "Epoch 4/25, Batch 431/2250, Loss: 0.3276\n",
      "Epoch 4/25, Batch 441/2250, Loss: 0.1046\n",
      "Epoch 4/25, Batch 451/2250, Loss: 0.2382\n",
      "Epoch 4/25, Batch 461/2250, Loss: 0.1265\n",
      "Epoch 4/25, Batch 471/2250, Loss: 0.2080\n",
      "Epoch 4/25, Batch 481/2250, Loss: 0.0954\n",
      "Epoch 4/25, Batch 491/2250, Loss: 0.1316\n",
      "Epoch 4/25, Batch 501/2250, Loss: 0.0661\n",
      "Epoch 4/25, Batch 511/2250, Loss: 0.1658\n",
      "Epoch 4/25, Batch 521/2250, Loss: 0.1350\n",
      "Epoch 4/25, Batch 531/2250, Loss: 0.1562\n",
      "Epoch 4/25, Batch 541/2250, Loss: 0.1554\n",
      "Epoch 4/25, Batch 551/2250, Loss: 0.1894\n",
      "Epoch 4/25, Batch 561/2250, Loss: 0.1630\n",
      "Epoch 4/25, Batch 571/2250, Loss: 0.2091\n",
      "Epoch 4/25, Batch 581/2250, Loss: 0.1016\n",
      "Epoch 4/25, Batch 591/2250, Loss: 0.0341\n",
      "Epoch 4/25, Batch 601/2250, Loss: 0.0975\n",
      "Epoch 4/25, Batch 611/2250, Loss: 0.1476\n",
      "Epoch 4/25, Batch 621/2250, Loss: 0.0535\n",
      "Epoch 4/25, Batch 631/2250, Loss: 0.1341\n",
      "Epoch 4/25, Batch 641/2250, Loss: 0.1455\n",
      "Epoch 4/25, Batch 651/2250, Loss: 0.0331\n",
      "Epoch 4/25, Batch 661/2250, Loss: 0.2044\n",
      "Epoch 4/25, Batch 671/2250, Loss: 0.1490\n",
      "Epoch 4/25, Batch 681/2250, Loss: 0.1099\n",
      "Epoch 4/25, Batch 691/2250, Loss: 0.2077\n",
      "Epoch 4/25, Batch 701/2250, Loss: 0.0820\n",
      "Epoch 4/25, Batch 711/2250, Loss: 0.0488\n",
      "Epoch 4/25, Batch 721/2250, Loss: 0.2020\n",
      "Epoch 4/25, Batch 731/2250, Loss: 0.0371\n",
      "Epoch 4/25, Batch 741/2250, Loss: 0.1219\n",
      "Epoch 4/25, Batch 751/2250, Loss: 0.1316\n",
      "Epoch 4/25, Batch 761/2250, Loss: 0.3296\n",
      "Epoch 4/25, Batch 771/2250, Loss: 0.2040\n",
      "Epoch 4/25, Batch 781/2250, Loss: 0.1484\n",
      "Epoch 4/25, Batch 791/2250, Loss: 0.2076\n",
      "Epoch 4/25, Batch 801/2250, Loss: 0.2047\n",
      "Epoch 4/25, Batch 811/2250, Loss: 0.0558\n",
      "Epoch 4/25, Batch 821/2250, Loss: 0.0819\n",
      "Epoch 4/25, Batch 831/2250, Loss: 0.1011\n",
      "Epoch 4/25, Batch 841/2250, Loss: 0.0984\n",
      "Epoch 4/25, Batch 851/2250, Loss: 0.0309\n",
      "Epoch 4/25, Batch 861/2250, Loss: 0.1775\n",
      "Epoch 4/25, Batch 871/2250, Loss: 0.1348\n",
      "Epoch 4/25, Batch 881/2250, Loss: 0.0527\n",
      "Epoch 4/25, Batch 891/2250, Loss: 0.0934\n",
      "Epoch 4/25, Batch 901/2250, Loss: 0.0459\n",
      "Epoch 4/25, Batch 911/2250, Loss: 0.2891\n",
      "Epoch 4/25, Batch 921/2250, Loss: 0.0611\n",
      "Epoch 4/25, Batch 931/2250, Loss: 0.1850\n",
      "Epoch 4/25, Batch 941/2250, Loss: 0.1464\n",
      "Epoch 4/25, Batch 951/2250, Loss: 0.2223\n",
      "Epoch 4/25, Batch 961/2250, Loss: 0.1499\n",
      "Epoch 4/25, Batch 971/2250, Loss: 0.2290\n",
      "Epoch 4/25, Batch 981/2250, Loss: 0.2459\n",
      "Epoch 4/25, Batch 991/2250, Loss: 0.2474\n",
      "Epoch 4/25, Batch 1001/2250, Loss: 0.1727\n",
      "Epoch 4/25, Batch 1011/2250, Loss: 0.2332\n",
      "Epoch 4/25, Batch 1021/2250, Loss: 0.1791\n",
      "Epoch 4/25, Batch 1031/2250, Loss: 0.0978\n",
      "Epoch 4/25, Batch 1041/2250, Loss: 0.1909\n",
      "Epoch 4/25, Batch 1051/2250, Loss: 0.0636\n",
      "Epoch 4/25, Batch 1061/2250, Loss: 0.1911\n",
      "Epoch 4/25, Batch 1071/2250, Loss: 0.0421\n",
      "Epoch 4/25, Batch 1081/2250, Loss: 0.0255\n",
      "Epoch 4/25, Batch 1091/2250, Loss: 0.0997\n",
      "Epoch 4/25, Batch 1101/2250, Loss: 0.2221\n",
      "Epoch 4/25, Batch 1111/2250, Loss: 0.0393\n",
      "Epoch 4/25, Batch 1121/2250, Loss: 0.2988\n",
      "Epoch 4/25, Batch 1131/2250, Loss: 0.1451\n",
      "Epoch 4/25, Batch 1141/2250, Loss: 0.1656\n",
      "Epoch 4/25, Batch 1151/2250, Loss: 0.3919\n",
      "Epoch 4/25, Batch 1161/2250, Loss: 0.1611\n",
      "Epoch 4/25, Batch 1171/2250, Loss: 0.1261\n",
      "Epoch 4/25, Batch 1181/2250, Loss: 0.2801\n",
      "Epoch 4/25, Batch 1191/2250, Loss: 0.1138\n",
      "Epoch 4/25, Batch 1201/2250, Loss: 0.1425\n",
      "Epoch 4/25, Batch 1211/2250, Loss: 0.0250\n",
      "Epoch 4/25, Batch 1221/2250, Loss: 0.1491\n",
      "Epoch 4/25, Batch 1231/2250, Loss: 0.3365\n",
      "Epoch 4/25, Batch 1241/2250, Loss: 0.1213\n",
      "Epoch 4/25, Batch 1251/2250, Loss: 0.2234\n",
      "Epoch 4/25, Batch 1261/2250, Loss: 0.1295\n",
      "Epoch 4/25, Batch 1271/2250, Loss: 0.0658\n",
      "Epoch 4/25, Batch 1281/2250, Loss: 0.0709\n",
      "Epoch 4/25, Batch 1291/2250, Loss: 0.2273\n",
      "Epoch 4/25, Batch 1301/2250, Loss: 0.1449\n",
      "Epoch 4/25, Batch 1311/2250, Loss: 0.1055\n",
      "Epoch 4/25, Batch 1321/2250, Loss: 0.1243\n",
      "Epoch 4/25, Batch 1331/2250, Loss: 0.1157\n",
      "Epoch 4/25, Batch 1341/2250, Loss: 0.0839\n",
      "Epoch 4/25, Batch 1351/2250, Loss: 0.0730\n",
      "Epoch 4/25, Batch 1361/2250, Loss: 0.2677\n",
      "Epoch 4/25, Batch 1371/2250, Loss: 0.1725\n",
      "Epoch 4/25, Batch 1381/2250, Loss: 0.0499\n",
      "Epoch 4/25, Batch 1391/2250, Loss: 0.0780\n",
      "Epoch 4/25, Batch 1401/2250, Loss: 0.0847\n",
      "Epoch 4/25, Batch 1411/2250, Loss: 0.0574\n",
      "Epoch 4/25, Batch 1421/2250, Loss: 0.0604\n",
      "Epoch 4/25, Batch 1431/2250, Loss: 0.1042\n",
      "Epoch 4/25, Batch 1441/2250, Loss: 0.2522\n",
      "Epoch 4/25, Batch 1451/2250, Loss: 0.0830\n",
      "Epoch 4/25, Batch 1461/2250, Loss: 0.1020\n",
      "Epoch 4/25, Batch 1471/2250, Loss: 0.1557\n",
      "Epoch 4/25, Batch 1481/2250, Loss: 0.0202\n",
      "Epoch 4/25, Batch 1491/2250, Loss: 0.1017\n",
      "Epoch 4/25, Batch 1501/2250, Loss: 0.2673\n",
      "Epoch 4/25, Batch 1511/2250, Loss: 0.1302\n",
      "Epoch 4/25, Batch 1521/2250, Loss: 0.0325\n",
      "Epoch 4/25, Batch 1531/2250, Loss: 0.1032\n",
      "Epoch 4/25, Batch 1541/2250, Loss: 0.1601\n",
      "Epoch 4/25, Batch 1551/2250, Loss: 0.1030\n",
      "Epoch 4/25, Batch 1561/2250, Loss: 0.0219\n",
      "Epoch 4/25, Batch 1571/2250, Loss: 0.0643\n",
      "Epoch 4/25, Batch 1581/2250, Loss: 0.2678\n",
      "Epoch 4/25, Batch 1591/2250, Loss: 0.0498\n",
      "Epoch 4/25, Batch 1601/2250, Loss: 0.0946\n",
      "Epoch 4/25, Batch 1611/2250, Loss: 0.1623\n",
      "Epoch 4/25, Batch 1621/2250, Loss: 0.0847\n",
      "Epoch 4/25, Batch 1631/2250, Loss: 0.1876\n",
      "Epoch 4/25, Batch 1641/2250, Loss: 0.2373\n",
      "Epoch 4/25, Batch 1651/2250, Loss: 0.0935\n",
      "Epoch 4/25, Batch 1661/2250, Loss: 0.1360\n",
      "Epoch 4/25, Batch 1671/2250, Loss: 0.2486\n",
      "Epoch 4/25, Batch 1681/2250, Loss: 0.1905\n",
      "Epoch 4/25, Batch 1691/2250, Loss: 0.0811\n",
      "Epoch 4/25, Batch 1701/2250, Loss: 0.0310\n",
      "Epoch 4/25, Batch 1711/2250, Loss: 0.1600\n",
      "Epoch 4/25, Batch 1721/2250, Loss: 0.0612\n",
      "Epoch 4/25, Batch 1731/2250, Loss: 0.0508\n",
      "Epoch 4/25, Batch 1741/2250, Loss: 0.1427\n",
      "Epoch 4/25, Batch 1751/2250, Loss: 0.0913\n",
      "Epoch 4/25, Batch 1761/2250, Loss: 0.1560\n",
      "Epoch 4/25, Batch 1771/2250, Loss: 0.0450\n",
      "Epoch 4/25, Batch 1781/2250, Loss: 0.0399\n",
      "Epoch 4/25, Batch 1791/2250, Loss: 0.2719\n",
      "Epoch 4/25, Batch 1801/2250, Loss: 0.1074\n",
      "Epoch 4/25, Batch 1811/2250, Loss: 0.0902\n",
      "Epoch 4/25, Batch 1821/2250, Loss: 0.2318\n",
      "Epoch 4/25, Batch 1831/2250, Loss: 0.1199\n",
      "Epoch 4/25, Batch 1841/2250, Loss: 0.0925\n",
      "Epoch 4/25, Batch 1851/2250, Loss: 0.0976\n",
      "Epoch 4/25, Batch 1861/2250, Loss: 0.0390\n",
      "Epoch 4/25, Batch 1871/2250, Loss: 0.0571\n",
      "Epoch 4/25, Batch 1881/2250, Loss: 0.1015\n",
      "Epoch 4/25, Batch 1891/2250, Loss: 0.1080\n",
      "Epoch 4/25, Batch 1901/2250, Loss: 0.1168\n",
      "Epoch 4/25, Batch 1911/2250, Loss: 0.0775\n",
      "Epoch 4/25, Batch 1921/2250, Loss: 0.0568\n",
      "Epoch 4/25, Batch 1931/2250, Loss: 0.1408\n",
      "Epoch 4/25, Batch 1941/2250, Loss: 0.0899\n",
      "Epoch 4/25, Batch 1951/2250, Loss: 0.0434\n",
      "Epoch 4/25, Batch 1961/2250, Loss: 0.0737\n",
      "Epoch 4/25, Batch 1971/2250, Loss: 0.2686\n",
      "Epoch 4/25, Batch 1981/2250, Loss: 0.2484\n",
      "Epoch 4/25, Batch 1991/2250, Loss: 0.1703\n",
      "Epoch 4/25, Batch 2001/2250, Loss: 0.1133\n",
      "Epoch 4/25, Batch 2011/2250, Loss: 0.1704\n",
      "Epoch 4/25, Batch 2021/2250, Loss: 0.0745\n",
      "Epoch 4/25, Batch 2031/2250, Loss: 0.0549\n",
      "Epoch 4/25, Batch 2041/2250, Loss: 0.1297\n",
      "Epoch 4/25, Batch 2051/2250, Loss: 0.0441\n",
      "Epoch 4/25, Batch 2061/2250, Loss: 0.0708\n",
      "Epoch 4/25, Batch 2071/2250, Loss: 0.1381\n",
      "Epoch 4/25, Batch 2081/2250, Loss: 0.0290\n",
      "Epoch 4/25, Batch 2091/2250, Loss: 0.1815\n",
      "Epoch 4/25, Batch 2101/2250, Loss: 0.1129\n",
      "Epoch 4/25, Batch 2111/2250, Loss: 0.1904\n",
      "Epoch 4/25, Batch 2121/2250, Loss: 0.0831\n",
      "Epoch 4/25, Batch 2131/2250, Loss: 0.0687\n",
      "Epoch 4/25, Batch 2141/2250, Loss: 0.1043\n",
      "Epoch 4/25, Batch 2151/2250, Loss: 0.0402\n",
      "Epoch 4/25, Batch 2161/2250, Loss: 0.1720\n",
      "Epoch 4/25, Batch 2171/2250, Loss: 0.1677\n",
      "Epoch 4/25, Batch 2181/2250, Loss: 0.0464\n",
      "Epoch 4/25, Batch 2191/2250, Loss: 0.0641\n",
      "Epoch 4/25, Batch 2201/2250, Loss: 0.1444\n",
      "Epoch 4/25, Batch 2211/2250, Loss: 0.2749\n",
      "Epoch 4/25, Batch 2221/2250, Loss: 0.1612\n",
      "Epoch 4/25, Batch 2231/2250, Loss: 0.0915\n",
      "Epoch 4/25, Batch 2241/2250, Loss: 0.1894\n",
      "Epoch 4/25:\n",
      "Train Loss: 0.1374, Train Acc: 94.57%\n",
      "Val Loss: 0.0913, Val Acc: 96.52%\n",
      "------------------------------------------------------------\n",
      "Starting epoch 5/25...\n",
      "Starting training loop with 2250 batches...\n",
      "Successfully loaded first batch: images shape torch.Size([32, 3, 224, 224]), labels shape torch.Size([32])\n",
      "Epoch 5/25, Batch 1/2250, Loss: 0.0763\n",
      "Epoch 5/25, Batch 11/2250, Loss: 0.1887\n",
      "Epoch 5/25, Batch 21/2250, Loss: 0.0135\n",
      "Epoch 5/25, Batch 31/2250, Loss: 0.0329\n",
      "Epoch 5/25, Batch 41/2250, Loss: 0.0652\n",
      "Epoch 5/25, Batch 51/2250, Loss: 0.1992\n",
      "Epoch 5/25, Batch 61/2250, Loss: 0.0648\n",
      "Epoch 5/25, Batch 71/2250, Loss: 0.0440\n",
      "Epoch 5/25, Batch 81/2250, Loss: 0.1063\n",
      "Epoch 5/25, Batch 91/2250, Loss: 0.0882\n",
      "Epoch 5/25, Batch 101/2250, Loss: 0.0601\n",
      "Epoch 5/25, Batch 111/2250, Loss: 0.0866\n",
      "Epoch 5/25, Batch 121/2250, Loss: 0.0152\n",
      "Epoch 5/25, Batch 131/2250, Loss: 0.2757\n",
      "Epoch 5/25, Batch 141/2250, Loss: 0.1486\n",
      "Epoch 5/25, Batch 151/2250, Loss: 0.1784\n",
      "Epoch 5/25, Batch 161/2250, Loss: 0.1791\n",
      "Epoch 5/25, Batch 171/2250, Loss: 0.0711\n",
      "Epoch 5/25, Batch 181/2250, Loss: 0.1449\n",
      "Epoch 5/25, Batch 191/2250, Loss: 0.1807\n",
      "Epoch 5/25, Batch 201/2250, Loss: 0.2455\n",
      "Epoch 5/25, Batch 211/2250, Loss: 0.0348\n",
      "Epoch 5/25, Batch 221/2250, Loss: 0.2430\n",
      "Epoch 5/25, Batch 231/2250, Loss: 0.1513\n",
      "Epoch 5/25, Batch 241/2250, Loss: 0.1641\n",
      "Epoch 5/25, Batch 251/2250, Loss: 0.0278\n",
      "Epoch 5/25, Batch 261/2250, Loss: 0.1402\n",
      "Epoch 5/25, Batch 271/2250, Loss: 0.0553\n",
      "Epoch 5/25, Batch 281/2250, Loss: 0.1638\n",
      "Epoch 5/25, Batch 291/2250, Loss: 0.2328\n",
      "Epoch 5/25, Batch 301/2250, Loss: 0.3221\n",
      "Epoch 5/25, Batch 311/2250, Loss: 0.0567\n",
      "Epoch 5/25, Batch 321/2250, Loss: 0.0262\n",
      "Epoch 5/25, Batch 331/2250, Loss: 0.0464\n",
      "Epoch 5/25, Batch 341/2250, Loss: 0.0819\n",
      "Epoch 5/25, Batch 351/2250, Loss: 0.0495\n",
      "Epoch 5/25, Batch 361/2250, Loss: 0.1081\n",
      "Epoch 5/25, Batch 371/2250, Loss: 0.1812\n",
      "Epoch 5/25, Batch 381/2250, Loss: 0.2119\n",
      "Epoch 5/25, Batch 391/2250, Loss: 0.0743\n",
      "Epoch 5/25, Batch 401/2250, Loss: 0.0841\n",
      "Epoch 5/25, Batch 411/2250, Loss: 0.0786\n",
      "Epoch 5/25, Batch 421/2250, Loss: 0.0037\n",
      "Epoch 5/25, Batch 431/2250, Loss: 0.1954\n",
      "Epoch 5/25, Batch 441/2250, Loss: 0.0662\n",
      "Epoch 5/25, Batch 451/2250, Loss: 0.2480\n",
      "Epoch 5/25, Batch 461/2250, Loss: 0.2152\n",
      "Epoch 5/25, Batch 471/2250, Loss: 0.0799\n",
      "Epoch 5/25, Batch 481/2250, Loss: 0.0878\n",
      "Epoch 5/25, Batch 491/2250, Loss: 0.1974\n",
      "Epoch 5/25, Batch 501/2250, Loss: 0.0470\n",
      "Epoch 5/25, Batch 511/2250, Loss: 0.0856\n",
      "Epoch 5/25, Batch 521/2250, Loss: 0.1039\n",
      "Epoch 5/25, Batch 531/2250, Loss: 0.1246\n",
      "Epoch 5/25, Batch 541/2250, Loss: 0.2828\n",
      "Epoch 5/25, Batch 551/2250, Loss: 0.1728\n",
      "Epoch 5/25, Batch 561/2250, Loss: 0.0861\n",
      "Epoch 5/25, Batch 571/2250, Loss: 0.2608\n",
      "Epoch 5/25, Batch 581/2250, Loss: 0.0743\n",
      "Epoch 5/25, Batch 591/2250, Loss: 0.0400\n",
      "Epoch 5/25, Batch 601/2250, Loss: 0.0822\n",
      "Epoch 5/25, Batch 611/2250, Loss: 0.1385\n",
      "Epoch 5/25, Batch 621/2250, Loss: 0.0547\n",
      "Epoch 5/25, Batch 631/2250, Loss: 0.1122\n",
      "Epoch 5/25, Batch 641/2250, Loss: 0.0958\n",
      "Epoch 5/25, Batch 651/2250, Loss: 0.0783\n",
      "Epoch 5/25, Batch 661/2250, Loss: 0.1800\n",
      "Epoch 5/25, Batch 671/2250, Loss: 0.0616\n",
      "Epoch 5/25, Batch 681/2250, Loss: 0.0941\n",
      "Epoch 5/25, Batch 691/2250, Loss: 0.0995\n",
      "Epoch 5/25, Batch 701/2250, Loss: 0.1834\n",
      "Epoch 5/25, Batch 711/2250, Loss: 0.0503\n",
      "Epoch 5/25, Batch 721/2250, Loss: 0.0513\n",
      "Epoch 5/25, Batch 731/2250, Loss: 0.1962\n",
      "Epoch 5/25, Batch 741/2250, Loss: 0.0768\n",
      "Epoch 5/25, Batch 751/2250, Loss: 0.1096\n",
      "Epoch 5/25, Batch 761/2250, Loss: 0.1711\n",
      "Epoch 5/25, Batch 771/2250, Loss: 0.1075\n",
      "Epoch 5/25, Batch 781/2250, Loss: 0.0412\n",
      "Epoch 5/25, Batch 791/2250, Loss: 0.1829\n",
      "Epoch 5/25, Batch 801/2250, Loss: 0.0278\n",
      "Epoch 5/25, Batch 811/2250, Loss: 0.0384\n",
      "Epoch 5/25, Batch 821/2250, Loss: 0.1529\n",
      "Epoch 5/25, Batch 831/2250, Loss: 0.1173\n",
      "Epoch 5/25, Batch 841/2250, Loss: 0.2919\n",
      "Epoch 5/25, Batch 851/2250, Loss: 0.1336\n",
      "Epoch 5/25, Batch 861/2250, Loss: 0.1664\n",
      "Epoch 5/25, Batch 871/2250, Loss: 0.1012\n",
      "Epoch 5/25, Batch 881/2250, Loss: 0.3208\n",
      "Epoch 5/25, Batch 891/2250, Loss: 0.1404\n",
      "Epoch 5/25, Batch 901/2250, Loss: 0.0704\n",
      "Epoch 5/25, Batch 911/2250, Loss: 0.1328\n",
      "Epoch 5/25, Batch 921/2250, Loss: 0.0791\n",
      "Epoch 5/25, Batch 931/2250, Loss: 0.0753\n",
      "Epoch 5/25, Batch 941/2250, Loss: 0.1493\n",
      "Epoch 5/25, Batch 951/2250, Loss: 0.1375\n",
      "Epoch 5/25, Batch 961/2250, Loss: 0.0373\n",
      "Epoch 5/25, Batch 971/2250, Loss: 0.1303\n",
      "Epoch 5/25, Batch 981/2250, Loss: 0.1288\n",
      "Epoch 5/25, Batch 991/2250, Loss: 0.1811\n",
      "Epoch 5/25, Batch 1001/2250, Loss: 0.1686\n",
      "Epoch 5/25, Batch 1011/2250, Loss: 0.1283\n",
      "Epoch 5/25, Batch 1021/2250, Loss: 0.0834\n",
      "Epoch 5/25, Batch 1031/2250, Loss: 0.1873\n",
      "Epoch 5/25, Batch 1041/2250, Loss: 0.0780\n",
      "Epoch 5/25, Batch 1051/2250, Loss: 0.1302\n",
      "Epoch 5/25, Batch 1061/2250, Loss: 0.0451\n",
      "Epoch 5/25, Batch 1071/2250, Loss: 0.0699\n",
      "Epoch 5/25, Batch 1081/2250, Loss: 0.0752\n",
      "Epoch 5/25, Batch 1091/2250, Loss: 0.1069\n",
      "Epoch 5/25, Batch 1101/2250, Loss: 0.0769\n",
      "Epoch 5/25, Batch 1111/2250, Loss: 0.1590\n",
      "Epoch 5/25, Batch 1121/2250, Loss: 0.1356\n",
      "Epoch 5/25, Batch 1131/2250, Loss: 0.3692\n",
      "Epoch 5/25, Batch 1141/2250, Loss: 0.1501\n",
      "Epoch 5/25, Batch 1151/2250, Loss: 0.0546\n",
      "Epoch 5/25, Batch 1161/2250, Loss: 0.0459\n",
      "Epoch 5/25, Batch 1171/2250, Loss: 0.0631\n",
      "Epoch 5/25, Batch 1181/2250, Loss: 0.2948\n",
      "Epoch 5/25, Batch 1191/2250, Loss: 0.0497\n",
      "Epoch 5/25, Batch 1201/2250, Loss: 0.1073\n",
      "Epoch 5/25, Batch 1211/2250, Loss: 0.1898\n",
      "Epoch 5/25, Batch 1221/2250, Loss: 0.0991\n",
      "Epoch 5/25, Batch 1231/2250, Loss: 0.1824\n",
      "Epoch 5/25, Batch 1241/2250, Loss: 0.0490\n",
      "Epoch 5/25, Batch 1251/2250, Loss: 0.0751\n",
      "Epoch 5/25, Batch 1261/2250, Loss: 0.1218\n",
      "Epoch 5/25, Batch 1271/2250, Loss: 0.1919\n",
      "Epoch 5/25, Batch 1281/2250, Loss: 0.1332\n",
      "Epoch 5/25, Batch 1291/2250, Loss: 0.3202\n",
      "Epoch 5/25, Batch 1301/2250, Loss: 0.0981\n",
      "Epoch 5/25, Batch 1311/2250, Loss: 0.2884\n",
      "Epoch 5/25, Batch 1321/2250, Loss: 0.2320\n",
      "Epoch 5/25, Batch 1331/2250, Loss: 0.2020\n",
      "Epoch 5/25, Batch 1341/2250, Loss: 0.1283\n",
      "Epoch 5/25, Batch 1351/2250, Loss: 0.0775\n",
      "Epoch 5/25, Batch 1361/2250, Loss: 0.1069\n",
      "Epoch 5/25, Batch 1371/2250, Loss: 0.0702\n",
      "Epoch 5/25, Batch 1381/2250, Loss: 0.1216\n",
      "Epoch 5/25, Batch 1391/2250, Loss: 0.2262\n",
      "Epoch 5/25, Batch 1401/2250, Loss: 0.0756\n",
      "Epoch 5/25, Batch 1411/2250, Loss: 0.0190\n",
      "Epoch 5/25, Batch 1421/2250, Loss: 0.0992\n",
      "Epoch 5/25, Batch 1431/2250, Loss: 0.0591\n",
      "Epoch 5/25, Batch 1441/2250, Loss: 0.0990\n",
      "Epoch 5/25, Batch 1451/2250, Loss: 0.0888\n",
      "Epoch 5/25, Batch 1461/2250, Loss: 0.0563\n",
      "Epoch 5/25, Batch 1471/2250, Loss: 0.1429\n",
      "Epoch 5/25, Batch 1481/2250, Loss: 0.0529\n",
      "Epoch 5/25, Batch 1491/2250, Loss: 0.0428\n",
      "Epoch 5/25, Batch 1501/2250, Loss: 0.0302\n",
      "Epoch 5/25, Batch 1511/2250, Loss: 0.0418\n",
      "Epoch 5/25, Batch 1521/2250, Loss: 0.0477\n",
      "Epoch 5/25, Batch 1531/2250, Loss: 0.1129\n",
      "Epoch 5/25, Batch 1541/2250, Loss: 0.1975\n",
      "Epoch 5/25, Batch 1551/2250, Loss: 0.1279\n",
      "Epoch 5/25, Batch 1561/2250, Loss: 0.1583\n",
      "Epoch 5/25, Batch 1571/2250, Loss: 0.1243\n",
      "Epoch 5/25, Batch 1581/2250, Loss: 0.0797\n",
      "Epoch 5/25, Batch 1591/2250, Loss: 0.0306\n",
      "Epoch 5/25, Batch 1601/2250, Loss: 0.0657\n",
      "Epoch 5/25, Batch 1611/2250, Loss: 0.0602\n",
      "Epoch 5/25, Batch 1621/2250, Loss: 0.1406\n",
      "Epoch 5/25, Batch 1631/2250, Loss: 0.2140\n",
      "Epoch 5/25, Batch 1641/2250, Loss: 0.0561\n",
      "Epoch 5/25, Batch 1651/2250, Loss: 0.1430\n",
      "Epoch 5/25, Batch 1661/2250, Loss: 0.1083\n",
      "Epoch 5/25, Batch 1671/2250, Loss: 0.0238\n",
      "Epoch 5/25, Batch 1681/2250, Loss: 0.1955\n",
      "Epoch 5/25, Batch 1691/2250, Loss: 0.0145\n",
      "Epoch 5/25, Batch 1701/2250, Loss: 0.1430\n",
      "Epoch 5/25, Batch 1711/2250, Loss: 0.1214\n",
      "Epoch 5/25, Batch 1721/2250, Loss: 0.1028\n",
      "Epoch 5/25, Batch 1731/2250, Loss: 0.2489\n",
      "Epoch 5/25, Batch 1741/2250, Loss: 0.0863\n",
      "Epoch 5/25, Batch 1751/2250, Loss: 0.0409\n",
      "Epoch 5/25, Batch 1761/2250, Loss: 0.0798\n",
      "Epoch 5/25, Batch 1771/2250, Loss: 0.0572\n",
      "Epoch 5/25, Batch 1781/2250, Loss: 0.1625\n",
      "Epoch 5/25, Batch 1791/2250, Loss: 0.1484\n",
      "Epoch 5/25, Batch 1801/2250, Loss: 0.1144\n",
      "Epoch 5/25, Batch 1811/2250, Loss: 0.0421\n",
      "Epoch 5/25, Batch 1821/2250, Loss: 0.1872\n",
      "Epoch 5/25, Batch 1831/2250, Loss: 0.1278\n",
      "Epoch 5/25, Batch 1841/2250, Loss: 0.2918\n",
      "Epoch 5/25, Batch 1851/2250, Loss: 0.0754\n",
      "Epoch 5/25, Batch 1861/2250, Loss: 0.1199\n",
      "Epoch 5/25, Batch 1871/2250, Loss: 0.0991\n",
      "Epoch 5/25, Batch 1881/2250, Loss: 0.1163\n",
      "Epoch 5/25, Batch 1891/2250, Loss: 0.0632\n",
      "Epoch 5/25, Batch 1901/2250, Loss: 0.1052\n",
      "Epoch 5/25, Batch 1911/2250, Loss: 0.1534\n",
      "Epoch 5/25, Batch 1921/2250, Loss: 0.0369\n",
      "Epoch 5/25, Batch 1931/2250, Loss: 0.2308\n",
      "Epoch 5/25, Batch 1941/2250, Loss: 0.0395\n",
      "Epoch 5/25, Batch 1951/2250, Loss: 0.0708\n",
      "Epoch 5/25, Batch 1961/2250, Loss: 0.0879\n",
      "Epoch 5/25, Batch 1971/2250, Loss: 0.1995\n",
      "Epoch 5/25, Batch 1981/2250, Loss: 0.0701\n",
      "Epoch 5/25, Batch 1991/2250, Loss: 0.1998\n",
      "Epoch 5/25, Batch 2001/2250, Loss: 0.0493\n",
      "Epoch 5/25, Batch 2011/2250, Loss: 0.1883\n",
      "Epoch 5/25, Batch 2021/2250, Loss: 0.1715\n",
      "Epoch 5/25, Batch 2031/2250, Loss: 0.1207\n",
      "Epoch 5/25, Batch 2041/2250, Loss: 0.0251\n",
      "Epoch 5/25, Batch 2051/2250, Loss: 0.0521\n",
      "Epoch 5/25, Batch 2061/2250, Loss: 0.2189\n",
      "Epoch 5/25, Batch 2071/2250, Loss: 0.0250\n",
      "Epoch 5/25, Batch 2081/2250, Loss: 0.2537\n",
      "Epoch 5/25, Batch 2091/2250, Loss: 0.0291\n",
      "Epoch 5/25, Batch 2101/2250, Loss: 0.1343\n",
      "Epoch 5/25, Batch 2111/2250, Loss: 0.0207\n",
      "Epoch 5/25, Batch 2121/2250, Loss: 0.1572\n",
      "Epoch 5/25, Batch 2131/2250, Loss: 0.1584\n",
      "Epoch 5/25, Batch 2141/2250, Loss: 0.0381\n",
      "Epoch 5/25, Batch 2151/2250, Loss: 0.1589\n",
      "Epoch 5/25, Batch 2161/2250, Loss: 0.2155\n",
      "Epoch 5/25, Batch 2171/2250, Loss: 0.1842\n",
      "Epoch 5/25, Batch 2181/2250, Loss: 0.0731\n",
      "Epoch 5/25, Batch 2191/2250, Loss: 0.1098\n",
      "Epoch 5/25, Batch 2201/2250, Loss: 0.0831\n",
      "Epoch 5/25, Batch 2211/2250, Loss: 0.1252\n",
      "Epoch 5/25, Batch 2221/2250, Loss: 0.0743\n",
      "Epoch 5/25, Batch 2231/2250, Loss: 0.0732\n",
      "Epoch 5/25, Batch 2241/2250, Loss: 0.0334\n",
      "Epoch 5/25:\n",
      "Train Loss: 0.1163, Train Acc: 95.42%\n",
      "Val Loss: 0.0777, Val Acc: 97.36%\n",
      "------------------------------------------------------------\n",
      "Starting epoch 6/25...\n",
      "Starting training loop with 2250 batches...\n",
      "Successfully loaded first batch: images shape torch.Size([32, 3, 224, 224]), labels shape torch.Size([32])\n",
      "Epoch 6/25, Batch 1/2250, Loss: 0.1516\n",
      "Epoch 6/25, Batch 11/2250, Loss: 0.0990\n",
      "Epoch 6/25, Batch 21/2250, Loss: 0.0628\n",
      "Epoch 6/25, Batch 31/2250, Loss: 0.0556\n",
      "Epoch 6/25, Batch 41/2250, Loss: 0.0535\n",
      "Epoch 6/25, Batch 51/2250, Loss: 0.0508\n",
      "Epoch 6/25, Batch 61/2250, Loss: 0.1154\n",
      "Epoch 6/25, Batch 71/2250, Loss: 0.0414\n",
      "Epoch 6/25, Batch 81/2250, Loss: 0.0135\n",
      "Epoch 6/25, Batch 91/2250, Loss: 0.0174\n",
      "Epoch 6/25, Batch 101/2250, Loss: 0.0205\n",
      "Epoch 6/25, Batch 111/2250, Loss: 0.1194\n",
      "Epoch 6/25, Batch 121/2250, Loss: 0.1108\n",
      "Epoch 6/25, Batch 131/2250, Loss: 0.1561\n",
      "Epoch 6/25, Batch 141/2250, Loss: 0.1048\n",
      "Epoch 6/25, Batch 151/2250, Loss: 0.0908\n",
      "Epoch 6/25, Batch 161/2250, Loss: 0.1103\n",
      "Epoch 6/25, Batch 171/2250, Loss: 0.0331\n",
      "Epoch 6/25, Batch 181/2250, Loss: 0.1463\n",
      "Epoch 6/25, Batch 191/2250, Loss: 0.2265\n",
      "Epoch 6/25, Batch 201/2250, Loss: 0.0705\n",
      "Epoch 6/25, Batch 211/2250, Loss: 0.0511\n",
      "Epoch 6/25, Batch 221/2250, Loss: 0.0558\n",
      "Epoch 6/25, Batch 231/2250, Loss: 0.0928\n",
      "Epoch 6/25, Batch 241/2250, Loss: 0.1783\n",
      "Epoch 6/25, Batch 251/2250, Loss: 0.0897\n",
      "Epoch 6/25, Batch 261/2250, Loss: 0.1978\n",
      "Epoch 6/25, Batch 271/2250, Loss: 0.2119\n",
      "Epoch 6/25, Batch 281/2250, Loss: 0.0829\n",
      "Epoch 6/25, Batch 291/2250, Loss: 0.1414\n",
      "Epoch 6/25, Batch 301/2250, Loss: 0.0681\n",
      "Epoch 6/25, Batch 311/2250, Loss: 0.0744\n",
      "Epoch 6/25, Batch 321/2250, Loss: 0.1419\n",
      "Epoch 6/25, Batch 331/2250, Loss: 0.0210\n",
      "Epoch 6/25, Batch 341/2250, Loss: 0.1998\n",
      "Epoch 6/25, Batch 351/2250, Loss: 0.1435\n",
      "Epoch 6/25, Batch 361/2250, Loss: 0.0362\n",
      "Epoch 6/25, Batch 371/2250, Loss: 0.0908\n",
      "Epoch 6/25, Batch 381/2250, Loss: 0.1174\n",
      "Epoch 6/25, Batch 391/2250, Loss: 0.1812\n",
      "Epoch 6/25, Batch 401/2250, Loss: 0.1885\n",
      "Epoch 6/25, Batch 411/2250, Loss: 0.0817\n",
      "Epoch 6/25, Batch 421/2250, Loss: 0.0619\n",
      "Epoch 6/25, Batch 431/2250, Loss: 0.0444\n",
      "Epoch 6/25, Batch 441/2250, Loss: 0.1279\n",
      "Epoch 6/25, Batch 451/2250, Loss: 0.0215\n",
      "Epoch 6/25, Batch 461/2250, Loss: 0.1132\n",
      "Epoch 6/25, Batch 471/2250, Loss: 0.0592\n",
      "Epoch 6/25, Batch 481/2250, Loss: 0.1743\n",
      "Epoch 6/25, Batch 491/2250, Loss: 0.0521\n",
      "Epoch 6/25, Batch 501/2250, Loss: 0.1287\n",
      "Epoch 6/25, Batch 511/2250, Loss: 0.2971\n",
      "Epoch 6/25, Batch 521/2250, Loss: 0.1317\n",
      "Epoch 6/25, Batch 531/2250, Loss: 0.2093\n",
      "Epoch 6/25, Batch 541/2250, Loss: 0.1442\n",
      "Epoch 6/25, Batch 551/2250, Loss: 0.1193\n",
      "Epoch 6/25, Batch 561/2250, Loss: 0.0095\n",
      "Epoch 6/25, Batch 571/2250, Loss: 0.0642\n",
      "Epoch 6/25, Batch 581/2250, Loss: 0.1610\n",
      "Epoch 6/25, Batch 591/2250, Loss: 0.0413\n",
      "Epoch 6/25, Batch 601/2250, Loss: 0.1412\n",
      "Epoch 6/25, Batch 611/2250, Loss: 0.2277\n",
      "Epoch 6/25, Batch 621/2250, Loss: 0.0434\n",
      "Epoch 6/25, Batch 631/2250, Loss: 0.1513\n",
      "Epoch 6/25, Batch 641/2250, Loss: 0.0387\n",
      "Epoch 6/25, Batch 651/2250, Loss: 0.0925\n",
      "Epoch 6/25, Batch 661/2250, Loss: 0.1800\n",
      "Epoch 6/25, Batch 671/2250, Loss: 0.0347\n",
      "Epoch 6/25, Batch 681/2250, Loss: 0.0908\n",
      "Epoch 6/25, Batch 691/2250, Loss: 0.0919\n",
      "Epoch 6/25, Batch 701/2250, Loss: 0.2846\n",
      "Epoch 6/25, Batch 711/2250, Loss: 0.0763\n",
      "Epoch 6/25, Batch 721/2250, Loss: 0.1248\n",
      "Epoch 6/25, Batch 731/2250, Loss: 0.1361\n",
      "Epoch 6/25, Batch 741/2250, Loss: 0.0816\n",
      "Epoch 6/25, Batch 751/2250, Loss: 0.1263\n",
      "Epoch 6/25, Batch 761/2250, Loss: 0.0761\n",
      "Epoch 6/25, Batch 771/2250, Loss: 0.0228\n",
      "Epoch 6/25, Batch 781/2250, Loss: 0.0733\n",
      "Epoch 6/25, Batch 791/2250, Loss: 0.0389\n",
      "Epoch 6/25, Batch 801/2250, Loss: 0.0663\n",
      "Epoch 6/25, Batch 811/2250, Loss: 0.2047\n",
      "Epoch 6/25, Batch 821/2250, Loss: 0.0674\n",
      "Epoch 6/25, Batch 831/2250, Loss: 0.0580\n",
      "Epoch 6/25, Batch 841/2250, Loss: 0.3650\n",
      "Epoch 6/25, Batch 851/2250, Loss: 0.1503\n",
      "Epoch 6/25, Batch 861/2250, Loss: 0.0338\n",
      "Epoch 6/25, Batch 871/2250, Loss: 0.0937\n",
      "Epoch 6/25, Batch 881/2250, Loss: 0.1109\n",
      "Epoch 6/25, Batch 891/2250, Loss: 0.0565\n",
      "Epoch 6/25, Batch 901/2250, Loss: 0.1470\n",
      "Epoch 6/25, Batch 911/2250, Loss: 0.0069\n",
      "Epoch 6/25, Batch 921/2250, Loss: 0.1335\n",
      "Epoch 6/25, Batch 931/2250, Loss: 0.0449\n",
      "Epoch 6/25, Batch 941/2250, Loss: 0.2010\n",
      "Epoch 6/25, Batch 951/2250, Loss: 0.0215\n",
      "Epoch 6/25, Batch 961/2250, Loss: 0.0865\n",
      "Epoch 6/25, Batch 971/2250, Loss: 0.0370\n",
      "Epoch 6/25, Batch 981/2250, Loss: 0.1586\n",
      "Epoch 6/25, Batch 991/2250, Loss: 0.0676\n",
      "Epoch 6/25, Batch 1001/2250, Loss: 0.1710\n",
      "Epoch 6/25, Batch 1011/2250, Loss: 0.1051\n",
      "Epoch 6/25, Batch 1021/2250, Loss: 0.0133\n",
      "Epoch 6/25, Batch 1031/2250, Loss: 0.0863\n",
      "Epoch 6/25, Batch 1041/2250, Loss: 0.1212\n",
      "Epoch 6/25, Batch 1051/2250, Loss: 0.1009\n",
      "Epoch 6/25, Batch 1061/2250, Loss: 0.0960\n",
      "Epoch 6/25, Batch 1071/2250, Loss: 0.0769\n",
      "Epoch 6/25, Batch 1081/2250, Loss: 0.1216\n",
      "Epoch 6/25, Batch 1091/2250, Loss: 0.2822\n",
      "Epoch 6/25, Batch 1101/2250, Loss: 0.0794\n",
      "Epoch 6/25, Batch 1111/2250, Loss: 0.3114\n",
      "Epoch 6/25, Batch 1121/2250, Loss: 0.1788\n",
      "Epoch 6/25, Batch 1131/2250, Loss: 0.0367\n",
      "Epoch 6/25, Batch 1141/2250, Loss: 0.0897\n",
      "Epoch 6/25, Batch 1151/2250, Loss: 0.1418\n",
      "Epoch 6/25, Batch 1161/2250, Loss: 0.0365\n",
      "Epoch 6/25, Batch 1171/2250, Loss: 0.0162\n",
      "Epoch 6/25, Batch 1181/2250, Loss: 0.0895\n",
      "Epoch 6/25, Batch 1191/2250, Loss: 0.0867\n",
      "Epoch 6/25, Batch 1201/2250, Loss: 0.2081\n",
      "Epoch 6/25, Batch 1211/2250, Loss: 0.1073\n",
      "Epoch 6/25, Batch 1221/2250, Loss: 0.0598\n",
      "Epoch 6/25, Batch 1231/2250, Loss: 0.0240\n",
      "Epoch 6/25, Batch 1241/2250, Loss: 0.0823\n",
      "Epoch 6/25, Batch 1251/2250, Loss: 0.1057\n",
      "Epoch 6/25, Batch 1261/2250, Loss: 0.0891\n",
      "Epoch 6/25, Batch 1271/2250, Loss: 0.0453\n",
      "Epoch 6/25, Batch 1281/2250, Loss: 0.0736\n",
      "Epoch 6/25, Batch 1291/2250, Loss: 0.0598\n",
      "Epoch 6/25, Batch 1301/2250, Loss: 0.0791\n",
      "Epoch 6/25, Batch 1311/2250, Loss: 0.0775\n",
      "Epoch 6/25, Batch 1321/2250, Loss: 0.1300\n",
      "Epoch 6/25, Batch 1331/2250, Loss: 0.0176\n",
      "Epoch 6/25, Batch 1341/2250, Loss: 0.1134\n",
      "Epoch 6/25, Batch 1351/2250, Loss: 0.1613\n",
      "Epoch 6/25, Batch 1361/2250, Loss: 0.0996\n",
      "Epoch 6/25, Batch 1371/2250, Loss: 0.0411\n",
      "Epoch 6/25, Batch 1381/2250, Loss: 0.0854\n",
      "Epoch 6/25, Batch 1391/2250, Loss: 0.1058\n",
      "Epoch 6/25, Batch 1401/2250, Loss: 0.1001\n",
      "Epoch 6/25, Batch 1411/2250, Loss: 0.0283\n",
      "Epoch 6/25, Batch 1421/2250, Loss: 0.1185\n",
      "Epoch 6/25, Batch 1431/2250, Loss: 0.0977\n",
      "Epoch 6/25, Batch 1441/2250, Loss: 0.1418\n",
      "Epoch 6/25, Batch 1451/2250, Loss: 0.0304\n",
      "Epoch 6/25, Batch 1461/2250, Loss: 0.0841\n",
      "Epoch 6/25, Batch 1471/2250, Loss: 0.0396\n",
      "Epoch 6/25, Batch 1481/2250, Loss: 0.1026\n",
      "Epoch 6/25, Batch 1491/2250, Loss: 0.2244\n",
      "Epoch 6/25, Batch 1501/2250, Loss: 0.0151\n",
      "Epoch 6/25, Batch 1511/2250, Loss: 0.0186\n",
      "Epoch 6/25, Batch 1521/2250, Loss: 0.0919\n",
      "Epoch 6/25, Batch 1531/2250, Loss: 0.0855\n",
      "Epoch 6/25, Batch 1541/2250, Loss: 0.1804\n",
      "Epoch 6/25, Batch 1551/2250, Loss: 0.0616\n",
      "Epoch 6/25, Batch 1561/2250, Loss: 0.0840\n",
      "Epoch 6/25, Batch 1571/2250, Loss: 0.0411\n",
      "Epoch 6/25, Batch 1581/2250, Loss: 0.0781\n",
      "Epoch 6/25, Batch 1591/2250, Loss: 0.0425\n",
      "Epoch 6/25, Batch 1601/2250, Loss: 0.1944\n",
      "Epoch 6/25, Batch 1611/2250, Loss: 0.1729\n",
      "Epoch 6/25, Batch 1621/2250, Loss: 0.0649\n",
      "Epoch 6/25, Batch 1631/2250, Loss: 0.0783\n",
      "Epoch 6/25, Batch 1641/2250, Loss: 0.1667\n",
      "Epoch 6/25, Batch 1651/2250, Loss: 0.0305\n",
      "Epoch 6/25, Batch 1661/2250, Loss: 0.0520\n",
      "Epoch 6/25, Batch 1671/2250, Loss: 0.0508\n",
      "Epoch 6/25, Batch 1681/2250, Loss: 0.2441\n",
      "Epoch 6/25, Batch 1691/2250, Loss: 0.0496\n",
      "Epoch 6/25, Batch 1701/2250, Loss: 0.1579\n",
      "Epoch 6/25, Batch 1711/2250, Loss: 0.0505\n",
      "Epoch 6/25, Batch 1721/2250, Loss: 0.1105\n",
      "Epoch 6/25, Batch 1731/2250, Loss: 0.0642\n",
      "Epoch 6/25, Batch 1741/2250, Loss: 0.0422\n",
      "Epoch 6/25, Batch 1751/2250, Loss: 0.1775\n",
      "Epoch 6/25, Batch 1761/2250, Loss: 0.3109\n",
      "Epoch 6/25, Batch 1771/2250, Loss: 0.1649\n",
      "Epoch 6/25, Batch 1781/2250, Loss: 0.2864\n",
      "Epoch 6/25, Batch 1791/2250, Loss: 0.0994\n",
      "Epoch 6/25, Batch 1801/2250, Loss: 0.0872\n",
      "Epoch 6/25, Batch 1811/2250, Loss: 0.0314\n",
      "Epoch 6/25, Batch 1821/2250, Loss: 0.0728\n",
      "Epoch 6/25, Batch 1831/2250, Loss: 0.0506\n",
      "Epoch 6/25, Batch 1841/2250, Loss: 0.0399\n",
      "Epoch 6/25, Batch 1851/2250, Loss: 0.0293\n",
      "Epoch 6/25, Batch 1861/2250, Loss: 0.0838\n",
      "Epoch 6/25, Batch 1871/2250, Loss: 0.0684\n",
      "Epoch 6/25, Batch 1881/2250, Loss: 0.1328\n",
      "Epoch 6/25, Batch 1891/2250, Loss: 0.1105\n",
      "Epoch 6/25, Batch 1901/2250, Loss: 0.2522\n",
      "Epoch 6/25, Batch 1911/2250, Loss: 0.0583\n",
      "Epoch 6/25, Batch 1921/2250, Loss: 0.0702\n",
      "Epoch 6/25, Batch 1931/2250, Loss: 0.0662\n",
      "Epoch 6/25, Batch 1941/2250, Loss: 0.0663\n",
      "Epoch 6/25, Batch 1951/2250, Loss: 0.2553\n",
      "Epoch 6/25, Batch 1961/2250, Loss: 0.1512\n",
      "Epoch 6/25, Batch 1971/2250, Loss: 0.1870\n",
      "Epoch 6/25, Batch 1981/2250, Loss: 0.1947\n",
      "Epoch 6/25, Batch 1991/2250, Loss: 0.1020\n",
      "Epoch 6/25, Batch 2001/2250, Loss: 0.1098\n",
      "Epoch 6/25, Batch 2011/2250, Loss: 0.1137\n",
      "Epoch 6/25, Batch 2021/2250, Loss: 0.0073\n",
      "Epoch 6/25, Batch 2031/2250, Loss: 0.0268\n",
      "Epoch 6/25, Batch 2041/2250, Loss: 0.1546\n",
      "Epoch 6/25, Batch 2051/2250, Loss: 0.1444\n",
      "Epoch 6/25, Batch 2061/2250, Loss: 0.0301\n",
      "Epoch 6/25, Batch 2071/2250, Loss: 0.1345\n",
      "Epoch 6/25, Batch 2081/2250, Loss: 0.1336\n",
      "Epoch 6/25, Batch 2091/2250, Loss: 0.0530\n",
      "Epoch 6/25, Batch 2101/2250, Loss: 0.0917\n",
      "Epoch 6/25, Batch 2111/2250, Loss: 0.0190\n",
      "Epoch 6/25, Batch 2121/2250, Loss: 0.0956\n",
      "Epoch 6/25, Batch 2131/2250, Loss: 0.0545\n",
      "Epoch 6/25, Batch 2141/2250, Loss: 0.2216\n",
      "Epoch 6/25, Batch 2151/2250, Loss: 0.1752\n",
      "Epoch 6/25, Batch 2161/2250, Loss: 0.1190\n",
      "Epoch 6/25, Batch 2171/2250, Loss: 0.1420\n",
      "Epoch 6/25, Batch 2181/2250, Loss: 0.1964\n",
      "Epoch 6/25, Batch 2191/2250, Loss: 0.1469\n",
      "Epoch 6/25, Batch 2201/2250, Loss: 0.0677\n",
      "Epoch 6/25, Batch 2211/2250, Loss: 0.0225\n",
      "Epoch 6/25, Batch 2221/2250, Loss: 0.1047\n",
      "Epoch 6/25, Batch 2231/2250, Loss: 0.0186\n",
      "Epoch 6/25, Batch 2241/2250, Loss: 0.0559\n",
      "Epoch 6/25:\n",
      "Train Loss: 0.1026, Train Acc: 96.04%\n",
      "Val Loss: 0.0616, Val Acc: 97.80%\n",
      "------------------------------------------------------------\n",
      "Starting epoch 7/25...\n",
      "Starting training loop with 2250 batches...\n",
      "Successfully loaded first batch: images shape torch.Size([32, 3, 224, 224]), labels shape torch.Size([32])\n",
      "Epoch 7/25, Batch 1/2250, Loss: 0.0338\n",
      "Epoch 7/25, Batch 11/2250, Loss: 0.0225\n",
      "Epoch 7/25, Batch 21/2250, Loss: 0.0939\n",
      "Epoch 7/25, Batch 31/2250, Loss: 0.0268\n",
      "Epoch 7/25, Batch 41/2250, Loss: 0.1290\n",
      "Epoch 7/25, Batch 51/2250, Loss: 0.0444\n",
      "Epoch 7/25, Batch 61/2250, Loss: 0.0783\n",
      "Epoch 7/25, Batch 71/2250, Loss: 0.1406\n",
      "Epoch 7/25, Batch 81/2250, Loss: 0.0449\n",
      "Epoch 7/25, Batch 91/2250, Loss: 0.0886\n",
      "Epoch 7/25, Batch 101/2250, Loss: 0.0559\n",
      "Epoch 7/25, Batch 111/2250, Loss: 0.0485\n",
      "Epoch 7/25, Batch 121/2250, Loss: 0.1549\n",
      "Epoch 7/25, Batch 131/2250, Loss: 0.0648\n",
      "Epoch 7/25, Batch 141/2250, Loss: 0.2719\n",
      "Epoch 7/25, Batch 151/2250, Loss: 0.0363\n",
      "Epoch 7/25, Batch 161/2250, Loss: 0.0047\n",
      "Epoch 7/25, Batch 171/2250, Loss: 0.0166\n",
      "Epoch 7/25, Batch 181/2250, Loss: 0.0365\n",
      "Epoch 7/25, Batch 191/2250, Loss: 0.0627\n",
      "Epoch 7/25, Batch 201/2250, Loss: 0.0667\n",
      "Epoch 7/25, Batch 211/2250, Loss: 0.0149\n",
      "Epoch 7/25, Batch 221/2250, Loss: 0.0704\n",
      "Epoch 7/25, Batch 231/2250, Loss: 0.0139\n",
      "Epoch 7/25, Batch 241/2250, Loss: 0.0510\n",
      "Epoch 7/25, Batch 251/2250, Loss: 0.0566\n",
      "Epoch 7/25, Batch 261/2250, Loss: 0.0671\n",
      "Epoch 7/25, Batch 271/2250, Loss: 0.1801\n",
      "Epoch 7/25, Batch 281/2250, Loss: 0.0449\n",
      "Epoch 7/25, Batch 291/2250, Loss: 0.0760\n",
      "Epoch 7/25, Batch 301/2250, Loss: 0.0626\n",
      "Epoch 7/25, Batch 311/2250, Loss: 0.3136\n",
      "Epoch 7/25, Batch 321/2250, Loss: 0.0420\n",
      "Epoch 7/25, Batch 331/2250, Loss: 0.1426\n",
      "Epoch 7/25, Batch 341/2250, Loss: 0.1499\n",
      "Epoch 7/25, Batch 351/2250, Loss: 0.1274\n",
      "Epoch 7/25, Batch 361/2250, Loss: 0.0772\n",
      "Epoch 7/25, Batch 371/2250, Loss: 0.1541\n",
      "Epoch 7/25, Batch 381/2250, Loss: 0.0903\n",
      "Epoch 7/25, Batch 391/2250, Loss: 0.0647\n",
      "Epoch 7/25, Batch 401/2250, Loss: 0.0224\n",
      "Epoch 7/25, Batch 411/2250, Loss: 0.2770\n",
      "Epoch 7/25, Batch 421/2250, Loss: 0.0498\n",
      "Epoch 7/25, Batch 431/2250, Loss: 0.0281\n",
      "Epoch 7/25, Batch 441/2250, Loss: 0.1429\n",
      "Epoch 7/25, Batch 451/2250, Loss: 0.0682\n",
      "Epoch 7/25, Batch 461/2250, Loss: 0.0536\n",
      "Epoch 7/25, Batch 471/2250, Loss: 0.0506\n",
      "Epoch 7/25, Batch 481/2250, Loss: 0.2921\n",
      "Epoch 7/25, Batch 491/2250, Loss: 0.0405\n",
      "Epoch 7/25, Batch 501/2250, Loss: 0.0721\n",
      "Epoch 7/25, Batch 511/2250, Loss: 0.3013\n",
      "Epoch 7/25, Batch 521/2250, Loss: 0.1152\n",
      "Epoch 7/25, Batch 531/2250, Loss: 0.0562\n",
      "Epoch 7/25, Batch 541/2250, Loss: 0.0174\n",
      "Epoch 7/25, Batch 551/2250, Loss: 0.0964\n",
      "Epoch 7/25, Batch 561/2250, Loss: 0.0914\n",
      "Epoch 7/25, Batch 571/2250, Loss: 0.2534\n",
      "Epoch 7/25, Batch 581/2250, Loss: 0.1333\n",
      "Epoch 7/25, Batch 591/2250, Loss: 0.0591\n",
      "Epoch 7/25, Batch 601/2250, Loss: 0.0418\n",
      "Epoch 7/25, Batch 611/2250, Loss: 0.0454\n",
      "Epoch 7/25, Batch 621/2250, Loss: 0.0970\n",
      "Epoch 7/25, Batch 631/2250, Loss: 0.2229\n",
      "Epoch 7/25, Batch 641/2250, Loss: 0.0968\n",
      "Epoch 7/25, Batch 651/2250, Loss: 0.1173\n",
      "Epoch 7/25, Batch 661/2250, Loss: 0.0221\n",
      "Epoch 7/25, Batch 671/2250, Loss: 0.0760\n",
      "Epoch 7/25, Batch 681/2250, Loss: 0.1801\n",
      "Epoch 7/25, Batch 691/2250, Loss: 0.0745\n",
      "Epoch 7/25, Batch 701/2250, Loss: 0.0432\n",
      "Epoch 7/25, Batch 711/2250, Loss: 0.0688\n",
      "Epoch 7/25, Batch 721/2250, Loss: 0.0557\n",
      "Epoch 7/25, Batch 731/2250, Loss: 0.0568\n",
      "Epoch 7/25, Batch 741/2250, Loss: 0.0545\n",
      "Epoch 7/25, Batch 751/2250, Loss: 0.0441\n",
      "Epoch 7/25, Batch 761/2250, Loss: 0.0613\n",
      "Epoch 7/25, Batch 771/2250, Loss: 0.0286\n",
      "Epoch 7/25, Batch 781/2250, Loss: 0.1239\n",
      "Epoch 7/25, Batch 791/2250, Loss: 0.0508\n",
      "Epoch 7/25, Batch 801/2250, Loss: 0.0736\n",
      "Epoch 7/25, Batch 811/2250, Loss: 0.2336\n",
      "Epoch 7/25, Batch 821/2250, Loss: 0.1840\n",
      "Epoch 7/25, Batch 831/2250, Loss: 0.0385\n",
      "Epoch 7/25, Batch 841/2250, Loss: 0.2082\n",
      "Epoch 7/25, Batch 851/2250, Loss: 0.0708\n",
      "Epoch 7/25, Batch 861/2250, Loss: 0.2474\n",
      "Epoch 7/25, Batch 871/2250, Loss: 0.0414\n",
      "Epoch 7/25, Batch 881/2250, Loss: 0.0440\n",
      "Epoch 7/25, Batch 891/2250, Loss: 0.0023\n",
      "Epoch 7/25, Batch 901/2250, Loss: 0.1430\n",
      "Epoch 7/25, Batch 911/2250, Loss: 0.1515\n",
      "Epoch 7/25, Batch 921/2250, Loss: 0.2170\n",
      "Epoch 7/25, Batch 931/2250, Loss: 0.0259\n",
      "Epoch 7/25, Batch 941/2250, Loss: 0.1073\n",
      "Epoch 7/25, Batch 951/2250, Loss: 0.0109\n",
      "Epoch 7/25, Batch 961/2250, Loss: 0.0243\n",
      "Epoch 7/25, Batch 971/2250, Loss: 0.1642\n",
      "Epoch 7/25, Batch 981/2250, Loss: 0.0492\n",
      "Epoch 7/25, Batch 991/2250, Loss: 0.0669\n",
      "Epoch 7/25, Batch 1001/2250, Loss: 0.0355\n",
      "Epoch 7/25, Batch 1011/2250, Loss: 0.1609\n",
      "Epoch 7/25, Batch 1021/2250, Loss: 0.0593\n",
      "Epoch 7/25, Batch 1031/2250, Loss: 0.0760\n",
      "Epoch 7/25, Batch 1041/2250, Loss: 0.0941\n",
      "Epoch 7/25, Batch 1051/2250, Loss: 0.0555\n",
      "Epoch 7/25, Batch 1061/2250, Loss: 0.0662\n",
      "Epoch 7/25, Batch 1071/2250, Loss: 0.1947\n",
      "Epoch 7/25, Batch 1081/2250, Loss: 0.0291\n",
      "Epoch 7/25, Batch 1091/2250, Loss: 0.0734\n",
      "Epoch 7/25, Batch 1101/2250, Loss: 0.0253\n",
      "Epoch 7/25, Batch 1111/2250, Loss: 0.0072\n",
      "Epoch 7/25, Batch 1121/2250, Loss: 0.0813\n",
      "Epoch 7/25, Batch 1131/2250, Loss: 0.0673\n",
      "Epoch 7/25, Batch 1141/2250, Loss: 0.1564\n",
      "Epoch 7/25, Batch 1151/2250, Loss: 0.0136\n",
      "Epoch 7/25, Batch 1161/2250, Loss: 0.1859\n",
      "Epoch 7/25, Batch 1171/2250, Loss: 0.2045\n",
      "Epoch 7/25, Batch 1181/2250, Loss: 0.1412\n",
      "Epoch 7/25, Batch 1191/2250, Loss: 0.1227\n",
      "Epoch 7/25, Batch 1201/2250, Loss: 0.0502\n",
      "Epoch 7/25, Batch 1211/2250, Loss: 0.3055\n",
      "Epoch 7/25, Batch 1221/2250, Loss: 0.0799\n",
      "Epoch 7/25, Batch 1231/2250, Loss: 0.0639\n",
      "Epoch 7/25, Batch 1241/2250, Loss: 0.0545\n",
      "Epoch 7/25, Batch 1251/2250, Loss: 0.0939\n",
      "Epoch 7/25, Batch 1261/2250, Loss: 0.1051\n",
      "Epoch 7/25, Batch 1271/2250, Loss: 0.1203\n",
      "Epoch 7/25, Batch 1281/2250, Loss: 0.0411\n",
      "Epoch 7/25, Batch 1291/2250, Loss: 0.1676\n",
      "Epoch 7/25, Batch 1301/2250, Loss: 0.1056\n",
      "Epoch 7/25, Batch 1311/2250, Loss: 0.2398\n",
      "Epoch 7/25, Batch 1321/2250, Loss: 0.1211\n",
      "Epoch 7/25, Batch 1331/2250, Loss: 0.0814\n",
      "Epoch 7/25, Batch 1341/2250, Loss: 0.0261\n",
      "Epoch 7/25, Batch 1351/2250, Loss: 0.2228\n",
      "Epoch 7/25, Batch 1361/2250, Loss: 0.1345\n",
      "Epoch 7/25, Batch 1371/2250, Loss: 0.0501\n",
      "Epoch 7/25, Batch 1381/2250, Loss: 0.1550\n",
      "Epoch 7/25, Batch 1391/2250, Loss: 0.0151\n",
      "Epoch 7/25, Batch 1401/2250, Loss: 0.1189\n",
      "Epoch 7/25, Batch 1411/2250, Loss: 0.0262\n",
      "Epoch 7/25, Batch 1421/2250, Loss: 0.0297\n",
      "Epoch 7/25, Batch 1431/2250, Loss: 0.2191\n",
      "Epoch 7/25, Batch 1441/2250, Loss: 0.0531\n",
      "Epoch 7/25, Batch 1451/2250, Loss: 0.0129\n",
      "Epoch 7/25, Batch 1461/2250, Loss: 0.1470\n",
      "Epoch 7/25, Batch 1471/2250, Loss: 0.3104\n",
      "Epoch 7/25, Batch 1481/2250, Loss: 0.1060\n",
      "Epoch 7/25, Batch 1491/2250, Loss: 0.0567\n",
      "Epoch 7/25, Batch 1501/2250, Loss: 0.0374\n",
      "Epoch 7/25, Batch 1511/2250, Loss: 0.0116\n",
      "Epoch 7/25, Batch 1521/2250, Loss: 0.0434\n",
      "Epoch 7/25, Batch 1531/2250, Loss: 0.0445\n",
      "Epoch 7/25, Batch 1541/2250, Loss: 0.0032\n",
      "Epoch 7/25, Batch 1551/2250, Loss: 0.0555\n",
      "Epoch 7/25, Batch 1561/2250, Loss: 0.1589\n",
      "Epoch 7/25, Batch 1571/2250, Loss: 0.0308\n",
      "Epoch 7/25, Batch 1581/2250, Loss: 0.0510\n",
      "Epoch 7/25, Batch 1591/2250, Loss: 0.1308\n",
      "Epoch 7/25, Batch 1601/2250, Loss: 0.1064\n",
      "Epoch 7/25, Batch 1611/2250, Loss: 0.0284\n",
      "Epoch 7/25, Batch 1621/2250, Loss: 0.0198\n",
      "Epoch 7/25, Batch 1631/2250, Loss: 0.1068\n",
      "Epoch 7/25, Batch 1641/2250, Loss: 0.1004\n",
      "Epoch 7/25, Batch 1651/2250, Loss: 0.0324\n",
      "Epoch 7/25, Batch 1661/2250, Loss: 0.1016\n",
      "Epoch 7/25, Batch 1671/2250, Loss: 0.0496\n",
      "Epoch 7/25, Batch 1681/2250, Loss: 0.2217\n",
      "Epoch 7/25, Batch 1691/2250, Loss: 0.0257\n",
      "Epoch 7/25, Batch 1701/2250, Loss: 0.1232\n",
      "Epoch 7/25, Batch 1711/2250, Loss: 0.0673\n",
      "Epoch 7/25, Batch 1721/2250, Loss: 0.0129\n",
      "Epoch 7/25, Batch 1731/2250, Loss: 0.0384\n",
      "Epoch 7/25, Batch 1741/2250, Loss: 0.0756\n",
      "Epoch 7/25, Batch 1751/2250, Loss: 0.0080\n",
      "Epoch 7/25, Batch 1761/2250, Loss: 0.0665\n",
      "Epoch 7/25, Batch 1771/2250, Loss: 0.0294\n",
      "Epoch 7/25, Batch 1781/2250, Loss: 0.0532\n",
      "Epoch 7/25, Batch 1791/2250, Loss: 0.0468\n",
      "Epoch 7/25, Batch 1801/2250, Loss: 0.1328\n",
      "Epoch 7/25, Batch 1811/2250, Loss: 0.0465\n",
      "Epoch 7/25, Batch 1821/2250, Loss: 0.0758\n",
      "Epoch 7/25, Batch 1831/2250, Loss: 0.2374\n",
      "Epoch 7/25, Batch 1841/2250, Loss: 0.1519\n",
      "Epoch 7/25, Batch 1851/2250, Loss: 0.0493\n",
      "Epoch 7/25, Batch 1861/2250, Loss: 0.0915\n",
      "Epoch 7/25, Batch 1871/2250, Loss: 0.0124\n",
      "Epoch 7/25, Batch 1881/2250, Loss: 0.0253\n",
      "Epoch 7/25, Batch 1891/2250, Loss: 0.0535\n",
      "Epoch 7/25, Batch 1901/2250, Loss: 0.0301\n",
      "Epoch 7/25, Batch 1911/2250, Loss: 0.0730\n",
      "Epoch 7/25, Batch 1921/2250, Loss: 0.0886\n",
      "Epoch 7/25, Batch 1931/2250, Loss: 0.0795\n",
      "Epoch 7/25, Batch 1941/2250, Loss: 0.1612\n",
      "Epoch 7/25, Batch 1951/2250, Loss: 0.2235\n",
      "Epoch 7/25, Batch 1961/2250, Loss: 0.0698\n",
      "Epoch 7/25, Batch 1971/2250, Loss: 0.0402\n",
      "Epoch 7/25, Batch 1981/2250, Loss: 0.0617\n",
      "Epoch 7/25, Batch 1991/2250, Loss: 0.0275\n",
      "Epoch 7/25, Batch 2001/2250, Loss: 0.0753\n",
      "Epoch 7/25, Batch 2011/2250, Loss: 0.1336\n",
      "Epoch 7/25, Batch 2021/2250, Loss: 0.0396\n",
      "Epoch 7/25, Batch 2031/2250, Loss: 0.2785\n",
      "Epoch 7/25, Batch 2041/2250, Loss: 0.1533\n",
      "Epoch 7/25, Batch 2051/2250, Loss: 0.2239\n",
      "Epoch 7/25, Batch 2061/2250, Loss: 0.1407\n",
      "Epoch 7/25, Batch 2071/2250, Loss: 0.0992\n",
      "Epoch 7/25, Batch 2081/2250, Loss: 0.0707\n",
      "Epoch 7/25, Batch 2091/2250, Loss: 0.0617\n",
      "Epoch 7/25, Batch 2101/2250, Loss: 0.0116\n",
      "Epoch 7/25, Batch 2111/2250, Loss: 0.0251\n",
      "Epoch 7/25, Batch 2121/2250, Loss: 0.0515\n",
      "Epoch 7/25, Batch 2131/2250, Loss: 0.1019\n",
      "Epoch 7/25, Batch 2141/2250, Loss: 0.0626\n",
      "Epoch 7/25, Batch 2151/2250, Loss: 0.0419\n",
      "Epoch 7/25, Batch 2161/2250, Loss: 0.0813\n",
      "Epoch 7/25, Batch 2171/2250, Loss: 0.1076\n",
      "Epoch 7/25, Batch 2181/2250, Loss: 0.0255\n",
      "Epoch 7/25, Batch 2191/2250, Loss: 0.0611\n",
      "Epoch 7/25, Batch 2201/2250, Loss: 0.0812\n",
      "Epoch 7/25, Batch 2211/2250, Loss: 0.0196\n",
      "Epoch 7/25, Batch 2221/2250, Loss: 0.0132\n",
      "Epoch 7/25, Batch 2231/2250, Loss: 0.0931\n",
      "Epoch 7/25, Batch 2241/2250, Loss: 0.0971\n",
      "Epoch 7/25:\n",
      "Train Loss: 0.0910, Train Acc: 96.55%\n",
      "Val Loss: 0.0731, Val Acc: 97.27%\n",
      "------------------------------------------------------------\n",
      "Starting epoch 8/25...\n",
      "Starting training loop with 2250 batches...\n",
      "Successfully loaded first batch: images shape torch.Size([32, 3, 224, 224]), labels shape torch.Size([32])\n",
      "Epoch 8/25, Batch 1/2250, Loss: 0.0725\n",
      "Epoch 8/25, Batch 11/2250, Loss: 0.1447\n",
      "Epoch 8/25, Batch 21/2250, Loss: 0.0267\n",
      "Epoch 8/25, Batch 31/2250, Loss: 0.0252\n",
      "Epoch 8/25, Batch 41/2250, Loss: 0.0679\n",
      "Epoch 8/25, Batch 51/2250, Loss: 0.0456\n",
      "Epoch 8/25, Batch 61/2250, Loss: 0.0108\n",
      "Epoch 8/25, Batch 71/2250, Loss: 0.0617\n",
      "Epoch 8/25, Batch 81/2250, Loss: 0.2492\n",
      "Epoch 8/25, Batch 91/2250, Loss: 0.0464\n",
      "Epoch 8/25, Batch 101/2250, Loss: 0.2218\n",
      "Epoch 8/25, Batch 111/2250, Loss: 0.0327\n",
      "Epoch 8/25, Batch 121/2250, Loss: 0.0456\n",
      "Epoch 8/25, Batch 131/2250, Loss: 0.0713\n",
      "Epoch 8/25, Batch 141/2250, Loss: 0.1407\n",
      "Epoch 8/25, Batch 151/2250, Loss: 0.0879\n",
      "Epoch 8/25, Batch 161/2250, Loss: 0.0217\n",
      "Epoch 8/25, Batch 171/2250, Loss: 0.1001\n",
      "Epoch 8/25, Batch 181/2250, Loss: 0.0251\n",
      "Epoch 8/25, Batch 191/2250, Loss: 0.0741\n",
      "Epoch 8/25, Batch 201/2250, Loss: 0.0323\n",
      "Epoch 8/25, Batch 211/2250, Loss: 0.0031\n",
      "Epoch 8/25, Batch 221/2250, Loss: 0.0082\n",
      "Epoch 8/25, Batch 231/2250, Loss: 0.0482\n",
      "Epoch 8/25, Batch 241/2250, Loss: 0.1076\n",
      "Epoch 8/25, Batch 251/2250, Loss: 0.2343\n",
      "Epoch 8/25, Batch 261/2250, Loss: 0.0870\n",
      "Epoch 8/25, Batch 271/2250, Loss: 0.1005\n",
      "Epoch 8/25, Batch 281/2250, Loss: 0.0180\n",
      "Epoch 8/25, Batch 291/2250, Loss: 0.0426\n",
      "Epoch 8/25, Batch 301/2250, Loss: 0.0620\n",
      "Epoch 8/25, Batch 311/2250, Loss: 0.1041\n",
      "Epoch 8/25, Batch 321/2250, Loss: 0.2243\n",
      "Epoch 8/25, Batch 331/2250, Loss: 0.1504\n",
      "Epoch 8/25, Batch 341/2250, Loss: 0.1540\n",
      "Epoch 8/25, Batch 351/2250, Loss: 0.1838\n",
      "Epoch 8/25, Batch 361/2250, Loss: 0.0178\n",
      "Epoch 8/25, Batch 371/2250, Loss: 0.1520\n",
      "Epoch 8/25, Batch 381/2250, Loss: 0.0378\n",
      "Epoch 8/25, Batch 391/2250, Loss: 0.2350\n",
      "Epoch 8/25, Batch 401/2250, Loss: 0.0591\n",
      "Epoch 8/25, Batch 411/2250, Loss: 0.0227\n",
      "Epoch 8/25, Batch 421/2250, Loss: 0.0783\n",
      "Epoch 8/25, Batch 431/2250, Loss: 0.0693\n",
      "Epoch 8/25, Batch 441/2250, Loss: 0.0449\n",
      "Epoch 8/25, Batch 451/2250, Loss: 0.1583\n",
      "Epoch 8/25, Batch 461/2250, Loss: 0.1249\n",
      "Epoch 8/25, Batch 471/2250, Loss: 0.0899\n",
      "Epoch 8/25, Batch 481/2250, Loss: 0.0124\n",
      "Epoch 8/25, Batch 491/2250, Loss: 0.1065\n",
      "Epoch 8/25, Batch 501/2250, Loss: 0.0492\n",
      "Epoch 8/25, Batch 511/2250, Loss: 0.0434\n",
      "Epoch 8/25, Batch 521/2250, Loss: 0.0860\n",
      "Epoch 8/25, Batch 531/2250, Loss: 0.0800\n",
      "Epoch 8/25, Batch 541/2250, Loss: 0.1647\n",
      "Epoch 8/25, Batch 551/2250, Loss: 0.1013\n",
      "Epoch 8/25, Batch 561/2250, Loss: 0.1324\n",
      "Epoch 8/25, Batch 571/2250, Loss: 0.1328\n",
      "Epoch 8/25, Batch 581/2250, Loss: 0.0541\n",
      "Epoch 8/25, Batch 591/2250, Loss: 0.1897\n",
      "Epoch 8/25, Batch 601/2250, Loss: 0.0319\n",
      "Epoch 8/25, Batch 611/2250, Loss: 0.0525\n",
      "Epoch 8/25, Batch 621/2250, Loss: 0.0457\n",
      "Epoch 8/25, Batch 631/2250, Loss: 0.0549\n",
      "Epoch 8/25, Batch 641/2250, Loss: 0.0207\n",
      "Epoch 8/25, Batch 651/2250, Loss: 0.0138\n",
      "Epoch 8/25, Batch 661/2250, Loss: 0.0813\n",
      "Epoch 8/25, Batch 671/2250, Loss: 0.0439\n",
      "Epoch 8/25, Batch 681/2250, Loss: 0.0690\n",
      "Epoch 8/25, Batch 691/2250, Loss: 0.0409\n",
      "Epoch 8/25, Batch 701/2250, Loss: 0.0951\n",
      "Epoch 8/25, Batch 711/2250, Loss: 0.0700\n",
      "Epoch 8/25, Batch 721/2250, Loss: 0.0327\n",
      "Epoch 8/25, Batch 731/2250, Loss: 0.0097\n",
      "Epoch 8/25, Batch 741/2250, Loss: 0.0740\n",
      "Epoch 8/25, Batch 751/2250, Loss: 0.0306\n",
      "Epoch 8/25, Batch 761/2250, Loss: 0.0356\n",
      "Epoch 8/25, Batch 771/2250, Loss: 0.1021\n",
      "Epoch 8/25, Batch 781/2250, Loss: 0.0342\n",
      "Epoch 8/25, Batch 791/2250, Loss: 0.0981\n",
      "Epoch 8/25, Batch 801/2250, Loss: 0.0616\n",
      "Epoch 8/25, Batch 811/2250, Loss: 0.0828\n",
      "Epoch 8/25, Batch 821/2250, Loss: 0.0267\n",
      "Epoch 8/25, Batch 831/2250, Loss: 0.1447\n",
      "Epoch 8/25, Batch 841/2250, Loss: 0.1535\n",
      "Epoch 8/25, Batch 851/2250, Loss: 0.0548\n",
      "Epoch 8/25, Batch 861/2250, Loss: 0.0301\n",
      "Epoch 8/25, Batch 871/2250, Loss: 0.0212\n",
      "Epoch 8/25, Batch 881/2250, Loss: 0.1506\n",
      "Epoch 8/25, Batch 891/2250, Loss: 0.0435\n",
      "Epoch 8/25, Batch 901/2250, Loss: 0.1669\n",
      "Epoch 8/25, Batch 911/2250, Loss: 0.0496\n",
      "Epoch 8/25, Batch 921/2250, Loss: 0.0922\n",
      "Epoch 8/25, Batch 931/2250, Loss: 0.0686\n",
      "Epoch 8/25, Batch 941/2250, Loss: 0.0100\n",
      "Epoch 8/25, Batch 951/2250, Loss: 0.0672\n",
      "Epoch 8/25, Batch 961/2250, Loss: 0.0079\n",
      "Epoch 8/25, Batch 971/2250, Loss: 0.0214\n",
      "Epoch 8/25, Batch 981/2250, Loss: 0.0547\n",
      "Epoch 8/25, Batch 991/2250, Loss: 0.1433\n",
      "Epoch 8/25, Batch 1001/2250, Loss: 0.0801\n",
      "Epoch 8/25, Batch 1011/2250, Loss: 0.0136\n",
      "Epoch 8/25, Batch 1021/2250, Loss: 0.0742\n",
      "Epoch 8/25, Batch 1031/2250, Loss: 0.0797\n",
      "Epoch 8/25, Batch 1041/2250, Loss: 0.1051\n",
      "Epoch 8/25, Batch 1051/2250, Loss: 0.2076\n",
      "Epoch 8/25, Batch 1061/2250, Loss: 0.1303\n",
      "Epoch 8/25, Batch 1071/2250, Loss: 0.2090\n",
      "Epoch 8/25, Batch 1081/2250, Loss: 0.0470\n",
      "Epoch 8/25, Batch 1091/2250, Loss: 0.1199\n",
      "Epoch 8/25, Batch 1101/2250, Loss: 0.2522\n",
      "Epoch 8/25, Batch 1111/2250, Loss: 0.0162\n",
      "Epoch 8/25, Batch 1121/2250, Loss: 0.0371\n",
      "Epoch 8/25, Batch 1131/2250, Loss: 0.0624\n",
      "Epoch 8/25, Batch 1141/2250, Loss: 0.0387\n",
      "Epoch 8/25, Batch 1151/2250, Loss: 0.0197\n",
      "Epoch 8/25, Batch 1161/2250, Loss: 0.2676\n",
      "Epoch 8/25, Batch 1171/2250, Loss: 0.0820\n",
      "Epoch 8/25, Batch 1181/2250, Loss: 0.0997\n",
      "Epoch 8/25, Batch 1191/2250, Loss: 0.0485\n",
      "Epoch 8/25, Batch 1201/2250, Loss: 0.0408\n",
      "Epoch 8/25, Batch 1211/2250, Loss: 0.0550\n",
      "Epoch 8/25, Batch 1221/2250, Loss: 0.2666\n",
      "Epoch 8/25, Batch 1231/2250, Loss: 0.0511\n",
      "Epoch 8/25, Batch 1241/2250, Loss: 0.0391\n",
      "Epoch 8/25, Batch 1251/2250, Loss: 0.0664\n",
      "Epoch 8/25, Batch 1261/2250, Loss: 0.0897\n",
      "Epoch 8/25, Batch 1271/2250, Loss: 0.0101\n",
      "Epoch 8/25, Batch 1281/2250, Loss: 0.1961\n",
      "Epoch 8/25, Batch 1291/2250, Loss: 0.1215\n",
      "Epoch 8/25, Batch 1301/2250, Loss: 0.0504\n",
      "Epoch 8/25, Batch 1311/2250, Loss: 0.0897\n",
      "Epoch 8/25, Batch 1321/2250, Loss: 0.1112\n",
      "Epoch 8/25, Batch 1331/2250, Loss: 0.0184\n",
      "Epoch 8/25, Batch 1341/2250, Loss: 0.0405\n",
      "Epoch 8/25, Batch 1351/2250, Loss: 0.0175\n",
      "Epoch 8/25, Batch 1361/2250, Loss: 0.0263\n",
      "Epoch 8/25, Batch 1371/2250, Loss: 0.1823\n",
      "Epoch 8/25, Batch 1381/2250, Loss: 0.1708\n",
      "Epoch 8/25, Batch 1391/2250, Loss: 0.0440\n",
      "Epoch 8/25, Batch 1401/2250, Loss: 0.0303\n",
      "Epoch 8/25, Batch 1411/2250, Loss: 0.0221\n",
      "Epoch 8/25, Batch 1421/2250, Loss: 0.0373\n",
      "Epoch 8/25, Batch 1431/2250, Loss: 0.0440\n",
      "Epoch 8/25, Batch 1441/2250, Loss: 0.0323\n",
      "Epoch 8/25, Batch 1451/2250, Loss: 0.0204\n",
      "Epoch 8/25, Batch 1461/2250, Loss: 0.0269\n",
      "Epoch 8/25, Batch 1471/2250, Loss: 0.0090\n",
      "Epoch 8/25, Batch 1481/2250, Loss: 0.1088\n",
      "Epoch 8/25, Batch 1491/2250, Loss: 0.1470\n",
      "Epoch 8/25, Batch 1501/2250, Loss: 0.1001\n",
      "Epoch 8/25, Batch 1511/2250, Loss: 0.0181\n",
      "Epoch 8/25, Batch 1521/2250, Loss: 0.1440\n",
      "Epoch 8/25, Batch 1531/2250, Loss: 0.0378\n",
      "Epoch 8/25, Batch 1541/2250, Loss: 0.1549\n",
      "Epoch 8/25, Batch 1551/2250, Loss: 0.0649\n",
      "Epoch 8/25, Batch 1561/2250, Loss: 0.0612\n",
      "Epoch 8/25, Batch 1571/2250, Loss: 0.0782\n",
      "Epoch 8/25, Batch 1581/2250, Loss: 0.0386\n",
      "Epoch 8/25, Batch 1591/2250, Loss: 0.0338\n",
      "Epoch 8/25, Batch 1601/2250, Loss: 0.0924\n",
      "Epoch 8/25, Batch 1611/2250, Loss: 0.0685\n",
      "Epoch 8/25, Batch 1621/2250, Loss: 0.0131\n",
      "Epoch 8/25, Batch 1631/2250, Loss: 0.0703\n",
      "Epoch 8/25, Batch 1641/2250, Loss: 0.0080\n",
      "Epoch 8/25, Batch 1651/2250, Loss: 0.0948\n",
      "Epoch 8/25, Batch 1661/2250, Loss: 0.0848\n",
      "Epoch 8/25, Batch 1671/2250, Loss: 0.1178\n",
      "Epoch 8/25, Batch 1681/2250, Loss: 0.3237\n",
      "Epoch 8/25, Batch 1691/2250, Loss: 0.0399\n",
      "Epoch 8/25, Batch 1701/2250, Loss: 0.1828\n",
      "Epoch 8/25, Batch 1711/2250, Loss: 0.0922\n",
      "Epoch 8/25, Batch 1721/2250, Loss: 0.0547\n",
      "Epoch 8/25, Batch 1731/2250, Loss: 0.0262\n",
      "Epoch 8/25, Batch 1741/2250, Loss: 0.0960\n",
      "Epoch 8/25, Batch 1751/2250, Loss: 0.0190\n",
      "Epoch 8/25, Batch 1761/2250, Loss: 0.0409\n",
      "Epoch 8/25, Batch 1771/2250, Loss: 0.0452\n",
      "Epoch 8/25, Batch 1781/2250, Loss: 0.0462\n",
      "Epoch 8/25, Batch 1791/2250, Loss: 0.1971\n",
      "Epoch 8/25, Batch 1801/2250, Loss: 0.1023\n",
      "Epoch 8/25, Batch 1811/2250, Loss: 0.0916\n",
      "Epoch 8/25, Batch 1821/2250, Loss: 0.0639\n",
      "Epoch 8/25, Batch 1831/2250, Loss: 0.0454\n",
      "Epoch 8/25, Batch 1841/2250, Loss: 0.0511\n",
      "Epoch 8/25, Batch 1851/2250, Loss: 0.0214\n",
      "Epoch 8/25, Batch 1861/2250, Loss: 0.2279\n",
      "Epoch 8/25, Batch 1871/2250, Loss: 0.0401\n",
      "Epoch 8/25, Batch 1881/2250, Loss: 0.1165\n",
      "Epoch 8/25, Batch 1891/2250, Loss: 0.0120\n",
      "Epoch 8/25, Batch 1901/2250, Loss: 0.1225\n",
      "Epoch 8/25, Batch 1911/2250, Loss: 0.1558\n",
      "Epoch 8/25, Batch 1921/2250, Loss: 0.0492\n",
      "Epoch 8/25, Batch 1931/2250, Loss: 0.0368\n",
      "Epoch 8/25, Batch 1941/2250, Loss: 0.0451\n",
      "Epoch 8/25, Batch 1951/2250, Loss: 0.1153\n",
      "Epoch 8/25, Batch 1961/2250, Loss: 0.0163\n",
      "Epoch 8/25, Batch 1971/2250, Loss: 0.0660\n",
      "Epoch 8/25, Batch 1981/2250, Loss: 0.1390\n",
      "Epoch 8/25, Batch 1991/2250, Loss: 0.0492\n",
      "Epoch 8/25, Batch 2001/2250, Loss: 0.0847\n",
      "Epoch 8/25, Batch 2011/2250, Loss: 0.0075\n",
      "Epoch 8/25, Batch 2021/2250, Loss: 0.0132\n",
      "Epoch 8/25, Batch 2031/2250, Loss: 0.0066\n",
      "Epoch 8/25, Batch 2041/2250, Loss: 0.0828\n",
      "Epoch 8/25, Batch 2051/2250, Loss: 0.0450\n",
      "Epoch 8/25, Batch 2061/2250, Loss: 0.1696\n",
      "Epoch 8/25, Batch 2071/2250, Loss: 0.0725\n",
      "Epoch 8/25, Batch 2081/2250, Loss: 0.0254\n",
      "Epoch 8/25, Batch 2091/2250, Loss: 0.1635\n",
      "Epoch 8/25, Batch 2101/2250, Loss: 0.0752\n",
      "Epoch 8/25, Batch 2111/2250, Loss: 0.3141\n",
      "Epoch 8/25, Batch 2121/2250, Loss: 0.0169\n",
      "Epoch 8/25, Batch 2131/2250, Loss: 0.0380\n",
      "Epoch 8/25, Batch 2141/2250, Loss: 0.0266\n",
      "Epoch 8/25, Batch 2151/2250, Loss: 0.1027\n",
      "Epoch 8/25, Batch 2161/2250, Loss: 0.1672\n",
      "Epoch 8/25, Batch 2171/2250, Loss: 0.1731\n",
      "Epoch 8/25, Batch 2181/2250, Loss: 0.0534\n",
      "Epoch 8/25, Batch 2191/2250, Loss: 0.0220\n",
      "Epoch 8/25, Batch 2201/2250, Loss: 0.0320\n",
      "Epoch 8/25, Batch 2211/2250, Loss: 0.0517\n",
      "Epoch 8/25, Batch 2221/2250, Loss: 0.0070\n",
      "Epoch 8/25, Batch 2231/2250, Loss: 0.0234\n",
      "Epoch 8/25, Batch 2241/2250, Loss: 0.0664\n",
      "Epoch 8/25:\n",
      "Train Loss: 0.0842, Train Acc: 96.78%\n",
      "Val Loss: 0.0531, Val Acc: 98.14%\n",
      "------------------------------------------------------------\n",
      "Starting epoch 9/25...\n",
      "Starting training loop with 2250 batches...\n",
      "Successfully loaded first batch: images shape torch.Size([32, 3, 224, 224]), labels shape torch.Size([32])\n",
      "Epoch 9/25, Batch 1/2250, Loss: 0.0750\n",
      "Epoch 9/25, Batch 11/2250, Loss: 0.0557\n",
      "Epoch 9/25, Batch 21/2250, Loss: 0.1406\n",
      "Epoch 9/25, Batch 31/2250, Loss: 0.0775\n",
      "Epoch 9/25, Batch 41/2250, Loss: 0.1173\n",
      "Epoch 9/25, Batch 51/2250, Loss: 0.2498\n",
      "Epoch 9/25, Batch 61/2250, Loss: 0.0762\n",
      "Epoch 9/25, Batch 71/2250, Loss: 0.0568\n",
      "Epoch 9/25, Batch 81/2250, Loss: 0.0911\n",
      "Epoch 9/25, Batch 91/2250, Loss: 0.0182\n",
      "Epoch 9/25, Batch 101/2250, Loss: 0.0648\n",
      "Epoch 9/25, Batch 111/2250, Loss: 0.0764\n",
      "Epoch 9/25, Batch 121/2250, Loss: 0.0942\n",
      "Epoch 9/25, Batch 131/2250, Loss: 0.0396\n",
      "Epoch 9/25, Batch 141/2250, Loss: 0.0241\n",
      "Epoch 9/25, Batch 151/2250, Loss: 0.0168\n",
      "Epoch 9/25, Batch 161/2250, Loss: 0.0168\n",
      "Epoch 9/25, Batch 171/2250, Loss: 0.0685\n",
      "Epoch 9/25, Batch 181/2250, Loss: 0.0425\n",
      "Epoch 9/25, Batch 191/2250, Loss: 0.0258\n",
      "Epoch 9/25, Batch 201/2250, Loss: 0.1443\n",
      "Epoch 9/25, Batch 211/2250, Loss: 0.0975\n",
      "Epoch 9/25, Batch 221/2250, Loss: 0.0424\n",
      "Epoch 9/25, Batch 231/2250, Loss: 0.0566\n",
      "Epoch 9/25, Batch 241/2250, Loss: 0.1117\n",
      "Epoch 9/25, Batch 251/2250, Loss: 0.1885\n",
      "Epoch 9/25, Batch 261/2250, Loss: 0.1157\n",
      "Epoch 9/25, Batch 271/2250, Loss: 0.0087\n",
      "Epoch 9/25, Batch 281/2250, Loss: 0.0699\n",
      "Epoch 9/25, Batch 291/2250, Loss: 0.0255\n",
      "Epoch 9/25, Batch 301/2250, Loss: 0.0293\n",
      "Epoch 9/25, Batch 311/2250, Loss: 0.3015\n",
      "Epoch 9/25, Batch 321/2250, Loss: 0.1849\n",
      "Epoch 9/25, Batch 331/2250, Loss: 0.0083\n",
      "Epoch 9/25, Batch 341/2250, Loss: 0.2343\n",
      "Epoch 9/25, Batch 351/2250, Loss: 0.0282\n",
      "Epoch 9/25, Batch 361/2250, Loss: 0.0423\n",
      "Epoch 9/25, Batch 371/2250, Loss: 0.0456\n",
      "Epoch 9/25, Batch 381/2250, Loss: 0.0875\n",
      "Epoch 9/25, Batch 391/2250, Loss: 0.0345\n",
      "Epoch 9/25, Batch 401/2250, Loss: 0.0888\n",
      "Epoch 9/25, Batch 411/2250, Loss: 0.0603\n",
      "Epoch 9/25, Batch 421/2250, Loss: 0.0320\n",
      "Epoch 9/25, Batch 431/2250, Loss: 0.0093\n",
      "Epoch 9/25, Batch 441/2250, Loss: 0.1796\n",
      "Epoch 9/25, Batch 451/2250, Loss: 0.0700\n",
      "Epoch 9/25, Batch 461/2250, Loss: 0.0162\n",
      "Epoch 9/25, Batch 471/2250, Loss: 0.0658\n",
      "Epoch 9/25, Batch 481/2250, Loss: 0.0439\n",
      "Epoch 9/25, Batch 491/2250, Loss: 0.2189\n",
      "Epoch 9/25, Batch 501/2250, Loss: 0.0373\n",
      "Epoch 9/25, Batch 511/2250, Loss: 0.0572\n",
      "Epoch 9/25, Batch 521/2250, Loss: 0.0992\n",
      "Epoch 9/25, Batch 531/2250, Loss: 0.1552\n",
      "Epoch 9/25, Batch 541/2250, Loss: 0.0488\n",
      "Epoch 9/25, Batch 551/2250, Loss: 0.0869\n",
      "Epoch 9/25, Batch 561/2250, Loss: 0.0117\n",
      "Epoch 9/25, Batch 571/2250, Loss: 0.0428\n",
      "Epoch 9/25, Batch 581/2250, Loss: 0.0966\n",
      "Epoch 9/25, Batch 591/2250, Loss: 0.1250\n",
      "Epoch 9/25, Batch 601/2250, Loss: 0.0471\n",
      "Epoch 9/25, Batch 611/2250, Loss: 0.0048\n",
      "Epoch 9/25, Batch 621/2250, Loss: 0.1995\n",
      "Epoch 9/25, Batch 631/2250, Loss: 0.1606\n",
      "Epoch 9/25, Batch 641/2250, Loss: 0.0361\n",
      "Epoch 9/25, Batch 651/2250, Loss: 0.0061\n",
      "Epoch 9/25, Batch 661/2250, Loss: 0.0584\n",
      "Epoch 9/25, Batch 671/2250, Loss: 0.1510\n",
      "Epoch 9/25, Batch 681/2250, Loss: 0.0901\n",
      "Epoch 9/25, Batch 691/2250, Loss: 0.0536\n",
      "Epoch 9/25, Batch 701/2250, Loss: 0.0093\n",
      "Epoch 9/25, Batch 711/2250, Loss: 0.0296\n",
      "Epoch 9/25, Batch 721/2250, Loss: 0.0349\n",
      "Epoch 9/25, Batch 731/2250, Loss: 0.4374\n",
      "Epoch 9/25, Batch 741/2250, Loss: 0.0492\n",
      "Epoch 9/25, Batch 751/2250, Loss: 0.0482\n",
      "Epoch 9/25, Batch 761/2250, Loss: 0.0096\n",
      "Epoch 9/25, Batch 771/2250, Loss: 0.0356\n",
      "Epoch 9/25, Batch 781/2250, Loss: 0.1171\n",
      "Epoch 9/25, Batch 791/2250, Loss: 0.1401\n",
      "Epoch 9/25, Batch 801/2250, Loss: 0.1404\n",
      "Epoch 9/25, Batch 811/2250, Loss: 0.2128\n",
      "Epoch 9/25, Batch 821/2250, Loss: 0.0173\n",
      "Epoch 9/25, Batch 831/2250, Loss: 0.0998\n",
      "Epoch 9/25, Batch 841/2250, Loss: 0.0467\n",
      "Epoch 9/25, Batch 851/2250, Loss: 0.0093\n",
      "Epoch 9/25, Batch 861/2250, Loss: 0.0792\n",
      "Epoch 9/25, Batch 871/2250, Loss: 0.1550\n",
      "Epoch 9/25, Batch 881/2250, Loss: 0.1932\n",
      "Epoch 9/25, Batch 891/2250, Loss: 0.0586\n",
      "Epoch 9/25, Batch 901/2250, Loss: 0.0416\n",
      "Epoch 9/25, Batch 911/2250, Loss: 0.0572\n",
      "Epoch 9/25, Batch 921/2250, Loss: 0.0919\n",
      "Epoch 9/25, Batch 931/2250, Loss: 0.1683\n",
      "Epoch 9/25, Batch 941/2250, Loss: 0.0686\n",
      "Epoch 9/25, Batch 951/2250, Loss: 0.0326\n",
      "Epoch 9/25, Batch 961/2250, Loss: 0.0375\n",
      "Epoch 9/25, Batch 971/2250, Loss: 0.0336\n",
      "Epoch 9/25, Batch 981/2250, Loss: 0.0188\n",
      "Epoch 9/25, Batch 991/2250, Loss: 0.0628\n",
      "Epoch 9/25, Batch 1001/2250, Loss: 0.0429\n",
      "Epoch 9/25, Batch 1011/2250, Loss: 0.1165\n",
      "Epoch 9/25, Batch 1021/2250, Loss: 0.0300\n",
      "Epoch 9/25, Batch 1031/2250, Loss: 0.0793\n",
      "Epoch 9/25, Batch 1041/2250, Loss: 0.0303\n",
      "Epoch 9/25, Batch 1051/2250, Loss: 0.0844\n",
      "Epoch 9/25, Batch 1061/2250, Loss: 0.0709\n",
      "Epoch 9/25, Batch 1071/2250, Loss: 0.2403\n",
      "Epoch 9/25, Batch 1081/2250, Loss: 0.0836\n",
      "Epoch 9/25, Batch 1091/2250, Loss: 0.0601\n",
      "Epoch 9/25, Batch 1101/2250, Loss: 0.0390\n",
      "Epoch 9/25, Batch 1111/2250, Loss: 0.0616\n",
      "Epoch 9/25, Batch 1121/2250, Loss: 0.0856\n",
      "Epoch 9/25, Batch 1131/2250, Loss: 0.2035\n",
      "Epoch 9/25, Batch 1141/2250, Loss: 0.0155\n",
      "Epoch 9/25, Batch 1151/2250, Loss: 0.0725\n",
      "Epoch 9/25, Batch 1161/2250, Loss: 0.0410\n",
      "Epoch 9/25, Batch 1171/2250, Loss: 0.0338\n",
      "Epoch 9/25, Batch 1181/2250, Loss: 0.0186\n",
      "Epoch 9/25, Batch 1191/2250, Loss: 0.1083\n",
      "Epoch 9/25, Batch 1201/2250, Loss: 0.0526\n",
      "Epoch 9/25, Batch 1211/2250, Loss: 0.0152\n",
      "Epoch 9/25, Batch 1221/2250, Loss: 0.0114\n",
      "Epoch 9/25, Batch 1231/2250, Loss: 0.0356\n",
      "Epoch 9/25, Batch 1241/2250, Loss: 0.1347\n",
      "Epoch 9/25, Batch 1251/2250, Loss: 0.0334\n",
      "Epoch 9/25, Batch 1261/2250, Loss: 0.0545\n",
      "Epoch 9/25, Batch 1271/2250, Loss: 0.0325\n",
      "Epoch 9/25, Batch 1281/2250, Loss: 0.0414\n",
      "Epoch 9/25, Batch 1291/2250, Loss: 0.0031\n",
      "Epoch 9/25, Batch 1301/2250, Loss: 0.0240\n",
      "Epoch 9/25, Batch 1311/2250, Loss: 0.2192\n",
      "Epoch 9/25, Batch 1321/2250, Loss: 0.1366\n",
      "Epoch 9/25, Batch 1331/2250, Loss: 0.0526\n",
      "Epoch 9/25, Batch 1341/2250, Loss: 0.0851\n",
      "Epoch 9/25, Batch 1351/2250, Loss: 0.1314\n",
      "Epoch 9/25, Batch 1361/2250, Loss: 0.0369\n",
      "Epoch 9/25, Batch 1371/2250, Loss: 0.0179\n",
      "Epoch 9/25, Batch 1381/2250, Loss: 0.0160\n",
      "Epoch 9/25, Batch 1391/2250, Loss: 0.1555\n",
      "Epoch 9/25, Batch 1401/2250, Loss: 0.1008\n",
      "Epoch 9/25, Batch 1411/2250, Loss: 0.0611\n",
      "Epoch 9/25, Batch 1421/2250, Loss: 0.0339\n",
      "Epoch 9/25, Batch 1431/2250, Loss: 0.1028\n",
      "Epoch 9/25, Batch 1441/2250, Loss: 0.4551\n",
      "Epoch 9/25, Batch 1451/2250, Loss: 0.0700\n",
      "Epoch 9/25, Batch 1461/2250, Loss: 0.0465\n",
      "Epoch 9/25, Batch 1471/2250, Loss: 0.0825\n",
      "Epoch 9/25, Batch 1481/2250, Loss: 0.0424\n",
      "Epoch 9/25, Batch 1491/2250, Loss: 0.0258\n",
      "Epoch 9/25, Batch 1501/2250, Loss: 0.0239\n",
      "Epoch 9/25, Batch 1511/2250, Loss: 0.0374\n",
      "Epoch 9/25, Batch 1521/2250, Loss: 0.0497\n",
      "Epoch 9/25, Batch 1531/2250, Loss: 0.0239\n",
      "Epoch 9/25, Batch 1541/2250, Loss: 0.0392\n",
      "Epoch 9/25, Batch 1551/2250, Loss: 0.0337\n",
      "Epoch 9/25, Batch 1561/2250, Loss: 0.0290\n",
      "Epoch 9/25, Batch 1571/2250, Loss: 0.0176\n",
      "Epoch 9/25, Batch 1581/2250, Loss: 0.0871\n",
      "Epoch 9/25, Batch 1591/2250, Loss: 0.0732\n",
      "Epoch 9/25, Batch 1601/2250, Loss: 0.0922\n",
      "Epoch 9/25, Batch 1611/2250, Loss: 0.0981\n",
      "Epoch 9/25, Batch 1621/2250, Loss: 0.0434\n",
      "Epoch 9/25, Batch 1631/2250, Loss: 0.0278\n",
      "Epoch 9/25, Batch 1641/2250, Loss: 0.0408\n",
      "Epoch 9/25, Batch 1651/2250, Loss: 0.0625\n",
      "Epoch 9/25, Batch 1661/2250, Loss: 0.0162\n",
      "Epoch 9/25, Batch 1671/2250, Loss: 0.0803\n",
      "Epoch 9/25, Batch 1681/2250, Loss: 0.1166\n",
      "Epoch 9/25, Batch 1691/2250, Loss: 0.0185\n",
      "Epoch 9/25, Batch 1701/2250, Loss: 0.0089\n",
      "Epoch 9/25, Batch 1711/2250, Loss: 0.0859\n",
      "Epoch 9/25, Batch 1721/2250, Loss: 0.0035\n",
      "Epoch 9/25, Batch 1731/2250, Loss: 0.0598\n",
      "Epoch 9/25, Batch 1741/2250, Loss: 0.0954\n",
      "Epoch 9/25, Batch 1751/2250, Loss: 0.0171\n",
      "Epoch 9/25, Batch 1761/2250, Loss: 0.0256\n",
      "Epoch 9/25, Batch 1771/2250, Loss: 0.0106\n",
      "Epoch 9/25, Batch 1781/2250, Loss: 0.0280\n",
      "Epoch 9/25, Batch 1791/2250, Loss: 0.0231\n",
      "Epoch 9/25, Batch 1801/2250, Loss: 0.0069\n",
      "Epoch 9/25, Batch 1811/2250, Loss: 0.1134\n",
      "Epoch 9/25, Batch 1821/2250, Loss: 0.0964\n",
      "Epoch 9/25, Batch 1831/2250, Loss: 0.0250\n",
      "Epoch 9/25, Batch 1841/2250, Loss: 0.0620\n",
      "Epoch 9/25, Batch 1851/2250, Loss: 0.1077\n",
      "Epoch 9/25, Batch 1861/2250, Loss: 0.0615\n",
      "Epoch 9/25, Batch 1871/2250, Loss: 0.1138\n",
      "Epoch 9/25, Batch 1881/2250, Loss: 0.0629\n",
      "Epoch 9/25, Batch 1891/2250, Loss: 0.0618\n",
      "Epoch 9/25, Batch 1901/2250, Loss: 0.3874\n",
      "Epoch 9/25, Batch 1911/2250, Loss: 0.0588\n",
      "Epoch 9/25, Batch 1921/2250, Loss: 0.1517\n",
      "Epoch 9/25, Batch 1931/2250, Loss: 0.0346\n",
      "Epoch 9/25, Batch 1941/2250, Loss: 0.0142\n",
      "Epoch 9/25, Batch 1951/2250, Loss: 0.0770\n",
      "Epoch 9/25, Batch 1961/2250, Loss: 0.0277\n",
      "Epoch 9/25, Batch 1971/2250, Loss: 0.0520\n",
      "Epoch 9/25, Batch 1981/2250, Loss: 0.0231\n",
      "Epoch 9/25, Batch 1991/2250, Loss: 0.0572\n",
      "Epoch 9/25, Batch 2001/2250, Loss: 0.0840\n",
      "Epoch 9/25, Batch 2011/2250, Loss: 0.2563\n",
      "Epoch 9/25, Batch 2021/2250, Loss: 0.0072\n",
      "Epoch 9/25, Batch 2031/2250, Loss: 0.0309\n",
      "Epoch 9/25, Batch 2041/2250, Loss: 0.0578\n",
      "Epoch 9/25, Batch 2051/2250, Loss: 0.0227\n",
      "Epoch 9/25, Batch 2061/2250, Loss: 0.0041\n",
      "Epoch 9/25, Batch 2071/2250, Loss: 0.0623\n",
      "Epoch 9/25, Batch 2081/2250, Loss: 0.0973\n",
      "Epoch 9/25, Batch 2091/2250, Loss: 0.2037\n",
      "Epoch 9/25, Batch 2101/2250, Loss: 0.0410\n",
      "Epoch 9/25, Batch 2111/2250, Loss: 0.0851\n",
      "Epoch 9/25, Batch 2121/2250, Loss: 0.0419\n",
      "Epoch 9/25, Batch 2131/2250, Loss: 0.1171\n",
      "Epoch 9/25, Batch 2141/2250, Loss: 0.1068\n",
      "Epoch 9/25, Batch 2151/2250, Loss: 0.0937\n",
      "Epoch 9/25, Batch 2161/2250, Loss: 0.0419\n",
      "Epoch 9/25, Batch 2171/2250, Loss: 0.0464\n",
      "Epoch 9/25, Batch 2181/2250, Loss: 0.0307\n",
      "Epoch 9/25, Batch 2191/2250, Loss: 0.0392\n",
      "Epoch 9/25, Batch 2201/2250, Loss: 0.0498\n",
      "Epoch 9/25, Batch 2211/2250, Loss: 0.0723\n",
      "Epoch 9/25, Batch 2221/2250, Loss: 0.0221\n",
      "Epoch 9/25, Batch 2231/2250, Loss: 0.0482\n",
      "Epoch 9/25, Batch 2241/2250, Loss: 0.0657\n",
      "Epoch 9/25:\n",
      "Train Loss: 0.0775, Train Acc: 97.02%\n",
      "Val Loss: 0.0633, Val Acc: 97.70%\n",
      "------------------------------------------------------------\n",
      "Starting epoch 10/25...\n",
      "Starting training loop with 2250 batches...\n",
      "Successfully loaded first batch: images shape torch.Size([32, 3, 224, 224]), labels shape torch.Size([32])\n",
      "Epoch 10/25, Batch 1/2250, Loss: 0.0098\n",
      "Epoch 10/25, Batch 11/2250, Loss: 0.0776\n",
      "Epoch 10/25, Batch 21/2250, Loss: 0.0143\n",
      "Epoch 10/25, Batch 31/2250, Loss: 0.0035\n",
      "Epoch 10/25, Batch 41/2250, Loss: 0.0610\n",
      "Epoch 10/25, Batch 51/2250, Loss: 0.0075\n",
      "Epoch 10/25, Batch 61/2250, Loss: 0.2085\n",
      "Epoch 10/25, Batch 71/2250, Loss: 0.0900\n",
      "Epoch 10/25, Batch 81/2250, Loss: 0.0480\n",
      "Epoch 10/25, Batch 91/2250, Loss: 0.0618\n",
      "Epoch 10/25, Batch 101/2250, Loss: 0.1071\n",
      "Epoch 10/25, Batch 111/2250, Loss: 0.1290\n",
      "Epoch 10/25, Batch 121/2250, Loss: 0.1089\n",
      "Epoch 10/25, Batch 131/2250, Loss: 0.0191\n",
      "Epoch 10/25, Batch 141/2250, Loss: 0.0260\n",
      "Epoch 10/25, Batch 151/2250, Loss: 0.0955\n",
      "Epoch 10/25, Batch 161/2250, Loss: 0.0961\n",
      "Epoch 10/25, Batch 171/2250, Loss: 0.1344\n",
      "Epoch 10/25, Batch 181/2250, Loss: 0.0626\n",
      "Epoch 10/25, Batch 191/2250, Loss: 0.0457\n",
      "Epoch 10/25, Batch 201/2250, Loss: 0.1287\n",
      "Epoch 10/25, Batch 211/2250, Loss: 0.2068\n",
      "Epoch 10/25, Batch 221/2250, Loss: 0.0363\n",
      "Epoch 10/25, Batch 231/2250, Loss: 0.0795\n",
      "Epoch 10/25, Batch 241/2250, Loss: 0.0364\n",
      "Epoch 10/25, Batch 251/2250, Loss: 0.0190\n",
      "Epoch 10/25, Batch 261/2250, Loss: 0.0685\n",
      "Epoch 10/25, Batch 271/2250, Loss: 0.0945\n",
      "Epoch 10/25, Batch 281/2250, Loss: 0.0954\n",
      "Epoch 10/25, Batch 291/2250, Loss: 0.0725\n",
      "Epoch 10/25, Batch 301/2250, Loss: 0.0462\n",
      "Epoch 10/25, Batch 311/2250, Loss: 0.1025\n",
      "Epoch 10/25, Batch 321/2250, Loss: 0.0051\n",
      "Epoch 10/25, Batch 331/2250, Loss: 0.1673\n",
      "Epoch 10/25, Batch 341/2250, Loss: 0.4140\n",
      "Epoch 10/25, Batch 351/2250, Loss: 0.0216\n",
      "Epoch 10/25, Batch 361/2250, Loss: 0.0576\n",
      "Epoch 10/25, Batch 371/2250, Loss: 0.0509\n",
      "Epoch 10/25, Batch 381/2250, Loss: 0.1014\n",
      "Epoch 10/25, Batch 391/2250, Loss: 0.1628\n",
      "Epoch 10/25, Batch 401/2250, Loss: 0.0418\n",
      "Epoch 10/25, Batch 411/2250, Loss: 0.0081\n",
      "Epoch 10/25, Batch 421/2250, Loss: 0.0593\n",
      "Epoch 10/25, Batch 431/2250, Loss: 0.0122\n",
      "Epoch 10/25, Batch 441/2250, Loss: 0.0519\n",
      "Epoch 10/25, Batch 451/2250, Loss: 0.0205\n",
      "Epoch 10/25, Batch 461/2250, Loss: 0.1243\n",
      "Epoch 10/25, Batch 471/2250, Loss: 0.0715\n",
      "Epoch 10/25, Batch 481/2250, Loss: 0.0365\n",
      "Epoch 10/25, Batch 491/2250, Loss: 0.0625\n",
      "Epoch 10/25, Batch 501/2250, Loss: 0.0633\n",
      "Epoch 10/25, Batch 511/2250, Loss: 0.0153\n",
      "Epoch 10/25, Batch 521/2250, Loss: 0.0163\n",
      "Epoch 10/25, Batch 531/2250, Loss: 0.1223\n",
      "Epoch 10/25, Batch 541/2250, Loss: 0.0050\n",
      "Epoch 10/25, Batch 551/2250, Loss: 0.0852\n",
      "Epoch 10/25, Batch 561/2250, Loss: 0.0018\n",
      "Epoch 10/25, Batch 571/2250, Loss: 0.0812\n",
      "Epoch 10/25, Batch 581/2250, Loss: 0.1464\n",
      "Epoch 10/25, Batch 591/2250, Loss: 0.0507\n",
      "Epoch 10/25, Batch 601/2250, Loss: 0.1494\n",
      "Epoch 10/25, Batch 611/2250, Loss: 0.0227\n",
      "Epoch 10/25, Batch 621/2250, Loss: 0.0256\n",
      "Epoch 10/25, Batch 631/2250, Loss: 0.0067\n",
      "Epoch 10/25, Batch 641/2250, Loss: 0.0044\n",
      "Epoch 10/25, Batch 651/2250, Loss: 0.0479\n",
      "Epoch 10/25, Batch 661/2250, Loss: 0.0073\n",
      "Epoch 10/25, Batch 671/2250, Loss: 0.0040\n",
      "Epoch 10/25, Batch 681/2250, Loss: 0.0155\n",
      "Epoch 10/25, Batch 691/2250, Loss: 0.1123\n",
      "Epoch 10/25, Batch 701/2250, Loss: 0.1207\n",
      "Epoch 10/25, Batch 711/2250, Loss: 0.0366\n",
      "Epoch 10/25, Batch 721/2250, Loss: 0.1081\n",
      "Epoch 10/25, Batch 731/2250, Loss: 0.0289\n",
      "Epoch 10/25, Batch 741/2250, Loss: 0.0238\n",
      "Epoch 10/25, Batch 751/2250, Loss: 0.0959\n",
      "Epoch 10/25, Batch 761/2250, Loss: 0.0650\n",
      "Epoch 10/25, Batch 771/2250, Loss: 0.0124\n",
      "Epoch 10/25, Batch 781/2250, Loss: 0.1064\n",
      "Epoch 10/25, Batch 791/2250, Loss: 0.2161\n",
      "Epoch 10/25, Batch 801/2250, Loss: 0.0120\n",
      "Epoch 10/25, Batch 811/2250, Loss: 0.0468\n",
      "Epoch 10/25, Batch 821/2250, Loss: 0.2506\n",
      "Epoch 10/25, Batch 831/2250, Loss: 0.0943\n",
      "Epoch 10/25, Batch 841/2250, Loss: 0.0197\n",
      "Epoch 10/25, Batch 851/2250, Loss: 0.0085\n",
      "Epoch 10/25, Batch 861/2250, Loss: 0.0230\n",
      "Epoch 10/25, Batch 871/2250, Loss: 0.0182\n",
      "Epoch 10/25, Batch 881/2250, Loss: 0.0068\n",
      "Epoch 10/25, Batch 891/2250, Loss: 0.0382\n",
      "Epoch 10/25, Batch 901/2250, Loss: 0.1011\n",
      "Epoch 10/25, Batch 911/2250, Loss: 0.0817\n",
      "Epoch 10/25, Batch 921/2250, Loss: 0.3220\n",
      "Epoch 10/25, Batch 931/2250, Loss: 0.0563\n",
      "Epoch 10/25, Batch 941/2250, Loss: 0.0810\n",
      "Epoch 10/25, Batch 951/2250, Loss: 0.0757\n",
      "Epoch 10/25, Batch 961/2250, Loss: 0.0589\n",
      "Epoch 10/25, Batch 971/2250, Loss: 0.1566\n",
      "Epoch 10/25, Batch 981/2250, Loss: 0.0346\n",
      "Epoch 10/25, Batch 991/2250, Loss: 0.2323\n",
      "Epoch 10/25, Batch 1001/2250, Loss: 0.0663\n",
      "Epoch 10/25, Batch 1011/2250, Loss: 0.0679\n",
      "Epoch 10/25, Batch 1021/2250, Loss: 0.1932\n",
      "Epoch 10/25, Batch 1031/2250, Loss: 0.0176\n",
      "Epoch 10/25, Batch 1041/2250, Loss: 0.1316\n",
      "Epoch 10/25, Batch 1051/2250, Loss: 0.0440\n",
      "Epoch 10/25, Batch 1061/2250, Loss: 0.0808\n",
      "Epoch 10/25, Batch 1071/2250, Loss: 0.1026\n",
      "Epoch 10/25, Batch 1081/2250, Loss: 0.0712\n",
      "Epoch 10/25, Batch 1091/2250, Loss: 0.0018\n",
      "Epoch 10/25, Batch 1101/2250, Loss: 0.0547\n",
      "Epoch 10/25, Batch 1111/2250, Loss: 0.0457\n",
      "Epoch 10/25, Batch 1121/2250, Loss: 0.0326\n",
      "Epoch 10/25, Batch 1131/2250, Loss: 0.1111\n",
      "Epoch 10/25, Batch 1141/2250, Loss: 0.0483\n",
      "Epoch 10/25, Batch 1151/2250, Loss: 0.0264\n",
      "Epoch 10/25, Batch 1161/2250, Loss: 0.0262\n",
      "Epoch 10/25, Batch 1171/2250, Loss: 0.0236\n",
      "Epoch 10/25, Batch 1181/2250, Loss: 0.0246\n",
      "Epoch 10/25, Batch 1191/2250, Loss: 0.0335\n",
      "Epoch 10/25, Batch 1201/2250, Loss: 0.0631\n",
      "Epoch 10/25, Batch 1211/2250, Loss: 0.1712\n",
      "Epoch 10/25, Batch 1221/2250, Loss: 0.0603\n",
      "Epoch 10/25, Batch 1231/2250, Loss: 0.0512\n",
      "Epoch 10/25, Batch 1241/2250, Loss: 0.0524\n",
      "Epoch 10/25, Batch 1251/2250, Loss: 0.0246\n",
      "Epoch 10/25, Batch 1261/2250, Loss: 0.1159\n",
      "Epoch 10/25, Batch 1271/2250, Loss: 0.0309\n",
      "Epoch 10/25, Batch 1281/2250, Loss: 0.1502\n",
      "Epoch 10/25, Batch 1291/2250, Loss: 0.1356\n",
      "Epoch 10/25, Batch 1301/2250, Loss: 0.0297\n",
      "Epoch 10/25, Batch 1311/2250, Loss: 0.1067\n",
      "Epoch 10/25, Batch 1321/2250, Loss: 0.0408\n",
      "Epoch 10/25, Batch 1331/2250, Loss: 0.1542\n",
      "Epoch 10/25, Batch 1341/2250, Loss: 0.0599\n",
      "Epoch 10/25, Batch 1351/2250, Loss: 0.1313\n",
      "Epoch 10/25, Batch 1361/2250, Loss: 0.0296\n",
      "Epoch 10/25, Batch 1371/2250, Loss: 0.0833\n",
      "Epoch 10/25, Batch 1381/2250, Loss: 0.4161\n",
      "Epoch 10/25, Batch 1391/2250, Loss: 0.1107\n",
      "Epoch 10/25, Batch 1401/2250, Loss: 0.1548\n",
      "Epoch 10/25, Batch 1411/2250, Loss: 0.0503\n",
      "Epoch 10/25, Batch 1421/2250, Loss: 0.0755\n",
      "Epoch 10/25, Batch 1431/2250, Loss: 0.0091\n",
      "Epoch 10/25, Batch 1441/2250, Loss: 0.0061\n",
      "Epoch 10/25, Batch 1451/2250, Loss: 0.1793\n",
      "Epoch 10/25, Batch 1461/2250, Loss: 0.0057\n",
      "Epoch 10/25, Batch 1471/2250, Loss: 0.0120\n",
      "Epoch 10/25, Batch 1481/2250, Loss: 0.0113\n",
      "Epoch 10/25, Batch 1491/2250, Loss: 0.0232\n",
      "Epoch 10/25, Batch 1501/2250, Loss: 0.0581\n",
      "Epoch 10/25, Batch 1511/2250, Loss: 0.0422\n",
      "Epoch 10/25, Batch 1521/2250, Loss: 0.1782\n",
      "Epoch 10/25, Batch 1531/2250, Loss: 0.0796\n",
      "Epoch 10/25, Batch 1541/2250, Loss: 0.0415\n",
      "Epoch 10/25, Batch 1551/2250, Loss: 0.0345\n",
      "Epoch 10/25, Batch 1561/2250, Loss: 0.0137\n",
      "Epoch 10/25, Batch 1571/2250, Loss: 0.0692\n",
      "Epoch 10/25, Batch 1581/2250, Loss: 0.0079\n",
      "Epoch 10/25, Batch 1591/2250, Loss: 0.0376\n",
      "Epoch 10/25, Batch 1601/2250, Loss: 0.0547\n",
      "Epoch 10/25, Batch 1611/2250, Loss: 0.0662\n",
      "Epoch 10/25, Batch 1621/2250, Loss: 0.0499\n",
      "Epoch 10/25, Batch 1631/2250, Loss: 0.0654\n",
      "Epoch 10/25, Batch 1641/2250, Loss: 0.0695\n",
      "Epoch 10/25, Batch 1651/2250, Loss: 0.1209\n",
      "Epoch 10/25, Batch 1661/2250, Loss: 0.0266\n",
      "Epoch 10/25, Batch 1671/2250, Loss: 0.0097\n",
      "Epoch 10/25, Batch 1681/2250, Loss: 0.0405\n",
      "Epoch 10/25, Batch 1691/2250, Loss: 0.0622\n",
      "Epoch 10/25, Batch 1701/2250, Loss: 0.0218\n",
      "Epoch 10/25, Batch 1711/2250, Loss: 0.0117\n",
      "Epoch 10/25, Batch 1721/2250, Loss: 0.2056\n",
      "Epoch 10/25, Batch 1731/2250, Loss: 0.0764\n",
      "Epoch 10/25, Batch 1741/2250, Loss: 0.2465\n",
      "Epoch 10/25, Batch 1751/2250, Loss: 0.0101\n",
      "Epoch 10/25, Batch 1761/2250, Loss: 0.0960\n",
      "Epoch 10/25, Batch 1771/2250, Loss: 0.1718\n",
      "Epoch 10/25, Batch 1781/2250, Loss: 0.0069\n",
      "Epoch 10/25, Batch 1791/2250, Loss: 0.0850\n",
      "Epoch 10/25, Batch 1801/2250, Loss: 0.0712\n",
      "Epoch 10/25, Batch 1811/2250, Loss: 0.0137\n",
      "Epoch 10/25, Batch 1821/2250, Loss: 0.0223\n",
      "Epoch 10/25, Batch 1831/2250, Loss: 0.1000\n",
      "Epoch 10/25, Batch 1841/2250, Loss: 0.0899\n",
      "Epoch 10/25, Batch 1851/2250, Loss: 0.0354\n",
      "Epoch 10/25, Batch 1861/2250, Loss: 0.0803\n",
      "Epoch 10/25, Batch 1871/2250, Loss: 0.1420\n",
      "Epoch 10/25, Batch 1881/2250, Loss: 0.0613\n",
      "Epoch 10/25, Batch 1891/2250, Loss: 0.1349\n",
      "Epoch 10/25, Batch 1901/2250, Loss: 0.0653\n",
      "Epoch 10/25, Batch 1911/2250, Loss: 0.2420\n",
      "Epoch 10/25, Batch 1921/2250, Loss: 0.1043\n",
      "Epoch 10/25, Batch 1931/2250, Loss: 0.0995\n",
      "Epoch 10/25, Batch 1941/2250, Loss: 0.0108\n",
      "Epoch 10/25, Batch 1951/2250, Loss: 0.0260\n",
      "Epoch 10/25, Batch 1961/2250, Loss: 0.0333\n",
      "Epoch 10/25, Batch 1971/2250, Loss: 0.0828\n",
      "Epoch 10/25, Batch 1981/2250, Loss: 0.0054\n",
      "Epoch 10/25, Batch 1991/2250, Loss: 0.0981\n",
      "Epoch 10/25, Batch 2001/2250, Loss: 0.0293\n",
      "Epoch 10/25, Batch 2011/2250, Loss: 0.0304\n",
      "Epoch 10/25, Batch 2021/2250, Loss: 0.0137\n",
      "Epoch 10/25, Batch 2031/2250, Loss: 0.1190\n",
      "Epoch 10/25, Batch 2041/2250, Loss: 0.0974\n",
      "Epoch 10/25, Batch 2051/2250, Loss: 0.1520\n",
      "Epoch 10/25, Batch 2061/2250, Loss: 0.0646\n",
      "Epoch 10/25, Batch 2071/2250, Loss: 0.0791\n",
      "Epoch 10/25, Batch 2081/2250, Loss: 0.0300\n",
      "Epoch 10/25, Batch 2091/2250, Loss: 0.0980\n",
      "Epoch 10/25, Batch 2101/2250, Loss: 0.1065\n",
      "Epoch 10/25, Batch 2111/2250, Loss: 0.0421\n",
      "Epoch 10/25, Batch 2121/2250, Loss: 0.0063\n",
      "Epoch 10/25, Batch 2131/2250, Loss: 0.0476\n",
      "Epoch 10/25, Batch 2141/2250, Loss: 0.0707\n",
      "Epoch 10/25, Batch 2151/2250, Loss: 0.0738\n",
      "Epoch 10/25, Batch 2161/2250, Loss: 0.0467\n",
      "Epoch 10/25, Batch 2171/2250, Loss: 0.0566\n",
      "Epoch 10/25, Batch 2181/2250, Loss: 0.0824\n",
      "Epoch 10/25, Batch 2191/2250, Loss: 0.0693\n",
      "Epoch 10/25, Batch 2201/2250, Loss: 0.0493\n",
      "Epoch 10/25, Batch 2211/2250, Loss: 0.0988\n",
      "Epoch 10/25, Batch 2221/2250, Loss: 0.0421\n",
      "Epoch 10/25, Batch 2231/2250, Loss: 0.1668\n",
      "Epoch 10/25, Batch 2241/2250, Loss: 0.1344\n",
      "Epoch 10/25:\n",
      "Train Loss: 0.0709, Train Acc: 97.36%\n",
      "Val Loss: 0.0587, Val Acc: 97.76%\n",
      "------------------------------------------------------------\n",
      "Starting epoch 11/25...\n",
      "Starting training loop with 2250 batches...\n",
      "Successfully loaded first batch: images shape torch.Size([32, 3, 224, 224]), labels shape torch.Size([32])\n",
      "Epoch 11/25, Batch 1/2250, Loss: 0.0861\n",
      "Epoch 11/25, Batch 11/2250, Loss: 0.0785\n",
      "Epoch 11/25, Batch 21/2250, Loss: 0.0682\n",
      "Epoch 11/25, Batch 31/2250, Loss: 0.0134\n",
      "Epoch 11/25, Batch 41/2250, Loss: 0.0645\n",
      "Epoch 11/25, Batch 51/2250, Loss: 0.0269\n",
      "Epoch 11/25, Batch 61/2250, Loss: 0.0440\n",
      "Epoch 11/25, Batch 71/2250, Loss: 0.0980\n",
      "Epoch 11/25, Batch 81/2250, Loss: 0.0583\n",
      "Epoch 11/25, Batch 91/2250, Loss: 0.0460\n",
      "Epoch 11/25, Batch 101/2250, Loss: 0.0286\n",
      "Epoch 11/25, Batch 111/2250, Loss: 0.0709\n",
      "Epoch 11/25, Batch 121/2250, Loss: 0.0148\n",
      "Epoch 11/25, Batch 131/2250, Loss: 0.0653\n",
      "Epoch 11/25, Batch 141/2250, Loss: 0.1056\n",
      "Epoch 11/25, Batch 151/2250, Loss: 0.0374\n",
      "Epoch 11/25, Batch 161/2250, Loss: 0.0110\n",
      "Epoch 11/25, Batch 171/2250, Loss: 0.0765\n",
      "Epoch 11/25, Batch 181/2250, Loss: 0.0168\n",
      "Epoch 11/25, Batch 191/2250, Loss: 0.0097\n",
      "Epoch 11/25, Batch 201/2250, Loss: 0.1269\n",
      "Epoch 11/25, Batch 211/2250, Loss: 0.0357\n",
      "Epoch 11/25, Batch 221/2250, Loss: 0.1106\n",
      "Epoch 11/25, Batch 231/2250, Loss: 0.0110\n",
      "Epoch 11/25, Batch 241/2250, Loss: 0.0097\n",
      "Epoch 11/25, Batch 251/2250, Loss: 0.0082\n",
      "Epoch 11/25, Batch 261/2250, Loss: 0.1141\n",
      "Epoch 11/25, Batch 271/2250, Loss: 0.0112\n",
      "Epoch 11/25, Batch 281/2250, Loss: 0.0432\n",
      "Epoch 11/25, Batch 291/2250, Loss: 0.0331\n",
      "Epoch 11/25, Batch 301/2250, Loss: 0.0492\n",
      "Epoch 11/25, Batch 311/2250, Loss: 0.0125\n",
      "Epoch 11/25, Batch 321/2250, Loss: 0.0169\n",
      "Epoch 11/25, Batch 331/2250, Loss: 0.0377\n",
      "Epoch 11/25, Batch 341/2250, Loss: 0.0168\n",
      "Epoch 11/25, Batch 351/2250, Loss: 0.1096\n",
      "Epoch 11/25, Batch 361/2250, Loss: 0.0652\n",
      "Epoch 11/25, Batch 371/2250, Loss: 0.0399\n",
      "Epoch 11/25, Batch 381/2250, Loss: 0.0237\n",
      "Epoch 11/25, Batch 391/2250, Loss: 0.1160\n",
      "Epoch 11/25, Batch 401/2250, Loss: 0.0902\n",
      "Epoch 11/25, Batch 411/2250, Loss: 0.0489\n",
      "Epoch 11/25, Batch 421/2250, Loss: 0.0224\n",
      "Epoch 11/25, Batch 431/2250, Loss: 0.0068\n",
      "Epoch 11/25, Batch 441/2250, Loss: 0.1600\n",
      "Epoch 11/25, Batch 451/2250, Loss: 0.1248\n",
      "Epoch 11/25, Batch 461/2250, Loss: 0.0686\n",
      "Epoch 11/25, Batch 471/2250, Loss: 0.1872\n",
      "Epoch 11/25, Batch 481/2250, Loss: 0.1145\n",
      "Epoch 11/25, Batch 491/2250, Loss: 0.0225\n",
      "Epoch 11/25, Batch 501/2250, Loss: 0.0791\n",
      "Epoch 11/25, Batch 511/2250, Loss: 0.0765\n",
      "Epoch 11/25, Batch 521/2250, Loss: 0.0329\n",
      "Epoch 11/25, Batch 531/2250, Loss: 0.0595\n",
      "Epoch 11/25, Batch 541/2250, Loss: 0.2089\n",
      "Epoch 11/25, Batch 551/2250, Loss: 0.0662\n",
      "Epoch 11/25, Batch 561/2250, Loss: 0.0745\n",
      "Epoch 11/25, Batch 571/2250, Loss: 0.0060\n",
      "Epoch 11/25, Batch 581/2250, Loss: 0.1102\n",
      "Epoch 11/25, Batch 591/2250, Loss: 0.0223\n",
      "Epoch 11/25, Batch 601/2250, Loss: 0.0924\n",
      "Epoch 11/25, Batch 611/2250, Loss: 0.0147\n",
      "Epoch 11/25, Batch 621/2250, Loss: 0.0268\n",
      "Epoch 11/25, Batch 631/2250, Loss: 0.0637\n",
      "Epoch 11/25, Batch 641/2250, Loss: 0.0139\n",
      "Epoch 11/25, Batch 651/2250, Loss: 0.1057\n",
      "Epoch 11/25, Batch 661/2250, Loss: 0.1028\n",
      "Epoch 11/25, Batch 671/2250, Loss: 0.0355\n",
      "Epoch 11/25, Batch 681/2250, Loss: 0.0141\n",
      "Epoch 11/25, Batch 691/2250, Loss: 0.0896\n",
      "Epoch 11/25, Batch 701/2250, Loss: 0.0328\n",
      "Epoch 11/25, Batch 711/2250, Loss: 0.0265\n",
      "Epoch 11/25, Batch 721/2250, Loss: 0.0061\n",
      "Epoch 11/25, Batch 731/2250, Loss: 0.0146\n",
      "Epoch 11/25, Batch 741/2250, Loss: 0.0451\n",
      "Epoch 11/25, Batch 751/2250, Loss: 0.0237\n",
      "Epoch 11/25, Batch 761/2250, Loss: 0.0564\n",
      "Epoch 11/25, Batch 771/2250, Loss: 0.0157\n",
      "Epoch 11/25, Batch 781/2250, Loss: 0.0615\n",
      "Epoch 11/25, Batch 791/2250, Loss: 0.0168\n",
      "Epoch 11/25, Batch 801/2250, Loss: 0.0203\n",
      "Epoch 11/25, Batch 811/2250, Loss: 0.0444\n",
      "Epoch 11/25, Batch 821/2250, Loss: 0.0232\n",
      "Epoch 11/25, Batch 831/2250, Loss: 0.0105\n",
      "Epoch 11/25, Batch 841/2250, Loss: 0.0794\n",
      "Epoch 11/25, Batch 851/2250, Loss: 0.0532\n",
      "Epoch 11/25, Batch 861/2250, Loss: 0.0712\n",
      "Epoch 11/25, Batch 871/2250, Loss: 0.0010\n",
      "Epoch 11/25, Batch 881/2250, Loss: 0.0899\n",
      "Epoch 11/25, Batch 891/2250, Loss: 0.1739\n",
      "Epoch 11/25, Batch 901/2250, Loss: 0.0781\n",
      "Epoch 11/25, Batch 911/2250, Loss: 0.0857\n",
      "Epoch 11/25, Batch 921/2250, Loss: 0.0391\n",
      "Epoch 11/25, Batch 931/2250, Loss: 0.1255\n",
      "Epoch 11/25, Batch 941/2250, Loss: 0.1614\n",
      "Epoch 11/25, Batch 951/2250, Loss: 0.2364\n",
      "Epoch 11/25, Batch 961/2250, Loss: 0.2583\n",
      "Epoch 11/25, Batch 971/2250, Loss: 0.0688\n",
      "Epoch 11/25, Batch 981/2250, Loss: 0.1362\n",
      "Epoch 11/25, Batch 991/2250, Loss: 0.0066\n",
      "Epoch 11/25, Batch 1001/2250, Loss: 0.0308\n",
      "Epoch 11/25, Batch 1011/2250, Loss: 0.1716\n",
      "Epoch 11/25, Batch 1021/2250, Loss: 0.0388\n",
      "Epoch 11/25, Batch 1031/2250, Loss: 0.0195\n",
      "Epoch 11/25, Batch 1041/2250, Loss: 0.0119\n",
      "Epoch 11/25, Batch 1051/2250, Loss: 0.0220\n",
      "Epoch 11/25, Batch 1061/2250, Loss: 0.0250\n",
      "Epoch 11/25, Batch 1071/2250, Loss: 0.1425\n",
      "Epoch 11/25, Batch 1081/2250, Loss: 0.0655\n",
      "Epoch 11/25, Batch 1091/2250, Loss: 0.0168\n",
      "Epoch 11/25, Batch 1101/2250, Loss: 0.0075\n",
      "Epoch 11/25, Batch 1111/2250, Loss: 0.0275\n",
      "Epoch 11/25, Batch 1121/2250, Loss: 0.1016\n",
      "Epoch 11/25, Batch 1131/2250, Loss: 0.0292\n",
      "Epoch 11/25, Batch 1141/2250, Loss: 0.1480\n",
      "Epoch 11/25, Batch 1151/2250, Loss: 0.0078\n",
      "Epoch 11/25, Batch 1161/2250, Loss: 0.0103\n",
      "Epoch 11/25, Batch 1171/2250, Loss: 0.0090\n",
      "Epoch 11/25, Batch 1181/2250, Loss: 0.0607\n",
      "Epoch 11/25, Batch 1191/2250, Loss: 0.0084\n",
      "Epoch 11/25, Batch 1201/2250, Loss: 0.0284\n",
      "Epoch 11/25, Batch 1211/2250, Loss: 0.0378\n",
      "Epoch 11/25, Batch 1221/2250, Loss: 0.0150\n",
      "Epoch 11/25, Batch 1231/2250, Loss: 0.0619\n",
      "Epoch 11/25, Batch 1241/2250, Loss: 0.0208\n",
      "Epoch 11/25, Batch 1251/2250, Loss: 0.1499\n",
      "Epoch 11/25, Batch 1261/2250, Loss: 0.0498\n",
      "Epoch 11/25, Batch 1271/2250, Loss: 0.0491\n",
      "Epoch 11/25, Batch 1281/2250, Loss: 0.1040\n",
      "Epoch 11/25, Batch 1291/2250, Loss: 0.0431\n",
      "Epoch 11/25, Batch 1301/2250, Loss: 0.0155\n",
      "Epoch 11/25, Batch 1311/2250, Loss: 0.0284\n",
      "Epoch 11/25, Batch 1321/2250, Loss: 0.1830\n",
      "Epoch 11/25, Batch 1331/2250, Loss: 0.0492\n",
      "Epoch 11/25, Batch 1341/2250, Loss: 0.0162\n",
      "Epoch 11/25, Batch 1351/2250, Loss: 0.2065\n",
      "Epoch 11/25, Batch 1361/2250, Loss: 0.1559\n",
      "Epoch 11/25, Batch 1371/2250, Loss: 0.1104\n",
      "Epoch 11/25, Batch 1381/2250, Loss: 0.2047\n",
      "Epoch 11/25, Batch 1391/2250, Loss: 0.1389\n",
      "Epoch 11/25, Batch 1401/2250, Loss: 0.0493\n",
      "Epoch 11/25, Batch 1411/2250, Loss: 0.0462\n",
      "Epoch 11/25, Batch 1421/2250, Loss: 0.0791\n",
      "Epoch 11/25, Batch 1431/2250, Loss: 0.0199\n",
      "Epoch 11/25, Batch 1441/2250, Loss: 0.0714\n",
      "Epoch 11/25, Batch 1451/2250, Loss: 0.0584\n",
      "Epoch 11/25, Batch 1461/2250, Loss: 0.0434\n",
      "Epoch 11/25, Batch 1471/2250, Loss: 0.0159\n",
      "Epoch 11/25, Batch 1481/2250, Loss: 0.0114\n",
      "Epoch 11/25, Batch 1491/2250, Loss: 0.0032\n",
      "Epoch 11/25, Batch 1501/2250, Loss: 0.0497\n",
      "Epoch 11/25, Batch 1511/2250, Loss: 0.0538\n",
      "Epoch 11/25, Batch 1521/2250, Loss: 0.0377\n",
      "Epoch 11/25, Batch 1531/2250, Loss: 0.0605\n",
      "Epoch 11/25, Batch 1541/2250, Loss: 0.0188\n",
      "Epoch 11/25, Batch 1551/2250, Loss: 0.0530\n",
      "Epoch 11/25, Batch 1561/2250, Loss: 0.0334\n",
      "Epoch 11/25, Batch 1571/2250, Loss: 0.0332\n",
      "Epoch 11/25, Batch 1581/2250, Loss: 0.1678\n",
      "Epoch 11/25, Batch 1591/2250, Loss: 0.0233\n",
      "Epoch 11/25, Batch 1601/2250, Loss: 0.0253\n",
      "Epoch 11/25, Batch 1611/2250, Loss: 0.0351\n",
      "Epoch 11/25, Batch 1621/2250, Loss: 0.0242\n",
      "Epoch 11/25, Batch 1631/2250, Loss: 0.0915\n",
      "Epoch 11/25, Batch 1641/2250, Loss: 0.0289\n",
      "Epoch 11/25, Batch 1651/2250, Loss: 0.0487\n",
      "Epoch 11/25, Batch 1661/2250, Loss: 0.0345\n",
      "Epoch 11/25, Batch 1671/2250, Loss: 0.1111\n",
      "Epoch 11/25, Batch 1681/2250, Loss: 0.0342\n",
      "Epoch 11/25, Batch 1691/2250, Loss: 0.0485\n",
      "Epoch 11/25, Batch 1701/2250, Loss: 0.0830\n",
      "Epoch 11/25, Batch 1711/2250, Loss: 0.1547\n",
      "Epoch 11/25, Batch 1721/2250, Loss: 0.0486\n",
      "Epoch 11/25, Batch 1731/2250, Loss: 0.0926\n",
      "Epoch 11/25, Batch 1741/2250, Loss: 0.1337\n",
      "Epoch 11/25, Batch 1751/2250, Loss: 0.0489\n",
      "Epoch 11/25, Batch 1761/2250, Loss: 0.1187\n",
      "Epoch 11/25, Batch 1771/2250, Loss: 0.0790\n",
      "Epoch 11/25, Batch 1781/2250, Loss: 0.0101\n",
      "Epoch 11/25, Batch 1791/2250, Loss: 0.0244\n",
      "Epoch 11/25, Batch 1801/2250, Loss: 0.1239\n",
      "Epoch 11/25, Batch 1811/2250, Loss: 0.0267\n",
      "Epoch 11/25, Batch 1821/2250, Loss: 0.0408\n",
      "Epoch 11/25, Batch 1831/2250, Loss: 0.0302\n",
      "Epoch 11/25, Batch 1841/2250, Loss: 0.0225\n",
      "Epoch 11/25, Batch 1851/2250, Loss: 0.0355\n",
      "Epoch 11/25, Batch 1861/2250, Loss: 0.0260\n",
      "Epoch 11/25, Batch 1871/2250, Loss: 0.0719\n",
      "Epoch 11/25, Batch 1881/2250, Loss: 0.3043\n",
      "Epoch 11/25, Batch 1891/2250, Loss: 0.0038\n",
      "Epoch 11/25, Batch 1901/2250, Loss: 0.1473\n",
      "Epoch 11/25, Batch 1911/2250, Loss: 0.0802\n",
      "Epoch 11/25, Batch 1921/2250, Loss: 0.0479\n",
      "Epoch 11/25, Batch 1931/2250, Loss: 0.0189\n",
      "Epoch 11/25, Batch 1941/2250, Loss: 0.0488\n",
      "Epoch 11/25, Batch 1951/2250, Loss: 0.1463\n",
      "Epoch 11/25, Batch 1961/2250, Loss: 0.0149\n",
      "Epoch 11/25, Batch 1971/2250, Loss: 0.0072\n",
      "Epoch 11/25, Batch 1981/2250, Loss: 0.0671\n",
      "Epoch 11/25, Batch 1991/2250, Loss: 0.0809\n",
      "Epoch 11/25, Batch 2001/2250, Loss: 0.0166\n",
      "Epoch 11/25, Batch 2011/2250, Loss: 0.0854\n",
      "Epoch 11/25, Batch 2021/2250, Loss: 0.0280\n",
      "Epoch 11/25, Batch 2031/2250, Loss: 0.0807\n",
      "Epoch 11/25, Batch 2041/2250, Loss: 0.0485\n",
      "Epoch 11/25, Batch 2051/2250, Loss: 0.0399\n",
      "Epoch 11/25, Batch 2061/2250, Loss: 0.0455\n",
      "Epoch 11/25, Batch 2071/2250, Loss: 0.0586\n",
      "Epoch 11/25, Batch 2081/2250, Loss: 0.0108\n",
      "Epoch 11/25, Batch 2091/2250, Loss: 0.0544\n",
      "Epoch 11/25, Batch 2101/2250, Loss: 0.0354\n",
      "Epoch 11/25, Batch 2111/2250, Loss: 0.1084\n",
      "Epoch 11/25, Batch 2121/2250, Loss: 0.0884\n",
      "Epoch 11/25, Batch 2131/2250, Loss: 0.1015\n",
      "Epoch 11/25, Batch 2141/2250, Loss: 0.0785\n",
      "Epoch 11/25, Batch 2151/2250, Loss: 0.0313\n",
      "Epoch 11/25, Batch 2161/2250, Loss: 0.0400\n",
      "Epoch 11/25, Batch 2171/2250, Loss: 0.0900\n",
      "Epoch 11/25, Batch 2181/2250, Loss: 0.0410\n",
      "Epoch 11/25, Batch 2191/2250, Loss: 0.0409\n",
      "Epoch 11/25, Batch 2201/2250, Loss: 0.0424\n",
      "Epoch 11/25, Batch 2211/2250, Loss: 0.0095\n",
      "Epoch 11/25, Batch 2221/2250, Loss: 0.1174\n",
      "Epoch 11/25, Batch 2231/2250, Loss: 0.0791\n",
      "Epoch 11/25, Batch 2241/2250, Loss: 0.0155\n",
      "Epoch 11/25:\n",
      "Train Loss: 0.0682, Train Acc: 97.47%\n",
      "Val Loss: 0.0578, Val Acc: 97.88%\n",
      "------------------------------------------------------------\n",
      "Starting epoch 12/25...\n",
      "Starting training loop with 2250 batches...\n",
      "Successfully loaded first batch: images shape torch.Size([32, 3, 224, 224]), labels shape torch.Size([32])\n",
      "Epoch 12/25, Batch 1/2250, Loss: 0.1421\n",
      "Epoch 12/25, Batch 11/2250, Loss: 0.0177\n",
      "Epoch 12/25, Batch 21/2250, Loss: 0.0487\n",
      "Epoch 12/25, Batch 31/2250, Loss: 0.0177\n",
      "Epoch 12/25, Batch 41/2250, Loss: 0.0183\n",
      "Epoch 12/25, Batch 51/2250, Loss: 0.1068\n",
      "Epoch 12/25, Batch 61/2250, Loss: 0.0085\n",
      "Epoch 12/25, Batch 71/2250, Loss: 0.0785\n",
      "Epoch 12/25, Batch 81/2250, Loss: 0.0213\n",
      "Epoch 12/25, Batch 91/2250, Loss: 0.0283\n",
      "Epoch 12/25, Batch 101/2250, Loss: 0.0160\n",
      "Epoch 12/25, Batch 111/2250, Loss: 0.0056\n",
      "Epoch 12/25, Batch 121/2250, Loss: 0.0503\n",
      "Epoch 12/25, Batch 131/2250, Loss: 0.0032\n",
      "Epoch 12/25, Batch 141/2250, Loss: 0.0057\n",
      "Epoch 12/25, Batch 151/2250, Loss: 0.0521\n",
      "Epoch 12/25, Batch 161/2250, Loss: 0.0052\n",
      "Epoch 12/25, Batch 171/2250, Loss: 0.0395\n",
      "Epoch 12/25, Batch 181/2250, Loss: 0.0997\n",
      "Epoch 12/25, Batch 191/2250, Loss: 0.0330\n",
      "Epoch 12/25, Batch 201/2250, Loss: 0.0443\n",
      "Epoch 12/25, Batch 211/2250, Loss: 0.0315\n",
      "Epoch 12/25, Batch 221/2250, Loss: 0.2489\n",
      "Epoch 12/25, Batch 231/2250, Loss: 0.0279\n",
      "Epoch 12/25, Batch 241/2250, Loss: 0.0747\n",
      "Epoch 12/25, Batch 251/2250, Loss: 0.0756\n",
      "Epoch 12/25, Batch 261/2250, Loss: 0.0354\n",
      "Epoch 12/25, Batch 271/2250, Loss: 0.0087\n",
      "Epoch 12/25, Batch 281/2250, Loss: 0.0434\n",
      "Epoch 12/25, Batch 291/2250, Loss: 0.0678\n",
      "Epoch 12/25, Batch 301/2250, Loss: 0.0214\n",
      "Epoch 12/25, Batch 311/2250, Loss: 0.0820\n",
      "Epoch 12/25, Batch 321/2250, Loss: 0.0500\n",
      "Epoch 12/25, Batch 331/2250, Loss: 0.0119\n",
      "Epoch 12/25, Batch 341/2250, Loss: 0.0682\n",
      "Epoch 12/25, Batch 351/2250, Loss: 0.0745\n",
      "Epoch 12/25, Batch 361/2250, Loss: 0.1358\n",
      "Epoch 12/25, Batch 371/2250, Loss: 0.0208\n",
      "Epoch 12/25, Batch 381/2250, Loss: 0.0073\n",
      "Epoch 12/25, Batch 391/2250, Loss: 0.1054\n",
      "Epoch 12/25, Batch 401/2250, Loss: 0.0644\n",
      "Epoch 12/25, Batch 411/2250, Loss: 0.0546\n",
      "Epoch 12/25, Batch 421/2250, Loss: 0.0258\n",
      "Epoch 12/25, Batch 431/2250, Loss: 0.0637\n",
      "Epoch 12/25, Batch 441/2250, Loss: 0.0284\n",
      "Epoch 12/25, Batch 451/2250, Loss: 0.0104\n",
      "Epoch 12/25, Batch 461/2250, Loss: 0.0366\n",
      "Epoch 12/25, Batch 471/2250, Loss: 0.0195\n",
      "Epoch 12/25, Batch 481/2250, Loss: 0.0144\n",
      "Epoch 12/25, Batch 491/2250, Loss: 0.0355\n",
      "Epoch 12/25, Batch 501/2250, Loss: 0.0096\n",
      "Epoch 12/25, Batch 511/2250, Loss: 0.1148\n",
      "Epoch 12/25, Batch 521/2250, Loss: 0.0620\n",
      "Epoch 12/25, Batch 531/2250, Loss: 0.1779\n",
      "Epoch 12/25, Batch 541/2250, Loss: 0.1422\n",
      "Epoch 12/25, Batch 551/2250, Loss: 0.0234\n",
      "Epoch 12/25, Batch 561/2250, Loss: 0.0211\n",
      "Epoch 12/25, Batch 571/2250, Loss: 0.0739\n",
      "Epoch 12/25, Batch 581/2250, Loss: 0.0177\n",
      "Epoch 12/25, Batch 591/2250, Loss: 0.0355\n",
      "Epoch 12/25, Batch 601/2250, Loss: 0.0150\n",
      "Epoch 12/25, Batch 611/2250, Loss: 0.0182\n",
      "Epoch 12/25, Batch 621/2250, Loss: 0.2226\n",
      "Epoch 12/25, Batch 631/2250, Loss: 0.0309\n",
      "Epoch 12/25, Batch 641/2250, Loss: 0.0779\n",
      "Epoch 12/25, Batch 651/2250, Loss: 0.0684\n",
      "Epoch 12/25, Batch 661/2250, Loss: 0.0178\n",
      "Epoch 12/25, Batch 671/2250, Loss: 0.0063\n",
      "Epoch 12/25, Batch 681/2250, Loss: 0.0061\n",
      "Epoch 12/25, Batch 691/2250, Loss: 0.0667\n",
      "Epoch 12/25, Batch 701/2250, Loss: 0.1201\n",
      "Epoch 12/25, Batch 711/2250, Loss: 0.1843\n",
      "Epoch 12/25, Batch 721/2250, Loss: 0.0360\n",
      "Epoch 12/25, Batch 731/2250, Loss: 0.0402\n",
      "Epoch 12/25, Batch 741/2250, Loss: 0.0464\n",
      "Epoch 12/25, Batch 751/2250, Loss: 0.0101\n",
      "Epoch 12/25, Batch 761/2250, Loss: 0.0688\n",
      "Epoch 12/25, Batch 771/2250, Loss: 0.2109\n",
      "Epoch 12/25, Batch 781/2250, Loss: 0.0099\n",
      "Epoch 12/25, Batch 791/2250, Loss: 0.0543\n",
      "Epoch 12/25, Batch 801/2250, Loss: 0.0528\n",
      "Epoch 12/25, Batch 811/2250, Loss: 0.2076\n",
      "Epoch 12/25, Batch 821/2250, Loss: 0.0201\n",
      "Epoch 12/25, Batch 831/2250, Loss: 0.0230\n",
      "Epoch 12/25, Batch 841/2250, Loss: 0.0248\n",
      "Epoch 12/25, Batch 851/2250, Loss: 0.3114\n",
      "Epoch 12/25, Batch 861/2250, Loss: 0.0237\n",
      "Epoch 12/25, Batch 871/2250, Loss: 0.0977\n",
      "Epoch 12/25, Batch 881/2250, Loss: 0.0148\n",
      "Epoch 12/25, Batch 891/2250, Loss: 0.0098\n",
      "Epoch 12/25, Batch 901/2250, Loss: 0.0262\n",
      "Epoch 12/25, Batch 911/2250, Loss: 0.0073\n",
      "Epoch 12/25, Batch 921/2250, Loss: 0.0091\n",
      "Epoch 12/25, Batch 931/2250, Loss: 0.0346\n",
      "Epoch 12/25, Batch 941/2250, Loss: 0.0224\n",
      "Epoch 12/25, Batch 951/2250, Loss: 0.2441\n",
      "Epoch 12/25, Batch 961/2250, Loss: 0.0438\n",
      "Epoch 12/25, Batch 971/2250, Loss: 0.1527\n",
      "Epoch 12/25, Batch 981/2250, Loss: 0.0153\n",
      "Epoch 12/25, Batch 991/2250, Loss: 0.0574\n",
      "Epoch 12/25, Batch 1001/2250, Loss: 0.0922\n",
      "Epoch 12/25, Batch 1011/2250, Loss: 0.1632\n",
      "Epoch 12/25, Batch 1021/2250, Loss: 0.0282\n",
      "Epoch 12/25, Batch 1031/2250, Loss: 0.0542\n",
      "Epoch 12/25, Batch 1041/2250, Loss: 0.0223\n",
      "Epoch 12/25, Batch 1051/2250, Loss: 0.0024\n",
      "Epoch 12/25, Batch 1061/2250, Loss: 0.0020\n",
      "Epoch 12/25, Batch 1071/2250, Loss: 0.0582\n",
      "Epoch 12/25, Batch 1081/2250, Loss: 0.0397\n",
      "Epoch 12/25, Batch 1091/2250, Loss: 0.0034\n",
      "Epoch 12/25, Batch 1101/2250, Loss: 0.0254\n",
      "Epoch 12/25, Batch 1111/2250, Loss: 0.0287\n",
      "Epoch 12/25, Batch 1121/2250, Loss: 0.0336\n",
      "Epoch 12/25, Batch 1131/2250, Loss: 0.0577\n",
      "Epoch 12/25, Batch 1141/2250, Loss: 0.1185\n",
      "Epoch 12/25, Batch 1151/2250, Loss: 0.0245\n",
      "Epoch 12/25, Batch 1161/2250, Loss: 0.0273\n",
      "Epoch 12/25, Batch 1171/2250, Loss: 0.0831\n",
      "Epoch 12/25, Batch 1181/2250, Loss: 0.0334\n",
      "Epoch 12/25, Batch 1191/2250, Loss: 0.3483\n",
      "Epoch 12/25, Batch 1201/2250, Loss: 0.0061\n",
      "Epoch 12/25, Batch 1211/2250, Loss: 0.0448\n",
      "Epoch 12/25, Batch 1221/2250, Loss: 0.0339\n",
      "Epoch 12/25, Batch 1231/2250, Loss: 0.0084\n",
      "Epoch 12/25, Batch 1241/2250, Loss: 0.1435\n",
      "Epoch 12/25, Batch 1251/2250, Loss: 0.2706\n",
      "Epoch 12/25, Batch 1261/2250, Loss: 0.0815\n",
      "Epoch 12/25, Batch 1271/2250, Loss: 0.0781\n",
      "Epoch 12/25, Batch 1281/2250, Loss: 0.0230\n",
      "Epoch 12/25, Batch 1291/2250, Loss: 0.0124\n",
      "Epoch 12/25, Batch 1301/2250, Loss: 0.1430\n",
      "Epoch 12/25, Batch 1311/2250, Loss: 0.0224\n",
      "Epoch 12/25, Batch 1321/2250, Loss: 0.0042\n",
      "Epoch 12/25, Batch 1331/2250, Loss: 0.0042\n",
      "Epoch 12/25, Batch 1341/2250, Loss: 0.0332\n",
      "Epoch 12/25, Batch 1351/2250, Loss: 0.1113\n",
      "Epoch 12/25, Batch 1361/2250, Loss: 0.0129\n",
      "Epoch 12/25, Batch 1371/2250, Loss: 0.0935\n",
      "Epoch 12/25, Batch 1381/2250, Loss: 0.0187\n",
      "Epoch 12/25, Batch 1391/2250, Loss: 0.0142\n",
      "Epoch 12/25, Batch 1401/2250, Loss: 0.1216\n",
      "Epoch 12/25, Batch 1411/2250, Loss: 0.0321\n",
      "Epoch 12/25, Batch 1421/2250, Loss: 0.0944\n",
      "Epoch 12/25, Batch 1431/2250, Loss: 0.0280\n",
      "Epoch 12/25, Batch 1441/2250, Loss: 0.0724\n",
      "Epoch 12/25, Batch 1451/2250, Loss: 0.0324\n",
      "Epoch 12/25, Batch 1461/2250, Loss: 0.1689\n",
      "Epoch 12/25, Batch 1471/2250, Loss: 0.0185\n",
      "Epoch 12/25, Batch 1481/2250, Loss: 0.0968\n",
      "Epoch 12/25, Batch 1491/2250, Loss: 0.0482\n",
      "Epoch 12/25, Batch 1501/2250, Loss: 0.0887\n",
      "Epoch 12/25, Batch 1511/2250, Loss: 0.0265\n",
      "Epoch 12/25, Batch 1521/2250, Loss: 0.0151\n",
      "Epoch 12/25, Batch 1531/2250, Loss: 0.0857\n",
      "Epoch 12/25, Batch 1541/2250, Loss: 0.0099\n",
      "Epoch 12/25, Batch 1551/2250, Loss: 0.0176\n",
      "Epoch 12/25, Batch 1561/2250, Loss: 0.0239\n",
      "Epoch 12/25, Batch 1571/2250, Loss: 0.0149\n",
      "Epoch 12/25, Batch 1581/2250, Loss: 0.0172\n",
      "Epoch 12/25, Batch 1591/2250, Loss: 0.0589\n",
      "Epoch 12/25, Batch 1601/2250, Loss: 0.0736\n",
      "Epoch 12/25, Batch 1611/2250, Loss: 0.0097\n",
      "Epoch 12/25, Batch 1621/2250, Loss: 0.0253\n",
      "Epoch 12/25, Batch 1631/2250, Loss: 0.0753\n",
      "Epoch 12/25, Batch 1641/2250, Loss: 0.0705\n",
      "Epoch 12/25, Batch 1651/2250, Loss: 0.1278\n",
      "Epoch 12/25, Batch 1661/2250, Loss: 0.1367\n",
      "Epoch 12/25, Batch 1671/2250, Loss: 0.0285\n",
      "Epoch 12/25, Batch 1681/2250, Loss: 0.0799\n",
      "Epoch 12/25, Batch 1691/2250, Loss: 0.0841\n",
      "Epoch 12/25, Batch 1701/2250, Loss: 0.0087\n",
      "Epoch 12/25, Batch 1711/2250, Loss: 0.0099\n",
      "Epoch 12/25, Batch 1721/2250, Loss: 0.0684\n",
      "Epoch 12/25, Batch 1731/2250, Loss: 0.2581\n",
      "Epoch 12/25, Batch 1741/2250, Loss: 0.0167\n",
      "Epoch 12/25, Batch 1751/2250, Loss: 0.0180\n",
      "Epoch 12/25, Batch 1761/2250, Loss: 0.0050\n",
      "Epoch 12/25, Batch 1771/2250, Loss: 0.0157\n",
      "Epoch 12/25, Batch 1781/2250, Loss: 0.0153\n",
      "Epoch 12/25, Batch 1791/2250, Loss: 0.2544\n",
      "Epoch 12/25, Batch 1801/2250, Loss: 0.0642\n",
      "Epoch 12/25, Batch 1811/2250, Loss: 0.1150\n",
      "Epoch 12/25, Batch 1821/2250, Loss: 0.2945\n",
      "Epoch 12/25, Batch 1831/2250, Loss: 0.0461\n",
      "Epoch 12/25, Batch 1841/2250, Loss: 0.2451\n",
      "Epoch 12/25, Batch 1851/2250, Loss: 0.0085\n",
      "Epoch 12/25, Batch 1861/2250, Loss: 0.0083\n",
      "Epoch 12/25, Batch 1871/2250, Loss: 0.2163\n",
      "Epoch 12/25, Batch 1881/2250, Loss: 0.0181\n",
      "Epoch 12/25, Batch 1891/2250, Loss: 0.1451\n",
      "Epoch 12/25, Batch 1901/2250, Loss: 0.0926\n",
      "Epoch 12/25, Batch 1911/2250, Loss: 0.0984\n",
      "Epoch 12/25, Batch 1921/2250, Loss: 0.0488\n",
      "Epoch 12/25, Batch 1931/2250, Loss: 0.0352\n",
      "Epoch 12/25, Batch 1941/2250, Loss: 0.0822\n",
      "Epoch 12/25, Batch 1951/2250, Loss: 0.0475\n",
      "Epoch 12/25, Batch 1961/2250, Loss: 0.0678\n",
      "Epoch 12/25, Batch 1971/2250, Loss: 0.0136\n",
      "Epoch 12/25, Batch 1981/2250, Loss: 0.0627\n",
      "Epoch 12/25, Batch 1991/2250, Loss: 0.0041\n",
      "Epoch 12/25, Batch 2001/2250, Loss: 0.0603\n",
      "Epoch 12/25, Batch 2011/2250, Loss: 0.1399\n",
      "Epoch 12/25, Batch 2021/2250, Loss: 0.0365\n",
      "Epoch 12/25, Batch 2031/2250, Loss: 0.2454\n",
      "Epoch 12/25, Batch 2041/2250, Loss: 0.2123\n",
      "Epoch 12/25, Batch 2051/2250, Loss: 0.0203\n",
      "Epoch 12/25, Batch 2061/2250, Loss: 0.1089\n",
      "Epoch 12/25, Batch 2071/2250, Loss: 0.0150\n",
      "Epoch 12/25, Batch 2081/2250, Loss: 0.0453\n",
      "Epoch 12/25, Batch 2091/2250, Loss: 0.0815\n",
      "Epoch 12/25, Batch 2101/2250, Loss: 0.2529\n",
      "Epoch 12/25, Batch 2111/2250, Loss: 0.0838\n",
      "Epoch 12/25, Batch 2121/2250, Loss: 0.0426\n",
      "Epoch 12/25, Batch 2131/2250, Loss: 0.0569\n",
      "Epoch 12/25, Batch 2141/2250, Loss: 0.0234\n",
      "Epoch 12/25, Batch 2151/2250, Loss: 0.1728\n",
      "Epoch 12/25, Batch 2161/2250, Loss: 0.0198\n",
      "Epoch 12/25, Batch 2171/2250, Loss: 0.0458\n",
      "Epoch 12/25, Batch 2181/2250, Loss: 0.0302\n",
      "Epoch 12/25, Batch 2191/2250, Loss: 0.0319\n",
      "Epoch 12/25, Batch 2201/2250, Loss: 0.1280\n",
      "Epoch 12/25, Batch 2211/2250, Loss: 0.0043\n",
      "Epoch 12/25, Batch 2221/2250, Loss: 0.0118\n",
      "Epoch 12/25, Batch 2231/2250, Loss: 0.0320\n",
      "Epoch 12/25, Batch 2241/2250, Loss: 0.0169\n",
      "Epoch 12/25:\n",
      "Train Loss: 0.0617, Train Acc: 97.74%\n",
      "Val Loss: 0.0507, Val Acc: 98.12%\n",
      "------------------------------------------------------------\n",
      "Starting epoch 13/25...\n",
      "Starting training loop with 2250 batches...\n",
      "Successfully loaded first batch: images shape torch.Size([32, 3, 224, 224]), labels shape torch.Size([32])\n",
      "Epoch 13/25, Batch 1/2250, Loss: 0.0462\n",
      "Epoch 13/25, Batch 11/2250, Loss: 0.0124\n",
      "Epoch 13/25, Batch 21/2250, Loss: 0.0123\n",
      "Epoch 13/25, Batch 31/2250, Loss: 0.0792\n",
      "Epoch 13/25, Batch 41/2250, Loss: 0.0698\n",
      "Epoch 13/25, Batch 51/2250, Loss: 0.0106\n",
      "Epoch 13/25, Batch 61/2250, Loss: 0.1299\n",
      "Epoch 13/25, Batch 71/2250, Loss: 0.1453\n",
      "Epoch 13/25, Batch 81/2250, Loss: 0.1276\n",
      "Epoch 13/25, Batch 91/2250, Loss: 0.0262\n",
      "Epoch 13/25, Batch 101/2250, Loss: 0.0244\n",
      "Epoch 13/25, Batch 111/2250, Loss: 0.0095\n",
      "Epoch 13/25, Batch 121/2250, Loss: 0.0154\n",
      "Epoch 13/25, Batch 131/2250, Loss: 0.0250\n",
      "Epoch 13/25, Batch 141/2250, Loss: 0.0018\n",
      "Epoch 13/25, Batch 151/2250, Loss: 0.1516\n",
      "Epoch 13/25, Batch 161/2250, Loss: 0.0430\n",
      "Epoch 13/25, Batch 171/2250, Loss: 0.2360\n",
      "Epoch 13/25, Batch 181/2250, Loss: 0.0578\n",
      "Epoch 13/25, Batch 191/2250, Loss: 0.0182\n",
      "Epoch 13/25, Batch 201/2250, Loss: 0.0152\n",
      "Epoch 13/25, Batch 211/2250, Loss: 0.0234\n",
      "Epoch 13/25, Batch 221/2250, Loss: 0.0679\n",
      "Epoch 13/25, Batch 231/2250, Loss: 0.1533\n",
      "Epoch 13/25, Batch 241/2250, Loss: 0.1161\n",
      "Epoch 13/25, Batch 251/2250, Loss: 0.0154\n",
      "Epoch 13/25, Batch 261/2250, Loss: 0.0021\n",
      "Epoch 13/25, Batch 271/2250, Loss: 0.0902\n",
      "Epoch 13/25, Batch 281/2250, Loss: 0.0850\n",
      "Epoch 13/25, Batch 291/2250, Loss: 0.0605\n",
      "Epoch 13/25, Batch 301/2250, Loss: 0.1017\n",
      "Epoch 13/25, Batch 311/2250, Loss: 0.2385\n",
      "Epoch 13/25, Batch 321/2250, Loss: 0.3551\n",
      "Epoch 13/25, Batch 331/2250, Loss: 0.0556\n",
      "Epoch 13/25, Batch 341/2250, Loss: 0.1081\n",
      "Epoch 13/25, Batch 351/2250, Loss: 0.0233\n",
      "Epoch 13/25, Batch 361/2250, Loss: 0.0413\n",
      "Epoch 13/25, Batch 371/2250, Loss: 0.0609\n",
      "Epoch 13/25, Batch 381/2250, Loss: 0.0389\n",
      "Epoch 13/25, Batch 391/2250, Loss: 0.0226\n",
      "Epoch 13/25, Batch 401/2250, Loss: 0.0383\n",
      "Epoch 13/25, Batch 411/2250, Loss: 0.0038\n",
      "Epoch 13/25, Batch 421/2250, Loss: 0.1076\n",
      "Epoch 13/25, Batch 431/2250, Loss: 0.0060\n",
      "Epoch 13/25, Batch 441/2250, Loss: 0.0627\n",
      "Epoch 13/25, Batch 451/2250, Loss: 0.2419\n",
      "Epoch 13/25, Batch 461/2250, Loss: 0.0292\n",
      "Epoch 13/25, Batch 471/2250, Loss: 0.0170\n",
      "Epoch 13/25, Batch 481/2250, Loss: 0.0138\n",
      "Epoch 13/25, Batch 491/2250, Loss: 0.0709\n",
      "Epoch 13/25, Batch 501/2250, Loss: 0.1287\n",
      "Epoch 13/25, Batch 511/2250, Loss: 0.0578\n",
      "Epoch 13/25, Batch 521/2250, Loss: 0.0091\n",
      "Epoch 13/25, Batch 531/2250, Loss: 0.0121\n",
      "Epoch 13/25, Batch 541/2250, Loss: 0.1180\n",
      "Epoch 13/25, Batch 551/2250, Loss: 0.0029\n",
      "Epoch 13/25, Batch 561/2250, Loss: 0.0650\n",
      "Epoch 13/25, Batch 571/2250, Loss: 0.0060\n",
      "Epoch 13/25, Batch 581/2250, Loss: 0.0835\n",
      "Epoch 13/25, Batch 591/2250, Loss: 0.1239\n",
      "Epoch 13/25, Batch 601/2250, Loss: 0.0882\n",
      "Epoch 13/25, Batch 611/2250, Loss: 0.0757\n",
      "Epoch 13/25, Batch 621/2250, Loss: 0.1220\n",
      "Epoch 13/25, Batch 631/2250, Loss: 0.0312\n",
      "Epoch 13/25, Batch 641/2250, Loss: 0.0722\n",
      "Epoch 13/25, Batch 651/2250, Loss: 0.0482\n",
      "Epoch 13/25, Batch 661/2250, Loss: 0.0143\n",
      "Epoch 13/25, Batch 671/2250, Loss: 0.2072\n",
      "Epoch 13/25, Batch 681/2250, Loss: 0.0085\n",
      "Epoch 13/25, Batch 691/2250, Loss: 0.0023\n",
      "Epoch 13/25, Batch 701/2250, Loss: 0.0055\n",
      "Epoch 13/25, Batch 711/2250, Loss: 0.1594\n",
      "Epoch 13/25, Batch 721/2250, Loss: 0.1576\n",
      "Epoch 13/25, Batch 731/2250, Loss: 0.0189\n",
      "Epoch 13/25, Batch 741/2250, Loss: 0.0214\n",
      "Epoch 13/25, Batch 751/2250, Loss: 0.1248\n",
      "Epoch 13/25, Batch 761/2250, Loss: 0.0233\n",
      "Epoch 13/25, Batch 771/2250, Loss: 0.0058\n",
      "Epoch 13/25, Batch 781/2250, Loss: 0.0491\n",
      "Epoch 13/25, Batch 791/2250, Loss: 0.0484\n",
      "Epoch 13/25, Batch 801/2250, Loss: 0.0959\n",
      "Epoch 13/25, Batch 811/2250, Loss: 0.1696\n",
      "Epoch 13/25, Batch 821/2250, Loss: 0.0045\n",
      "Epoch 13/25, Batch 831/2250, Loss: 0.0441\n",
      "Epoch 13/25, Batch 841/2250, Loss: 0.0659\n",
      "Epoch 13/25, Batch 851/2250, Loss: 0.1230\n",
      "Epoch 13/25, Batch 861/2250, Loss: 0.1288\n",
      "Epoch 13/25, Batch 871/2250, Loss: 0.1429\n",
      "Epoch 13/25, Batch 881/2250, Loss: 0.1954\n",
      "Epoch 13/25, Batch 891/2250, Loss: 0.0250\n",
      "Epoch 13/25, Batch 901/2250, Loss: 0.0457\n",
      "Epoch 13/25, Batch 911/2250, Loss: 0.0429\n",
      "Epoch 13/25, Batch 921/2250, Loss: 0.0081\n",
      "Epoch 13/25, Batch 931/2250, Loss: 0.0332\n",
      "Epoch 13/25, Batch 941/2250, Loss: 0.0188\n",
      "Epoch 13/25, Batch 951/2250, Loss: 0.0334\n",
      "Epoch 13/25, Batch 961/2250, Loss: 0.0071\n",
      "Epoch 13/25, Batch 971/2250, Loss: 0.0137\n",
      "Epoch 13/25, Batch 981/2250, Loss: 0.0458\n",
      "Epoch 13/25, Batch 991/2250, Loss: 0.0431\n",
      "Epoch 13/25, Batch 1001/2250, Loss: 0.0144\n",
      "Epoch 13/25, Batch 1011/2250, Loss: 0.0128\n",
      "Epoch 13/25, Batch 1021/2250, Loss: 0.1325\n",
      "Epoch 13/25, Batch 1031/2250, Loss: 0.1416\n",
      "Epoch 13/25, Batch 1041/2250, Loss: 0.0093\n",
      "Epoch 13/25, Batch 1051/2250, Loss: 0.0029\n",
      "Epoch 13/25, Batch 1061/2250, Loss: 0.0023\n",
      "Epoch 13/25, Batch 1071/2250, Loss: 0.0974\n",
      "Epoch 13/25, Batch 1081/2250, Loss: 0.0088\n",
      "Epoch 13/25, Batch 1091/2250, Loss: 0.0928\n",
      "Epoch 13/25, Batch 1101/2250, Loss: 0.0026\n",
      "Epoch 13/25, Batch 1111/2250, Loss: 0.0590\n",
      "Epoch 13/25, Batch 1121/2250, Loss: 0.0078\n",
      "Epoch 13/25, Batch 1131/2250, Loss: 0.0339\n",
      "Epoch 13/25, Batch 1141/2250, Loss: 0.0175\n",
      "Epoch 13/25, Batch 1151/2250, Loss: 0.1968\n",
      "Epoch 13/25, Batch 1161/2250, Loss: 0.0611\n",
      "Epoch 13/25, Batch 1171/2250, Loss: 0.0329\n",
      "Epoch 13/25, Batch 1181/2250, Loss: 0.0169\n",
      "Epoch 13/25, Batch 1191/2250, Loss: 0.0672\n",
      "Epoch 13/25, Batch 1201/2250, Loss: 0.0615\n",
      "Epoch 13/25, Batch 1211/2250, Loss: 0.0420\n",
      "Epoch 13/25, Batch 1221/2250, Loss: 0.0263\n",
      "Epoch 13/25, Batch 1231/2250, Loss: 0.0017\n",
      "Epoch 13/25, Batch 1241/2250, Loss: 0.0793\n",
      "Epoch 13/25, Batch 1251/2250, Loss: 0.0173\n",
      "Epoch 13/25, Batch 1261/2250, Loss: 0.1361\n",
      "Epoch 13/25, Batch 1271/2250, Loss: 0.1715\n",
      "Epoch 13/25, Batch 1281/2250, Loss: 0.0575\n",
      "Epoch 13/25, Batch 1291/2250, Loss: 0.0049\n",
      "Epoch 13/25, Batch 1301/2250, Loss: 0.1358\n",
      "Epoch 13/25, Batch 1311/2250, Loss: 0.0210\n",
      "Epoch 13/25, Batch 1321/2250, Loss: 0.0797\n",
      "Epoch 13/25, Batch 1331/2250, Loss: 0.0301\n",
      "Epoch 13/25, Batch 1341/2250, Loss: 0.2516\n",
      "Epoch 13/25, Batch 1351/2250, Loss: 0.0687\n",
      "Epoch 13/25, Batch 1361/2250, Loss: 0.0055\n",
      "Epoch 13/25, Batch 1371/2250, Loss: 0.1971\n",
      "Epoch 13/25, Batch 1381/2250, Loss: 0.0066\n",
      "Epoch 13/25, Batch 1391/2250, Loss: 0.0363\n",
      "Epoch 13/25, Batch 1401/2250, Loss: 0.0396\n",
      "Epoch 13/25, Batch 1411/2250, Loss: 0.0142\n",
      "Epoch 13/25, Batch 1421/2250, Loss: 0.0077\n",
      "Epoch 13/25, Batch 1431/2250, Loss: 0.0340\n",
      "Epoch 13/25, Batch 1441/2250, Loss: 0.1268\n",
      "Epoch 13/25, Batch 1451/2250, Loss: 0.1652\n",
      "Epoch 13/25, Batch 1461/2250, Loss: 0.1130\n",
      "Epoch 13/25, Batch 1471/2250, Loss: 0.0573\n",
      "Epoch 13/25, Batch 1481/2250, Loss: 0.0128\n",
      "Epoch 13/25, Batch 1491/2250, Loss: 0.0407\n",
      "Epoch 13/25, Batch 1501/2250, Loss: 0.0313\n",
      "Epoch 13/25, Batch 1511/2250, Loss: 0.2236\n",
      "Epoch 13/25, Batch 1521/2250, Loss: 0.0074\n",
      "Epoch 13/25, Batch 1531/2250, Loss: 0.0511\n",
      "Epoch 13/25, Batch 1541/2250, Loss: 0.0616\n",
      "Epoch 13/25, Batch 1551/2250, Loss: 0.0200\n",
      "Epoch 13/25, Batch 1561/2250, Loss: 0.0173\n",
      "Epoch 13/25, Batch 1571/2250, Loss: 0.0389\n",
      "Epoch 13/25, Batch 1581/2250, Loss: 0.0034\n",
      "Epoch 13/25, Batch 1591/2250, Loss: 0.0295\n",
      "Epoch 13/25, Batch 1601/2250, Loss: 0.0063\n",
      "Epoch 13/25, Batch 1611/2250, Loss: 0.1785\n",
      "Epoch 13/25, Batch 1621/2250, Loss: 0.0299\n",
      "Epoch 13/25, Batch 1631/2250, Loss: 0.0508\n",
      "Epoch 13/25, Batch 1641/2250, Loss: 0.0382\n",
      "Epoch 13/25, Batch 1651/2250, Loss: 0.0395\n",
      "Epoch 13/25, Batch 1661/2250, Loss: 0.1936\n",
      "Epoch 13/25, Batch 1671/2250, Loss: 0.0737\n",
      "Epoch 13/25, Batch 1681/2250, Loss: 0.0067\n",
      "Epoch 13/25, Batch 1691/2250, Loss: 0.0193\n",
      "Epoch 13/25, Batch 1701/2250, Loss: 0.1235\n",
      "Epoch 13/25, Batch 1711/2250, Loss: 0.1127\n",
      "Epoch 13/25, Batch 1721/2250, Loss: 0.0500\n",
      "Epoch 13/25, Batch 1731/2250, Loss: 0.0157\n",
      "Epoch 13/25, Batch 1741/2250, Loss: 0.1030\n",
      "Epoch 13/25, Batch 1751/2250, Loss: 0.1630\n",
      "Epoch 13/25, Batch 1761/2250, Loss: 0.0085\n",
      "Epoch 13/25, Batch 1771/2250, Loss: 0.0789\n",
      "Epoch 13/25, Batch 1781/2250, Loss: 0.0336\n",
      "Epoch 13/25, Batch 1791/2250, Loss: 0.2904\n",
      "Epoch 13/25, Batch 1801/2250, Loss: 0.0119\n",
      "Epoch 13/25, Batch 1811/2250, Loss: 0.0222\n",
      "Epoch 13/25, Batch 1821/2250, Loss: 0.0325\n",
      "Epoch 13/25, Batch 1831/2250, Loss: 0.1088\n",
      "Epoch 13/25, Batch 1841/2250, Loss: 0.0462\n",
      "Epoch 13/25, Batch 1851/2250, Loss: 0.0072\n",
      "Epoch 13/25, Batch 1861/2250, Loss: 0.1836\n",
      "Epoch 13/25, Batch 1871/2250, Loss: 0.0187\n",
      "Epoch 13/25, Batch 1881/2250, Loss: 0.0684\n",
      "Epoch 13/25, Batch 1891/2250, Loss: 0.1632\n",
      "Epoch 13/25, Batch 1901/2250, Loss: 0.0179\n",
      "Epoch 13/25, Batch 1911/2250, Loss: 0.0642\n",
      "Epoch 13/25, Batch 1921/2250, Loss: 0.0083\n",
      "Epoch 13/25, Batch 1931/2250, Loss: 0.0088\n",
      "Epoch 13/25, Batch 1941/2250, Loss: 0.1213\n",
      "Epoch 13/25, Batch 1951/2250, Loss: 0.0117\n",
      "Epoch 13/25, Batch 1961/2250, Loss: 0.0775\n",
      "Epoch 13/25, Batch 1971/2250, Loss: 0.2141\n",
      "Epoch 13/25, Batch 1981/2250, Loss: 0.0316\n",
      "Epoch 13/25, Batch 1991/2250, Loss: 0.0337\n",
      "Epoch 13/25, Batch 2001/2250, Loss: 0.0170\n",
      "Epoch 13/25, Batch 2011/2250, Loss: 0.0121\n",
      "Epoch 13/25, Batch 2021/2250, Loss: 0.0025\n",
      "Epoch 13/25, Batch 2031/2250, Loss: 0.0196\n",
      "Epoch 13/25, Batch 2041/2250, Loss: 0.0306\n",
      "Epoch 13/25, Batch 2051/2250, Loss: 0.0392\n",
      "Epoch 13/25, Batch 2061/2250, Loss: 0.0213\n",
      "Epoch 13/25, Batch 2071/2250, Loss: 0.0024\n",
      "Epoch 13/25, Batch 2081/2250, Loss: 0.0062\n",
      "Epoch 13/25, Batch 2091/2250, Loss: 0.0299\n",
      "Epoch 13/25, Batch 2101/2250, Loss: 0.0117\n",
      "Epoch 13/25, Batch 2111/2250, Loss: 0.1992\n",
      "Epoch 13/25, Batch 2121/2250, Loss: 0.0695\n",
      "Epoch 13/25, Batch 2131/2250, Loss: 0.0332\n",
      "Epoch 13/25, Batch 2141/2250, Loss: 0.0370\n",
      "Epoch 13/25, Batch 2151/2250, Loss: 0.1501\n",
      "Epoch 13/25, Batch 2161/2250, Loss: 0.1685\n",
      "Epoch 13/25, Batch 2171/2250, Loss: 0.1292\n",
      "Epoch 13/25, Batch 2181/2250, Loss: 0.0947\n",
      "Epoch 13/25, Batch 2191/2250, Loss: 0.0256\n",
      "Epoch 13/25, Batch 2201/2250, Loss: 0.0177\n",
      "Epoch 13/25, Batch 2211/2250, Loss: 0.0251\n",
      "Epoch 13/25, Batch 2221/2250, Loss: 0.1350\n",
      "Epoch 13/25, Batch 2231/2250, Loss: 0.0367\n",
      "Epoch 13/25, Batch 2241/2250, Loss: 0.0531\n",
      "Epoch 13/25:\n",
      "Train Loss: 0.0586, Train Acc: 97.85%\n",
      "Val Loss: 0.0496, Val Acc: 98.28%\n",
      "------------------------------------------------------------\n",
      "Starting epoch 14/25...\n",
      "Starting training loop with 2250 batches...\n",
      "Successfully loaded first batch: images shape torch.Size([32, 3, 224, 224]), labels shape torch.Size([32])\n",
      "Epoch 14/25, Batch 1/2250, Loss: 0.0660\n",
      "Epoch 14/25, Batch 11/2250, Loss: 0.0123\n",
      "Epoch 14/25, Batch 21/2250, Loss: 0.0749\n",
      "Epoch 14/25, Batch 31/2250, Loss: 0.1406\n",
      "Epoch 14/25, Batch 41/2250, Loss: 0.0195\n",
      "Epoch 14/25, Batch 51/2250, Loss: 0.0042\n",
      "Epoch 14/25, Batch 61/2250, Loss: 0.0137\n",
      "Epoch 14/25, Batch 71/2250, Loss: 0.0233\n",
      "Epoch 14/25, Batch 81/2250, Loss: 0.0133\n",
      "Epoch 14/25, Batch 91/2250, Loss: 0.0615\n",
      "Epoch 14/25, Batch 101/2250, Loss: 0.0154\n",
      "Epoch 14/25, Batch 111/2250, Loss: 0.0083\n",
      "Epoch 14/25, Batch 121/2250, Loss: 0.0018\n",
      "Epoch 14/25, Batch 131/2250, Loss: 0.0227\n",
      "Epoch 14/25, Batch 141/2250, Loss: 0.1199\n",
      "Epoch 14/25, Batch 151/2250, Loss: 0.0261\n",
      "Epoch 14/25, Batch 161/2250, Loss: 0.0914\n",
      "Epoch 14/25, Batch 171/2250, Loss: 0.0886\n",
      "Epoch 14/25, Batch 181/2250, Loss: 0.0805\n",
      "Epoch 14/25, Batch 191/2250, Loss: 0.0350\n",
      "Epoch 14/25, Batch 201/2250, Loss: 0.0676\n",
      "Epoch 14/25, Batch 211/2250, Loss: 0.0205\n",
      "Epoch 14/25, Batch 221/2250, Loss: 0.0019\n",
      "Epoch 14/25, Batch 231/2250, Loss: 0.1405\n",
      "Epoch 14/25, Batch 241/2250, Loss: 0.0313\n",
      "Epoch 14/25, Batch 251/2250, Loss: 0.0870\n",
      "Epoch 14/25, Batch 261/2250, Loss: 0.0058\n",
      "Epoch 14/25, Batch 271/2250, Loss: 0.0475\n",
      "Epoch 14/25, Batch 281/2250, Loss: 0.1857\n",
      "Epoch 14/25, Batch 291/2250, Loss: 0.1218\n",
      "Epoch 14/25, Batch 301/2250, Loss: 0.0499\n",
      "Epoch 14/25, Batch 311/2250, Loss: 0.0043\n",
      "Epoch 14/25, Batch 321/2250, Loss: 0.0119\n",
      "Epoch 14/25, Batch 331/2250, Loss: 0.1097\n",
      "Epoch 14/25, Batch 341/2250, Loss: 0.0779\n",
      "Epoch 14/25, Batch 351/2250, Loss: 0.2125\n",
      "Epoch 14/25, Batch 361/2250, Loss: 0.0237\n",
      "Epoch 14/25, Batch 371/2250, Loss: 0.0446\n",
      "Epoch 14/25, Batch 381/2250, Loss: 0.0290\n",
      "Epoch 14/25, Batch 391/2250, Loss: 0.0110\n",
      "Epoch 14/25, Batch 401/2250, Loss: 0.0553\n",
      "Epoch 14/25, Batch 411/2250, Loss: 0.1318\n",
      "Epoch 14/25, Batch 421/2250, Loss: 0.0325\n",
      "Epoch 14/25, Batch 431/2250, Loss: 0.0036\n",
      "Epoch 14/25, Batch 441/2250, Loss: 0.1357\n",
      "Epoch 14/25, Batch 451/2250, Loss: 0.0187\n",
      "Epoch 14/25, Batch 461/2250, Loss: 0.1283\n",
      "Epoch 14/25, Batch 471/2250, Loss: 0.1892\n",
      "Epoch 14/25, Batch 481/2250, Loss: 0.1136\n",
      "Epoch 14/25, Batch 491/2250, Loss: 0.0394\n",
      "Epoch 14/25, Batch 501/2250, Loss: 0.0676\n",
      "Epoch 14/25, Batch 511/2250, Loss: 0.0835\n",
      "Epoch 14/25, Batch 521/2250, Loss: 0.0114\n",
      "Epoch 14/25, Batch 531/2250, Loss: 0.0448\n",
      "Epoch 14/25, Batch 541/2250, Loss: 0.0161\n",
      "Epoch 14/25, Batch 551/2250, Loss: 0.0340\n",
      "Epoch 14/25, Batch 561/2250, Loss: 0.0863\n",
      "Epoch 14/25, Batch 571/2250, Loss: 0.1522\n",
      "Epoch 14/25, Batch 581/2250, Loss: 0.0404\n",
      "Epoch 14/25, Batch 591/2250, Loss: 0.1148\n",
      "Epoch 14/25, Batch 601/2250, Loss: 0.1754\n",
      "Epoch 14/25, Batch 611/2250, Loss: 0.0098\n",
      "Epoch 14/25, Batch 621/2250, Loss: 0.1071\n",
      "Epoch 14/25, Batch 631/2250, Loss: 0.2082\n",
      "Epoch 14/25, Batch 641/2250, Loss: 0.0940\n",
      "Epoch 14/25, Batch 651/2250, Loss: 0.0126\n",
      "Epoch 14/25, Batch 661/2250, Loss: 0.0272\n",
      "Epoch 14/25, Batch 671/2250, Loss: 0.0286\n",
      "Epoch 14/25, Batch 681/2250, Loss: 0.0179\n",
      "Epoch 14/25, Batch 691/2250, Loss: 0.1406\n",
      "Epoch 14/25, Batch 701/2250, Loss: 0.0926\n",
      "Epoch 14/25, Batch 711/2250, Loss: 0.0662\n",
      "Epoch 14/25, Batch 721/2250, Loss: 0.0095\n",
      "Epoch 14/25, Batch 731/2250, Loss: 0.0537\n",
      "Epoch 14/25, Batch 741/2250, Loss: 0.0972\n",
      "Epoch 14/25, Batch 751/2250, Loss: 0.0247\n",
      "Epoch 14/25, Batch 761/2250, Loss: 0.1525\n",
      "Epoch 14/25, Batch 771/2250, Loss: 0.2338\n",
      "Epoch 14/25, Batch 781/2250, Loss: 0.0351\n",
      "Epoch 14/25, Batch 791/2250, Loss: 0.1718\n",
      "Epoch 14/25, Batch 801/2250, Loss: 0.0710\n",
      "Epoch 14/25, Batch 811/2250, Loss: 0.0996\n",
      "Epoch 14/25, Batch 821/2250, Loss: 0.1400\n",
      "Epoch 14/25, Batch 831/2250, Loss: 0.0577\n",
      "Epoch 14/25, Batch 841/2250, Loss: 0.0143\n",
      "Epoch 14/25, Batch 851/2250, Loss: 0.0675\n",
      "Epoch 14/25, Batch 861/2250, Loss: 0.0111\n",
      "Epoch 14/25, Batch 871/2250, Loss: 0.1593\n",
      "Epoch 14/25, Batch 881/2250, Loss: 0.0117\n",
      "Epoch 14/25, Batch 891/2250, Loss: 0.1749\n",
      "Epoch 14/25, Batch 901/2250, Loss: 0.0884\n",
      "Epoch 14/25, Batch 911/2250, Loss: 0.0112\n",
      "Epoch 14/25, Batch 921/2250, Loss: 0.0106\n",
      "Epoch 14/25, Batch 931/2250, Loss: 0.0827\n",
      "Epoch 14/25, Batch 941/2250, Loss: 0.0325\n",
      "Epoch 14/25, Batch 951/2250, Loss: 0.1056\n",
      "Epoch 14/25, Batch 961/2250, Loss: 0.0185\n",
      "Epoch 14/25, Batch 971/2250, Loss: 0.0082\n",
      "Epoch 14/25, Batch 981/2250, Loss: 0.2306\n",
      "Epoch 14/25, Batch 991/2250, Loss: 0.0851\n",
      "Epoch 14/25, Batch 1001/2250, Loss: 0.0374\n",
      "Epoch 14/25, Batch 1011/2250, Loss: 0.0471\n",
      "Epoch 14/25, Batch 1021/2250, Loss: 0.0212\n",
      "Epoch 14/25, Batch 1031/2250, Loss: 0.0177\n",
      "Epoch 14/25, Batch 1041/2250, Loss: 0.0412\n",
      "Epoch 14/25, Batch 1051/2250, Loss: 0.1152\n",
      "Epoch 14/25, Batch 1061/2250, Loss: 0.0341\n",
      "Epoch 14/25, Batch 1071/2250, Loss: 0.0343\n",
      "Epoch 14/25, Batch 1081/2250, Loss: 0.0079\n",
      "Epoch 14/25, Batch 1091/2250, Loss: 0.0110\n",
      "Epoch 14/25, Batch 1101/2250, Loss: 0.0259\n",
      "Epoch 14/25, Batch 1111/2250, Loss: 0.0051\n",
      "Epoch 14/25, Batch 1121/2250, Loss: 0.0335\n",
      "Epoch 14/25, Batch 1131/2250, Loss: 0.0443\n",
      "Epoch 14/25, Batch 1141/2250, Loss: 0.0022\n",
      "Epoch 14/25, Batch 1151/2250, Loss: 0.1097\n",
      "Epoch 14/25, Batch 1161/2250, Loss: 0.0322\n",
      "Epoch 14/25, Batch 1171/2250, Loss: 0.0577\n",
      "Epoch 14/25, Batch 1181/2250, Loss: 0.0656\n",
      "Epoch 14/25, Batch 1191/2250, Loss: 0.0056\n",
      "Epoch 14/25, Batch 1201/2250, Loss: 0.0212\n",
      "Epoch 14/25, Batch 1211/2250, Loss: 0.0229\n",
      "Epoch 14/25, Batch 1221/2250, Loss: 0.1086\n",
      "Epoch 14/25, Batch 1231/2250, Loss: 0.0050\n",
      "Epoch 14/25, Batch 1241/2250, Loss: 0.0068\n",
      "Epoch 14/25, Batch 1251/2250, Loss: 0.0822\n",
      "Epoch 14/25, Batch 1261/2250, Loss: 0.1235\n",
      "Epoch 14/25, Batch 1271/2250, Loss: 0.0607\n",
      "Epoch 14/25, Batch 1281/2250, Loss: 0.0156\n",
      "Epoch 14/25, Batch 1291/2250, Loss: 0.0352\n",
      "Epoch 14/25, Batch 1301/2250, Loss: 0.2390\n",
      "Epoch 14/25, Batch 1311/2250, Loss: 0.0238\n",
      "Epoch 14/25, Batch 1321/2250, Loss: 0.0748\n",
      "Epoch 14/25, Batch 1331/2250, Loss: 0.1843\n",
      "Epoch 14/25, Batch 1341/2250, Loss: 0.0774\n",
      "Epoch 14/25, Batch 1351/2250, Loss: 0.0502\n",
      "Epoch 14/25, Batch 1361/2250, Loss: 0.0055\n",
      "Epoch 14/25, Batch 1371/2250, Loss: 0.0844\n",
      "Epoch 14/25, Batch 1381/2250, Loss: 0.0066\n",
      "Epoch 14/25, Batch 1391/2250, Loss: 0.0156\n",
      "Epoch 14/25, Batch 1401/2250, Loss: 0.0337\n",
      "Epoch 14/25, Batch 1411/2250, Loss: 0.0874\n",
      "Epoch 14/25, Batch 1421/2250, Loss: 0.0643\n",
      "Epoch 14/25, Batch 1431/2250, Loss: 0.0415\n",
      "Epoch 14/25, Batch 1441/2250, Loss: 0.0019\n",
      "Epoch 14/25, Batch 1451/2250, Loss: 0.0159\n",
      "Epoch 14/25, Batch 1461/2250, Loss: 0.0185\n",
      "Epoch 14/25, Batch 1471/2250, Loss: 0.1033\n",
      "Epoch 14/25, Batch 1481/2250, Loss: 0.1565\n",
      "Epoch 14/25, Batch 1491/2250, Loss: 0.0061\n",
      "Epoch 14/25, Batch 1501/2250, Loss: 0.0207\n",
      "Epoch 14/25, Batch 1511/2250, Loss: 0.0901\n",
      "Epoch 14/25, Batch 1521/2250, Loss: 0.1521\n",
      "Epoch 14/25, Batch 1531/2250, Loss: 0.0212\n",
      "Epoch 14/25, Batch 1541/2250, Loss: 0.0470\n",
      "Epoch 14/25, Batch 1551/2250, Loss: 0.0459\n",
      "Epoch 14/25, Batch 1561/2250, Loss: 0.1619\n",
      "Epoch 14/25, Batch 1571/2250, Loss: 0.1552\n",
      "Epoch 14/25, Batch 1581/2250, Loss: 0.0174\n",
      "Epoch 14/25, Batch 1591/2250, Loss: 0.0566\n",
      "Epoch 14/25, Batch 1601/2250, Loss: 0.0416\n",
      "Epoch 14/25, Batch 1611/2250, Loss: 0.0416\n",
      "Epoch 14/25, Batch 1621/2250, Loss: 0.0175\n",
      "Epoch 14/25, Batch 1631/2250, Loss: 0.0325\n",
      "Epoch 14/25, Batch 1641/2250, Loss: 0.1181\n",
      "Epoch 14/25, Batch 1651/2250, Loss: 0.0476\n",
      "Epoch 14/25, Batch 1661/2250, Loss: 0.0608\n",
      "Epoch 14/25, Batch 1671/2250, Loss: 0.0774\n",
      "Epoch 14/25, Batch 1681/2250, Loss: 0.0259\n",
      "Epoch 14/25, Batch 1691/2250, Loss: 0.0378\n",
      "Epoch 14/25, Batch 1701/2250, Loss: 0.0176\n",
      "Epoch 14/25, Batch 1711/2250, Loss: 0.0668\n",
      "Epoch 14/25, Batch 1721/2250, Loss: 0.0157\n",
      "Epoch 14/25, Batch 1731/2250, Loss: 0.0062\n",
      "Epoch 14/25, Batch 1741/2250, Loss: 0.0244\n",
      "Epoch 14/25, Batch 1751/2250, Loss: 0.2203\n",
      "Epoch 14/25, Batch 1761/2250, Loss: 0.1047\n",
      "Epoch 14/25, Batch 1771/2250, Loss: 0.0317\n",
      "Epoch 14/25, Batch 1781/2250, Loss: 0.0554\n",
      "Epoch 14/25, Batch 1791/2250, Loss: 0.0091\n",
      "Epoch 14/25, Batch 1801/2250, Loss: 0.0107\n",
      "Epoch 14/25, Batch 1811/2250, Loss: 0.0117\n",
      "Epoch 14/25, Batch 1821/2250, Loss: 0.0078\n",
      "Epoch 14/25, Batch 1831/2250, Loss: 0.1024\n",
      "Epoch 14/25, Batch 1841/2250, Loss: 0.0312\n",
      "Epoch 14/25, Batch 1851/2250, Loss: 0.0463\n",
      "Epoch 14/25, Batch 1861/2250, Loss: 0.0130\n",
      "Epoch 14/25, Batch 1871/2250, Loss: 0.0972\n",
      "Epoch 14/25, Batch 1881/2250, Loss: 0.1055\n",
      "Epoch 14/25, Batch 1891/2250, Loss: 0.0177\n",
      "Epoch 14/25, Batch 1901/2250, Loss: 0.0265\n",
      "Epoch 14/25, Batch 1911/2250, Loss: 0.1045\n",
      "Epoch 14/25, Batch 1921/2250, Loss: 0.0384\n",
      "Epoch 14/25, Batch 1931/2250, Loss: 0.1339\n",
      "Epoch 14/25, Batch 1941/2250, Loss: 0.0782\n",
      "Epoch 14/25, Batch 1951/2250, Loss: 0.1309\n",
      "Epoch 14/25, Batch 1961/2250, Loss: 0.0208\n",
      "Epoch 14/25, Batch 1971/2250, Loss: 0.0471\n",
      "Epoch 14/25, Batch 1981/2250, Loss: 0.0295\n",
      "Epoch 14/25, Batch 1991/2250, Loss: 0.0307\n",
      "Epoch 14/25, Batch 2001/2250, Loss: 0.0286\n",
      "Epoch 14/25, Batch 2011/2250, Loss: 0.0669\n",
      "Epoch 14/25, Batch 2021/2250, Loss: 0.0772\n",
      "Epoch 14/25, Batch 2031/2250, Loss: 0.0701\n",
      "Epoch 14/25, Batch 2041/2250, Loss: 0.0062\n",
      "Epoch 14/25, Batch 2051/2250, Loss: 0.1297\n",
      "Epoch 14/25, Batch 2061/2250, Loss: 0.0709\n",
      "Epoch 14/25, Batch 2071/2250, Loss: 0.0548\n",
      "Epoch 14/25, Batch 2081/2250, Loss: 0.0206\n",
      "Epoch 14/25, Batch 2091/2250, Loss: 0.1344\n",
      "Epoch 14/25, Batch 2101/2250, Loss: 0.0630\n",
      "Epoch 14/25, Batch 2111/2250, Loss: 0.0257\n",
      "Epoch 14/25, Batch 2121/2250, Loss: 0.1036\n",
      "Epoch 14/25, Batch 2131/2250, Loss: 0.0286\n",
      "Epoch 14/25, Batch 2141/2250, Loss: 0.0159\n",
      "Epoch 14/25, Batch 2151/2250, Loss: 0.0202\n",
      "Epoch 14/25, Batch 2161/2250, Loss: 0.0157\n",
      "Epoch 14/25, Batch 2171/2250, Loss: 0.0279\n",
      "Epoch 14/25, Batch 2181/2250, Loss: 0.1112\n",
      "Epoch 14/25, Batch 2191/2250, Loss: 0.0146\n",
      "Epoch 14/25, Batch 2201/2250, Loss: 0.0206\n",
      "Epoch 14/25, Batch 2211/2250, Loss: 0.0127\n",
      "Epoch 14/25, Batch 2221/2250, Loss: 0.0521\n",
      "Epoch 14/25, Batch 2231/2250, Loss: 0.0594\n",
      "Epoch 14/25, Batch 2241/2250, Loss: 0.0101\n",
      "Epoch 14/25:\n",
      "Train Loss: 0.0560, Train Acc: 97.92%\n",
      "Val Loss: 0.0556, Val Acc: 97.97%\n",
      "------------------------------------------------------------\n",
      "Starting epoch 15/25...\n",
      "Starting training loop with 2250 batches...\n",
      "Successfully loaded first batch: images shape torch.Size([32, 3, 224, 224]), labels shape torch.Size([32])\n",
      "Epoch 15/25, Batch 1/2250, Loss: 0.0244\n",
      "Epoch 15/25, Batch 11/2250, Loss: 0.2539\n",
      "Epoch 15/25, Batch 21/2250, Loss: 0.0197\n",
      "Epoch 15/25, Batch 31/2250, Loss: 0.0357\n",
      "Epoch 15/25, Batch 41/2250, Loss: 0.0674\n",
      "Epoch 15/25, Batch 51/2250, Loss: 0.0438\n",
      "Epoch 15/25, Batch 61/2250, Loss: 0.0490\n",
      "Epoch 15/25, Batch 71/2250, Loss: 0.0039\n",
      "Epoch 15/25, Batch 81/2250, Loss: 0.1093\n",
      "Epoch 15/25, Batch 91/2250, Loss: 0.0223\n",
      "Epoch 15/25, Batch 101/2250, Loss: 0.0173\n",
      "Epoch 15/25, Batch 111/2250, Loss: 0.0164\n",
      "Epoch 15/25, Batch 121/2250, Loss: 0.1610\n",
      "Epoch 15/25, Batch 131/2250, Loss: 0.0192\n",
      "Epoch 15/25, Batch 141/2250, Loss: 0.0066\n",
      "Epoch 15/25, Batch 151/2250, Loss: 0.0157\n",
      "Epoch 15/25, Batch 161/2250, Loss: 0.0024\n",
      "Epoch 15/25, Batch 171/2250, Loss: 0.0390\n",
      "Epoch 15/25, Batch 181/2250, Loss: 0.0092\n",
      "Epoch 15/25, Batch 191/2250, Loss: 0.0965\n",
      "Epoch 15/25, Batch 201/2250, Loss: 0.0708\n",
      "Epoch 15/25, Batch 211/2250, Loss: 0.0183\n",
      "Epoch 15/25, Batch 221/2250, Loss: 0.0305\n",
      "Epoch 15/25, Batch 231/2250, Loss: 0.1091\n",
      "Epoch 15/25, Batch 241/2250, Loss: 0.0155\n",
      "Epoch 15/25, Batch 251/2250, Loss: 0.0212\n",
      "Epoch 15/25, Batch 261/2250, Loss: 0.0302\n",
      "Epoch 15/25, Batch 271/2250, Loss: 0.1213\n",
      "Epoch 15/25, Batch 281/2250, Loss: 0.1368\n",
      "Epoch 15/25, Batch 291/2250, Loss: 0.0693\n",
      "Epoch 15/25, Batch 301/2250, Loss: 0.0226\n",
      "Epoch 15/25, Batch 311/2250, Loss: 0.0114\n",
      "Epoch 15/25, Batch 321/2250, Loss: 0.0430\n",
      "Epoch 15/25, Batch 331/2250, Loss: 0.1003\n",
      "Epoch 15/25, Batch 341/2250, Loss: 0.0409\n",
      "Epoch 15/25, Batch 351/2250, Loss: 0.0311\n",
      "Epoch 15/25, Batch 361/2250, Loss: 0.0130\n",
      "Epoch 15/25, Batch 371/2250, Loss: 0.0481\n",
      "Epoch 15/25, Batch 381/2250, Loss: 0.0786\n",
      "Epoch 15/25, Batch 391/2250, Loss: 0.0548\n",
      "Epoch 15/25, Batch 401/2250, Loss: 0.0284\n",
      "Epoch 15/25, Batch 411/2250, Loss: 0.0358\n",
      "Epoch 15/25, Batch 421/2250, Loss: 0.2111\n",
      "Epoch 15/25, Batch 431/2250, Loss: 0.0098\n",
      "Epoch 15/25, Batch 441/2250, Loss: 0.1069\n",
      "Epoch 15/25, Batch 451/2250, Loss: 0.1496\n",
      "Epoch 15/25, Batch 461/2250, Loss: 0.0719\n",
      "Epoch 15/25, Batch 471/2250, Loss: 0.0098\n",
      "Epoch 15/25, Batch 481/2250, Loss: 0.0122\n",
      "Epoch 15/25, Batch 491/2250, Loss: 0.0229\n",
      "Epoch 15/25, Batch 501/2250, Loss: 0.0061\n",
      "Epoch 15/25, Batch 511/2250, Loss: 0.0678\n",
      "Epoch 15/25, Batch 521/2250, Loss: 0.0147\n",
      "Epoch 15/25, Batch 531/2250, Loss: 0.0156\n",
      "Epoch 15/25, Batch 541/2250, Loss: 0.0387\n",
      "Epoch 15/25, Batch 551/2250, Loss: 0.0017\n",
      "Epoch 15/25, Batch 561/2250, Loss: 0.1451\n",
      "Epoch 15/25, Batch 571/2250, Loss: 0.1006\n",
      "Epoch 15/25, Batch 581/2250, Loss: 0.0204\n",
      "Epoch 15/25, Batch 591/2250, Loss: 0.1714\n",
      "Epoch 15/25, Batch 601/2250, Loss: 0.0553\n",
      "Epoch 15/25, Batch 611/2250, Loss: 0.0618\n",
      "Epoch 15/25, Batch 621/2250, Loss: 0.0098\n",
      "Epoch 15/25, Batch 631/2250, Loss: 0.0027\n",
      "Epoch 15/25, Batch 641/2250, Loss: 0.0789\n",
      "Epoch 15/25, Batch 651/2250, Loss: 0.0151\n",
      "Epoch 15/25, Batch 661/2250, Loss: 0.0304\n",
      "Epoch 15/25, Batch 671/2250, Loss: 0.0035\n",
      "Epoch 15/25, Batch 681/2250, Loss: 0.0418\n",
      "Epoch 15/25, Batch 691/2250, Loss: 0.0372\n",
      "Epoch 15/25, Batch 701/2250, Loss: 0.0349\n",
      "Epoch 15/25, Batch 711/2250, Loss: 0.0087\n",
      "Epoch 15/25, Batch 721/2250, Loss: 0.0887\n",
      "Epoch 15/25, Batch 731/2250, Loss: 0.1701\n",
      "Epoch 15/25, Batch 741/2250, Loss: 0.0424\n",
      "Epoch 15/25, Batch 751/2250, Loss: 0.0144\n",
      "Epoch 15/25, Batch 761/2250, Loss: 0.0976\n",
      "Epoch 15/25, Batch 771/2250, Loss: 0.0668\n",
      "Epoch 15/25, Batch 781/2250, Loss: 0.0572\n",
      "Epoch 15/25, Batch 791/2250, Loss: 0.0091\n",
      "Epoch 15/25, Batch 801/2250, Loss: 0.0130\n",
      "Epoch 15/25, Batch 811/2250, Loss: 0.0387\n",
      "Epoch 15/25, Batch 821/2250, Loss: 0.0022\n",
      "Epoch 15/25, Batch 831/2250, Loss: 0.0039\n",
      "Epoch 15/25, Batch 841/2250, Loss: 0.0856\n",
      "Epoch 15/25, Batch 851/2250, Loss: 0.0294\n",
      "Epoch 15/25, Batch 861/2250, Loss: 0.0852\n",
      "Epoch 15/25, Batch 871/2250, Loss: 0.1315\n",
      "Epoch 15/25, Batch 881/2250, Loss: 0.0236\n",
      "Epoch 15/25, Batch 891/2250, Loss: 0.0085\n",
      "Epoch 15/25, Batch 901/2250, Loss: 0.0941\n",
      "Epoch 15/25, Batch 911/2250, Loss: 0.0257\n",
      "Epoch 15/25, Batch 921/2250, Loss: 0.0108\n",
      "Epoch 15/25, Batch 931/2250, Loss: 0.0111\n",
      "Epoch 15/25, Batch 941/2250, Loss: 0.0013\n",
      "Epoch 15/25, Batch 951/2250, Loss: 0.0058\n",
      "Epoch 15/25, Batch 961/2250, Loss: 0.0594\n",
      "Epoch 15/25, Batch 971/2250, Loss: 0.0617\n",
      "Epoch 15/25, Batch 981/2250, Loss: 0.0111\n",
      "Epoch 15/25, Batch 991/2250, Loss: 0.0404\n",
      "Epoch 15/25, Batch 1001/2250, Loss: 0.0779\n",
      "Epoch 15/25, Batch 1011/2250, Loss: 0.0210\n",
      "Epoch 15/25, Batch 1021/2250, Loss: 0.0166\n",
      "Epoch 15/25, Batch 1031/2250, Loss: 0.1174\n",
      "Epoch 15/25, Batch 1041/2250, Loss: 0.0076\n",
      "Epoch 15/25, Batch 1051/2250, Loss: 0.0647\n",
      "Epoch 15/25, Batch 1061/2250, Loss: 0.0504\n",
      "Epoch 15/25, Batch 1071/2250, Loss: 0.0027\n",
      "Epoch 15/25, Batch 1081/2250, Loss: 0.0248\n",
      "Epoch 15/25, Batch 1091/2250, Loss: 0.0059\n",
      "Epoch 15/25, Batch 1101/2250, Loss: 0.0134\n",
      "Epoch 15/25, Batch 1111/2250, Loss: 0.1844\n",
      "Epoch 15/25, Batch 1121/2250, Loss: 0.1295\n",
      "Epoch 15/25, Batch 1131/2250, Loss: 0.2367\n",
      "Epoch 15/25, Batch 1141/2250, Loss: 0.0504\n",
      "Epoch 15/25, Batch 1151/2250, Loss: 0.0440\n",
      "Epoch 15/25, Batch 1161/2250, Loss: 0.1246\n",
      "Epoch 15/25, Batch 1171/2250, Loss: 0.0633\n",
      "Epoch 15/25, Batch 1181/2250, Loss: 0.0093\n",
      "Epoch 15/25, Batch 1191/2250, Loss: 0.0702\n",
      "Epoch 15/25, Batch 1201/2250, Loss: 0.1402\n",
      "Epoch 15/25, Batch 1211/2250, Loss: 0.0182\n",
      "Epoch 15/25, Batch 1221/2250, Loss: 0.0403\n",
      "Epoch 15/25, Batch 1231/2250, Loss: 0.1285\n",
      "Epoch 15/25, Batch 1241/2250, Loss: 0.0523\n",
      "Epoch 15/25, Batch 1251/2250, Loss: 0.0579\n",
      "Epoch 15/25, Batch 1261/2250, Loss: 0.0039\n",
      "Epoch 15/25, Batch 1271/2250, Loss: 0.0235\n",
      "Epoch 15/25, Batch 1281/2250, Loss: 0.1840\n",
      "Epoch 15/25, Batch 1291/2250, Loss: 0.0349\n",
      "Epoch 15/25, Batch 1301/2250, Loss: 0.0191\n",
      "Epoch 15/25, Batch 1311/2250, Loss: 0.1030\n",
      "Epoch 15/25, Batch 1321/2250, Loss: 0.0622\n",
      "Epoch 15/25, Batch 1331/2250, Loss: 0.0632\n",
      "Epoch 15/25, Batch 1341/2250, Loss: 0.0416\n",
      "Epoch 15/25, Batch 1351/2250, Loss: 0.0332\n",
      "Epoch 15/25, Batch 1361/2250, Loss: 0.0252\n",
      "Epoch 15/25, Batch 1371/2250, Loss: 0.0446\n",
      "Epoch 15/25, Batch 1381/2250, Loss: 0.0065\n",
      "Epoch 15/25, Batch 1391/2250, Loss: 0.0367\n",
      "Epoch 15/25, Batch 1401/2250, Loss: 0.1095\n",
      "Epoch 15/25, Batch 1411/2250, Loss: 0.0059\n",
      "Epoch 15/25, Batch 1421/2250, Loss: 0.0026\n",
      "Epoch 15/25, Batch 1431/2250, Loss: 0.0567\n",
      "Epoch 15/25, Batch 1441/2250, Loss: 0.1233\n",
      "Epoch 15/25, Batch 1451/2250, Loss: 0.0567\n",
      "Epoch 15/25, Batch 1461/2250, Loss: 0.0824\n",
      "Epoch 15/25, Batch 1471/2250, Loss: 0.0179\n",
      "Epoch 15/25, Batch 1481/2250, Loss: 0.0148\n",
      "Epoch 15/25, Batch 1491/2250, Loss: 0.4306\n",
      "Epoch 15/25, Batch 1501/2250, Loss: 0.0287\n",
      "Epoch 15/25, Batch 1511/2250, Loss: 0.1076\n",
      "Epoch 15/25, Batch 1521/2250, Loss: 0.0206\n",
      "Epoch 15/25, Batch 1531/2250, Loss: 0.2555\n",
      "Epoch 15/25, Batch 1541/2250, Loss: 0.0607\n",
      "Epoch 15/25, Batch 1551/2250, Loss: 0.0250\n",
      "Epoch 15/25, Batch 1561/2250, Loss: 0.0906\n",
      "Epoch 15/25, Batch 1571/2250, Loss: 0.0075\n",
      "Epoch 15/25, Batch 1581/2250, Loss: 0.0824\n",
      "Epoch 15/25, Batch 1591/2250, Loss: 0.0528\n",
      "Epoch 15/25, Batch 1601/2250, Loss: 0.0573\n",
      "Epoch 15/25, Batch 1611/2250, Loss: 0.0602\n",
      "Epoch 15/25, Batch 1621/2250, Loss: 0.0370\n",
      "Epoch 15/25, Batch 1631/2250, Loss: 0.0068\n",
      "Epoch 15/25, Batch 1641/2250, Loss: 0.0353\n",
      "Epoch 15/25, Batch 1651/2250, Loss: 0.0260\n",
      "Epoch 15/25, Batch 1661/2250, Loss: 0.0125\n",
      "Epoch 15/25, Batch 1671/2250, Loss: 0.0426\n",
      "Epoch 15/25, Batch 1681/2250, Loss: 0.0395\n",
      "Epoch 15/25, Batch 1691/2250, Loss: 0.0176\n",
      "Epoch 15/25, Batch 1701/2250, Loss: 0.0041\n",
      "Epoch 15/25, Batch 1711/2250, Loss: 0.0163\n",
      "Epoch 15/25, Batch 1721/2250, Loss: 0.0299\n",
      "Epoch 15/25, Batch 1731/2250, Loss: 0.0453\n",
      "Epoch 15/25, Batch 1741/2250, Loss: 0.0111\n",
      "Epoch 15/25, Batch 1751/2250, Loss: 0.0013\n",
      "Epoch 15/25, Batch 1761/2250, Loss: 0.0164\n",
      "Epoch 15/25, Batch 1771/2250, Loss: 0.0072\n",
      "Epoch 15/25, Batch 1781/2250, Loss: 0.0287\n",
      "Epoch 15/25, Batch 1791/2250, Loss: 0.1271\n",
      "Epoch 15/25, Batch 1801/2250, Loss: 0.0134\n",
      "Epoch 15/25, Batch 1811/2250, Loss: 0.1932\n",
      "Epoch 15/25, Batch 1821/2250, Loss: 0.0751\n",
      "Epoch 15/25, Batch 1831/2250, Loss: 0.0461\n",
      "Epoch 15/25, Batch 1841/2250, Loss: 0.0843\n",
      "Epoch 15/25, Batch 1851/2250, Loss: 0.0114\n",
      "Epoch 15/25, Batch 1861/2250, Loss: 0.0972\n",
      "Epoch 15/25, Batch 1871/2250, Loss: 0.0081\n",
      "Epoch 15/25, Batch 1881/2250, Loss: 0.0190\n",
      "Epoch 15/25, Batch 1891/2250, Loss: 0.1573\n",
      "Epoch 15/25, Batch 1901/2250, Loss: 0.0035\n",
      "Epoch 15/25, Batch 1911/2250, Loss: 0.0730\n",
      "Epoch 15/25, Batch 1921/2250, Loss: 0.1316\n",
      "Epoch 15/25, Batch 1931/2250, Loss: 0.2518\n",
      "Epoch 15/25, Batch 1941/2250, Loss: 0.0066\n",
      "Epoch 15/25, Batch 1951/2250, Loss: 0.0750\n",
      "Epoch 15/25, Batch 1961/2250, Loss: 0.0815\n",
      "Epoch 15/25, Batch 1971/2250, Loss: 0.0092\n",
      "Epoch 15/25, Batch 1981/2250, Loss: 0.0280\n",
      "Epoch 15/25, Batch 1991/2250, Loss: 0.0580\n",
      "Epoch 15/25, Batch 2001/2250, Loss: 0.0031\n",
      "Epoch 15/25, Batch 2011/2250, Loss: 0.1357\n",
      "Epoch 15/25, Batch 2021/2250, Loss: 0.0028\n",
      "Epoch 15/25, Batch 2031/2250, Loss: 0.0367\n",
      "Epoch 15/25, Batch 2041/2250, Loss: 0.0015\n",
      "Epoch 15/25, Batch 2051/2250, Loss: 0.0810\n",
      "Epoch 15/25, Batch 2061/2250, Loss: 0.0352\n",
      "Epoch 15/25, Batch 2071/2250, Loss: 0.0521\n",
      "Epoch 15/25, Batch 2081/2250, Loss: 0.0505\n",
      "Epoch 15/25, Batch 2091/2250, Loss: 0.0320\n",
      "Epoch 15/25, Batch 2101/2250, Loss: 0.0299\n",
      "Epoch 15/25, Batch 2111/2250, Loss: 0.0289\n",
      "Epoch 15/25, Batch 2121/2250, Loss: 0.0118\n",
      "Epoch 15/25, Batch 2131/2250, Loss: 0.0544\n",
      "Epoch 15/25, Batch 2141/2250, Loss: 0.0255\n",
      "Epoch 15/25, Batch 2151/2250, Loss: 0.0047\n",
      "Epoch 15/25, Batch 2161/2250, Loss: 0.0704\n",
      "Epoch 15/25, Batch 2171/2250, Loss: 0.0362\n",
      "Epoch 15/25, Batch 2181/2250, Loss: 0.0063\n",
      "Epoch 15/25, Batch 2191/2250, Loss: 0.0244\n",
      "Epoch 15/25, Batch 2201/2250, Loss: 0.0045\n",
      "Epoch 15/25, Batch 2211/2250, Loss: 0.0191\n",
      "Epoch 15/25, Batch 2221/2250, Loss: 0.0506\n",
      "Epoch 15/25, Batch 2231/2250, Loss: 0.0203\n",
      "Epoch 15/25, Batch 2241/2250, Loss: 0.0222\n",
      "Epoch 15/25:\n",
      "Train Loss: 0.0540, Train Acc: 98.03%\n",
      "Val Loss: 0.0495, Val Acc: 98.21%\n",
      "------------------------------------------------------------\n",
      "Starting epoch 16/25...\n",
      "Starting training loop with 2250 batches...\n",
      "Successfully loaded first batch: images shape torch.Size([32, 3, 224, 224]), labels shape torch.Size([32])\n",
      "Epoch 16/25, Batch 1/2250, Loss: 0.0283\n",
      "Epoch 16/25, Batch 11/2250, Loss: 0.0653\n",
      "Epoch 16/25, Batch 21/2250, Loss: 0.0084\n",
      "Epoch 16/25, Batch 31/2250, Loss: 0.0108\n",
      "Epoch 16/25, Batch 41/2250, Loss: 0.0332\n",
      "Epoch 16/25, Batch 51/2250, Loss: 0.0006\n",
      "Epoch 16/25, Batch 61/2250, Loss: 0.0342\n",
      "Epoch 16/25, Batch 71/2250, Loss: 0.0088\n",
      "Epoch 16/25, Batch 81/2250, Loss: 0.0033\n",
      "Epoch 16/25, Batch 91/2250, Loss: 0.0026\n",
      "Epoch 16/25, Batch 101/2250, Loss: 0.0052\n",
      "Epoch 16/25, Batch 111/2250, Loss: 0.2550\n",
      "Epoch 16/25, Batch 121/2250, Loss: 0.0147\n",
      "Epoch 16/25, Batch 131/2250, Loss: 0.1470\n",
      "Epoch 16/25, Batch 141/2250, Loss: 0.0316\n",
      "Epoch 16/25, Batch 151/2250, Loss: 0.0076\n",
      "Epoch 16/25, Batch 161/2250, Loss: 0.0138\n",
      "Epoch 16/25, Batch 171/2250, Loss: 0.1362\n",
      "Epoch 16/25, Batch 181/2250, Loss: 0.1478\n",
      "Epoch 16/25, Batch 191/2250, Loss: 0.0125\n",
      "Epoch 16/25, Batch 201/2250, Loss: 0.0738\n",
      "Epoch 16/25, Batch 211/2250, Loss: 0.0348\n",
      "Epoch 16/25, Batch 221/2250, Loss: 0.2642\n",
      "Epoch 16/25, Batch 231/2250, Loss: 0.0624\n",
      "Epoch 16/25, Batch 241/2250, Loss: 0.0269\n",
      "Epoch 16/25, Batch 251/2250, Loss: 0.0132\n",
      "Epoch 16/25, Batch 261/2250, Loss: 0.0293\n",
      "Epoch 16/25, Batch 271/2250, Loss: 0.0759\n",
      "Epoch 16/25, Batch 281/2250, Loss: 0.0083\n",
      "Epoch 16/25, Batch 291/2250, Loss: 0.0103\n",
      "Epoch 16/25, Batch 301/2250, Loss: 0.1063\n",
      "Epoch 16/25, Batch 311/2250, Loss: 0.1212\n",
      "Epoch 16/25, Batch 321/2250, Loss: 0.0016\n",
      "Epoch 16/25, Batch 331/2250, Loss: 0.0046\n",
      "Epoch 16/25, Batch 341/2250, Loss: 0.0094\n",
      "Epoch 16/25, Batch 351/2250, Loss: 0.0258\n",
      "Epoch 16/25, Batch 361/2250, Loss: 0.0338\n",
      "Epoch 16/25, Batch 371/2250, Loss: 0.0080\n",
      "Epoch 16/25, Batch 381/2250, Loss: 0.0840\n",
      "Epoch 16/25, Batch 391/2250, Loss: 0.0099\n",
      "Epoch 16/25, Batch 401/2250, Loss: 0.0763\n",
      "Epoch 16/25, Batch 411/2250, Loss: 0.0171\n",
      "Epoch 16/25, Batch 421/2250, Loss: 0.0081\n",
      "Epoch 16/25, Batch 431/2250, Loss: 0.0409\n",
      "Epoch 16/25, Batch 441/2250, Loss: 0.0927\n",
      "Epoch 16/25, Batch 451/2250, Loss: 0.0706\n",
      "Epoch 16/25, Batch 461/2250, Loss: 0.0175\n",
      "Epoch 16/25, Batch 471/2250, Loss: 0.0286\n",
      "Epoch 16/25, Batch 481/2250, Loss: 0.0212\n",
      "Epoch 16/25, Batch 491/2250, Loss: 0.0159\n",
      "Epoch 16/25, Batch 501/2250, Loss: 0.0877\n",
      "Epoch 16/25, Batch 511/2250, Loss: 0.0010\n",
      "Epoch 16/25, Batch 521/2250, Loss: 0.0504\n",
      "Epoch 16/25, Batch 531/2250, Loss: 0.0379\n",
      "Epoch 16/25, Batch 541/2250, Loss: 0.0140\n",
      "Epoch 16/25, Batch 551/2250, Loss: 0.0163\n",
      "Epoch 16/25, Batch 561/2250, Loss: 0.0145\n",
      "Epoch 16/25, Batch 571/2250, Loss: 0.0448\n",
      "Epoch 16/25, Batch 581/2250, Loss: 0.0022\n",
      "Epoch 16/25, Batch 591/2250, Loss: 0.0246\n",
      "Epoch 16/25, Batch 601/2250, Loss: 0.0636\n",
      "Epoch 16/25, Batch 611/2250, Loss: 0.0353\n",
      "Epoch 16/25, Batch 621/2250, Loss: 0.0125\n",
      "Epoch 16/25, Batch 631/2250, Loss: 0.0186\n",
      "Epoch 16/25, Batch 641/2250, Loss: 0.0729\n",
      "Epoch 16/25, Batch 651/2250, Loss: 0.0141\n",
      "Epoch 16/25, Batch 661/2250, Loss: 0.0610\n",
      "Epoch 16/25, Batch 671/2250, Loss: 0.0388\n",
      "Epoch 16/25, Batch 681/2250, Loss: 0.0661\n",
      "Epoch 16/25, Batch 691/2250, Loss: 0.0376\n",
      "Epoch 16/25, Batch 701/2250, Loss: 0.0315\n",
      "Epoch 16/25, Batch 711/2250, Loss: 0.1385\n",
      "Epoch 16/25, Batch 721/2250, Loss: 0.0045\n",
      "Epoch 16/25, Batch 731/2250, Loss: 0.0173\n",
      "Epoch 16/25, Batch 741/2250, Loss: 0.0917\n",
      "Epoch 16/25, Batch 751/2250, Loss: 0.0025\n",
      "Epoch 16/25, Batch 761/2250, Loss: 0.0046\n",
      "Epoch 16/25, Batch 771/2250, Loss: 0.0066\n",
      "Epoch 16/25, Batch 781/2250, Loss: 0.0035\n",
      "Epoch 16/25, Batch 791/2250, Loss: 0.0128\n",
      "Epoch 16/25, Batch 801/2250, Loss: 0.0034\n",
      "Epoch 16/25, Batch 811/2250, Loss: 0.1099\n",
      "Epoch 16/25, Batch 821/2250, Loss: 0.0281\n",
      "Epoch 16/25, Batch 831/2250, Loss: 0.1064\n",
      "Epoch 16/25, Batch 841/2250, Loss: 0.0127\n",
      "Epoch 16/25, Batch 851/2250, Loss: 0.0922\n",
      "Epoch 16/25, Batch 861/2250, Loss: 0.0238\n",
      "Epoch 16/25, Batch 871/2250, Loss: 0.0427\n",
      "Epoch 16/25, Batch 881/2250, Loss: 0.0344\n",
      "Epoch 16/25, Batch 891/2250, Loss: 0.0230\n",
      "Epoch 16/25, Batch 901/2250, Loss: 0.0324\n",
      "Epoch 16/25, Batch 911/2250, Loss: 0.0247\n",
      "Epoch 16/25, Batch 921/2250, Loss: 0.0832\n",
      "Epoch 16/25, Batch 931/2250, Loss: 0.0608\n",
      "Epoch 16/25, Batch 941/2250, Loss: 0.0044\n",
      "Epoch 16/25, Batch 951/2250, Loss: 0.0761\n",
      "Epoch 16/25, Batch 961/2250, Loss: 0.0362\n",
      "Epoch 16/25, Batch 971/2250, Loss: 0.0628\n",
      "Epoch 16/25, Batch 981/2250, Loss: 0.0504\n",
      "Epoch 16/25, Batch 991/2250, Loss: 0.0918\n",
      "Epoch 16/25, Batch 1001/2250, Loss: 0.1260\n",
      "Epoch 16/25, Batch 1011/2250, Loss: 0.0400\n",
      "Epoch 16/25, Batch 1021/2250, Loss: 0.0123\n",
      "Epoch 16/25, Batch 1031/2250, Loss: 0.0988\n",
      "Epoch 16/25, Batch 1041/2250, Loss: 0.1542\n",
      "Epoch 16/25, Batch 1051/2250, Loss: 0.0266\n",
      "Epoch 16/25, Batch 1061/2250, Loss: 0.0101\n",
      "Epoch 16/25, Batch 1071/2250, Loss: 0.0153\n",
      "Epoch 16/25, Batch 1081/2250, Loss: 0.0843\n",
      "Epoch 16/25, Batch 1091/2250, Loss: 0.0035\n",
      "Epoch 16/25, Batch 1101/2250, Loss: 0.1241\n",
      "Epoch 16/25, Batch 1111/2250, Loss: 0.2110\n",
      "Epoch 16/25, Batch 1121/2250, Loss: 0.0316\n",
      "Epoch 16/25, Batch 1131/2250, Loss: 0.0055\n",
      "Epoch 16/25, Batch 1141/2250, Loss: 0.0151\n",
      "Epoch 16/25, Batch 1151/2250, Loss: 0.0635\n",
      "Epoch 16/25, Batch 1161/2250, Loss: 0.0653\n",
      "Epoch 16/25, Batch 1171/2250, Loss: 0.1533\n",
      "Epoch 16/25, Batch 1181/2250, Loss: 0.0193\n",
      "Epoch 16/25, Batch 1191/2250, Loss: 0.0304\n",
      "Epoch 16/25, Batch 1201/2250, Loss: 0.0227\n",
      "Epoch 16/25, Batch 1211/2250, Loss: 0.0303\n",
      "Epoch 16/25, Batch 1221/2250, Loss: 0.0021\n",
      "Epoch 16/25, Batch 1231/2250, Loss: 0.1813\n",
      "Epoch 16/25, Batch 1241/2250, Loss: 0.0235\n",
      "Epoch 16/25, Batch 1251/2250, Loss: 0.0208\n",
      "Epoch 16/25, Batch 1261/2250, Loss: 0.0034\n",
      "Epoch 16/25, Batch 1271/2250, Loss: 0.0037\n",
      "Epoch 16/25, Batch 1281/2250, Loss: 0.0088\n",
      "Epoch 16/25, Batch 1291/2250, Loss: 0.1006\n",
      "Epoch 16/25, Batch 1301/2250, Loss: 0.0063\n",
      "Epoch 16/25, Batch 1311/2250, Loss: 0.1115\n",
      "Epoch 16/25, Batch 1321/2250, Loss: 0.0538\n",
      "Epoch 16/25, Batch 1331/2250, Loss: 0.0165\n",
      "Epoch 16/25, Batch 1341/2250, Loss: 0.0447\n",
      "Epoch 16/25, Batch 1351/2250, Loss: 0.1791\n",
      "Epoch 16/25, Batch 1361/2250, Loss: 0.0952\n",
      "Epoch 16/25, Batch 1371/2250, Loss: 0.0501\n",
      "Epoch 16/25, Batch 1381/2250, Loss: 0.0492\n",
      "Epoch 16/25, Batch 1391/2250, Loss: 0.0072\n",
      "Epoch 16/25, Batch 1401/2250, Loss: 0.0289\n",
      "Epoch 16/25, Batch 1411/2250, Loss: 0.0053\n",
      "Epoch 16/25, Batch 1421/2250, Loss: 0.0479\n",
      "Epoch 16/25, Batch 1431/2250, Loss: 0.0236\n",
      "Epoch 16/25, Batch 1441/2250, Loss: 0.0207\n",
      "Epoch 16/25, Batch 1451/2250, Loss: 0.1310\n",
      "Epoch 16/25, Batch 1461/2250, Loss: 0.0115\n",
      "Epoch 16/25, Batch 1471/2250, Loss: 0.0030\n",
      "Epoch 16/25, Batch 1481/2250, Loss: 0.0066\n",
      "Epoch 16/25, Batch 1491/2250, Loss: 0.0314\n",
      "Epoch 16/25, Batch 1501/2250, Loss: 0.0545\n",
      "Epoch 16/25, Batch 1511/2250, Loss: 0.0147\n",
      "Epoch 16/25, Batch 1521/2250, Loss: 0.0395\n",
      "Epoch 16/25, Batch 1531/2250, Loss: 0.0305\n",
      "Epoch 16/25, Batch 1541/2250, Loss: 0.1645\n",
      "Epoch 16/25, Batch 1551/2250, Loss: 0.1686\n",
      "Epoch 16/25, Batch 1561/2250, Loss: 0.0232\n",
      "Epoch 16/25, Batch 1571/2250, Loss: 0.0380\n",
      "Epoch 16/25, Batch 1581/2250, Loss: 0.0669\n",
      "Epoch 16/25, Batch 1591/2250, Loss: 0.0154\n",
      "Epoch 16/25, Batch 1601/2250, Loss: 0.2470\n",
      "Epoch 16/25, Batch 1611/2250, Loss: 0.0620\n",
      "Epoch 16/25, Batch 1621/2250, Loss: 0.0095\n",
      "Epoch 16/25, Batch 1631/2250, Loss: 0.0925\n",
      "Epoch 16/25, Batch 1641/2250, Loss: 0.0026\n",
      "Epoch 16/25, Batch 1651/2250, Loss: 0.0216\n",
      "Epoch 16/25, Batch 1661/2250, Loss: 0.0147\n",
      "Epoch 16/25, Batch 1671/2250, Loss: 0.1020\n",
      "Epoch 16/25, Batch 1681/2250, Loss: 0.0026\n",
      "Epoch 16/25, Batch 1691/2250, Loss: 0.0144\n",
      "Epoch 16/25, Batch 1701/2250, Loss: 0.0207\n",
      "Epoch 16/25, Batch 1711/2250, Loss: 0.0659\n",
      "Epoch 16/25, Batch 1721/2250, Loss: 0.2795\n",
      "Epoch 16/25, Batch 1731/2250, Loss: 0.0168\n",
      "Epoch 16/25, Batch 1741/2250, Loss: 0.0763\n",
      "Epoch 16/25, Batch 1751/2250, Loss: 0.0036\n",
      "Epoch 16/25, Batch 1761/2250, Loss: 0.0018\n",
      "Epoch 16/25, Batch 1771/2250, Loss: 0.1954\n",
      "Epoch 16/25, Batch 1781/2250, Loss: 0.0858\n",
      "Epoch 16/25, Batch 1791/2250, Loss: 0.0068\n",
      "Epoch 16/25, Batch 1801/2250, Loss: 0.0035\n",
      "Epoch 16/25, Batch 1811/2250, Loss: 0.1423\n",
      "Epoch 16/25, Batch 1821/2250, Loss: 0.1327\n",
      "Epoch 16/25, Batch 1831/2250, Loss: 0.0150\n",
      "Epoch 16/25, Batch 1841/2250, Loss: 0.0526\n",
      "Epoch 16/25, Batch 1851/2250, Loss: 0.0199\n",
      "Epoch 16/25, Batch 1861/2250, Loss: 0.0458\n",
      "Epoch 16/25, Batch 1871/2250, Loss: 0.0311\n",
      "Epoch 16/25, Batch 1881/2250, Loss: 0.0423\n",
      "Epoch 16/25, Batch 1891/2250, Loss: 0.0114\n",
      "Epoch 16/25, Batch 1901/2250, Loss: 0.0258\n",
      "Epoch 16/25, Batch 1911/2250, Loss: 0.0907\n",
      "Epoch 16/25, Batch 1921/2250, Loss: 0.0628\n",
      "Epoch 16/25, Batch 1931/2250, Loss: 0.0182\n",
      "Epoch 16/25, Batch 1941/2250, Loss: 0.0199\n",
      "Epoch 16/25, Batch 1951/2250, Loss: 0.1238\n",
      "Epoch 16/25, Batch 1961/2250, Loss: 0.1835\n",
      "Epoch 16/25, Batch 1971/2250, Loss: 0.0718\n",
      "Epoch 16/25, Batch 1981/2250, Loss: 0.0182\n",
      "Epoch 16/25, Batch 1991/2250, Loss: 0.0381\n",
      "Epoch 16/25, Batch 2001/2250, Loss: 0.0328\n",
      "Epoch 16/25, Batch 2011/2250, Loss: 0.0090\n",
      "Epoch 16/25, Batch 2021/2250, Loss: 0.0179\n",
      "Epoch 16/25, Batch 2031/2250, Loss: 0.0109\n",
      "Epoch 16/25, Batch 2041/2250, Loss: 0.0432\n",
      "Epoch 16/25, Batch 2051/2250, Loss: 0.1318\n",
      "Epoch 16/25, Batch 2061/2250, Loss: 0.0427\n",
      "Epoch 16/25, Batch 2071/2250, Loss: 0.0031\n",
      "Epoch 16/25, Batch 2081/2250, Loss: 0.0116\n",
      "Epoch 16/25, Batch 2091/2250, Loss: 0.0217\n",
      "Epoch 16/25, Batch 2101/2250, Loss: 0.0579\n",
      "Epoch 16/25, Batch 2111/2250, Loss: 0.1100\n",
      "Epoch 16/25, Batch 2121/2250, Loss: 0.0324\n",
      "Epoch 16/25, Batch 2131/2250, Loss: 0.0127\n",
      "Epoch 16/25, Batch 2141/2250, Loss: 0.0034\n",
      "Epoch 16/25, Batch 2151/2250, Loss: 0.0830\n",
      "Epoch 16/25, Batch 2161/2250, Loss: 0.0319\n",
      "Epoch 16/25, Batch 2171/2250, Loss: 0.2270\n",
      "Epoch 16/25, Batch 2181/2250, Loss: 0.0326\n",
      "Epoch 16/25, Batch 2191/2250, Loss: 0.0015\n",
      "Epoch 16/25, Batch 2201/2250, Loss: 0.0461\n",
      "Epoch 16/25, Batch 2211/2250, Loss: 0.0255\n",
      "Epoch 16/25, Batch 2221/2250, Loss: 0.0707\n",
      "Epoch 16/25, Batch 2231/2250, Loss: 0.0532\n",
      "Epoch 16/25, Batch 2241/2250, Loss: 0.0356\n",
      "Epoch 16/25:\n",
      "Train Loss: 0.0520, Train Acc: 98.10%\n",
      "Val Loss: 0.0510, Val Acc: 98.15%\n",
      "------------------------------------------------------------\n",
      "Starting epoch 17/25...\n",
      "Starting training loop with 2250 batches...\n",
      "Successfully loaded first batch: images shape torch.Size([32, 3, 224, 224]), labels shape torch.Size([32])\n",
      "Epoch 17/25, Batch 1/2250, Loss: 0.0296\n",
      "Epoch 17/25, Batch 11/2250, Loss: 0.0499\n",
      "Epoch 17/25, Batch 21/2250, Loss: 0.0025\n",
      "Epoch 17/25, Batch 31/2250, Loss: 0.0052\n",
      "Epoch 17/25, Batch 41/2250, Loss: 0.0277\n",
      "Epoch 17/25, Batch 51/2250, Loss: 0.0038\n",
      "Epoch 17/25, Batch 61/2250, Loss: 0.1644\n",
      "Epoch 17/25, Batch 71/2250, Loss: 0.0090\n",
      "Epoch 17/25, Batch 81/2250, Loss: 0.0204\n",
      "Epoch 17/25, Batch 91/2250, Loss: 0.0120\n",
      "Epoch 17/25, Batch 101/2250, Loss: 0.0124\n",
      "Epoch 17/25, Batch 111/2250, Loss: 0.0124\n",
      "Epoch 17/25, Batch 121/2250, Loss: 0.0286\n",
      "Epoch 17/25, Batch 131/2250, Loss: 0.0612\n",
      "Epoch 17/25, Batch 141/2250, Loss: 0.0777\n",
      "Epoch 17/25, Batch 151/2250, Loss: 0.0527\n",
      "Epoch 17/25, Batch 161/2250, Loss: 0.0416\n",
      "Epoch 17/25, Batch 171/2250, Loss: 0.0685\n",
      "Epoch 17/25, Batch 181/2250, Loss: 0.0232\n",
      "Epoch 17/25, Batch 191/2250, Loss: 0.0153\n",
      "Epoch 17/25, Batch 201/2250, Loss: 0.2325\n",
      "Epoch 17/25, Batch 211/2250, Loss: 0.1125\n",
      "Epoch 17/25, Batch 221/2250, Loss: 0.0134\n",
      "Epoch 17/25, Batch 231/2250, Loss: 0.0309\n",
      "Epoch 17/25, Batch 241/2250, Loss: 0.0464\n",
      "Epoch 17/25, Batch 251/2250, Loss: 0.0124\n",
      "Epoch 17/25, Batch 261/2250, Loss: 0.0931\n",
      "Epoch 17/25, Batch 271/2250, Loss: 0.1005\n",
      "Epoch 17/25, Batch 281/2250, Loss: 0.0281\n",
      "Epoch 17/25, Batch 291/2250, Loss: 0.0819\n",
      "Epoch 17/25, Batch 301/2250, Loss: 0.0225\n",
      "Epoch 17/25, Batch 311/2250, Loss: 0.0574\n",
      "Epoch 17/25, Batch 321/2250, Loss: 0.0233\n",
      "Epoch 17/25, Batch 331/2250, Loss: 0.0187\n",
      "Epoch 17/25, Batch 341/2250, Loss: 0.0567\n",
      "Epoch 17/25, Batch 351/2250, Loss: 0.0058\n",
      "Epoch 17/25, Batch 361/2250, Loss: 0.2441\n",
      "Epoch 17/25, Batch 371/2250, Loss: 0.0539\n",
      "Epoch 17/25, Batch 381/2250, Loss: 0.0118\n",
      "Epoch 17/25, Batch 391/2250, Loss: 0.0674\n",
      "Epoch 17/25, Batch 401/2250, Loss: 0.0102\n",
      "Epoch 17/25, Batch 411/2250, Loss: 0.0743\n",
      "Epoch 17/25, Batch 421/2250, Loss: 0.0187\n",
      "Epoch 17/25, Batch 431/2250, Loss: 0.2052\n",
      "Epoch 17/25, Batch 441/2250, Loss: 0.0191\n",
      "Epoch 17/25, Batch 451/2250, Loss: 0.0191\n",
      "Epoch 17/25, Batch 461/2250, Loss: 0.0091\n",
      "Epoch 17/25, Batch 471/2250, Loss: 0.0867\n",
      "Epoch 17/25, Batch 481/2250, Loss: 0.0140\n",
      "Epoch 17/25, Batch 491/2250, Loss: 0.0281\n",
      "Epoch 17/25, Batch 501/2250, Loss: 0.0774\n",
      "Epoch 17/25, Batch 511/2250, Loss: 0.0150\n",
      "Epoch 17/25, Batch 521/2250, Loss: 0.0234\n",
      "Epoch 17/25, Batch 531/2250, Loss: 0.0226\n",
      "Epoch 17/25, Batch 541/2250, Loss: 0.0876\n",
      "Epoch 17/25, Batch 551/2250, Loss: 0.0071\n",
      "Epoch 17/25, Batch 561/2250, Loss: 0.1225\n",
      "Epoch 17/25, Batch 571/2250, Loss: 0.0456\n",
      "Epoch 17/25, Batch 581/2250, Loss: 0.0046\n",
      "Epoch 17/25, Batch 591/2250, Loss: 0.0363\n",
      "Epoch 17/25, Batch 601/2250, Loss: 0.0141\n",
      "Epoch 17/25, Batch 611/2250, Loss: 0.0113\n",
      "Epoch 17/25, Batch 621/2250, Loss: 0.0035\n",
      "Epoch 17/25, Batch 631/2250, Loss: 0.0802\n",
      "Epoch 17/25, Batch 641/2250, Loss: 0.0113\n",
      "Epoch 17/25, Batch 651/2250, Loss: 0.0363\n",
      "Epoch 17/25, Batch 661/2250, Loss: 0.0599\n",
      "Epoch 17/25, Batch 671/2250, Loss: 0.1515\n",
      "Epoch 17/25, Batch 681/2250, Loss: 0.0419\n",
      "Epoch 17/25, Batch 691/2250, Loss: 0.0055\n",
      "Epoch 17/25, Batch 701/2250, Loss: 0.0688\n",
      "Epoch 17/25, Batch 711/2250, Loss: 0.0029\n",
      "Epoch 17/25, Batch 721/2250, Loss: 0.0362\n",
      "Epoch 17/25, Batch 731/2250, Loss: 0.0229\n",
      "Epoch 17/25, Batch 741/2250, Loss: 0.0457\n",
      "Epoch 17/25, Batch 751/2250, Loss: 0.0459\n",
      "Epoch 17/25, Batch 761/2250, Loss: 0.0080\n",
      "Epoch 17/25, Batch 771/2250, Loss: 0.0860\n",
      "Epoch 17/25, Batch 781/2250, Loss: 0.0029\n",
      "Epoch 17/25, Batch 791/2250, Loss: 0.0378\n",
      "Epoch 17/25, Batch 801/2250, Loss: 0.1441\n",
      "Epoch 17/25, Batch 811/2250, Loss: 0.0142\n",
      "Epoch 17/25, Batch 821/2250, Loss: 0.1074\n",
      "Epoch 17/25, Batch 831/2250, Loss: 0.0022\n",
      "Epoch 17/25, Batch 841/2250, Loss: 0.0005\n",
      "Epoch 17/25, Batch 851/2250, Loss: 0.0185\n",
      "Epoch 17/25, Batch 861/2250, Loss: 0.0683\n",
      "Epoch 17/25, Batch 871/2250, Loss: 0.1592\n",
      "Epoch 17/25, Batch 881/2250, Loss: 0.0945\n",
      "Epoch 17/25, Batch 891/2250, Loss: 0.0030\n",
      "Epoch 17/25, Batch 901/2250, Loss: 0.0582\n",
      "Epoch 17/25, Batch 911/2250, Loss: 0.1951\n",
      "Epoch 17/25, Batch 921/2250, Loss: 0.0408\n",
      "Epoch 17/25, Batch 931/2250, Loss: 0.0187\n",
      "Epoch 17/25, Batch 941/2250, Loss: 0.0737\n",
      "Epoch 17/25, Batch 951/2250, Loss: 0.1123\n",
      "Epoch 17/25, Batch 961/2250, Loss: 0.0459\n",
      "Epoch 17/25, Batch 971/2250, Loss: 0.0146\n",
      "Epoch 17/25, Batch 981/2250, Loss: 0.0186\n",
      "Epoch 17/25, Batch 991/2250, Loss: 0.1880\n",
      "Epoch 17/25, Batch 1001/2250, Loss: 0.0209\n",
      "Epoch 17/25, Batch 1011/2250, Loss: 0.0152\n",
      "Epoch 17/25, Batch 1021/2250, Loss: 0.0198\n",
      "Epoch 17/25, Batch 1031/2250, Loss: 0.2147\n",
      "Epoch 17/25, Batch 1041/2250, Loss: 0.0144\n",
      "Epoch 17/25, Batch 1051/2250, Loss: 0.0728\n",
      "Epoch 17/25, Batch 1061/2250, Loss: 0.0051\n",
      "Epoch 17/25, Batch 1071/2250, Loss: 0.0026\n",
      "Epoch 17/25, Batch 1081/2250, Loss: 0.0367\n",
      "Epoch 17/25, Batch 1091/2250, Loss: 0.0508\n",
      "Epoch 17/25, Batch 1101/2250, Loss: 0.0979\n",
      "Epoch 17/25, Batch 1111/2250, Loss: 0.1577\n",
      "Epoch 17/25, Batch 1121/2250, Loss: 0.0137\n",
      "Epoch 17/25, Batch 1131/2250, Loss: 0.0655\n",
      "Epoch 17/25, Batch 1141/2250, Loss: 0.0041\n",
      "Epoch 17/25, Batch 1151/2250, Loss: 0.0364\n",
      "Epoch 17/25, Batch 1161/2250, Loss: 0.1771\n",
      "Epoch 17/25, Batch 1171/2250, Loss: 0.0023\n",
      "Epoch 17/25, Batch 1181/2250, Loss: 0.0715\n",
      "Epoch 17/25, Batch 1191/2250, Loss: 0.0457\n",
      "Epoch 17/25, Batch 1201/2250, Loss: 0.0038\n",
      "Epoch 17/25, Batch 1211/2250, Loss: 0.0739\n",
      "Epoch 17/25, Batch 1221/2250, Loss: 0.0519\n",
      "Epoch 17/25, Batch 1231/2250, Loss: 0.0123\n",
      "Epoch 17/25, Batch 1241/2250, Loss: 0.0141\n",
      "Epoch 17/25, Batch 1251/2250, Loss: 0.0113\n",
      "Epoch 17/25, Batch 1261/2250, Loss: 0.1343\n",
      "Epoch 17/25, Batch 1271/2250, Loss: 0.0214\n",
      "Epoch 17/25, Batch 1281/2250, Loss: 0.0574\n",
      "Epoch 17/25, Batch 1291/2250, Loss: 0.0825\n",
      "Epoch 17/25, Batch 1301/2250, Loss: 0.0128\n",
      "Epoch 17/25, Batch 1311/2250, Loss: 0.0149\n",
      "Epoch 17/25, Batch 1321/2250, Loss: 0.0746\n",
      "Epoch 17/25, Batch 1331/2250, Loss: 0.0161\n",
      "Epoch 17/25, Batch 1341/2250, Loss: 0.1312\n",
      "Epoch 17/25, Batch 1351/2250, Loss: 0.0982\n",
      "Epoch 17/25, Batch 1361/2250, Loss: 0.0088\n",
      "Epoch 17/25, Batch 1371/2250, Loss: 0.0365\n",
      "Epoch 17/25, Batch 1381/2250, Loss: 0.0593\n",
      "Epoch 17/25, Batch 1391/2250, Loss: 0.0710\n",
      "Epoch 17/25, Batch 1401/2250, Loss: 0.0961\n",
      "Epoch 17/25, Batch 1411/2250, Loss: 0.0265\n",
      "Epoch 17/25, Batch 1421/2250, Loss: 0.0391\n",
      "Epoch 17/25, Batch 1431/2250, Loss: 0.1388\n",
      "Epoch 17/25, Batch 1441/2250, Loss: 0.0559\n",
      "Epoch 17/25, Batch 1451/2250, Loss: 0.0157\n",
      "Epoch 17/25, Batch 1461/2250, Loss: 0.0832\n",
      "Epoch 17/25, Batch 1471/2250, Loss: 0.1547\n",
      "Epoch 17/25, Batch 1481/2250, Loss: 0.0959\n",
      "Epoch 17/25, Batch 1491/2250, Loss: 0.1752\n",
      "Epoch 17/25, Batch 1501/2250, Loss: 0.0968\n",
      "Epoch 17/25, Batch 1511/2250, Loss: 0.0256\n",
      "Epoch 17/25, Batch 1521/2250, Loss: 0.0039\n",
      "Epoch 17/25, Batch 1531/2250, Loss: 0.0236\n",
      "Epoch 17/25, Batch 1541/2250, Loss: 0.0834\n",
      "Epoch 17/25, Batch 1551/2250, Loss: 0.0043\n",
      "Epoch 17/25, Batch 1561/2250, Loss: 0.1113\n",
      "Epoch 17/25, Batch 1571/2250, Loss: 0.0220\n",
      "Epoch 17/25, Batch 1581/2250, Loss: 0.0553\n",
      "Epoch 17/25, Batch 1591/2250, Loss: 0.0256\n",
      "Epoch 17/25, Batch 1601/2250, Loss: 0.1817\n",
      "Epoch 17/25, Batch 1611/2250, Loss: 0.0163\n",
      "Epoch 17/25, Batch 1621/2250, Loss: 0.0260\n",
      "Epoch 17/25, Batch 1631/2250, Loss: 0.0193\n",
      "Epoch 17/25, Batch 1641/2250, Loss: 0.1299\n",
      "Epoch 17/25, Batch 1651/2250, Loss: 0.0079\n",
      "Epoch 17/25, Batch 1661/2250, Loss: 0.0830\n",
      "Epoch 17/25, Batch 1671/2250, Loss: 0.0499\n",
      "Epoch 17/25, Batch 1681/2250, Loss: 0.0403\n",
      "Epoch 17/25, Batch 1691/2250, Loss: 0.0807\n",
      "Epoch 17/25, Batch 1701/2250, Loss: 0.1121\n",
      "Epoch 17/25, Batch 1711/2250, Loss: 0.0040\n",
      "Epoch 17/25, Batch 1721/2250, Loss: 0.1727\n",
      "Epoch 17/25, Batch 1731/2250, Loss: 0.0283\n",
      "Epoch 17/25, Batch 1741/2250, Loss: 0.0459\n",
      "Epoch 17/25, Batch 1751/2250, Loss: 0.0485\n",
      "Epoch 17/25, Batch 1761/2250, Loss: 0.0819\n",
      "Epoch 17/25, Batch 1771/2250, Loss: 0.0795\n",
      "Epoch 17/25, Batch 1781/2250, Loss: 0.0234\n",
      "Epoch 17/25, Batch 1791/2250, Loss: 0.0102\n",
      "Epoch 17/25, Batch 1801/2250, Loss: 0.0314\n",
      "Epoch 17/25, Batch 1811/2250, Loss: 0.0185\n",
      "Epoch 17/25, Batch 1821/2250, Loss: 0.0079\n",
      "Epoch 17/25, Batch 1831/2250, Loss: 0.0256\n",
      "Epoch 17/25, Batch 1841/2250, Loss: 0.0134\n",
      "Epoch 17/25, Batch 1851/2250, Loss: 0.0488\n",
      "Epoch 17/25, Batch 1861/2250, Loss: 0.0114\n",
      "Epoch 17/25, Batch 1871/2250, Loss: 0.0142\n",
      "Epoch 17/25, Batch 1881/2250, Loss: 0.0609\n",
      "Epoch 17/25, Batch 1891/2250, Loss: 0.0190\n",
      "Epoch 17/25, Batch 1901/2250, Loss: 0.0330\n",
      "Epoch 17/25, Batch 1911/2250, Loss: 0.0455\n",
      "Epoch 17/25, Batch 1921/2250, Loss: 0.0716\n",
      "Epoch 17/25, Batch 1931/2250, Loss: 0.0020\n",
      "Epoch 17/25, Batch 1941/2250, Loss: 0.0013\n",
      "Epoch 17/25, Batch 1951/2250, Loss: 0.0583\n",
      "Epoch 17/25, Batch 1961/2250, Loss: 0.0165\n",
      "Epoch 17/25, Batch 1971/2250, Loss: 0.0879\n",
      "Epoch 17/25, Batch 1981/2250, Loss: 0.1435\n",
      "Epoch 17/25, Batch 1991/2250, Loss: 0.0088\n",
      "Epoch 17/25, Batch 2001/2250, Loss: 0.0809\n",
      "Epoch 17/25, Batch 2011/2250, Loss: 0.0153\n",
      "Epoch 17/25, Batch 2021/2250, Loss: 0.0775\n",
      "Epoch 17/25, Batch 2031/2250, Loss: 0.0544\n",
      "Epoch 17/25, Batch 2041/2250, Loss: 0.1633\n",
      "Epoch 17/25, Batch 2051/2250, Loss: 0.0023\n",
      "Epoch 17/25, Batch 2061/2250, Loss: 0.0411\n",
      "Epoch 17/25, Batch 2071/2250, Loss: 0.0068\n",
      "Epoch 17/25, Batch 2081/2250, Loss: 0.0704\n",
      "Epoch 17/25, Batch 2091/2250, Loss: 0.0160\n",
      "Epoch 17/25, Batch 2101/2250, Loss: 0.1213\n",
      "Epoch 17/25, Batch 2111/2250, Loss: 0.2026\n",
      "Epoch 17/25, Batch 2121/2250, Loss: 0.0103\n",
      "Epoch 17/25, Batch 2131/2250, Loss: 0.0539\n",
      "Epoch 17/25, Batch 2141/2250, Loss: 0.0566\n",
      "Epoch 17/25, Batch 2151/2250, Loss: 0.0387\n",
      "Epoch 17/25, Batch 2161/2250, Loss: 0.0053\n",
      "Epoch 17/25, Batch 2171/2250, Loss: 0.0940\n",
      "Epoch 17/25, Batch 2181/2250, Loss: 0.0738\n",
      "Epoch 17/25, Batch 2191/2250, Loss: 0.0273\n",
      "Epoch 17/25, Batch 2201/2250, Loss: 0.0026\n",
      "Epoch 17/25, Batch 2211/2250, Loss: 0.0017\n",
      "Epoch 17/25, Batch 2221/2250, Loss: 0.0292\n",
      "Epoch 17/25, Batch 2231/2250, Loss: 0.0492\n",
      "Epoch 17/25, Batch 2241/2250, Loss: 0.0065\n",
      "Epoch 17/25:\n",
      "Train Loss: 0.0496, Train Acc: 98.14%\n",
      "Val Loss: 0.0489, Val Acc: 98.32%\n",
      "------------------------------------------------------------\n",
      "Starting epoch 18/25...\n",
      "Starting training loop with 2250 batches...\n",
      "Successfully loaded first batch: images shape torch.Size([32, 3, 224, 224]), labels shape torch.Size([32])\n",
      "Epoch 18/25, Batch 1/2250, Loss: 0.0047\n",
      "Epoch 18/25, Batch 11/2250, Loss: 0.0295\n",
      "Epoch 18/25, Batch 21/2250, Loss: 0.0044\n",
      "Epoch 18/25, Batch 31/2250, Loss: 0.2953\n",
      "Epoch 18/25, Batch 41/2250, Loss: 0.0972\n",
      "Epoch 18/25, Batch 51/2250, Loss: 0.0372\n",
      "Epoch 18/25, Batch 61/2250, Loss: 0.0415\n",
      "Epoch 18/25, Batch 71/2250, Loss: 0.0054\n",
      "Epoch 18/25, Batch 81/2250, Loss: 0.1159\n",
      "Epoch 18/25, Batch 91/2250, Loss: 0.1017\n",
      "Epoch 18/25, Batch 101/2250, Loss: 0.0131\n",
      "Epoch 18/25, Batch 111/2250, Loss: 0.0010\n",
      "Epoch 18/25, Batch 121/2250, Loss: 0.0249\n",
      "Epoch 18/25, Batch 131/2250, Loss: 0.1120\n",
      "Epoch 18/25, Batch 141/2250, Loss: 0.1759\n",
      "Epoch 18/25, Batch 151/2250, Loss: 0.0363\n",
      "Epoch 18/25, Batch 161/2250, Loss: 0.0219\n",
      "Epoch 18/25, Batch 171/2250, Loss: 0.0045\n",
      "Epoch 18/25, Batch 181/2250, Loss: 0.0128\n",
      "Epoch 18/25, Batch 191/2250, Loss: 0.0448\n",
      "Epoch 18/25, Batch 201/2250, Loss: 0.0369\n",
      "Epoch 18/25, Batch 211/2250, Loss: 0.0046\n",
      "Epoch 18/25, Batch 221/2250, Loss: 0.0492\n",
      "Epoch 18/25, Batch 231/2250, Loss: 0.1881\n",
      "Epoch 18/25, Batch 241/2250, Loss: 0.0074\n",
      "Epoch 18/25, Batch 251/2250, Loss: 0.0076\n",
      "Epoch 18/25, Batch 261/2250, Loss: 0.0315\n",
      "Epoch 18/25, Batch 271/2250, Loss: 0.0050\n",
      "Epoch 18/25, Batch 281/2250, Loss: 0.0534\n",
      "Epoch 18/25, Batch 291/2250, Loss: 0.0835\n",
      "Epoch 18/25, Batch 301/2250, Loss: 0.0546\n",
      "Epoch 18/25, Batch 311/2250, Loss: 0.0228\n",
      "Epoch 18/25, Batch 321/2250, Loss: 0.0417\n",
      "Epoch 18/25, Batch 331/2250, Loss: 0.1927\n",
      "Epoch 18/25, Batch 341/2250, Loss: 0.0225\n",
      "Epoch 18/25, Batch 351/2250, Loss: 0.1576\n",
      "Epoch 18/25, Batch 361/2250, Loss: 0.1756\n",
      "Epoch 18/25, Batch 371/2250, Loss: 0.0993\n",
      "Epoch 18/25, Batch 381/2250, Loss: 0.0277\n",
      "Epoch 18/25, Batch 391/2250, Loss: 0.0037\n",
      "Epoch 18/25, Batch 401/2250, Loss: 0.0433\n",
      "Epoch 18/25, Batch 411/2250, Loss: 0.0018\n",
      "Epoch 18/25, Batch 421/2250, Loss: 0.0238\n",
      "Epoch 18/25, Batch 431/2250, Loss: 0.0196\n",
      "Epoch 18/25, Batch 441/2250, Loss: 0.0635\n",
      "Epoch 18/25, Batch 451/2250, Loss: 0.0507\n",
      "Epoch 18/25, Batch 461/2250, Loss: 0.0239\n",
      "Epoch 18/25, Batch 471/2250, Loss: 0.0178\n",
      "Epoch 18/25, Batch 481/2250, Loss: 0.0077\n",
      "Epoch 18/25, Batch 491/2250, Loss: 0.0156\n",
      "Epoch 18/25, Batch 501/2250, Loss: 0.0535\n",
      "Epoch 18/25, Batch 511/2250, Loss: 0.0285\n",
      "Epoch 18/25, Batch 521/2250, Loss: 0.0536\n",
      "Epoch 18/25, Batch 531/2250, Loss: 0.0234\n",
      "Epoch 18/25, Batch 541/2250, Loss: 0.1418\n",
      "Epoch 18/25, Batch 551/2250, Loss: 0.0181\n",
      "Epoch 18/25, Batch 561/2250, Loss: 0.0775\n",
      "Epoch 18/25, Batch 571/2250, Loss: 0.0070\n",
      "Epoch 18/25, Batch 581/2250, Loss: 0.0850\n",
      "Epoch 18/25, Batch 591/2250, Loss: 0.0055\n",
      "Epoch 18/25, Batch 601/2250, Loss: 0.0118\n",
      "Epoch 18/25, Batch 611/2250, Loss: 0.0774\n",
      "Epoch 18/25, Batch 621/2250, Loss: 0.0920\n",
      "Epoch 18/25, Batch 631/2250, Loss: 0.0087\n",
      "Epoch 18/25, Batch 641/2250, Loss: 0.1355\n",
      "Epoch 18/25, Batch 651/2250, Loss: 0.0178\n",
      "Epoch 18/25, Batch 661/2250, Loss: 0.0130\n",
      "Epoch 18/25, Batch 671/2250, Loss: 0.0124\n",
      "Epoch 18/25, Batch 681/2250, Loss: 0.0021\n",
      "Epoch 18/25, Batch 691/2250, Loss: 0.0301\n",
      "Epoch 18/25, Batch 701/2250, Loss: 0.0136\n",
      "Epoch 18/25, Batch 711/2250, Loss: 0.0020\n",
      "Epoch 18/25, Batch 721/2250, Loss: 0.0072\n",
      "Epoch 18/25, Batch 731/2250, Loss: 0.0066\n",
      "Epoch 18/25, Batch 741/2250, Loss: 0.0250\n",
      "Epoch 18/25, Batch 751/2250, Loss: 0.0134\n",
      "Epoch 18/25, Batch 761/2250, Loss: 0.0090\n",
      "Epoch 18/25, Batch 771/2250, Loss: 0.0033\n",
      "Epoch 18/25, Batch 781/2250, Loss: 0.0117\n",
      "Epoch 18/25, Batch 791/2250, Loss: 0.0638\n",
      "Epoch 18/25, Batch 801/2250, Loss: 0.0808\n",
      "Epoch 18/25, Batch 811/2250, Loss: 0.0369\n",
      "Epoch 18/25, Batch 821/2250, Loss: 0.0180\n",
      "Epoch 18/25, Batch 831/2250, Loss: 0.0174\n",
      "Epoch 18/25, Batch 841/2250, Loss: 0.0064\n",
      "Epoch 18/25, Batch 851/2250, Loss: 0.1347\n",
      "Epoch 18/25, Batch 861/2250, Loss: 0.0032\n",
      "Epoch 18/25, Batch 871/2250, Loss: 0.0045\n",
      "Epoch 18/25, Batch 881/2250, Loss: 0.1174\n",
      "Epoch 18/25, Batch 891/2250, Loss: 0.0103\n",
      "Epoch 18/25, Batch 901/2250, Loss: 0.0578\n",
      "Epoch 18/25, Batch 911/2250, Loss: 0.0007\n",
      "Epoch 18/25, Batch 921/2250, Loss: 0.0091\n",
      "Epoch 18/25, Batch 931/2250, Loss: 0.1253\n",
      "Epoch 18/25, Batch 941/2250, Loss: 0.1502\n",
      "Epoch 18/25, Batch 951/2250, Loss: 0.0494\n",
      "Epoch 18/25, Batch 961/2250, Loss: 0.0031\n",
      "Epoch 18/25, Batch 971/2250, Loss: 0.1806\n",
      "Epoch 18/25, Batch 981/2250, Loss: 0.0822\n",
      "Epoch 18/25, Batch 991/2250, Loss: 0.0594\n",
      "Epoch 18/25, Batch 1001/2250, Loss: 0.2031\n",
      "Epoch 18/25, Batch 1011/2250, Loss: 0.0282\n",
      "Epoch 18/25, Batch 1021/2250, Loss: 0.1705\n",
      "Epoch 18/25, Batch 1031/2250, Loss: 0.0052\n",
      "Epoch 18/25, Batch 1041/2250, Loss: 0.0018\n",
      "Epoch 18/25, Batch 1051/2250, Loss: 0.0089\n",
      "Epoch 18/25, Batch 1061/2250, Loss: 0.0183\n",
      "Epoch 18/25, Batch 1071/2250, Loss: 0.0610\n",
      "Epoch 18/25, Batch 1081/2250, Loss: 0.0389\n",
      "Epoch 18/25, Batch 1091/2250, Loss: 0.0350\n",
      "Epoch 18/25, Batch 1101/2250, Loss: 0.0294\n",
      "Epoch 18/25, Batch 1111/2250, Loss: 0.0511\n",
      "Epoch 18/25, Batch 1121/2250, Loss: 0.0035\n",
      "Epoch 18/25, Batch 1131/2250, Loss: 0.1012\n",
      "Epoch 18/25, Batch 1141/2250, Loss: 0.0018\n",
      "Epoch 18/25, Batch 1151/2250, Loss: 0.2148\n",
      "Epoch 18/25, Batch 1161/2250, Loss: 0.0298\n",
      "Epoch 18/25, Batch 1171/2250, Loss: 0.0236\n",
      "Epoch 18/25, Batch 1181/2250, Loss: 0.0702\n",
      "Epoch 18/25, Batch 1191/2250, Loss: 0.0297\n",
      "Epoch 18/25, Batch 1201/2250, Loss: 0.1003\n",
      "Epoch 18/25, Batch 1211/2250, Loss: 0.0124\n",
      "Epoch 18/25, Batch 1221/2250, Loss: 0.0134\n",
      "Epoch 18/25, Batch 1231/2250, Loss: 0.0110\n",
      "Epoch 18/25, Batch 1241/2250, Loss: 0.0026\n",
      "Epoch 18/25, Batch 1251/2250, Loss: 0.1738\n",
      "Epoch 18/25, Batch 1261/2250, Loss: 0.0091\n",
      "Epoch 18/25, Batch 1271/2250, Loss: 0.2176\n",
      "Epoch 18/25, Batch 1281/2250, Loss: 0.0285\n",
      "Epoch 18/25, Batch 1291/2250, Loss: 0.0253\n",
      "Epoch 18/25, Batch 1301/2250, Loss: 0.0021\n",
      "Epoch 18/25, Batch 1311/2250, Loss: 0.0025\n",
      "Epoch 18/25, Batch 1321/2250, Loss: 0.0138\n",
      "Epoch 18/25, Batch 1331/2250, Loss: 0.1006\n",
      "Epoch 18/25, Batch 1341/2250, Loss: 0.1715\n",
      "Epoch 18/25, Batch 1351/2250, Loss: 0.0803\n",
      "Epoch 18/25, Batch 1361/2250, Loss: 0.0101\n",
      "Epoch 18/25, Batch 1371/2250, Loss: 0.0170\n",
      "Epoch 18/25, Batch 1381/2250, Loss: 0.0153\n",
      "Epoch 18/25, Batch 1391/2250, Loss: 0.0523\n",
      "Epoch 18/25, Batch 1401/2250, Loss: 0.0054\n",
      "Epoch 18/25, Batch 1411/2250, Loss: 0.0027\n",
      "Epoch 18/25, Batch 1421/2250, Loss: 0.0390\n",
      "Epoch 18/25, Batch 1431/2250, Loss: 0.0103\n",
      "Epoch 18/25, Batch 1441/2250, Loss: 0.1085\n",
      "Epoch 18/25, Batch 1451/2250, Loss: 0.1259\n",
      "Epoch 18/25, Batch 1461/2250, Loss: 0.0587\n",
      "Epoch 18/25, Batch 1471/2250, Loss: 0.0115\n",
      "Epoch 18/25, Batch 1481/2250, Loss: 0.0923\n",
      "Epoch 18/25, Batch 1491/2250, Loss: 0.0277\n",
      "Epoch 18/25, Batch 1501/2250, Loss: 0.0597\n",
      "Epoch 18/25, Batch 1511/2250, Loss: 0.0294\n",
      "Epoch 18/25, Batch 1521/2250, Loss: 0.0645\n",
      "Epoch 18/25, Batch 1531/2250, Loss: 0.0111\n",
      "Epoch 18/25, Batch 1541/2250, Loss: 0.0476\n",
      "Epoch 18/25, Batch 1551/2250, Loss: 0.0245\n",
      "Epoch 18/25, Batch 1561/2250, Loss: 0.0204\n",
      "Epoch 18/25, Batch 1571/2250, Loss: 0.0339\n",
      "Epoch 18/25, Batch 1581/2250, Loss: 0.0362\n",
      "Epoch 18/25, Batch 1591/2250, Loss: 0.0332\n",
      "Epoch 18/25, Batch 1601/2250, Loss: 0.0015\n",
      "Epoch 18/25, Batch 1611/2250, Loss: 0.1336\n",
      "Epoch 18/25, Batch 1621/2250, Loss: 0.0306\n",
      "Epoch 18/25, Batch 1631/2250, Loss: 0.1012\n",
      "Epoch 18/25, Batch 1641/2250, Loss: 0.0428\n",
      "Epoch 18/25, Batch 1651/2250, Loss: 0.0092\n",
      "Epoch 18/25, Batch 1661/2250, Loss: 0.0071\n",
      "Epoch 18/25, Batch 1671/2250, Loss: 0.1376\n",
      "Epoch 18/25, Batch 1681/2250, Loss: 0.0091\n",
      "Epoch 18/25, Batch 1691/2250, Loss: 0.0079\n",
      "Epoch 18/25, Batch 1701/2250, Loss: 0.0958\n",
      "Epoch 18/25, Batch 1711/2250, Loss: 0.1056\n",
      "Epoch 18/25, Batch 1721/2250, Loss: 0.0032\n",
      "Epoch 18/25, Batch 1731/2250, Loss: 0.0136\n",
      "Epoch 18/25, Batch 1741/2250, Loss: 0.1229\n",
      "Epoch 18/25, Batch 1751/2250, Loss: 0.0023\n",
      "Epoch 18/25, Batch 1761/2250, Loss: 0.0078\n",
      "Epoch 18/25, Batch 1771/2250, Loss: 0.0080\n",
      "Epoch 18/25, Batch 1781/2250, Loss: 0.0092\n",
      "Epoch 18/25, Batch 1791/2250, Loss: 0.0077\n",
      "Epoch 18/25, Batch 1801/2250, Loss: 0.0245\n",
      "Epoch 18/25, Batch 1811/2250, Loss: 0.0047\n",
      "Epoch 18/25, Batch 1821/2250, Loss: 0.0317\n",
      "Epoch 18/25, Batch 1831/2250, Loss: 0.0063\n",
      "Epoch 18/25, Batch 1841/2250, Loss: 0.0569\n",
      "Epoch 18/25, Batch 1851/2250, Loss: 0.3180\n",
      "Epoch 18/25, Batch 1861/2250, Loss: 0.0142\n",
      "Epoch 18/25, Batch 1871/2250, Loss: 0.0079\n",
      "Epoch 18/25, Batch 1881/2250, Loss: 0.0219\n",
      "Epoch 18/25, Batch 1891/2250, Loss: 0.0103\n",
      "Epoch 18/25, Batch 1901/2250, Loss: 0.0375\n",
      "Epoch 18/25, Batch 1911/2250, Loss: 0.0431\n",
      "Epoch 18/25, Batch 1921/2250, Loss: 0.0137\n",
      "Epoch 18/25, Batch 1931/2250, Loss: 0.0757\n",
      "Epoch 18/25, Batch 1941/2250, Loss: 0.0255\n",
      "Epoch 18/25, Batch 1951/2250, Loss: 0.0167\n",
      "Epoch 18/25, Batch 1961/2250, Loss: 0.0059\n",
      "Epoch 18/25, Batch 1971/2250, Loss: 0.1697\n",
      "Epoch 18/25, Batch 1981/2250, Loss: 0.0063\n",
      "Epoch 18/25, Batch 1991/2250, Loss: 0.0335\n",
      "Epoch 18/25, Batch 2001/2250, Loss: 0.0918\n",
      "Epoch 18/25, Batch 2011/2250, Loss: 0.0165\n",
      "Epoch 18/25, Batch 2021/2250, Loss: 0.0249\n",
      "Epoch 18/25, Batch 2031/2250, Loss: 0.0141\n",
      "Epoch 18/25, Batch 2041/2250, Loss: 0.0388\n",
      "Epoch 18/25, Batch 2051/2250, Loss: 0.0499\n",
      "Epoch 18/25, Batch 2061/2250, Loss: 0.0197\n",
      "Epoch 18/25, Batch 2071/2250, Loss: 0.2055\n",
      "Epoch 18/25, Batch 2081/2250, Loss: 0.1381\n",
      "Epoch 18/25, Batch 2091/2250, Loss: 0.0303\n",
      "Epoch 18/25, Batch 2101/2250, Loss: 0.0955\n",
      "Epoch 18/25, Batch 2111/2250, Loss: 0.0446\n",
      "Epoch 18/25, Batch 2121/2250, Loss: 0.0351\n",
      "Epoch 18/25, Batch 2131/2250, Loss: 0.0708\n",
      "Epoch 18/25, Batch 2141/2250, Loss: 0.0414\n",
      "Epoch 18/25, Batch 2151/2250, Loss: 0.0472\n",
      "Epoch 18/25, Batch 2161/2250, Loss: 0.0251\n",
      "Epoch 18/25, Batch 2171/2250, Loss: 0.0213\n",
      "Epoch 18/25, Batch 2181/2250, Loss: 0.1002\n",
      "Epoch 18/25, Batch 2191/2250, Loss: 0.1242\n",
      "Epoch 18/25, Batch 2201/2250, Loss: 0.0046\n",
      "Epoch 18/25, Batch 2211/2250, Loss: 0.0262\n",
      "Epoch 18/25, Batch 2221/2250, Loss: 0.0487\n",
      "Epoch 18/25, Batch 2231/2250, Loss: 0.0077\n",
      "Epoch 18/25, Batch 2241/2250, Loss: 0.0519\n",
      "Epoch 18/25:\n",
      "Train Loss: 0.0462, Train Acc: 98.33%\n",
      "Val Loss: 0.0399, Val Acc: 98.60%\n",
      "------------------------------------------------------------\n",
      "Starting epoch 19/25...\n",
      "Starting training loop with 2250 batches...\n",
      "Successfully loaded first batch: images shape torch.Size([32, 3, 224, 224]), labels shape torch.Size([32])\n",
      "Epoch 19/25, Batch 1/2250, Loss: 0.0467\n",
      "Epoch 19/25, Batch 11/2250, Loss: 0.0322\n",
      "Epoch 19/25, Batch 21/2250, Loss: 0.1442\n",
      "Epoch 19/25, Batch 31/2250, Loss: 0.0108\n",
      "Epoch 19/25, Batch 41/2250, Loss: 0.0044\n",
      "Epoch 19/25, Batch 51/2250, Loss: 0.1051\n",
      "Epoch 19/25, Batch 61/2250, Loss: 0.0399\n",
      "Epoch 19/25, Batch 71/2250, Loss: 0.1461\n",
      "Epoch 19/25, Batch 81/2250, Loss: 0.0073\n",
      "Epoch 19/25, Batch 91/2250, Loss: 0.0156\n",
      "Epoch 19/25, Batch 101/2250, Loss: 0.1204\n",
      "Epoch 19/25, Batch 111/2250, Loss: 0.0067\n",
      "Epoch 19/25, Batch 121/2250, Loss: 0.0157\n",
      "Epoch 19/25, Batch 131/2250, Loss: 0.0147\n",
      "Epoch 19/25, Batch 141/2250, Loss: 0.0522\n",
      "Epoch 19/25, Batch 151/2250, Loss: 0.0013\n",
      "Epoch 19/25, Batch 161/2250, Loss: 0.0641\n",
      "Epoch 19/25, Batch 171/2250, Loss: 0.3137\n",
      "Epoch 19/25, Batch 181/2250, Loss: 0.0486\n",
      "Epoch 19/25, Batch 191/2250, Loss: 0.0663\n",
      "Epoch 19/25, Batch 201/2250, Loss: 0.0672\n",
      "Epoch 19/25, Batch 211/2250, Loss: 0.0424\n",
      "Epoch 19/25, Batch 221/2250, Loss: 0.0426\n",
      "Epoch 19/25, Batch 231/2250, Loss: 0.0038\n",
      "Epoch 19/25, Batch 241/2250, Loss: 0.0103\n",
      "Epoch 19/25, Batch 251/2250, Loss: 0.0492\n",
      "Epoch 19/25, Batch 261/2250, Loss: 0.0428\n",
      "Epoch 19/25, Batch 271/2250, Loss: 0.0031\n",
      "Epoch 19/25, Batch 281/2250, Loss: 0.0171\n",
      "Epoch 19/25, Batch 291/2250, Loss: 0.1007\n",
      "Epoch 19/25, Batch 301/2250, Loss: 0.0420\n",
      "Epoch 19/25, Batch 311/2250, Loss: 0.0192\n",
      "Epoch 19/25, Batch 321/2250, Loss: 0.0085\n",
      "Epoch 19/25, Batch 331/2250, Loss: 0.0411\n",
      "Epoch 19/25, Batch 341/2250, Loss: 0.0025\n",
      "Epoch 19/25, Batch 351/2250, Loss: 0.0238\n",
      "Epoch 19/25, Batch 361/2250, Loss: 0.0220\n",
      "Epoch 19/25, Batch 371/2250, Loss: 0.1023\n",
      "Epoch 19/25, Batch 381/2250, Loss: 0.0048\n",
      "Epoch 19/25, Batch 391/2250, Loss: 0.0527\n",
      "Epoch 19/25, Batch 401/2250, Loss: 0.0217\n",
      "Epoch 19/25, Batch 411/2250, Loss: 0.0043\n",
      "Epoch 19/25, Batch 421/2250, Loss: 0.0024\n",
      "Epoch 19/25, Batch 431/2250, Loss: 0.0590\n",
      "Epoch 19/25, Batch 441/2250, Loss: 0.0198\n",
      "Epoch 19/25, Batch 451/2250, Loss: 0.0356\n",
      "Epoch 19/25, Batch 461/2250, Loss: 0.0074\n",
      "Epoch 19/25, Batch 471/2250, Loss: 0.0972\n",
      "Epoch 19/25, Batch 481/2250, Loss: 0.0105\n",
      "Epoch 19/25, Batch 491/2250, Loss: 0.0144\n",
      "Epoch 19/25, Batch 501/2250, Loss: 0.0164\n",
      "Epoch 19/25, Batch 511/2250, Loss: 0.0037\n",
      "Epoch 19/25, Batch 521/2250, Loss: 0.0178\n",
      "Epoch 19/25, Batch 531/2250, Loss: 0.0470\n",
      "Epoch 19/25, Batch 541/2250, Loss: 0.1954\n",
      "Epoch 19/25, Batch 551/2250, Loss: 0.0113\n",
      "Epoch 19/25, Batch 561/2250, Loss: 0.0051\n",
      "Epoch 19/25, Batch 571/2250, Loss: 0.1354\n",
      "Epoch 19/25, Batch 581/2250, Loss: 0.0266\n",
      "Epoch 19/25, Batch 591/2250, Loss: 0.0280\n",
      "Epoch 19/25, Batch 601/2250, Loss: 0.0049\n",
      "Epoch 19/25, Batch 611/2250, Loss: 0.0258\n",
      "Epoch 19/25, Batch 621/2250, Loss: 0.0042\n",
      "Epoch 19/25, Batch 631/2250, Loss: 0.0044\n",
      "Epoch 19/25, Batch 641/2250, Loss: 0.0203\n",
      "Epoch 19/25, Batch 651/2250, Loss: 0.1250\n",
      "Epoch 19/25, Batch 661/2250, Loss: 0.0389\n",
      "Epoch 19/25, Batch 671/2250, Loss: 0.0205\n",
      "Epoch 19/25, Batch 681/2250, Loss: 0.0394\n",
      "Epoch 19/25, Batch 691/2250, Loss: 0.0262\n",
      "Epoch 19/25, Batch 701/2250, Loss: 0.1528\n",
      "Epoch 19/25, Batch 711/2250, Loss: 0.1041\n",
      "Epoch 19/25, Batch 721/2250, Loss: 0.0129\n",
      "Epoch 19/25, Batch 731/2250, Loss: 0.0117\n",
      "Epoch 19/25, Batch 741/2250, Loss: 0.0070\n",
      "Epoch 19/25, Batch 751/2250, Loss: 0.0883\n",
      "Epoch 19/25, Batch 761/2250, Loss: 0.1089\n",
      "Epoch 19/25, Batch 771/2250, Loss: 0.0405\n",
      "Epoch 19/25, Batch 781/2250, Loss: 0.0402\n",
      "Epoch 19/25, Batch 791/2250, Loss: 0.0683\n",
      "Epoch 19/25, Batch 801/2250, Loss: 0.0262\n",
      "Epoch 19/25, Batch 811/2250, Loss: 0.0839\n",
      "Epoch 19/25, Batch 821/2250, Loss: 0.0159\n",
      "Epoch 19/25, Batch 831/2250, Loss: 0.0124\n",
      "Epoch 19/25, Batch 841/2250, Loss: 0.0330\n",
      "Epoch 19/25, Batch 851/2250, Loss: 0.0078\n",
      "Epoch 19/25, Batch 861/2250, Loss: 0.0191\n",
      "Epoch 19/25, Batch 871/2250, Loss: 0.0045\n",
      "Epoch 19/25, Batch 881/2250, Loss: 0.1380\n",
      "Epoch 19/25, Batch 891/2250, Loss: 0.0788\n",
      "Epoch 19/25, Batch 901/2250, Loss: 0.0084\n",
      "Epoch 19/25, Batch 911/2250, Loss: 0.0149\n",
      "Epoch 19/25, Batch 921/2250, Loss: 0.0867\n",
      "Epoch 19/25, Batch 931/2250, Loss: 0.0007\n",
      "Epoch 19/25, Batch 941/2250, Loss: 0.0333\n",
      "Epoch 19/25, Batch 951/2250, Loss: 0.1821\n",
      "Epoch 19/25, Batch 961/2250, Loss: 0.0473\n",
      "Epoch 19/25, Batch 971/2250, Loss: 0.0968\n",
      "Epoch 19/25, Batch 981/2250, Loss: 0.0331\n",
      "Epoch 19/25, Batch 991/2250, Loss: 0.0049\n",
      "Epoch 19/25, Batch 1001/2250, Loss: 0.0356\n",
      "Epoch 19/25, Batch 1011/2250, Loss: 0.0498\n",
      "Epoch 19/25, Batch 1021/2250, Loss: 0.0101\n",
      "Epoch 19/25, Batch 1031/2250, Loss: 0.0220\n",
      "Epoch 19/25, Batch 1041/2250, Loss: 0.0355\n",
      "Epoch 19/25, Batch 1051/2250, Loss: 0.0144\n",
      "Epoch 19/25, Batch 1061/2250, Loss: 0.0416\n",
      "Epoch 19/25, Batch 1071/2250, Loss: 0.0925\n",
      "Epoch 19/25, Batch 1081/2250, Loss: 0.0107\n",
      "Epoch 19/25, Batch 1091/2250, Loss: 0.0299\n",
      "Epoch 19/25, Batch 1101/2250, Loss: 0.0492\n",
      "Epoch 19/25, Batch 1111/2250, Loss: 0.1178\n",
      "Epoch 19/25, Batch 1121/2250, Loss: 0.1230\n",
      "Epoch 19/25, Batch 1131/2250, Loss: 0.0046\n",
      "Epoch 19/25, Batch 1141/2250, Loss: 0.0635\n",
      "Epoch 19/25, Batch 1151/2250, Loss: 0.1384\n",
      "Epoch 19/25, Batch 1161/2250, Loss: 0.0706\n",
      "Epoch 19/25, Batch 1171/2250, Loss: 0.0612\n",
      "Epoch 19/25, Batch 1181/2250, Loss: 0.0879\n",
      "Epoch 19/25, Batch 1191/2250, Loss: 0.0463\n",
      "Epoch 19/25, Batch 1201/2250, Loss: 0.0224\n",
      "Epoch 19/25, Batch 1211/2250, Loss: 0.0036\n",
      "Epoch 19/25, Batch 1221/2250, Loss: 0.0207\n",
      "Epoch 19/25, Batch 1231/2250, Loss: 0.0185\n",
      "Epoch 19/25, Batch 1241/2250, Loss: 0.0203\n",
      "Epoch 19/25, Batch 1251/2250, Loss: 0.1051\n",
      "Epoch 19/25, Batch 1261/2250, Loss: 0.0113\n",
      "Epoch 19/25, Batch 1271/2250, Loss: 0.0272\n",
      "Epoch 19/25, Batch 1281/2250, Loss: 0.0134\n",
      "Epoch 19/25, Batch 1291/2250, Loss: 0.0337\n",
      "Epoch 19/25, Batch 1301/2250, Loss: 0.0789\n",
      "Epoch 19/25, Batch 1311/2250, Loss: 0.0444\n",
      "Epoch 19/25, Batch 1321/2250, Loss: 0.0098\n",
      "Epoch 19/25, Batch 1331/2250, Loss: 0.0057\n",
      "Epoch 19/25, Batch 1341/2250, Loss: 0.1382\n",
      "Epoch 19/25, Batch 1351/2250, Loss: 0.0312\n",
      "Epoch 19/25, Batch 1361/2250, Loss: 0.0941\n",
      "Epoch 19/25, Batch 1371/2250, Loss: 0.0207\n",
      "Epoch 19/25, Batch 1381/2250, Loss: 0.0108\n",
      "Epoch 19/25, Batch 1391/2250, Loss: 0.0107\n",
      "Epoch 19/25, Batch 1401/2250, Loss: 0.1884\n",
      "Epoch 19/25, Batch 1411/2250, Loss: 0.0041\n",
      "Epoch 19/25, Batch 1421/2250, Loss: 0.0443\n",
      "Epoch 19/25, Batch 1431/2250, Loss: 0.0099\n",
      "Epoch 19/25, Batch 1441/2250, Loss: 0.0145\n",
      "Epoch 19/25, Batch 1451/2250, Loss: 0.2418\n",
      "Epoch 19/25, Batch 1461/2250, Loss: 0.0064\n",
      "Epoch 19/25, Batch 1471/2250, Loss: 0.0337\n",
      "Epoch 19/25, Batch 1481/2250, Loss: 0.2431\n",
      "Epoch 19/25, Batch 1491/2250, Loss: 0.0584\n",
      "Epoch 19/25, Batch 1501/2250, Loss: 0.0252\n",
      "Epoch 19/25, Batch 1511/2250, Loss: 0.0169\n",
      "Epoch 19/25, Batch 1521/2250, Loss: 0.0344\n",
      "Epoch 19/25, Batch 1531/2250, Loss: 0.0176\n",
      "Epoch 19/25, Batch 1541/2250, Loss: 0.0422\n",
      "Epoch 19/25, Batch 1551/2250, Loss: 0.0511\n",
      "Epoch 19/25, Batch 1561/2250, Loss: 0.0085\n",
      "Epoch 19/25, Batch 1571/2250, Loss: 0.0944\n",
      "Epoch 19/25, Batch 1581/2250, Loss: 0.1002\n",
      "Epoch 19/25, Batch 1591/2250, Loss: 0.0135\n",
      "Epoch 19/25, Batch 1601/2250, Loss: 0.0185\n",
      "Epoch 19/25, Batch 1611/2250, Loss: 0.0306\n",
      "Epoch 19/25, Batch 1621/2250, Loss: 0.0059\n",
      "Epoch 19/25, Batch 1631/2250, Loss: 0.0067\n",
      "Epoch 19/25, Batch 1641/2250, Loss: 0.0096\n",
      "Epoch 19/25, Batch 1651/2250, Loss: 0.0415\n",
      "Epoch 19/25, Batch 1661/2250, Loss: 0.0097\n",
      "Epoch 19/25, Batch 1671/2250, Loss: 0.0010\n",
      "Epoch 19/25, Batch 1681/2250, Loss: 0.0030\n",
      "Epoch 19/25, Batch 1691/2250, Loss: 0.0077\n",
      "Epoch 19/25, Batch 1701/2250, Loss: 0.0217\n",
      "Epoch 19/25, Batch 1711/2250, Loss: 0.1793\n",
      "Epoch 19/25, Batch 1721/2250, Loss: 0.0143\n",
      "Epoch 19/25, Batch 1731/2250, Loss: 0.0349\n",
      "Epoch 19/25, Batch 1741/2250, Loss: 0.1281\n",
      "Epoch 19/25, Batch 1751/2250, Loss: 0.0246\n",
      "Epoch 19/25, Batch 1761/2250, Loss: 0.0080\n",
      "Epoch 19/25, Batch 1771/2250, Loss: 0.0789\n",
      "Epoch 19/25, Batch 1781/2250, Loss: 0.0046\n",
      "Epoch 19/25, Batch 1791/2250, Loss: 0.0270\n",
      "Epoch 19/25, Batch 1801/2250, Loss: 0.0259\n",
      "Epoch 19/25, Batch 1811/2250, Loss: 0.0055\n",
      "Epoch 19/25, Batch 1821/2250, Loss: 0.1427\n",
      "Epoch 19/25, Batch 1831/2250, Loss: 0.0031\n",
      "Epoch 19/25, Batch 1841/2250, Loss: 0.0657\n",
      "Epoch 19/25, Batch 1851/2250, Loss: 0.0099\n",
      "Epoch 19/25, Batch 1861/2250, Loss: 0.0513\n",
      "Epoch 19/25, Batch 1871/2250, Loss: 0.0910\n",
      "Epoch 19/25, Batch 1881/2250, Loss: 0.1219\n",
      "Epoch 19/25, Batch 1891/2250, Loss: 0.0267\n",
      "Epoch 19/25, Batch 1901/2250, Loss: 0.0210\n",
      "Epoch 19/25, Batch 1911/2250, Loss: 0.1146\n",
      "Epoch 19/25, Batch 1921/2250, Loss: 0.0766\n",
      "Epoch 19/25, Batch 1931/2250, Loss: 0.0194\n",
      "Epoch 19/25, Batch 1941/2250, Loss: 0.0195\n",
      "Epoch 19/25, Batch 1951/2250, Loss: 0.0390\n",
      "Epoch 19/25, Batch 1961/2250, Loss: 0.0079\n",
      "Epoch 19/25, Batch 1971/2250, Loss: 0.0046\n",
      "Epoch 19/25, Batch 1981/2250, Loss: 0.1664\n",
      "Epoch 19/25, Batch 1991/2250, Loss: 0.1034\n",
      "Epoch 19/25, Batch 2001/2250, Loss: 0.0009\n",
      "Epoch 19/25, Batch 2011/2250, Loss: 0.0280\n",
      "Epoch 19/25, Batch 2021/2250, Loss: 0.0028\n",
      "Epoch 19/25, Batch 2031/2250, Loss: 0.1074\n",
      "Epoch 19/25, Batch 2041/2250, Loss: 0.0143\n",
      "Epoch 19/25, Batch 2051/2250, Loss: 0.0087\n",
      "Epoch 19/25, Batch 2061/2250, Loss: 0.0024\n",
      "Epoch 19/25, Batch 2071/2250, Loss: 0.0126\n",
      "Epoch 19/25, Batch 2081/2250, Loss: 0.0313\n",
      "Epoch 19/25, Batch 2091/2250, Loss: 0.0702\n",
      "Epoch 19/25, Batch 2101/2250, Loss: 0.0237\n",
      "Epoch 19/25, Batch 2111/2250, Loss: 0.0272\n",
      "Epoch 19/25, Batch 2121/2250, Loss: 0.1637\n",
      "Epoch 19/25, Batch 2131/2250, Loss: 0.0182\n",
      "Epoch 19/25, Batch 2141/2250, Loss: 0.0037\n",
      "Epoch 19/25, Batch 2151/2250, Loss: 0.0704\n",
      "Epoch 19/25, Batch 2161/2250, Loss: 0.0050\n",
      "Epoch 19/25, Batch 2171/2250, Loss: 0.1908\n",
      "Epoch 19/25, Batch 2181/2250, Loss: 0.0094\n",
      "Epoch 19/25, Batch 2191/2250, Loss: 0.0650\n",
      "Epoch 19/25, Batch 2201/2250, Loss: 0.0407\n",
      "Epoch 19/25, Batch 2211/2250, Loss: 0.0999\n",
      "Epoch 19/25, Batch 2221/2250, Loss: 0.1043\n",
      "Epoch 19/25, Batch 2231/2250, Loss: 0.0106\n",
      "Epoch 19/25, Batch 2241/2250, Loss: 0.0230\n",
      "Epoch 19/25:\n",
      "Train Loss: 0.0477, Train Acc: 98.26%\n",
      "Val Loss: 0.0293, Val Acc: 98.91%\n",
      "------------------------------------------------------------\n",
      "Starting epoch 20/25...\n",
      "Starting training loop with 2250 batches...\n",
      "Successfully loaded first batch: images shape torch.Size([32, 3, 224, 224]), labels shape torch.Size([32])\n",
      "Epoch 20/25, Batch 1/2250, Loss: 0.0508\n",
      "Epoch 20/25, Batch 11/2250, Loss: 0.0019\n",
      "Epoch 20/25, Batch 21/2250, Loss: 0.0354\n",
      "Epoch 20/25, Batch 31/2250, Loss: 0.0403\n",
      "Epoch 20/25, Batch 41/2250, Loss: 0.2232\n",
      "Epoch 20/25, Batch 51/2250, Loss: 0.1378\n",
      "Epoch 20/25, Batch 61/2250, Loss: 0.0081\n",
      "Epoch 20/25, Batch 71/2250, Loss: 0.0069\n",
      "Epoch 20/25, Batch 81/2250, Loss: 0.0200\n",
      "Epoch 20/25, Batch 91/2250, Loss: 0.0236\n",
      "Epoch 20/25, Batch 101/2250, Loss: 0.0160\n",
      "Epoch 20/25, Batch 111/2250, Loss: 0.0419\n",
      "Epoch 20/25, Batch 121/2250, Loss: 0.0331\n",
      "Epoch 20/25, Batch 131/2250, Loss: 0.0010\n",
      "Epoch 20/25, Batch 141/2250, Loss: 0.0090\n",
      "Epoch 20/25, Batch 151/2250, Loss: 0.1158\n",
      "Epoch 20/25, Batch 161/2250, Loss: 0.0279\n",
      "Epoch 20/25, Batch 171/2250, Loss: 0.0135\n",
      "Epoch 20/25, Batch 181/2250, Loss: 0.1311\n",
      "Epoch 20/25, Batch 191/2250, Loss: 0.0351\n",
      "Epoch 20/25, Batch 201/2250, Loss: 0.1166\n",
      "Epoch 20/25, Batch 211/2250, Loss: 0.0215\n",
      "Epoch 20/25, Batch 221/2250, Loss: 0.0037\n",
      "Epoch 20/25, Batch 231/2250, Loss: 0.0579\n",
      "Epoch 20/25, Batch 241/2250, Loss: 0.0041\n",
      "Epoch 20/25, Batch 251/2250, Loss: 0.0079\n",
      "Epoch 20/25, Batch 261/2250, Loss: 0.0041\n",
      "Epoch 20/25, Batch 271/2250, Loss: 0.1085\n",
      "Epoch 20/25, Batch 281/2250, Loss: 0.0366\n",
      "Epoch 20/25, Batch 291/2250, Loss: 0.0304\n",
      "Epoch 20/25, Batch 301/2250, Loss: 0.0526\n",
      "Epoch 20/25, Batch 311/2250, Loss: 0.0019\n",
      "Epoch 20/25, Batch 321/2250, Loss: 0.0607\n",
      "Epoch 20/25, Batch 331/2250, Loss: 0.0111\n",
      "Epoch 20/25, Batch 341/2250, Loss: 0.0028\n",
      "Epoch 20/25, Batch 351/2250, Loss: 0.0176\n",
      "Epoch 20/25, Batch 361/2250, Loss: 0.0811\n",
      "Epoch 20/25, Batch 371/2250, Loss: 0.0127\n",
      "Epoch 20/25, Batch 381/2250, Loss: 0.2259\n",
      "Epoch 20/25, Batch 391/2250, Loss: 0.0140\n",
      "Epoch 20/25, Batch 401/2250, Loss: 0.0202\n",
      "Epoch 20/25, Batch 411/2250, Loss: 0.0509\n",
      "Epoch 20/25, Batch 421/2250, Loss: 0.0471\n",
      "Epoch 20/25, Batch 431/2250, Loss: 0.0008\n",
      "Epoch 20/25, Batch 441/2250, Loss: 0.1589\n",
      "Epoch 20/25, Batch 451/2250, Loss: 0.1086\n",
      "Epoch 20/25, Batch 461/2250, Loss: 0.0800\n",
      "Epoch 20/25, Batch 471/2250, Loss: 0.1239\n",
      "Epoch 20/25, Batch 481/2250, Loss: 0.0175\n",
      "Epoch 20/25, Batch 491/2250, Loss: 0.0133\n",
      "Epoch 20/25, Batch 501/2250, Loss: 0.0139\n",
      "Epoch 20/25, Batch 511/2250, Loss: 0.0037\n",
      "Epoch 20/25, Batch 521/2250, Loss: 0.0543\n",
      "Epoch 20/25, Batch 531/2250, Loss: 0.1754\n",
      "Epoch 20/25, Batch 541/2250, Loss: 0.0039\n",
      "Epoch 20/25, Batch 551/2250, Loss: 0.0444\n",
      "Epoch 20/25, Batch 561/2250, Loss: 0.0028\n",
      "Epoch 20/25, Batch 571/2250, Loss: 0.0006\n",
      "Epoch 20/25, Batch 581/2250, Loss: 0.0146\n",
      "Epoch 20/25, Batch 591/2250, Loss: 0.0695\n",
      "Epoch 20/25, Batch 601/2250, Loss: 0.0972\n",
      "Epoch 20/25, Batch 611/2250, Loss: 0.0042\n",
      "Epoch 20/25, Batch 621/2250, Loss: 0.0363\n",
      "Epoch 20/25, Batch 631/2250, Loss: 0.0042\n",
      "Epoch 20/25, Batch 641/2250, Loss: 0.0065\n",
      "Epoch 20/25, Batch 651/2250, Loss: 0.0011\n",
      "Epoch 20/25, Batch 661/2250, Loss: 0.0285\n",
      "Epoch 20/25, Batch 671/2250, Loss: 0.0262\n",
      "Epoch 20/25, Batch 681/2250, Loss: 0.0120\n",
      "Epoch 20/25, Batch 691/2250, Loss: 0.0972\n",
      "Epoch 20/25, Batch 701/2250, Loss: 0.0073\n",
      "Epoch 20/25, Batch 711/2250, Loss: 0.0068\n",
      "Epoch 20/25, Batch 721/2250, Loss: 0.0278\n",
      "Epoch 20/25, Batch 731/2250, Loss: 0.0026\n",
      "Epoch 20/25, Batch 741/2250, Loss: 0.0024\n",
      "Epoch 20/25, Batch 751/2250, Loss: 0.0451\n",
      "Epoch 20/25, Batch 761/2250, Loss: 0.0227\n",
      "Epoch 20/25, Batch 771/2250, Loss: 0.0218\n",
      "Epoch 20/25, Batch 781/2250, Loss: 0.1058\n",
      "Epoch 20/25, Batch 791/2250, Loss: 0.0175\n",
      "Epoch 20/25, Batch 801/2250, Loss: 0.0327\n",
      "Epoch 20/25, Batch 811/2250, Loss: 0.0190\n",
      "Epoch 20/25, Batch 821/2250, Loss: 0.0522\n",
      "Epoch 20/25, Batch 831/2250, Loss: 0.0103\n",
      "Epoch 20/25, Batch 841/2250, Loss: 0.0215\n",
      "Epoch 20/25, Batch 851/2250, Loss: 0.0128\n",
      "Epoch 20/25, Batch 861/2250, Loss: 0.0024\n",
      "Epoch 20/25, Batch 871/2250, Loss: 0.0377\n",
      "Epoch 20/25, Batch 881/2250, Loss: 0.0665\n",
      "Epoch 20/25, Batch 891/2250, Loss: 0.0867\n",
      "Epoch 20/25, Batch 901/2250, Loss: 0.0010\n",
      "Epoch 20/25, Batch 911/2250, Loss: 0.0073\n",
      "Epoch 20/25, Batch 921/2250, Loss: 0.0177\n",
      "Epoch 20/25, Batch 931/2250, Loss: 0.1684\n",
      "Epoch 20/25, Batch 941/2250, Loss: 0.0173\n",
      "Epoch 20/25, Batch 951/2250, Loss: 0.1183\n",
      "Epoch 20/25, Batch 961/2250, Loss: 0.0760\n",
      "Epoch 20/25, Batch 971/2250, Loss: 0.0145\n",
      "Epoch 20/25, Batch 981/2250, Loss: 0.0131\n",
      "Epoch 20/25, Batch 991/2250, Loss: 0.0251\n",
      "Epoch 20/25, Batch 1001/2250, Loss: 0.0090\n",
      "Epoch 20/25, Batch 1011/2250, Loss: 0.0327\n",
      "Epoch 20/25, Batch 1021/2250, Loss: 0.0271\n",
      "Epoch 20/25, Batch 1031/2250, Loss: 0.0047\n",
      "Epoch 20/25, Batch 1041/2250, Loss: 0.0059\n",
      "Epoch 20/25, Batch 1051/2250, Loss: 0.1295\n",
      "Epoch 20/25, Batch 1061/2250, Loss: 0.0104\n",
      "Epoch 20/25, Batch 1071/2250, Loss: 0.0966\n",
      "Epoch 20/25, Batch 1081/2250, Loss: 0.0753\n",
      "Epoch 20/25, Batch 1091/2250, Loss: 0.0543\n",
      "Epoch 20/25, Batch 1101/2250, Loss: 0.0229\n",
      "Epoch 20/25, Batch 1111/2250, Loss: 0.0194\n",
      "Epoch 20/25, Batch 1121/2250, Loss: 0.0653\n",
      "Epoch 20/25, Batch 1131/2250, Loss: 0.0221\n",
      "Epoch 20/25, Batch 1141/2250, Loss: 0.1621\n",
      "Epoch 20/25, Batch 1151/2250, Loss: 0.0030\n",
      "Epoch 20/25, Batch 1161/2250, Loss: 0.0292\n",
      "Epoch 20/25, Batch 1171/2250, Loss: 0.0089\n",
      "Epoch 20/25, Batch 1181/2250, Loss: 0.0026\n",
      "Epoch 20/25, Batch 1191/2250, Loss: 0.1118\n",
      "Epoch 20/25, Batch 1201/2250, Loss: 0.2135\n",
      "Epoch 20/25, Batch 1211/2250, Loss: 0.0653\n",
      "Epoch 20/25, Batch 1221/2250, Loss: 0.0031\n",
      "Epoch 20/25, Batch 1231/2250, Loss: 0.0934\n",
      "Epoch 20/25, Batch 1241/2250, Loss: 0.0200\n",
      "Epoch 20/25, Batch 1251/2250, Loss: 0.0114\n",
      "Epoch 20/25, Batch 1261/2250, Loss: 0.0007\n",
      "Epoch 20/25, Batch 1271/2250, Loss: 0.0227\n",
      "Epoch 20/25, Batch 1281/2250, Loss: 0.0021\n",
      "Epoch 20/25, Batch 1291/2250, Loss: 0.0124\n",
      "Epoch 20/25, Batch 1301/2250, Loss: 0.1037\n",
      "Epoch 20/25, Batch 1311/2250, Loss: 0.0074\n",
      "Epoch 20/25, Batch 1321/2250, Loss: 0.0091\n",
      "Epoch 20/25, Batch 1331/2250, Loss: 0.0388\n",
      "Epoch 20/25, Batch 1341/2250, Loss: 0.0064\n",
      "Epoch 20/25, Batch 1351/2250, Loss: 0.1891\n",
      "Epoch 20/25, Batch 1361/2250, Loss: 0.0741\n",
      "Epoch 20/25, Batch 1371/2250, Loss: 0.1260\n",
      "Epoch 20/25, Batch 1381/2250, Loss: 0.0715\n",
      "Epoch 20/25, Batch 1391/2250, Loss: 0.0586\n",
      "Epoch 20/25, Batch 1401/2250, Loss: 0.0179\n",
      "Epoch 20/25, Batch 1411/2250, Loss: 0.0019\n",
      "Epoch 20/25, Batch 1421/2250, Loss: 0.0196\n",
      "Epoch 20/25, Batch 1431/2250, Loss: 0.0045\n",
      "Epoch 20/25, Batch 1441/2250, Loss: 0.0871\n",
      "Epoch 20/25, Batch 1451/2250, Loss: 0.1882\n",
      "Epoch 20/25, Batch 1461/2250, Loss: 0.0049\n",
      "Epoch 20/25, Batch 1471/2250, Loss: 0.0050\n",
      "Epoch 20/25, Batch 1481/2250, Loss: 0.0641\n",
      "Epoch 20/25, Batch 1491/2250, Loss: 0.0080\n",
      "Epoch 20/25, Batch 1501/2250, Loss: 0.0580\n",
      "Epoch 20/25, Batch 1511/2250, Loss: 0.0446\n",
      "Epoch 20/25, Batch 1521/2250, Loss: 0.2446\n",
      "Epoch 20/25, Batch 1531/2250, Loss: 0.0151\n",
      "Epoch 20/25, Batch 1541/2250, Loss: 0.1668\n",
      "Epoch 20/25, Batch 1551/2250, Loss: 0.0292\n",
      "Epoch 20/25, Batch 1561/2250, Loss: 0.0344\n",
      "Epoch 20/25, Batch 1571/2250, Loss: 0.0156\n",
      "Epoch 20/25, Batch 1581/2250, Loss: 0.0112\n",
      "Epoch 20/25, Batch 1591/2250, Loss: 0.0187\n",
      "Epoch 20/25, Batch 1601/2250, Loss: 0.0788\n",
      "Epoch 20/25, Batch 1611/2250, Loss: 0.0050\n",
      "Epoch 20/25, Batch 1621/2250, Loss: 0.0465\n",
      "Epoch 20/25, Batch 1631/2250, Loss: 0.0079\n",
      "Epoch 20/25, Batch 1641/2250, Loss: 0.0118\n",
      "Epoch 20/25, Batch 1651/2250, Loss: 0.0031\n",
      "Epoch 20/25, Batch 1661/2250, Loss: 0.1525\n",
      "Epoch 20/25, Batch 1671/2250, Loss: 0.0014\n",
      "Epoch 20/25, Batch 1681/2250, Loss: 0.0296\n",
      "Epoch 20/25, Batch 1691/2250, Loss: 0.0016\n",
      "Epoch 20/25, Batch 1701/2250, Loss: 0.2247\n",
      "Epoch 20/25, Batch 1711/2250, Loss: 0.1147\n",
      "Epoch 20/25, Batch 1721/2250, Loss: 0.0858\n",
      "Epoch 20/25, Batch 1731/2250, Loss: 0.0075\n",
      "Epoch 20/25, Batch 1741/2250, Loss: 0.0304\n",
      "Epoch 20/25, Batch 1751/2250, Loss: 0.0166\n",
      "Epoch 20/25, Batch 1761/2250, Loss: 0.0193\n",
      "Epoch 20/25, Batch 1771/2250, Loss: 0.0079\n",
      "Epoch 20/25, Batch 1781/2250, Loss: 0.0680\n",
      "Epoch 20/25, Batch 1791/2250, Loss: 0.0128\n",
      "Epoch 20/25, Batch 1801/2250, Loss: 0.0022\n",
      "Epoch 20/25, Batch 1811/2250, Loss: 0.0464\n",
      "Epoch 20/25, Batch 1821/2250, Loss: 0.0103\n",
      "Epoch 20/25, Batch 1831/2250, Loss: 0.0067\n",
      "Epoch 20/25, Batch 1841/2250, Loss: 0.0087\n",
      "Epoch 20/25, Batch 1851/2250, Loss: 0.0829\n",
      "Epoch 20/25, Batch 1861/2250, Loss: 0.0785\n",
      "Epoch 20/25, Batch 1871/2250, Loss: 0.0023\n",
      "Epoch 20/25, Batch 1881/2250, Loss: 0.0333\n",
      "Epoch 20/25, Batch 1891/2250, Loss: 0.0934\n",
      "Epoch 20/25, Batch 1901/2250, Loss: 0.0343\n",
      "Epoch 20/25, Batch 1911/2250, Loss: 0.0169\n",
      "Epoch 20/25, Batch 1921/2250, Loss: 0.0808\n",
      "Epoch 20/25, Batch 1931/2250, Loss: 0.0117\n",
      "Epoch 20/25, Batch 1941/2250, Loss: 0.0014\n",
      "Epoch 20/25, Batch 1951/2250, Loss: 0.0436\n",
      "Epoch 20/25, Batch 1961/2250, Loss: 0.0054\n",
      "Epoch 20/25, Batch 1971/2250, Loss: 0.0818\n",
      "Epoch 20/25, Batch 1981/2250, Loss: 0.0059\n",
      "Epoch 20/25, Batch 1991/2250, Loss: 0.0482\n",
      "Epoch 20/25, Batch 2001/2250, Loss: 0.0648\n",
      "Epoch 20/25, Batch 2011/2250, Loss: 0.0264\n",
      "Epoch 20/25, Batch 2021/2250, Loss: 0.0122\n",
      "Epoch 20/25, Batch 2031/2250, Loss: 0.0056\n",
      "Epoch 20/25, Batch 2041/2250, Loss: 0.0506\n",
      "Epoch 20/25, Batch 2051/2250, Loss: 0.0829\n",
      "Epoch 20/25, Batch 2061/2250, Loss: 0.0102\n",
      "Epoch 20/25, Batch 2071/2250, Loss: 0.0127\n",
      "Epoch 20/25, Batch 2081/2250, Loss: 0.0092\n",
      "Epoch 20/25, Batch 2091/2250, Loss: 0.0098\n",
      "Epoch 20/25, Batch 2101/2250, Loss: 0.0531\n",
      "Epoch 20/25, Batch 2111/2250, Loss: 0.0036\n",
      "Epoch 20/25, Batch 2121/2250, Loss: 0.0716\n",
      "Epoch 20/25, Batch 2131/2250, Loss: 0.0759\n",
      "Epoch 20/25, Batch 2141/2250, Loss: 0.0129\n",
      "Epoch 20/25, Batch 2151/2250, Loss: 0.0359\n",
      "Epoch 20/25, Batch 2161/2250, Loss: 0.0068\n",
      "Epoch 20/25, Batch 2171/2250, Loss: 0.0124\n",
      "Epoch 20/25, Batch 2181/2250, Loss: 0.0089\n",
      "Epoch 20/25, Batch 2191/2250, Loss: 0.0042\n",
      "Epoch 20/25, Batch 2201/2250, Loss: 0.0235\n",
      "Epoch 20/25, Batch 2211/2250, Loss: 0.0046\n",
      "Epoch 20/25, Batch 2221/2250, Loss: 0.0011\n",
      "Epoch 20/25, Batch 2231/2250, Loss: 0.0012\n",
      "Epoch 20/25, Batch 2241/2250, Loss: 0.0599\n",
      "Epoch 20/25:\n",
      "Train Loss: 0.0437, Train Acc: 98.38%\n",
      "Val Loss: 0.0687, Val Acc: 97.52%\n",
      "------------------------------------------------------------\n",
      "Starting epoch 21/25...\n",
      "Starting training loop with 2250 batches...\n",
      "Successfully loaded first batch: images shape torch.Size([32, 3, 224, 224]), labels shape torch.Size([32])\n",
      "Epoch 21/25, Batch 1/2250, Loss: 0.0424\n",
      "Epoch 21/25, Batch 11/2250, Loss: 0.0087\n",
      "Epoch 21/25, Batch 21/2250, Loss: 0.0243\n",
      "Epoch 21/25, Batch 31/2250, Loss: 0.0716\n",
      "Epoch 21/25, Batch 41/2250, Loss: 0.0074\n",
      "Epoch 21/25, Batch 51/2250, Loss: 0.0052\n",
      "Epoch 21/25, Batch 61/2250, Loss: 0.0367\n",
      "Epoch 21/25, Batch 71/2250, Loss: 0.0042\n",
      "Epoch 21/25, Batch 81/2250, Loss: 0.0233\n",
      "Epoch 21/25, Batch 91/2250, Loss: 0.0861\n",
      "Epoch 21/25, Batch 101/2250, Loss: 0.0808\n",
      "Epoch 21/25, Batch 111/2250, Loss: 0.0123\n",
      "Epoch 21/25, Batch 121/2250, Loss: 0.2113\n",
      "Epoch 21/25, Batch 131/2250, Loss: 0.0555\n",
      "Epoch 21/25, Batch 141/2250, Loss: 0.0032\n",
      "Epoch 21/25, Batch 151/2250, Loss: 0.1824\n",
      "Epoch 21/25, Batch 161/2250, Loss: 0.0247\n",
      "Epoch 21/25, Batch 171/2250, Loss: 0.0166\n",
      "Epoch 21/25, Batch 181/2250, Loss: 0.0305\n",
      "Epoch 21/25, Batch 191/2250, Loss: 0.0119\n",
      "Epoch 21/25, Batch 201/2250, Loss: 0.0142\n",
      "Epoch 21/25, Batch 211/2250, Loss: 0.1592\n",
      "Epoch 21/25, Batch 221/2250, Loss: 0.0227\n",
      "Epoch 21/25, Batch 231/2250, Loss: 0.2305\n",
      "Epoch 21/25, Batch 241/2250, Loss: 0.0258\n",
      "Epoch 21/25, Batch 251/2250, Loss: 0.0025\n",
      "Epoch 21/25, Batch 261/2250, Loss: 0.0732\n",
      "Epoch 21/25, Batch 271/2250, Loss: 0.0327\n",
      "Epoch 21/25, Batch 281/2250, Loss: 0.0376\n",
      "Epoch 21/25, Batch 291/2250, Loss: 0.0004\n",
      "Epoch 21/25, Batch 301/2250, Loss: 0.0093\n",
      "Epoch 21/25, Batch 311/2250, Loss: 0.0013\n",
      "Epoch 21/25, Batch 321/2250, Loss: 0.0103\n",
      "Epoch 21/25, Batch 331/2250, Loss: 0.0139\n",
      "Epoch 21/25, Batch 341/2250, Loss: 0.0804\n",
      "Epoch 21/25, Batch 351/2250, Loss: 0.0019\n",
      "Epoch 21/25, Batch 361/2250, Loss: 0.0088\n",
      "Epoch 21/25, Batch 371/2250, Loss: 0.0124\n",
      "Epoch 21/25, Batch 381/2250, Loss: 0.1244\n",
      "Epoch 21/25, Batch 391/2250, Loss: 0.0168\n",
      "Epoch 21/25, Batch 401/2250, Loss: 0.0501\n",
      "Epoch 21/25, Batch 411/2250, Loss: 0.1302\n",
      "Epoch 21/25, Batch 421/2250, Loss: 0.0411\n",
      "Epoch 21/25, Batch 431/2250, Loss: 0.0295\n",
      "Epoch 21/25, Batch 441/2250, Loss: 0.0168\n",
      "Epoch 21/25, Batch 451/2250, Loss: 0.1649\n",
      "Epoch 21/25, Batch 461/2250, Loss: 0.0923\n",
      "Epoch 21/25, Batch 471/2250, Loss: 0.0131\n",
      "Epoch 21/25, Batch 481/2250, Loss: 0.0676\n",
      "Epoch 21/25, Batch 491/2250, Loss: 0.1271\n",
      "Epoch 21/25, Batch 501/2250, Loss: 0.0206\n",
      "Epoch 21/25, Batch 511/2250, Loss: 0.0070\n",
      "Epoch 21/25, Batch 521/2250, Loss: 0.0152\n",
      "Epoch 21/25, Batch 531/2250, Loss: 0.2933\n",
      "Epoch 21/25, Batch 541/2250, Loss: 0.1111\n",
      "Epoch 21/25, Batch 551/2250, Loss: 0.0181\n",
      "Epoch 21/25, Batch 561/2250, Loss: 0.0172\n",
      "Epoch 21/25, Batch 571/2250, Loss: 0.0432\n",
      "Epoch 21/25, Batch 581/2250, Loss: 0.1970\n",
      "Epoch 21/25, Batch 591/2250, Loss: 0.0427\n",
      "Epoch 21/25, Batch 601/2250, Loss: 0.2666\n",
      "Epoch 21/25, Batch 611/2250, Loss: 0.0128\n",
      "Epoch 21/25, Batch 621/2250, Loss: 0.0121\n",
      "Epoch 21/25, Batch 631/2250, Loss: 0.0284\n",
      "Epoch 21/25, Batch 641/2250, Loss: 0.0085\n",
      "Epoch 21/25, Batch 651/2250, Loss: 0.0164\n",
      "Epoch 21/25, Batch 661/2250, Loss: 0.0631\n",
      "Epoch 21/25, Batch 671/2250, Loss: 0.0074\n",
      "Epoch 21/25, Batch 681/2250, Loss: 0.0247\n",
      "Epoch 21/25, Batch 691/2250, Loss: 0.0022\n",
      "Epoch 21/25, Batch 701/2250, Loss: 0.0181\n",
      "Epoch 21/25, Batch 711/2250, Loss: 0.0327\n",
      "Epoch 21/25, Batch 721/2250, Loss: 0.0988\n",
      "Epoch 21/25, Batch 731/2250, Loss: 0.0560\n",
      "Epoch 21/25, Batch 741/2250, Loss: 0.0620\n",
      "Epoch 21/25, Batch 751/2250, Loss: 0.0203\n",
      "Epoch 21/25, Batch 761/2250, Loss: 0.0109\n",
      "Epoch 21/25, Batch 771/2250, Loss: 0.0765\n",
      "Epoch 21/25, Batch 781/2250, Loss: 0.0155\n",
      "Epoch 21/25, Batch 791/2250, Loss: 0.0123\n",
      "Epoch 21/25, Batch 801/2250, Loss: 0.0037\n",
      "Epoch 21/25, Batch 811/2250, Loss: 0.0291\n",
      "Epoch 21/25, Batch 821/2250, Loss: 0.0899\n",
      "Epoch 21/25, Batch 831/2250, Loss: 0.0303\n",
      "Epoch 21/25, Batch 841/2250, Loss: 0.0215\n",
      "Epoch 21/25, Batch 851/2250, Loss: 0.0190\n",
      "Epoch 21/25, Batch 861/2250, Loss: 0.0248\n",
      "Epoch 21/25, Batch 871/2250, Loss: 0.0926\n",
      "Epoch 21/25, Batch 881/2250, Loss: 0.0052\n",
      "Epoch 21/25, Batch 891/2250, Loss: 0.0198\n",
      "Epoch 21/25, Batch 901/2250, Loss: 0.0142\n",
      "Epoch 21/25, Batch 911/2250, Loss: 0.0429\n",
      "Epoch 21/25, Batch 921/2250, Loss: 0.0061\n",
      "Epoch 21/25, Batch 931/2250, Loss: 0.1607\n",
      "Epoch 21/25, Batch 941/2250, Loss: 0.0099\n",
      "Epoch 21/25, Batch 951/2250, Loss: 0.0111\n",
      "Epoch 21/25, Batch 961/2250, Loss: 0.1727\n",
      "Epoch 21/25, Batch 971/2250, Loss: 0.0147\n",
      "Epoch 21/25, Batch 981/2250, Loss: 0.0158\n",
      "Epoch 21/25, Batch 991/2250, Loss: 0.0030\n",
      "Epoch 21/25, Batch 1001/2250, Loss: 0.0897\n",
      "Epoch 21/25, Batch 1011/2250, Loss: 0.0056\n",
      "Epoch 21/25, Batch 1021/2250, Loss: 0.0215\n",
      "Epoch 21/25, Batch 1031/2250, Loss: 0.0155\n",
      "Epoch 21/25, Batch 1041/2250, Loss: 0.2514\n",
      "Epoch 21/25, Batch 1051/2250, Loss: 0.1086\n",
      "Epoch 21/25, Batch 1061/2250, Loss: 0.1177\n",
      "Epoch 21/25, Batch 1071/2250, Loss: 0.0183\n",
      "Epoch 21/25, Batch 1081/2250, Loss: 0.0092\n",
      "Epoch 21/25, Batch 1091/2250, Loss: 0.1528\n",
      "Epoch 21/25, Batch 1101/2250, Loss: 0.0327\n",
      "Epoch 21/25, Batch 1111/2250, Loss: 0.0395\n",
      "Epoch 21/25, Batch 1121/2250, Loss: 0.0075\n",
      "Epoch 21/25, Batch 1131/2250, Loss: 0.0364\n",
      "Epoch 21/25, Batch 1141/2250, Loss: 0.0043\n",
      "Epoch 21/25, Batch 1151/2250, Loss: 0.0302\n",
      "Epoch 21/25, Batch 1161/2250, Loss: 0.1334\n",
      "Epoch 21/25, Batch 1171/2250, Loss: 0.0114\n",
      "Epoch 21/25, Batch 1181/2250, Loss: 0.0041\n",
      "Epoch 21/25, Batch 1191/2250, Loss: 0.0080\n",
      "Epoch 21/25, Batch 1201/2250, Loss: 0.0037\n",
      "Epoch 21/25, Batch 1211/2250, Loss: 0.0209\n",
      "Epoch 21/25, Batch 1221/2250, Loss: 0.1573\n",
      "Epoch 21/25, Batch 1231/2250, Loss: 0.0058\n",
      "Epoch 21/25, Batch 1241/2250, Loss: 0.0078\n",
      "Epoch 21/25, Batch 1251/2250, Loss: 0.0080\n",
      "Epoch 21/25, Batch 1261/2250, Loss: 0.0844\n",
      "Epoch 21/25, Batch 1271/2250, Loss: 0.0578\n",
      "Epoch 21/25, Batch 1281/2250, Loss: 0.0271\n",
      "Epoch 21/25, Batch 1291/2250, Loss: 0.0019\n",
      "Epoch 21/25, Batch 1301/2250, Loss: 0.0115\n",
      "Epoch 21/25, Batch 1311/2250, Loss: 0.0219\n",
      "Epoch 21/25, Batch 1321/2250, Loss: 0.1046\n",
      "Epoch 21/25, Batch 1331/2250, Loss: 0.0280\n",
      "Epoch 21/25, Batch 1341/2250, Loss: 0.0173\n",
      "Epoch 21/25, Batch 1351/2250, Loss: 0.0043\n",
      "Epoch 21/25, Batch 1361/2250, Loss: 0.2355\n",
      "Epoch 21/25, Batch 1371/2250, Loss: 0.0972\n",
      "Epoch 21/25, Batch 1381/2250, Loss: 0.0187\n",
      "Epoch 21/25, Batch 1391/2250, Loss: 0.1382\n",
      "Epoch 21/25, Batch 1401/2250, Loss: 0.0381\n",
      "Epoch 21/25, Batch 1411/2250, Loss: 0.0131\n",
      "Epoch 21/25, Batch 1421/2250, Loss: 0.0259\n",
      "Epoch 21/25, Batch 1431/2250, Loss: 0.0090\n",
      "Epoch 21/25, Batch 1441/2250, Loss: 0.0768\n",
      "Epoch 21/25, Batch 1451/2250, Loss: 0.0212\n",
      "Epoch 21/25, Batch 1461/2250, Loss: 0.0337\n",
      "Epoch 21/25, Batch 1471/2250, Loss: 0.0086\n",
      "Epoch 21/25, Batch 1481/2250, Loss: 0.0028\n",
      "Epoch 21/25, Batch 1491/2250, Loss: 0.1005\n",
      "Epoch 21/25, Batch 1501/2250, Loss: 0.0176\n",
      "Epoch 21/25, Batch 1511/2250, Loss: 0.0319\n",
      "Epoch 21/25, Batch 1521/2250, Loss: 0.0564\n",
      "Epoch 21/25, Batch 1531/2250, Loss: 0.0133\n",
      "Epoch 21/25, Batch 1541/2250, Loss: 0.0334\n",
      "Epoch 21/25, Batch 1551/2250, Loss: 0.0303\n",
      "Epoch 21/25, Batch 1561/2250, Loss: 0.0368\n",
      "Epoch 21/25, Batch 1571/2250, Loss: 0.0406\n",
      "Epoch 21/25, Batch 1581/2250, Loss: 0.0180\n",
      "Epoch 21/25, Batch 1591/2250, Loss: 0.0035\n",
      "Epoch 21/25, Batch 1601/2250, Loss: 0.0120\n",
      "Epoch 21/25, Batch 1611/2250, Loss: 0.0100\n",
      "Epoch 21/25, Batch 1621/2250, Loss: 0.0012\n",
      "Epoch 21/25, Batch 1631/2250, Loss: 0.0067\n",
      "Epoch 21/25, Batch 1641/2250, Loss: 0.1215\n",
      "Epoch 21/25, Batch 1651/2250, Loss: 0.0155\n",
      "Epoch 21/25, Batch 1661/2250, Loss: 0.0858\n",
      "Epoch 21/25, Batch 1671/2250, Loss: 0.0028\n",
      "Epoch 21/25, Batch 1681/2250, Loss: 0.0386\n",
      "Epoch 21/25, Batch 1691/2250, Loss: 0.0220\n",
      "Epoch 21/25, Batch 1701/2250, Loss: 0.0179\n",
      "Epoch 21/25, Batch 1711/2250, Loss: 0.0720\n",
      "Epoch 21/25, Batch 1721/2250, Loss: 0.0169\n",
      "Epoch 21/25, Batch 1731/2250, Loss: 0.1136\n",
      "Epoch 21/25, Batch 1741/2250, Loss: 0.0258\n",
      "Epoch 21/25, Batch 1751/2250, Loss: 0.0399\n",
      "Epoch 21/25, Batch 1761/2250, Loss: 0.0113\n",
      "Epoch 21/25, Batch 1771/2250, Loss: 0.0803\n",
      "Epoch 21/25, Batch 1781/2250, Loss: 0.1486\n",
      "Epoch 21/25, Batch 1791/2250, Loss: 0.1667\n",
      "Epoch 21/25, Batch 1801/2250, Loss: 0.1902\n",
      "Epoch 21/25, Batch 1811/2250, Loss: 0.0282\n",
      "Epoch 21/25, Batch 1821/2250, Loss: 0.0194\n",
      "Epoch 21/25, Batch 1831/2250, Loss: 0.0144\n",
      "Epoch 21/25, Batch 1841/2250, Loss: 0.0067\n",
      "Epoch 21/25, Batch 1851/2250, Loss: 0.0009\n",
      "Epoch 21/25, Batch 1861/2250, Loss: 0.0800\n",
      "Epoch 21/25, Batch 1871/2250, Loss: 0.0715\n",
      "Epoch 21/25, Batch 1881/2250, Loss: 0.0476\n",
      "Epoch 21/25, Batch 1891/2250, Loss: 0.0051\n",
      "Epoch 21/25, Batch 1901/2250, Loss: 0.0835\n",
      "Epoch 21/25, Batch 1911/2250, Loss: 0.0262\n",
      "Epoch 21/25, Batch 1921/2250, Loss: 0.0155\n",
      "Epoch 21/25, Batch 1931/2250, Loss: 0.0099\n",
      "Epoch 21/25, Batch 1941/2250, Loss: 0.0447\n",
      "Epoch 21/25, Batch 1951/2250, Loss: 0.0990\n",
      "Epoch 21/25, Batch 1961/2250, Loss: 0.0446\n",
      "Epoch 21/25, Batch 1971/2250, Loss: 0.0049\n",
      "Epoch 21/25, Batch 1981/2250, Loss: 0.0077\n",
      "Epoch 21/25, Batch 1991/2250, Loss: 0.0926\n",
      "Epoch 21/25, Batch 2001/2250, Loss: 0.0408\n",
      "Epoch 21/25, Batch 2011/2250, Loss: 0.0611\n",
      "Epoch 21/25, Batch 2021/2250, Loss: 0.0857\n",
      "Epoch 21/25, Batch 2031/2250, Loss: 0.0886\n",
      "Epoch 21/25, Batch 2041/2250, Loss: 0.1854\n",
      "Epoch 21/25, Batch 2051/2250, Loss: 0.0155\n",
      "Epoch 21/25, Batch 2061/2250, Loss: 0.1654\n",
      "Epoch 21/25, Batch 2071/2250, Loss: 0.0020\n",
      "Epoch 21/25, Batch 2081/2250, Loss: 0.1102\n",
      "Epoch 21/25, Batch 2091/2250, Loss: 0.0326\n",
      "Epoch 21/25, Batch 2101/2250, Loss: 0.1683\n",
      "Epoch 21/25, Batch 2111/2250, Loss: 0.1826\n",
      "Epoch 21/25, Batch 2121/2250, Loss: 0.0274\n",
      "Epoch 21/25, Batch 2131/2250, Loss: 0.0069\n",
      "Epoch 21/25, Batch 2141/2250, Loss: 0.1824\n",
      "Epoch 21/25, Batch 2151/2250, Loss: 0.1046\n",
      "Epoch 21/25, Batch 2161/2250, Loss: 0.0970\n",
      "Epoch 21/25, Batch 2171/2250, Loss: 0.0457\n",
      "Epoch 21/25, Batch 2181/2250, Loss: 0.0492\n",
      "Epoch 21/25, Batch 2191/2250, Loss: 0.0052\n",
      "Epoch 21/25, Batch 2201/2250, Loss: 0.0195\n",
      "Epoch 21/25, Batch 2211/2250, Loss: 0.0012\n",
      "Epoch 21/25, Batch 2221/2250, Loss: 0.0858\n",
      "Epoch 21/25, Batch 2231/2250, Loss: 0.0960\n",
      "Epoch 21/25, Batch 2241/2250, Loss: 0.1166\n",
      "Epoch 21/25:\n",
      "Train Loss: 0.0437, Train Acc: 98.41%\n",
      "Val Loss: 0.0332, Val Acc: 98.77%\n",
      "------------------------------------------------------------\n",
      "Starting epoch 22/25...\n",
      "Starting training loop with 2250 batches...\n",
      "Successfully loaded first batch: images shape torch.Size([32, 3, 224, 224]), labels shape torch.Size([32])\n",
      "Epoch 22/25, Batch 1/2250, Loss: 0.0082\n",
      "Epoch 22/25, Batch 11/2250, Loss: 0.1218\n",
      "Epoch 22/25, Batch 21/2250, Loss: 0.1442\n",
      "Epoch 22/25, Batch 31/2250, Loss: 0.0065\n",
      "Epoch 22/25, Batch 41/2250, Loss: 0.0083\n",
      "Epoch 22/25, Batch 51/2250, Loss: 0.0142\n",
      "Epoch 22/25, Batch 61/2250, Loss: 0.0020\n",
      "Epoch 22/25, Batch 71/2250, Loss: 0.0057\n",
      "Epoch 22/25, Batch 81/2250, Loss: 0.0013\n",
      "Epoch 22/25, Batch 91/2250, Loss: 0.0590\n",
      "Epoch 22/25, Batch 101/2250, Loss: 0.0164\n",
      "Epoch 22/25, Batch 111/2250, Loss: 0.0348\n",
      "Epoch 22/25, Batch 121/2250, Loss: 0.0601\n",
      "Epoch 22/25, Batch 131/2250, Loss: 0.0608\n",
      "Epoch 22/25, Batch 141/2250, Loss: 0.0233\n",
      "Epoch 22/25, Batch 151/2250, Loss: 0.0312\n",
      "Epoch 22/25, Batch 161/2250, Loss: 0.0025\n",
      "Epoch 22/25, Batch 171/2250, Loss: 0.0044\n",
      "Epoch 22/25, Batch 181/2250, Loss: 0.0016\n",
      "Epoch 22/25, Batch 191/2250, Loss: 0.0232\n",
      "Epoch 22/25, Batch 201/2250, Loss: 0.0231\n",
      "Epoch 22/25, Batch 211/2250, Loss: 0.0242\n",
      "Epoch 22/25, Batch 221/2250, Loss: 0.0237\n",
      "Epoch 22/25, Batch 231/2250, Loss: 0.0061\n",
      "Epoch 22/25, Batch 241/2250, Loss: 0.1079\n",
      "Epoch 22/25, Batch 251/2250, Loss: 0.0603\n",
      "Epoch 22/25, Batch 261/2250, Loss: 0.0751\n",
      "Epoch 22/25, Batch 271/2250, Loss: 0.0489\n",
      "Epoch 22/25, Batch 281/2250, Loss: 0.0125\n",
      "Epoch 22/25, Batch 291/2250, Loss: 0.1280\n",
      "Epoch 22/25, Batch 301/2250, Loss: 0.1701\n",
      "Epoch 22/25, Batch 311/2250, Loss: 0.0200\n",
      "Epoch 22/25, Batch 321/2250, Loss: 0.0002\n",
      "Epoch 22/25, Batch 331/2250, Loss: 0.0084\n",
      "Epoch 22/25, Batch 341/2250, Loss: 0.0592\n",
      "Epoch 22/25, Batch 351/2250, Loss: 0.0236\n",
      "Epoch 22/25, Batch 361/2250, Loss: 0.0152\n",
      "Epoch 22/25, Batch 371/2250, Loss: 0.0001\n",
      "Epoch 22/25, Batch 381/2250, Loss: 0.0005\n",
      "Epoch 22/25, Batch 391/2250, Loss: 0.0043\n",
      "Epoch 22/25, Batch 401/2250, Loss: 0.0056\n",
      "Epoch 22/25, Batch 411/2250, Loss: 0.0429\n",
      "Epoch 22/25, Batch 421/2250, Loss: 0.0084\n",
      "Epoch 22/25, Batch 431/2250, Loss: 0.0177\n",
      "Epoch 22/25, Batch 441/2250, Loss: 0.0043\n",
      "Epoch 22/25, Batch 451/2250, Loss: 0.0078\n",
      "Epoch 22/25, Batch 461/2250, Loss: 0.0135\n",
      "Epoch 22/25, Batch 471/2250, Loss: 0.0426\n",
      "Epoch 22/25, Batch 481/2250, Loss: 0.0776\n",
      "Epoch 22/25, Batch 491/2250, Loss: 0.0090\n",
      "Epoch 22/25, Batch 501/2250, Loss: 0.0236\n",
      "Epoch 22/25, Batch 511/2250, Loss: 0.0115\n",
      "Epoch 22/25, Batch 521/2250, Loss: 0.0168\n",
      "Epoch 22/25, Batch 531/2250, Loss: 0.2376\n",
      "Epoch 22/25, Batch 541/2250, Loss: 0.0110\n",
      "Epoch 22/25, Batch 551/2250, Loss: 0.0052\n",
      "Epoch 22/25, Batch 561/2250, Loss: 0.0033\n",
      "Epoch 22/25, Batch 571/2250, Loss: 0.0354\n",
      "Epoch 22/25, Batch 581/2250, Loss: 0.0165\n",
      "Epoch 22/25, Batch 591/2250, Loss: 0.0007\n",
      "Epoch 22/25, Batch 601/2250, Loss: 0.0079\n",
      "Epoch 22/25, Batch 611/2250, Loss: 0.1109\n",
      "Epoch 22/25, Batch 621/2250, Loss: 0.0387\n",
      "Epoch 22/25, Batch 631/2250, Loss: 0.0549\n",
      "Epoch 22/25, Batch 641/2250, Loss: 0.0703\n",
      "Epoch 22/25, Batch 651/2250, Loss: 0.0171\n",
      "Epoch 22/25, Batch 661/2250, Loss: 0.0094\n",
      "Epoch 22/25, Batch 671/2250, Loss: 0.0567\n",
      "Epoch 22/25, Batch 681/2250, Loss: 0.0111\n",
      "Epoch 22/25, Batch 691/2250, Loss: 0.0298\n",
      "Epoch 22/25, Batch 701/2250, Loss: 0.0018\n",
      "Epoch 22/25, Batch 711/2250, Loss: 0.0214\n",
      "Epoch 22/25, Batch 721/2250, Loss: 0.0015\n",
      "Epoch 22/25, Batch 731/2250, Loss: 0.0840\n",
      "Epoch 22/25, Batch 741/2250, Loss: 0.0203\n",
      "Epoch 22/25, Batch 751/2250, Loss: 0.0073\n",
      "Epoch 22/25, Batch 761/2250, Loss: 0.0517\n",
      "Epoch 22/25, Batch 771/2250, Loss: 0.0677\n",
      "Epoch 22/25, Batch 781/2250, Loss: 0.0121\n",
      "Epoch 22/25, Batch 791/2250, Loss: 0.0023\n",
      "Epoch 22/25, Batch 801/2250, Loss: 0.0462\n",
      "Epoch 22/25, Batch 811/2250, Loss: 0.0278\n",
      "Epoch 22/25, Batch 821/2250, Loss: 0.0290\n",
      "Epoch 22/25, Batch 831/2250, Loss: 0.0127\n",
      "Epoch 22/25, Batch 841/2250, Loss: 0.0555\n",
      "Epoch 22/25, Batch 851/2250, Loss: 0.0005\n",
      "Epoch 22/25, Batch 861/2250, Loss: 0.0155\n",
      "Epoch 22/25, Batch 871/2250, Loss: 0.0111\n",
      "Epoch 22/25, Batch 881/2250, Loss: 0.0032\n",
      "Epoch 22/25, Batch 891/2250, Loss: 0.0027\n",
      "Epoch 22/25, Batch 901/2250, Loss: 0.0082\n",
      "Epoch 22/25, Batch 911/2250, Loss: 0.0067\n",
      "Epoch 22/25, Batch 921/2250, Loss: 0.0580\n",
      "Epoch 22/25, Batch 931/2250, Loss: 0.0482\n",
      "Epoch 22/25, Batch 941/2250, Loss: 0.0301\n",
      "Epoch 22/25, Batch 951/2250, Loss: 0.0502\n",
      "Epoch 22/25, Batch 961/2250, Loss: 0.0303\n",
      "Epoch 22/25, Batch 971/2250, Loss: 0.0574\n",
      "Epoch 22/25, Batch 981/2250, Loss: 0.1259\n",
      "Epoch 22/25, Batch 991/2250, Loss: 0.0216\n",
      "Epoch 22/25, Batch 1001/2250, Loss: 0.0153\n",
      "Epoch 22/25, Batch 1011/2250, Loss: 0.0149\n",
      "Epoch 22/25, Batch 1021/2250, Loss: 0.0458\n",
      "Epoch 22/25, Batch 1031/2250, Loss: 0.0083\n",
      "Epoch 22/25, Batch 1041/2250, Loss: 0.0333\n",
      "Epoch 22/25, Batch 1051/2250, Loss: 0.0053\n",
      "Epoch 22/25, Batch 1061/2250, Loss: 0.0042\n",
      "Epoch 22/25, Batch 1071/2250, Loss: 0.0045\n",
      "Epoch 22/25, Batch 1081/2250, Loss: 0.1000\n",
      "Epoch 22/25, Batch 1091/2250, Loss: 0.0516\n",
      "Epoch 22/25, Batch 1101/2250, Loss: 0.0536\n",
      "Epoch 22/25, Batch 1111/2250, Loss: 0.0118\n",
      "Epoch 22/25, Batch 1121/2250, Loss: 0.0229\n",
      "Epoch 22/25, Batch 1131/2250, Loss: 0.0117\n",
      "Epoch 22/25, Batch 1141/2250, Loss: 0.1085\n",
      "Epoch 22/25, Batch 1151/2250, Loss: 0.0131\n",
      "Epoch 22/25, Batch 1161/2250, Loss: 0.0698\n",
      "Epoch 22/25, Batch 1171/2250, Loss: 0.0127\n",
      "Epoch 22/25, Batch 1181/2250, Loss: 0.0538\n",
      "Epoch 22/25, Batch 1191/2250, Loss: 0.0093\n",
      "Epoch 22/25, Batch 1201/2250, Loss: 0.0247\n",
      "Epoch 22/25, Batch 1211/2250, Loss: 0.1403\n",
      "Epoch 22/25, Batch 1221/2250, Loss: 0.0193\n",
      "Epoch 22/25, Batch 1231/2250, Loss: 0.0320\n",
      "Epoch 22/25, Batch 1241/2250, Loss: 0.0293\n",
      "Epoch 22/25, Batch 1251/2250, Loss: 0.0441\n",
      "Epoch 22/25, Batch 1261/2250, Loss: 0.0025\n",
      "Epoch 22/25, Batch 1271/2250, Loss: 0.0897\n",
      "Epoch 22/25, Batch 1281/2250, Loss: 0.0147\n",
      "Epoch 22/25, Batch 1291/2250, Loss: 0.0095\n",
      "Epoch 22/25, Batch 1301/2250, Loss: 0.0995\n",
      "Epoch 22/25, Batch 1311/2250, Loss: 0.0485\n",
      "Epoch 22/25, Batch 1321/2250, Loss: 0.0030\n",
      "Epoch 22/25, Batch 1331/2250, Loss: 0.0021\n",
      "Epoch 22/25, Batch 1341/2250, Loss: 0.0178\n",
      "Epoch 22/25, Batch 1351/2250, Loss: 0.0296\n",
      "Epoch 22/25, Batch 1361/2250, Loss: 0.0107\n",
      "Epoch 22/25, Batch 1371/2250, Loss: 0.0475\n",
      "Epoch 22/25, Batch 1381/2250, Loss: 0.0197\n",
      "Epoch 22/25, Batch 1391/2250, Loss: 0.0636\n",
      "Epoch 22/25, Batch 1401/2250, Loss: 0.0152\n",
      "Epoch 22/25, Batch 1411/2250, Loss: 0.1469\n",
      "Epoch 22/25, Batch 1421/2250, Loss: 0.0292\n",
      "Epoch 22/25, Batch 1431/2250, Loss: 0.0046\n",
      "Epoch 22/25, Batch 1441/2250, Loss: 0.0926\n",
      "Epoch 22/25, Batch 1451/2250, Loss: 0.0071\n",
      "Epoch 22/25, Batch 1461/2250, Loss: 0.0184\n",
      "Epoch 22/25, Batch 1471/2250, Loss: 0.2838\n",
      "Epoch 22/25, Batch 1481/2250, Loss: 0.0126\n",
      "Epoch 22/25, Batch 1491/2250, Loss: 0.0229\n",
      "Epoch 22/25, Batch 1501/2250, Loss: 0.0076\n",
      "Epoch 22/25, Batch 1511/2250, Loss: 0.0246\n",
      "Epoch 22/25, Batch 1521/2250, Loss: 0.0167\n",
      "Epoch 22/25, Batch 1531/2250, Loss: 0.1008\n",
      "Epoch 22/25, Batch 1541/2250, Loss: 0.0130\n",
      "Epoch 22/25, Batch 1551/2250, Loss: 0.0088\n",
      "Epoch 22/25, Batch 1561/2250, Loss: 0.0924\n",
      "Epoch 22/25, Batch 1571/2250, Loss: 0.0833\n",
      "Epoch 22/25, Batch 1581/2250, Loss: 0.0053\n",
      "Epoch 22/25, Batch 1591/2250, Loss: 0.0616\n",
      "Epoch 22/25, Batch 1601/2250, Loss: 0.0140\n",
      "Epoch 22/25, Batch 1611/2250, Loss: 0.0397\n",
      "Epoch 22/25, Batch 1621/2250, Loss: 0.3073\n",
      "Epoch 22/25, Batch 1631/2250, Loss: 0.1491\n",
      "Epoch 22/25, Batch 1641/2250, Loss: 0.0080\n",
      "Epoch 22/25, Batch 1651/2250, Loss: 0.0228\n",
      "Epoch 22/25, Batch 1661/2250, Loss: 0.0330\n",
      "Epoch 22/25, Batch 1671/2250, Loss: 0.0141\n",
      "Epoch 22/25, Batch 1681/2250, Loss: 0.0042\n",
      "Epoch 22/25, Batch 1691/2250, Loss: 0.0054\n",
      "Epoch 22/25, Batch 1701/2250, Loss: 0.0407\n",
      "Epoch 22/25, Batch 1711/2250, Loss: 0.0078\n",
      "Epoch 22/25, Batch 1721/2250, Loss: 0.0452\n",
      "Epoch 22/25, Batch 1731/2250, Loss: 0.0255\n",
      "Epoch 22/25, Batch 1741/2250, Loss: 0.0091\n",
      "Epoch 22/25, Batch 1751/2250, Loss: 0.0580\n",
      "Epoch 22/25, Batch 1761/2250, Loss: 0.0149\n",
      "Epoch 22/25, Batch 1771/2250, Loss: 0.0312\n",
      "Epoch 22/25, Batch 1781/2250, Loss: 0.0761\n",
      "Epoch 22/25, Batch 1791/2250, Loss: 0.0254\n",
      "Epoch 22/25, Batch 1801/2250, Loss: 0.0083\n",
      "Epoch 22/25, Batch 1811/2250, Loss: 0.0498\n",
      "Epoch 22/25, Batch 1821/2250, Loss: 0.1092\n",
      "Epoch 22/25, Batch 1831/2250, Loss: 0.0178\n",
      "Epoch 22/25, Batch 1841/2250, Loss: 0.0045\n",
      "Epoch 22/25, Batch 1851/2250, Loss: 0.0149\n",
      "Epoch 22/25, Batch 1861/2250, Loss: 0.2032\n",
      "Epoch 22/25, Batch 1871/2250, Loss: 0.0234\n",
      "Epoch 22/25, Batch 1881/2250, Loss: 0.0684\n",
      "Epoch 22/25, Batch 1891/2250, Loss: 0.0071\n",
      "Epoch 22/25, Batch 1901/2250, Loss: 0.0360\n",
      "Epoch 22/25, Batch 1911/2250, Loss: 0.0803\n",
      "Epoch 22/25, Batch 1921/2250, Loss: 0.0324\n",
      "Epoch 22/25, Batch 1931/2250, Loss: 0.0214\n",
      "Epoch 22/25, Batch 1941/2250, Loss: 0.0093\n",
      "Epoch 22/25, Batch 1951/2250, Loss: 0.0347\n",
      "Epoch 22/25, Batch 1961/2250, Loss: 0.1119\n",
      "Epoch 22/25, Batch 1971/2250, Loss: 0.0486\n",
      "Epoch 22/25, Batch 1981/2250, Loss: 0.0513\n",
      "Epoch 22/25, Batch 1991/2250, Loss: 0.0063\n",
      "Epoch 22/25, Batch 2001/2250, Loss: 0.2599\n",
      "Epoch 22/25, Batch 2011/2250, Loss: 0.0123\n",
      "Epoch 22/25, Batch 2021/2250, Loss: 0.0102\n",
      "Epoch 22/25, Batch 2031/2250, Loss: 0.0019\n",
      "Epoch 22/25, Batch 2041/2250, Loss: 0.0111\n",
      "Epoch 22/25, Batch 2051/2250, Loss: 0.0077\n",
      "Epoch 22/25, Batch 2061/2250, Loss: 0.2246\n",
      "Epoch 22/25, Batch 2071/2250, Loss: 0.0252\n",
      "Epoch 22/25, Batch 2081/2250, Loss: 0.2044\n",
      "Epoch 22/25, Batch 2091/2250, Loss: 0.0591\n",
      "Epoch 22/25, Batch 2101/2250, Loss: 0.0412\n",
      "Epoch 22/25, Batch 2111/2250, Loss: 0.0157\n",
      "Epoch 22/25, Batch 2121/2250, Loss: 0.0463\n",
      "Epoch 22/25, Batch 2131/2250, Loss: 0.0220\n",
      "Epoch 22/25, Batch 2141/2250, Loss: 0.0145\n",
      "Epoch 22/25, Batch 2151/2250, Loss: 0.0765\n",
      "Epoch 22/25, Batch 2161/2250, Loss: 0.0411\n",
      "Epoch 22/25, Batch 2171/2250, Loss: 0.1540\n",
      "Epoch 22/25, Batch 2181/2250, Loss: 0.0688\n",
      "Epoch 22/25, Batch 2191/2250, Loss: 0.0508\n",
      "Epoch 22/25, Batch 2201/2250, Loss: 0.0159\n",
      "Epoch 22/25, Batch 2211/2250, Loss: 0.0125\n",
      "Epoch 22/25, Batch 2221/2250, Loss: 0.0056\n",
      "Epoch 22/25, Batch 2231/2250, Loss: 0.0207\n",
      "Epoch 22/25, Batch 2241/2250, Loss: 0.0368\n",
      "Epoch 22/25:\n",
      "Train Loss: 0.0400, Train Acc: 98.57%\n",
      "Val Loss: 0.0389, Val Acc: 98.62%\n",
      "------------------------------------------------------------\n",
      "Starting epoch 23/25...\n",
      "Starting training loop with 2250 batches...\n",
      "Successfully loaded first batch: images shape torch.Size([32, 3, 224, 224]), labels shape torch.Size([32])\n",
      "Epoch 23/25, Batch 1/2250, Loss: 0.0156\n",
      "Epoch 23/25, Batch 11/2250, Loss: 0.0219\n",
      "Epoch 23/25, Batch 21/2250, Loss: 0.0241\n",
      "Epoch 23/25, Batch 31/2250, Loss: 0.0068\n",
      "Epoch 23/25, Batch 41/2250, Loss: 0.0801\n",
      "Epoch 23/25, Batch 51/2250, Loss: 0.0006\n",
      "Epoch 23/25, Batch 61/2250, Loss: 0.0026\n",
      "Epoch 23/25, Batch 71/2250, Loss: 0.0177\n",
      "Epoch 23/25, Batch 81/2250, Loss: 0.0392\n",
      "Epoch 23/25, Batch 91/2250, Loss: 0.0021\n",
      "Epoch 23/25, Batch 101/2250, Loss: 0.0040\n",
      "Epoch 23/25, Batch 111/2250, Loss: 0.0456\n",
      "Epoch 23/25, Batch 121/2250, Loss: 0.0246\n",
      "Epoch 23/25, Batch 131/2250, Loss: 0.1026\n",
      "Epoch 23/25, Batch 141/2250, Loss: 0.0101\n",
      "Epoch 23/25, Batch 151/2250, Loss: 0.0471\n",
      "Epoch 23/25, Batch 161/2250, Loss: 0.0200\n",
      "Epoch 23/25, Batch 171/2250, Loss: 0.0006\n",
      "Epoch 23/25, Batch 181/2250, Loss: 0.0127\n",
      "Epoch 23/25, Batch 191/2250, Loss: 0.0039\n",
      "Epoch 23/25, Batch 201/2250, Loss: 0.0016\n",
      "Epoch 23/25, Batch 211/2250, Loss: 0.0112\n",
      "Epoch 23/25, Batch 221/2250, Loss: 0.0238\n",
      "Epoch 23/25, Batch 231/2250, Loss: 0.0391\n",
      "Epoch 23/25, Batch 241/2250, Loss: 0.0410\n",
      "Epoch 23/25, Batch 251/2250, Loss: 0.0373\n",
      "Epoch 23/25, Batch 261/2250, Loss: 0.0150\n",
      "Epoch 23/25, Batch 271/2250, Loss: 0.0426\n",
      "Epoch 23/25, Batch 281/2250, Loss: 0.0046\n",
      "Epoch 23/25, Batch 291/2250, Loss: 0.0042\n",
      "Epoch 23/25, Batch 301/2250, Loss: 0.0208\n",
      "Epoch 23/25, Batch 311/2250, Loss: 0.0018\n",
      "Epoch 23/25, Batch 321/2250, Loss: 0.0457\n",
      "Epoch 23/25, Batch 331/2250, Loss: 0.0053\n",
      "Epoch 23/25, Batch 341/2250, Loss: 0.0062\n",
      "Epoch 23/25, Batch 351/2250, Loss: 0.0298\n",
      "Epoch 23/25, Batch 361/2250, Loss: 0.0200\n",
      "Epoch 23/25, Batch 371/2250, Loss: 0.0034\n",
      "Epoch 23/25, Batch 381/2250, Loss: 0.0668\n",
      "Epoch 23/25, Batch 391/2250, Loss: 0.0025\n",
      "Epoch 23/25, Batch 401/2250, Loss: 0.0124\n",
      "Epoch 23/25, Batch 411/2250, Loss: 0.0161\n",
      "Epoch 23/25, Batch 421/2250, Loss: 0.0782\n",
      "Epoch 23/25, Batch 431/2250, Loss: 0.0388\n",
      "Epoch 23/25, Batch 441/2250, Loss: 0.0028\n",
      "Epoch 23/25, Batch 451/2250, Loss: 0.0269\n",
      "Epoch 23/25, Batch 461/2250, Loss: 0.0041\n",
      "Epoch 23/25, Batch 471/2250, Loss: 0.1482\n",
      "Epoch 23/25, Batch 481/2250, Loss: 0.0079\n",
      "Epoch 23/25, Batch 491/2250, Loss: 0.2245\n",
      "Epoch 23/25, Batch 501/2250, Loss: 0.0430\n",
      "Epoch 23/25, Batch 511/2250, Loss: 0.0381\n",
      "Epoch 23/25, Batch 521/2250, Loss: 0.0014\n",
      "Epoch 23/25, Batch 531/2250, Loss: 0.0807\n",
      "Epoch 23/25, Batch 541/2250, Loss: 0.0368\n",
      "Epoch 23/25, Batch 551/2250, Loss: 0.0742\n",
      "Epoch 23/25, Batch 561/2250, Loss: 0.0727\n",
      "Epoch 23/25, Batch 571/2250, Loss: 0.0441\n",
      "Epoch 23/25, Batch 581/2250, Loss: 0.0185\n",
      "Epoch 23/25, Batch 591/2250, Loss: 0.0648\n",
      "Epoch 23/25, Batch 601/2250, Loss: 0.0177\n",
      "Epoch 23/25, Batch 611/2250, Loss: 0.0417\n",
      "Epoch 23/25, Batch 621/2250, Loss: 0.0134\n",
      "Epoch 23/25, Batch 631/2250, Loss: 0.0750\n",
      "Epoch 23/25, Batch 641/2250, Loss: 0.0015\n",
      "Epoch 23/25, Batch 651/2250, Loss: 0.0642\n",
      "Epoch 23/25, Batch 661/2250, Loss: 0.0050\n",
      "Epoch 23/25, Batch 671/2250, Loss: 0.0029\n",
      "Epoch 23/25, Batch 681/2250, Loss: 0.0010\n",
      "Epoch 23/25, Batch 691/2250, Loss: 0.0056\n",
      "Epoch 23/25, Batch 701/2250, Loss: 0.0515\n",
      "Epoch 23/25, Batch 711/2250, Loss: 0.0055\n",
      "Epoch 23/25, Batch 721/2250, Loss: 0.0208\n",
      "Epoch 23/25, Batch 731/2250, Loss: 0.0820\n",
      "Epoch 23/25, Batch 741/2250, Loss: 0.0256\n",
      "Epoch 23/25, Batch 751/2250, Loss: 0.0401\n",
      "Epoch 23/25, Batch 761/2250, Loss: 0.0093\n",
      "Epoch 23/25, Batch 771/2250, Loss: 0.0024\n",
      "Epoch 23/25, Batch 781/2250, Loss: 0.0093\n",
      "Epoch 23/25, Batch 791/2250, Loss: 0.0093\n",
      "Epoch 23/25, Batch 801/2250, Loss: 0.0025\n",
      "Epoch 23/25, Batch 811/2250, Loss: 0.0140\n",
      "Epoch 23/25, Batch 821/2250, Loss: 0.0122\n",
      "Epoch 23/25, Batch 831/2250, Loss: 0.0135\n",
      "Epoch 23/25, Batch 841/2250, Loss: 0.0181\n",
      "Epoch 23/25, Batch 851/2250, Loss: 0.1990\n",
      "Epoch 23/25, Batch 861/2250, Loss: 0.0418\n",
      "Epoch 23/25, Batch 871/2250, Loss: 0.0171\n",
      "Epoch 23/25, Batch 881/2250, Loss: 0.0376\n",
      "Epoch 23/25, Batch 891/2250, Loss: 0.0412\n",
      "Epoch 23/25, Batch 901/2250, Loss: 0.0131\n",
      "Epoch 23/25, Batch 911/2250, Loss: 0.0072\n",
      "Epoch 23/25, Batch 921/2250, Loss: 0.0038\n",
      "Epoch 23/25, Batch 931/2250, Loss: 0.1147\n",
      "Epoch 23/25, Batch 941/2250, Loss: 0.0024\n",
      "Epoch 23/25, Batch 951/2250, Loss: 0.0051\n",
      "Epoch 23/25, Batch 961/2250, Loss: 0.1156\n",
      "Epoch 23/25, Batch 971/2250, Loss: 0.0786\n",
      "Epoch 23/25, Batch 981/2250, Loss: 0.0071\n",
      "Epoch 23/25, Batch 991/2250, Loss: 0.0310\n",
      "Epoch 23/25, Batch 1001/2250, Loss: 0.0010\n",
      "Epoch 23/25, Batch 1011/2250, Loss: 0.2182\n",
      "Epoch 23/25, Batch 1021/2250, Loss: 0.0075\n",
      "Epoch 23/25, Batch 1031/2250, Loss: 0.0617\n",
      "Epoch 23/25, Batch 1041/2250, Loss: 0.0009\n",
      "Epoch 23/25, Batch 1051/2250, Loss: 0.0211\n",
      "Epoch 23/25, Batch 1061/2250, Loss: 0.0166\n",
      "Epoch 23/25, Batch 1071/2250, Loss: 0.0087\n",
      "Epoch 23/25, Batch 1081/2250, Loss: 0.0203\n",
      "Epoch 23/25, Batch 1091/2250, Loss: 0.0039\n",
      "Epoch 23/25, Batch 1101/2250, Loss: 0.0203\n",
      "Epoch 23/25, Batch 1111/2250, Loss: 0.0571\n",
      "Epoch 23/25, Batch 1121/2250, Loss: 0.0091\n",
      "Epoch 23/25, Batch 1131/2250, Loss: 0.0056\n",
      "Epoch 23/25, Batch 1141/2250, Loss: 0.0210\n",
      "Epoch 23/25, Batch 1151/2250, Loss: 0.0027\n",
      "Epoch 23/25, Batch 1161/2250, Loss: 0.0267\n",
      "Epoch 23/25, Batch 1171/2250, Loss: 0.0330\n",
      "Epoch 23/25, Batch 1181/2250, Loss: 0.0093\n",
      "Epoch 23/25, Batch 1191/2250, Loss: 0.0938\n",
      "Epoch 23/25, Batch 1201/2250, Loss: 0.0264\n",
      "Epoch 23/25, Batch 1211/2250, Loss: 0.0089\n",
      "Epoch 23/25, Batch 1221/2250, Loss: 0.0066\n",
      "Epoch 23/25, Batch 1231/2250, Loss: 0.0843\n",
      "Epoch 23/25, Batch 1241/2250, Loss: 0.1615\n",
      "Epoch 23/25, Batch 1251/2250, Loss: 0.0345\n",
      "Epoch 23/25, Batch 1261/2250, Loss: 0.0823\n",
      "Epoch 23/25, Batch 1271/2250, Loss: 0.0214\n",
      "Epoch 23/25, Batch 1281/2250, Loss: 0.0173\n",
      "Epoch 23/25, Batch 1291/2250, Loss: 0.0437\n",
      "Epoch 23/25, Batch 1301/2250, Loss: 0.0377\n",
      "Epoch 23/25, Batch 1311/2250, Loss: 0.0496\n",
      "Epoch 23/25, Batch 1321/2250, Loss: 0.1259\n",
      "Epoch 23/25, Batch 1331/2250, Loss: 0.0128\n",
      "Epoch 23/25, Batch 1341/2250, Loss: 0.0046\n",
      "Epoch 23/25, Batch 1351/2250, Loss: 0.0116\n",
      "Epoch 23/25, Batch 1361/2250, Loss: 0.0208\n",
      "Epoch 23/25, Batch 1371/2250, Loss: 0.0041\n",
      "Epoch 23/25, Batch 1381/2250, Loss: 0.0191\n",
      "Epoch 23/25, Batch 1391/2250, Loss: 0.0828\n",
      "Epoch 23/25, Batch 1401/2250, Loss: 0.0264\n",
      "Epoch 23/25, Batch 1411/2250, Loss: 0.0036\n",
      "Epoch 23/25, Batch 1421/2250, Loss: 0.0082\n",
      "Epoch 23/25, Batch 1431/2250, Loss: 0.0129\n",
      "Epoch 23/25, Batch 1441/2250, Loss: 0.2963\n",
      "Epoch 23/25, Batch 1451/2250, Loss: 0.0202\n",
      "Epoch 23/25, Batch 1461/2250, Loss: 0.0141\n",
      "Epoch 23/25, Batch 1471/2250, Loss: 0.0199\n",
      "Epoch 23/25, Batch 1481/2250, Loss: 0.1379\n",
      "Epoch 23/25, Batch 1491/2250, Loss: 0.0203\n",
      "Epoch 23/25, Batch 1501/2250, Loss: 0.0142\n",
      "Epoch 23/25, Batch 1511/2250, Loss: 0.0503\n",
      "Epoch 23/25, Batch 1521/2250, Loss: 0.0045\n",
      "Epoch 23/25, Batch 1531/2250, Loss: 0.0207\n",
      "Epoch 23/25, Batch 1541/2250, Loss: 0.0171\n",
      "Epoch 23/25, Batch 1551/2250, Loss: 0.0123\n",
      "Epoch 23/25, Batch 1561/2250, Loss: 0.0073\n",
      "Epoch 23/25, Batch 1571/2250, Loss: 0.0276\n",
      "Epoch 23/25, Batch 1581/2250, Loss: 0.0869\n",
      "Epoch 23/25, Batch 1591/2250, Loss: 0.0139\n",
      "Epoch 23/25, Batch 1601/2250, Loss: 0.1922\n",
      "Epoch 23/25, Batch 1611/2250, Loss: 0.0015\n",
      "Epoch 23/25, Batch 1621/2250, Loss: 0.0045\n",
      "Epoch 23/25, Batch 1631/2250, Loss: 0.0078\n",
      "Epoch 23/25, Batch 1641/2250, Loss: 0.0372\n",
      "Epoch 23/25, Batch 1651/2250, Loss: 0.0048\n",
      "Epoch 23/25, Batch 1661/2250, Loss: 0.0271\n",
      "Epoch 23/25, Batch 1671/2250, Loss: 0.0962\n",
      "Epoch 23/25, Batch 1681/2250, Loss: 0.0123\n",
      "Epoch 23/25, Batch 1691/2250, Loss: 0.0125\n",
      "Epoch 23/25, Batch 1701/2250, Loss: 0.0684\n",
      "Epoch 23/25, Batch 1711/2250, Loss: 0.0364\n",
      "Epoch 23/25, Batch 1721/2250, Loss: 0.1098\n",
      "Epoch 23/25, Batch 1731/2250, Loss: 0.0040\n",
      "Epoch 23/25, Batch 1741/2250, Loss: 0.0369\n",
      "Epoch 23/25, Batch 1751/2250, Loss: 0.1601\n",
      "Epoch 23/25, Batch 1761/2250, Loss: 0.0053\n",
      "Epoch 23/25, Batch 1771/2250, Loss: 0.1145\n",
      "Epoch 23/25, Batch 1781/2250, Loss: 0.0524\n",
      "Epoch 23/25, Batch 1791/2250, Loss: 0.0070\n",
      "Epoch 23/25, Batch 1801/2250, Loss: 0.0022\n",
      "Epoch 23/25, Batch 1811/2250, Loss: 0.0029\n",
      "Epoch 23/25, Batch 1821/2250, Loss: 0.0162\n",
      "Epoch 23/25, Batch 1831/2250, Loss: 0.0139\n",
      "Epoch 23/25, Batch 1841/2250, Loss: 0.0027\n",
      "Epoch 23/25, Batch 1851/2250, Loss: 0.0162\n",
      "Epoch 23/25, Batch 1861/2250, Loss: 0.0766\n",
      "Epoch 23/25, Batch 1871/2250, Loss: 0.0330\n",
      "Epoch 23/25, Batch 1881/2250, Loss: 0.0033\n",
      "Epoch 23/25, Batch 1891/2250, Loss: 0.0635\n",
      "Epoch 23/25, Batch 1901/2250, Loss: 0.0044\n",
      "Epoch 23/25, Batch 1911/2250, Loss: 0.0115\n",
      "Epoch 23/25, Batch 1921/2250, Loss: 0.0011\n",
      "Epoch 23/25, Batch 1931/2250, Loss: 0.0025\n",
      "Epoch 23/25, Batch 1941/2250, Loss: 0.0033\n",
      "Epoch 23/25, Batch 1951/2250, Loss: 0.0133\n",
      "Epoch 23/25, Batch 1961/2250, Loss: 0.0054\n",
      "Epoch 23/25, Batch 1971/2250, Loss: 0.0574\n",
      "Epoch 23/25, Batch 1981/2250, Loss: 0.0112\n",
      "Epoch 23/25, Batch 1991/2250, Loss: 0.0143\n",
      "Epoch 23/25, Batch 2001/2250, Loss: 0.1552\n",
      "Epoch 23/25, Batch 2011/2250, Loss: 0.0085\n",
      "Epoch 23/25, Batch 2021/2250, Loss: 0.0095\n",
      "Epoch 23/25, Batch 2031/2250, Loss: 0.0022\n",
      "Epoch 23/25, Batch 2041/2250, Loss: 0.0454\n",
      "Epoch 23/25, Batch 2051/2250, Loss: 0.0528\n",
      "Epoch 23/25, Batch 2061/2250, Loss: 0.0098\n",
      "Epoch 23/25, Batch 2071/2250, Loss: 0.0229\n",
      "Epoch 23/25, Batch 2081/2250, Loss: 0.0163\n",
      "Epoch 23/25, Batch 2091/2250, Loss: 0.0377\n",
      "Epoch 23/25, Batch 2101/2250, Loss: 0.0454\n",
      "Epoch 23/25, Batch 2111/2250, Loss: 0.0521\n",
      "Epoch 23/25, Batch 2121/2250, Loss: 0.1813\n",
      "Epoch 23/25, Batch 2131/2250, Loss: 0.0255\n",
      "Epoch 23/25, Batch 2141/2250, Loss: 0.0186\n",
      "Epoch 23/25, Batch 2151/2250, Loss: 0.0103\n",
      "Epoch 23/25, Batch 2161/2250, Loss: 0.0647\n",
      "Epoch 23/25, Batch 2171/2250, Loss: 0.0565\n",
      "Epoch 23/25, Batch 2181/2250, Loss: 0.0917\n",
      "Epoch 23/25, Batch 2191/2250, Loss: 0.0179\n",
      "Epoch 23/25, Batch 2201/2250, Loss: 0.0757\n",
      "Epoch 23/25, Batch 2211/2250, Loss: 0.0570\n",
      "Epoch 23/25, Batch 2221/2250, Loss: 0.0138\n",
      "Epoch 23/25, Batch 2231/2250, Loss: 0.0045\n",
      "Epoch 23/25, Batch 2241/2250, Loss: 0.0344\n",
      "Epoch 23/25:\n",
      "Train Loss: 0.0406, Train Acc: 98.58%\n",
      "Val Loss: 0.0350, Val Acc: 98.75%\n",
      "------------------------------------------------------------\n",
      "Starting epoch 24/25...\n",
      "Starting training loop with 2250 batches...\n",
      "Successfully loaded first batch: images shape torch.Size([32, 3, 224, 224]), labels shape torch.Size([32])\n",
      "Epoch 24/25, Batch 1/2250, Loss: 0.0502\n",
      "Epoch 24/25, Batch 11/2250, Loss: 0.0048\n",
      "Epoch 24/25, Batch 21/2250, Loss: 0.0029\n",
      "Epoch 24/25, Batch 31/2250, Loss: 0.0047\n",
      "Epoch 24/25, Batch 41/2250, Loss: 0.0116\n",
      "Epoch 24/25, Batch 51/2250, Loss: 0.0033\n",
      "Epoch 24/25, Batch 61/2250, Loss: 0.0564\n",
      "Epoch 24/25, Batch 71/2250, Loss: 0.0100\n",
      "Epoch 24/25, Batch 81/2250, Loss: 0.0146\n",
      "Epoch 24/25, Batch 91/2250, Loss: 0.0024\n",
      "Epoch 24/25, Batch 101/2250, Loss: 0.0039\n",
      "Epoch 24/25, Batch 111/2250, Loss: 0.0975\n",
      "Epoch 24/25, Batch 121/2250, Loss: 0.0039\n",
      "Epoch 24/25, Batch 131/2250, Loss: 0.0852\n",
      "Epoch 24/25, Batch 141/2250, Loss: 0.0076\n",
      "Epoch 24/25, Batch 151/2250, Loss: 0.0571\n",
      "Epoch 24/25, Batch 161/2250, Loss: 0.0344\n",
      "Epoch 24/25, Batch 171/2250, Loss: 0.0730\n",
      "Epoch 24/25, Batch 181/2250, Loss: 0.0060\n",
      "Epoch 24/25, Batch 191/2250, Loss: 0.0040\n",
      "Epoch 24/25, Batch 201/2250, Loss: 0.0004\n",
      "Epoch 24/25, Batch 211/2250, Loss: 0.0106\n",
      "Epoch 24/25, Batch 221/2250, Loss: 0.0121\n",
      "Epoch 24/25, Batch 231/2250, Loss: 0.0399\n",
      "Epoch 24/25, Batch 241/2250, Loss: 0.0304\n",
      "Epoch 24/25, Batch 251/2250, Loss: 0.0039\n",
      "Epoch 24/25, Batch 261/2250, Loss: 0.0248\n",
      "Epoch 24/25, Batch 271/2250, Loss: 0.0612\n",
      "Epoch 24/25, Batch 281/2250, Loss: 0.0529\n",
      "Epoch 24/25, Batch 291/2250, Loss: 0.0262\n",
      "Epoch 24/25, Batch 301/2250, Loss: 0.0198\n",
      "Epoch 24/25, Batch 311/2250, Loss: 0.0135\n",
      "Epoch 24/25, Batch 321/2250, Loss: 0.0051\n",
      "Epoch 24/25, Batch 331/2250, Loss: 0.0249\n",
      "Epoch 24/25, Batch 341/2250, Loss: 0.0438\n",
      "Epoch 24/25, Batch 351/2250, Loss: 0.0671\n",
      "Epoch 24/25, Batch 361/2250, Loss: 0.0999\n",
      "Epoch 24/25, Batch 371/2250, Loss: 0.1533\n",
      "Epoch 24/25, Batch 381/2250, Loss: 0.0500\n",
      "Epoch 24/25, Batch 391/2250, Loss: 0.0050\n",
      "Epoch 24/25, Batch 401/2250, Loss: 0.0367\n",
      "Epoch 24/25, Batch 411/2250, Loss: 0.0132\n",
      "Epoch 24/25, Batch 421/2250, Loss: 0.0030\n",
      "Epoch 24/25, Batch 431/2250, Loss: 0.0781\n",
      "Epoch 24/25, Batch 441/2250, Loss: 0.0233\n",
      "Epoch 24/25, Batch 451/2250, Loss: 0.0249\n",
      "Epoch 24/25, Batch 461/2250, Loss: 0.0125\n",
      "Epoch 24/25, Batch 471/2250, Loss: 0.0012\n",
      "Epoch 24/25, Batch 481/2250, Loss: 0.0359\n",
      "Epoch 24/25, Batch 491/2250, Loss: 0.0029\n",
      "Epoch 24/25, Batch 501/2250, Loss: 0.0373\n",
      "Epoch 24/25, Batch 511/2250, Loss: 0.1036\n",
      "Epoch 24/25, Batch 521/2250, Loss: 0.0032\n",
      "Epoch 24/25, Batch 531/2250, Loss: 0.0217\n",
      "Epoch 24/25, Batch 541/2250, Loss: 0.0097\n",
      "Epoch 24/25, Batch 551/2250, Loss: 0.0007\n",
      "Epoch 24/25, Batch 561/2250, Loss: 0.0049\n",
      "Epoch 24/25, Batch 571/2250, Loss: 0.0073\n",
      "Epoch 24/25, Batch 581/2250, Loss: 0.1001\n",
      "Epoch 24/25, Batch 591/2250, Loss: 0.0047\n",
      "Epoch 24/25, Batch 601/2250, Loss: 0.0172\n",
      "Epoch 24/25, Batch 611/2250, Loss: 0.0196\n",
      "Epoch 24/25, Batch 621/2250, Loss: 0.0053\n",
      "Epoch 24/25, Batch 631/2250, Loss: 0.0142\n",
      "Epoch 24/25, Batch 641/2250, Loss: 0.0145\n",
      "Epoch 24/25, Batch 651/2250, Loss: 0.1402\n",
      "Epoch 24/25, Batch 661/2250, Loss: 0.0029\n",
      "Epoch 24/25, Batch 671/2250, Loss: 0.0074\n",
      "Epoch 24/25, Batch 681/2250, Loss: 0.1188\n",
      "Epoch 24/25, Batch 691/2250, Loss: 0.0008\n",
      "Epoch 24/25, Batch 701/2250, Loss: 0.0062\n",
      "Epoch 24/25, Batch 711/2250, Loss: 0.1107\n",
      "Epoch 24/25, Batch 721/2250, Loss: 0.0010\n",
      "Epoch 24/25, Batch 731/2250, Loss: 0.0050\n",
      "Epoch 24/25, Batch 741/2250, Loss: 0.0020\n",
      "Epoch 24/25, Batch 751/2250, Loss: 0.0621\n",
      "Epoch 24/25, Batch 761/2250, Loss: 0.0265\n",
      "Epoch 24/25, Batch 771/2250, Loss: 0.0274\n",
      "Epoch 24/25, Batch 781/2250, Loss: 0.0705\n",
      "Epoch 24/25, Batch 791/2250, Loss: 0.0106\n",
      "Epoch 24/25, Batch 801/2250, Loss: 0.0146\n",
      "Epoch 24/25, Batch 811/2250, Loss: 0.0063\n",
      "Epoch 24/25, Batch 821/2250, Loss: 0.0250\n",
      "Epoch 24/25, Batch 831/2250, Loss: 0.0643\n",
      "Epoch 24/25, Batch 841/2250, Loss: 0.0100\n",
      "Epoch 24/25, Batch 851/2250, Loss: 0.0018\n",
      "Epoch 24/25, Batch 861/2250, Loss: 0.0394\n",
      "Epoch 24/25, Batch 871/2250, Loss: 0.0175\n",
      "Epoch 24/25, Batch 881/2250, Loss: 0.0281\n",
      "Epoch 24/25, Batch 891/2250, Loss: 0.0181\n",
      "Epoch 24/25, Batch 901/2250, Loss: 0.1316\n",
      "Epoch 24/25, Batch 911/2250, Loss: 0.0127\n",
      "Epoch 24/25, Batch 921/2250, Loss: 0.0073\n",
      "Epoch 24/25, Batch 931/2250, Loss: 0.0094\n",
      "Epoch 24/25, Batch 941/2250, Loss: 0.0463\n",
      "Epoch 24/25, Batch 951/2250, Loss: 0.0250\n",
      "Epoch 24/25, Batch 961/2250, Loss: 0.0080\n",
      "Epoch 24/25, Batch 971/2250, Loss: 0.0012\n",
      "Epoch 24/25, Batch 981/2250, Loss: 0.0469\n",
      "Epoch 24/25, Batch 991/2250, Loss: 0.0737\n",
      "Epoch 24/25, Batch 1001/2250, Loss: 0.1827\n",
      "Epoch 24/25, Batch 1011/2250, Loss: 0.1542\n",
      "Epoch 24/25, Batch 1021/2250, Loss: 0.0058\n",
      "Epoch 24/25, Batch 1031/2250, Loss: 0.0711\n",
      "Epoch 24/25, Batch 1041/2250, Loss: 0.0177\n",
      "Epoch 24/25, Batch 1051/2250, Loss: 0.0170\n",
      "Epoch 24/25, Batch 1061/2250, Loss: 0.0224\n",
      "Epoch 24/25, Batch 1071/2250, Loss: 0.0090\n",
      "Epoch 24/25, Batch 1081/2250, Loss: 0.0083\n",
      "Epoch 24/25, Batch 1091/2250, Loss: 0.0333\n",
      "Epoch 24/25, Batch 1101/2250, Loss: 0.0012\n",
      "Epoch 24/25, Batch 1111/2250, Loss: 0.0981\n",
      "Epoch 24/25, Batch 1121/2250, Loss: 0.0024\n",
      "Epoch 24/25, Batch 1131/2250, Loss: 0.0973\n",
      "Epoch 24/25, Batch 1141/2250, Loss: 0.0607\n",
      "Epoch 24/25, Batch 1151/2250, Loss: 0.0274\n",
      "Epoch 24/25, Batch 1161/2250, Loss: 0.0386\n",
      "Epoch 24/25, Batch 1171/2250, Loss: 0.1302\n",
      "Epoch 24/25, Batch 1181/2250, Loss: 0.0687\n",
      "Epoch 24/25, Batch 1191/2250, Loss: 0.0407\n",
      "Epoch 24/25, Batch 1201/2250, Loss: 0.0017\n",
      "Epoch 24/25, Batch 1211/2250, Loss: 0.0060\n",
      "Epoch 24/25, Batch 1221/2250, Loss: 0.1182\n",
      "Epoch 24/25, Batch 1231/2250, Loss: 0.0095\n",
      "Epoch 24/25, Batch 1241/2250, Loss: 0.0201\n",
      "Epoch 24/25, Batch 1251/2250, Loss: 0.0057\n",
      "Epoch 24/25, Batch 1261/2250, Loss: 0.0039\n",
      "Epoch 24/25, Batch 1271/2250, Loss: 0.0860\n",
      "Epoch 24/25, Batch 1281/2250, Loss: 0.0081\n",
      "Epoch 24/25, Batch 1291/2250, Loss: 0.0240\n",
      "Epoch 24/25, Batch 1301/2250, Loss: 0.0015\n",
      "Epoch 24/25, Batch 1311/2250, Loss: 0.0103\n",
      "Epoch 24/25, Batch 1321/2250, Loss: 0.0014\n",
      "Epoch 24/25, Batch 1331/2250, Loss: 0.0148\n",
      "Epoch 24/25, Batch 1341/2250, Loss: 0.0160\n",
      "Epoch 24/25, Batch 1351/2250, Loss: 0.0496\n",
      "Epoch 24/25, Batch 1361/2250, Loss: 0.0351\n",
      "Epoch 24/25, Batch 1371/2250, Loss: 0.0080\n",
      "Epoch 24/25, Batch 1381/2250, Loss: 0.0412\n",
      "Epoch 24/25, Batch 1391/2250, Loss: 0.0302\n",
      "Epoch 24/25, Batch 1401/2250, Loss: 0.0274\n",
      "Epoch 24/25, Batch 1411/2250, Loss: 0.3065\n",
      "Epoch 24/25, Batch 1421/2250, Loss: 0.0960\n",
      "Epoch 24/25, Batch 1431/2250, Loss: 0.0683\n",
      "Epoch 24/25, Batch 1441/2250, Loss: 0.0174\n",
      "Epoch 24/25, Batch 1451/2250, Loss: 0.0027\n",
      "Epoch 24/25, Batch 1461/2250, Loss: 0.0256\n",
      "Epoch 24/25, Batch 1471/2250, Loss: 0.0190\n",
      "Epoch 24/25, Batch 1481/2250, Loss: 0.1128\n",
      "Epoch 24/25, Batch 1491/2250, Loss: 0.0844\n",
      "Epoch 24/25, Batch 1501/2250, Loss: 0.1278\n",
      "Epoch 24/25, Batch 1511/2250, Loss: 0.0278\n",
      "Epoch 24/25, Batch 1521/2250, Loss: 0.0200\n",
      "Epoch 24/25, Batch 1531/2250, Loss: 0.1247\n",
      "Epoch 24/25, Batch 1541/2250, Loss: 0.0113\n",
      "Epoch 24/25, Batch 1551/2250, Loss: 0.0096\n",
      "Epoch 24/25, Batch 1561/2250, Loss: 0.1268\n",
      "Epoch 24/25, Batch 1571/2250, Loss: 0.0203\n",
      "Epoch 24/25, Batch 1581/2250, Loss: 0.0586\n",
      "Epoch 24/25, Batch 1591/2250, Loss: 0.0403\n",
      "Epoch 24/25, Batch 1601/2250, Loss: 0.0237\n",
      "Epoch 24/25, Batch 1611/2250, Loss: 0.0012\n",
      "Epoch 24/25, Batch 1621/2250, Loss: 0.0305\n",
      "Epoch 24/25, Batch 1631/2250, Loss: 0.1512\n",
      "Epoch 24/25, Batch 1641/2250, Loss: 0.0056\n",
      "Epoch 24/25, Batch 1651/2250, Loss: 0.0057\n",
      "Epoch 24/25, Batch 1661/2250, Loss: 0.0099\n",
      "Epoch 24/25, Batch 1671/2250, Loss: 0.0754\n",
      "Epoch 24/25, Batch 1681/2250, Loss: 0.2622\n",
      "Epoch 24/25, Batch 1691/2250, Loss: 0.0621\n",
      "Epoch 24/25, Batch 1701/2250, Loss: 0.2074\n",
      "Epoch 24/25, Batch 1711/2250, Loss: 0.0080\n",
      "Epoch 24/25, Batch 1721/2250, Loss: 0.0198\n",
      "Epoch 24/25, Batch 1731/2250, Loss: 0.0948\n",
      "Epoch 24/25, Batch 1741/2250, Loss: 0.0336\n",
      "Epoch 24/25, Batch 1751/2250, Loss: 0.1737\n",
      "Epoch 24/25, Batch 1761/2250, Loss: 0.0011\n",
      "Epoch 24/25, Batch 1771/2250, Loss: 0.0790\n",
      "Epoch 24/25, Batch 1781/2250, Loss: 0.0017\n",
      "Epoch 24/25, Batch 1791/2250, Loss: 0.0307\n",
      "Epoch 24/25, Batch 1801/2250, Loss: 0.0503\n",
      "Epoch 24/25, Batch 1811/2250, Loss: 0.0033\n",
      "Epoch 24/25, Batch 1821/2250, Loss: 0.0083\n",
      "Epoch 24/25, Batch 1831/2250, Loss: 0.0819\n",
      "Epoch 24/25, Batch 1841/2250, Loss: 0.0041\n",
      "Epoch 24/25, Batch 1851/2250, Loss: 0.0014\n",
      "Epoch 24/25, Batch 1861/2250, Loss: 0.0026\n",
      "Epoch 24/25, Batch 1871/2250, Loss: 0.0178\n",
      "Epoch 24/25, Batch 1881/2250, Loss: 0.0215\n",
      "Epoch 24/25, Batch 1891/2250, Loss: 0.0035\n",
      "Epoch 24/25, Batch 1901/2250, Loss: 0.0034\n",
      "Epoch 24/25, Batch 1911/2250, Loss: 0.0299\n",
      "Epoch 24/25, Batch 1921/2250, Loss: 0.0035\n",
      "Epoch 24/25, Batch 1931/2250, Loss: 0.0518\n",
      "Epoch 24/25, Batch 1941/2250, Loss: 0.0033\n",
      "Epoch 24/25, Batch 1951/2250, Loss: 0.0055\n",
      "Epoch 24/25, Batch 1961/2250, Loss: 0.0033\n",
      "Epoch 24/25, Batch 1971/2250, Loss: 0.0064\n",
      "Epoch 24/25, Batch 1981/2250, Loss: 0.0062\n",
      "Epoch 24/25, Batch 1991/2250, Loss: 0.1810\n",
      "Epoch 24/25, Batch 2001/2250, Loss: 0.0162\n",
      "Epoch 24/25, Batch 2011/2250, Loss: 0.0368\n",
      "Epoch 24/25, Batch 2021/2250, Loss: 0.0372\n",
      "Epoch 24/25, Batch 2031/2250, Loss: 0.0156\n",
      "Epoch 24/25, Batch 2041/2250, Loss: 0.0775\n",
      "Epoch 24/25, Batch 2051/2250, Loss: 0.0033\n",
      "Epoch 24/25, Batch 2061/2250, Loss: 0.0244\n",
      "Epoch 24/25, Batch 2071/2250, Loss: 0.1663\n",
      "Epoch 24/25, Batch 2081/2250, Loss: 0.0172\n",
      "Epoch 24/25, Batch 2091/2250, Loss: 0.0081\n",
      "Epoch 24/25, Batch 2101/2250, Loss: 0.0667\n",
      "Epoch 24/25, Batch 2111/2250, Loss: 0.0028\n",
      "Epoch 24/25, Batch 2121/2250, Loss: 0.0293\n",
      "Epoch 24/25, Batch 2131/2250, Loss: 0.1469\n",
      "Epoch 24/25, Batch 2141/2250, Loss: 0.0111\n",
      "Epoch 24/25, Batch 2151/2250, Loss: 0.0005\n",
      "Epoch 24/25, Batch 2161/2250, Loss: 0.0059\n",
      "Epoch 24/25, Batch 2171/2250, Loss: 0.0336\n",
      "Epoch 24/25, Batch 2181/2250, Loss: 0.0020\n",
      "Epoch 24/25, Batch 2191/2250, Loss: 0.0151\n",
      "Epoch 24/25, Batch 2201/2250, Loss: 0.0037\n",
      "Epoch 24/25, Batch 2211/2250, Loss: 0.0066\n",
      "Epoch 24/25, Batch 2221/2250, Loss: 0.1726\n",
      "Epoch 24/25, Batch 2231/2250, Loss: 0.0277\n",
      "Epoch 24/25, Batch 2241/2250, Loss: 0.0673\n",
      "Epoch 24/25:\n",
      "Train Loss: 0.0401, Train Acc: 98.53%\n",
      "Val Loss: 0.0402, Val Acc: 98.45%\n",
      "------------------------------------------------------------\n",
      "Starting epoch 25/25...\n",
      "Starting training loop with 2250 batches...\n",
      "Successfully loaded first batch: images shape torch.Size([32, 3, 224, 224]), labels shape torch.Size([32])\n",
      "Epoch 25/25, Batch 1/2250, Loss: 0.1258\n",
      "Epoch 25/25, Batch 11/2250, Loss: 0.0324\n",
      "Epoch 25/25, Batch 21/2250, Loss: 0.0201\n",
      "Epoch 25/25, Batch 31/2250, Loss: 0.0040\n",
      "Epoch 25/25, Batch 41/2250, Loss: 0.0813\n",
      "Epoch 25/25, Batch 51/2250, Loss: 0.0050\n",
      "Epoch 25/25, Batch 61/2250, Loss: 0.0782\n",
      "Epoch 25/25, Batch 71/2250, Loss: 0.0651\n",
      "Epoch 25/25, Batch 81/2250, Loss: 0.0140\n",
      "Epoch 25/25, Batch 91/2250, Loss: 0.0208\n",
      "Epoch 25/25, Batch 101/2250, Loss: 0.0025\n",
      "Epoch 25/25, Batch 111/2250, Loss: 0.0063\n",
      "Epoch 25/25, Batch 121/2250, Loss: 0.0228\n",
      "Epoch 25/25, Batch 131/2250, Loss: 0.0072\n",
      "Epoch 25/25, Batch 141/2250, Loss: 0.0123\n",
      "Epoch 25/25, Batch 151/2250, Loss: 0.1573\n",
      "Epoch 25/25, Batch 161/2250, Loss: 0.0341\n",
      "Epoch 25/25, Batch 171/2250, Loss: 0.0121\n",
      "Epoch 25/25, Batch 181/2250, Loss: 0.0038\n",
      "Epoch 25/25, Batch 191/2250, Loss: 0.0135\n",
      "Epoch 25/25, Batch 201/2250, Loss: 0.0131\n",
      "Epoch 25/25, Batch 211/2250, Loss: 0.1400\n",
      "Epoch 25/25, Batch 221/2250, Loss: 0.0261\n",
      "Epoch 25/25, Batch 231/2250, Loss: 0.0161\n",
      "Epoch 25/25, Batch 241/2250, Loss: 0.0026\n",
      "Epoch 25/25, Batch 251/2250, Loss: 0.0228\n",
      "Epoch 25/25, Batch 261/2250, Loss: 0.0394\n",
      "Epoch 25/25, Batch 271/2250, Loss: 0.0059\n",
      "Epoch 25/25, Batch 281/2250, Loss: 0.0342\n",
      "Epoch 25/25, Batch 291/2250, Loss: 0.0049\n",
      "Epoch 25/25, Batch 301/2250, Loss: 0.1326\n",
      "Epoch 25/25, Batch 311/2250, Loss: 0.0553\n",
      "Epoch 25/25, Batch 321/2250, Loss: 0.0028\n",
      "Epoch 25/25, Batch 331/2250, Loss: 0.0130\n",
      "Epoch 25/25, Batch 341/2250, Loss: 0.0162\n",
      "Epoch 25/25, Batch 351/2250, Loss: 0.0843\n",
      "Epoch 25/25, Batch 361/2250, Loss: 0.0358\n",
      "Epoch 25/25, Batch 371/2250, Loss: 0.0790\n",
      "Epoch 25/25, Batch 381/2250, Loss: 0.0576\n",
      "Epoch 25/25, Batch 391/2250, Loss: 0.1252\n",
      "Epoch 25/25, Batch 401/2250, Loss: 0.0352\n",
      "Epoch 25/25, Batch 411/2250, Loss: 0.0864\n",
      "Epoch 25/25, Batch 421/2250, Loss: 0.0685\n",
      "Epoch 25/25, Batch 431/2250, Loss: 0.1002\n",
      "Epoch 25/25, Batch 441/2250, Loss: 0.0292\n",
      "Epoch 25/25, Batch 451/2250, Loss: 0.0627\n",
      "Epoch 25/25, Batch 461/2250, Loss: 0.0069\n",
      "Epoch 25/25, Batch 471/2250, Loss: 0.0017\n",
      "Epoch 25/25, Batch 481/2250, Loss: 0.0083\n",
      "Epoch 25/25, Batch 491/2250, Loss: 0.0553\n",
      "Epoch 25/25, Batch 501/2250, Loss: 0.0153\n",
      "Epoch 25/25, Batch 511/2250, Loss: 0.0029\n",
      "Epoch 25/25, Batch 521/2250, Loss: 0.0172\n",
      "Epoch 25/25, Batch 531/2250, Loss: 0.0056\n",
      "Epoch 25/25, Batch 541/2250, Loss: 0.0130\n",
      "Epoch 25/25, Batch 551/2250, Loss: 0.0061\n",
      "Epoch 25/25, Batch 561/2250, Loss: 0.0393\n",
      "Epoch 25/25, Batch 571/2250, Loss: 0.0219\n",
      "Epoch 25/25, Batch 581/2250, Loss: 0.0076\n",
      "Epoch 25/25, Batch 591/2250, Loss: 0.0123\n",
      "Epoch 25/25, Batch 601/2250, Loss: 0.0298\n",
      "Epoch 25/25, Batch 611/2250, Loss: 0.0811\n",
      "Epoch 25/25, Batch 621/2250, Loss: 0.0559\n",
      "Epoch 25/25, Batch 631/2250, Loss: 0.0496\n",
      "Epoch 25/25, Batch 641/2250, Loss: 0.0018\n",
      "Epoch 25/25, Batch 651/2250, Loss: 0.0852\n",
      "Epoch 25/25, Batch 661/2250, Loss: 0.0055\n",
      "Epoch 25/25, Batch 671/2250, Loss: 0.1388\n",
      "Epoch 25/25, Batch 681/2250, Loss: 0.0285\n",
      "Epoch 25/25, Batch 691/2250, Loss: 0.0015\n",
      "Epoch 25/25, Batch 701/2250, Loss: 0.0170\n",
      "Epoch 25/25, Batch 711/2250, Loss: 0.0016\n",
      "Epoch 25/25, Batch 721/2250, Loss: 0.0030\n",
      "Epoch 25/25, Batch 731/2250, Loss: 0.0401\n",
      "Epoch 25/25, Batch 741/2250, Loss: 0.0051\n",
      "Epoch 25/25, Batch 751/2250, Loss: 0.0114\n",
      "Epoch 25/25, Batch 761/2250, Loss: 0.0291\n",
      "Epoch 25/25, Batch 771/2250, Loss: 0.0141\n",
      "Epoch 25/25, Batch 781/2250, Loss: 0.0337\n",
      "Epoch 25/25, Batch 791/2250, Loss: 0.0383\n",
      "Epoch 25/25, Batch 801/2250, Loss: 0.0254\n",
      "Epoch 25/25, Batch 811/2250, Loss: 0.0281\n",
      "Epoch 25/25, Batch 821/2250, Loss: 0.0619\n",
      "Epoch 25/25, Batch 831/2250, Loss: 0.0391\n",
      "Epoch 25/25, Batch 841/2250, Loss: 0.0024\n",
      "Epoch 25/25, Batch 851/2250, Loss: 0.0057\n",
      "Epoch 25/25, Batch 861/2250, Loss: 0.0126\n",
      "Epoch 25/25, Batch 871/2250, Loss: 0.0045\n",
      "Epoch 25/25, Batch 881/2250, Loss: 0.1306\n",
      "Epoch 25/25, Batch 891/2250, Loss: 0.0229\n",
      "Epoch 25/25, Batch 901/2250, Loss: 0.0023\n",
      "Epoch 25/25, Batch 911/2250, Loss: 0.0256\n",
      "Epoch 25/25, Batch 921/2250, Loss: 0.0007\n",
      "Epoch 25/25, Batch 931/2250, Loss: 0.0100\n",
      "Epoch 25/25, Batch 941/2250, Loss: 0.0255\n",
      "Epoch 25/25, Batch 951/2250, Loss: 0.2040\n",
      "Epoch 25/25, Batch 961/2250, Loss: 0.0974\n",
      "Epoch 25/25, Batch 971/2250, Loss: 0.0092\n",
      "Epoch 25/25, Batch 981/2250, Loss: 0.0147\n",
      "Epoch 25/25, Batch 991/2250, Loss: 0.0235\n",
      "Epoch 25/25, Batch 1001/2250, Loss: 0.0729\n",
      "Epoch 25/25, Batch 1011/2250, Loss: 0.0005\n",
      "Epoch 25/25, Batch 1021/2250, Loss: 0.0222\n",
      "Epoch 25/25, Batch 1031/2250, Loss: 0.0292\n",
      "Epoch 25/25, Batch 1041/2250, Loss: 0.0030\n",
      "Epoch 25/25, Batch 1051/2250, Loss: 0.0012\n",
      "Epoch 25/25, Batch 1061/2250, Loss: 0.0337\n",
      "Epoch 25/25, Batch 1071/2250, Loss: 0.0094\n",
      "Epoch 25/25, Batch 1081/2250, Loss: 0.0254\n",
      "Epoch 25/25, Batch 1091/2250, Loss: 0.0038\n",
      "Epoch 25/25, Batch 1101/2250, Loss: 0.0413\n",
      "Epoch 25/25, Batch 1111/2250, Loss: 0.0189\n",
      "Epoch 25/25, Batch 1121/2250, Loss: 0.0096\n",
      "Epoch 25/25, Batch 1131/2250, Loss: 0.1110\n",
      "Epoch 25/25, Batch 1141/2250, Loss: 0.0589\n",
      "Epoch 25/25, Batch 1151/2250, Loss: 0.0029\n",
      "Epoch 25/25, Batch 1161/2250, Loss: 0.0023\n",
      "Epoch 25/25, Batch 1171/2250, Loss: 0.0483\n",
      "Epoch 25/25, Batch 1181/2250, Loss: 0.0019\n",
      "Epoch 25/25, Batch 1191/2250, Loss: 0.0150\n",
      "Epoch 25/25, Batch 1201/2250, Loss: 0.0953\n",
      "Epoch 25/25, Batch 1211/2250, Loss: 0.0409\n",
      "Epoch 25/25, Batch 1221/2250, Loss: 0.0188\n",
      "Epoch 25/25, Batch 1231/2250, Loss: 0.0226\n",
      "Epoch 25/25, Batch 1241/2250, Loss: 0.0281\n",
      "Epoch 25/25, Batch 1251/2250, Loss: 0.0352\n",
      "Epoch 25/25, Batch 1261/2250, Loss: 0.1526\n",
      "Epoch 25/25, Batch 1271/2250, Loss: 0.0078\n",
      "Epoch 25/25, Batch 1281/2250, Loss: 0.0300\n",
      "Epoch 25/25, Batch 1291/2250, Loss: 0.0397\n",
      "Epoch 25/25, Batch 1301/2250, Loss: 0.0076\n",
      "Epoch 25/25, Batch 1311/2250, Loss: 0.0251\n",
      "Epoch 25/25, Batch 1321/2250, Loss: 0.0029\n",
      "Epoch 25/25, Batch 1331/2250, Loss: 0.0498\n",
      "Epoch 25/25, Batch 1341/2250, Loss: 0.3346\n",
      "Epoch 25/25, Batch 1351/2250, Loss: 0.0078\n",
      "Epoch 25/25, Batch 1361/2250, Loss: 0.0200\n",
      "Epoch 25/25, Batch 1371/2250, Loss: 0.0239\n",
      "Epoch 25/25, Batch 1381/2250, Loss: 0.0725\n",
      "Epoch 25/25, Batch 1391/2250, Loss: 0.0048\n",
      "Epoch 25/25, Batch 1401/2250, Loss: 0.0154\n",
      "Epoch 25/25, Batch 1411/2250, Loss: 0.0058\n",
      "Epoch 25/25, Batch 1421/2250, Loss: 0.0082\n",
      "Epoch 25/25, Batch 1431/2250, Loss: 0.1749\n",
      "Epoch 25/25, Batch 1441/2250, Loss: 0.1065\n",
      "Epoch 25/25, Batch 1451/2250, Loss: 0.0384\n",
      "Epoch 25/25, Batch 1461/2250, Loss: 0.0279\n",
      "Epoch 25/25, Batch 1471/2250, Loss: 0.0018\n",
      "Epoch 25/25, Batch 1481/2250, Loss: 0.0044\n",
      "Epoch 25/25, Batch 1491/2250, Loss: 0.0002\n",
      "Epoch 25/25, Batch 1501/2250, Loss: 0.0785\n",
      "Epoch 25/25, Batch 1511/2250, Loss: 0.0176\n",
      "Epoch 25/25, Batch 1521/2250, Loss: 0.3237\n",
      "Epoch 25/25, Batch 1531/2250, Loss: 0.1768\n",
      "Epoch 25/25, Batch 1541/2250, Loss: 0.0137\n",
      "Epoch 25/25, Batch 1551/2250, Loss: 0.0330\n",
      "Epoch 25/25, Batch 1561/2250, Loss: 0.0081\n",
      "Epoch 25/25, Batch 1571/2250, Loss: 0.0377\n",
      "Epoch 25/25, Batch 1581/2250, Loss: 0.0046\n",
      "Epoch 25/25, Batch 1591/2250, Loss: 0.0107\n",
      "Epoch 25/25, Batch 1601/2250, Loss: 0.0030\n",
      "Epoch 25/25, Batch 1611/2250, Loss: 0.0120\n",
      "Epoch 25/25, Batch 1621/2250, Loss: 0.0073\n",
      "Epoch 25/25, Batch 1631/2250, Loss: 0.0577\n",
      "Epoch 25/25, Batch 1641/2250, Loss: 0.0124\n",
      "Epoch 25/25, Batch 1651/2250, Loss: 0.1029\n",
      "Epoch 25/25, Batch 1661/2250, Loss: 0.0145\n",
      "Epoch 25/25, Batch 1671/2250, Loss: 0.1023\n",
      "Epoch 25/25, Batch 1681/2250, Loss: 0.0123\n",
      "Epoch 25/25, Batch 1691/2250, Loss: 0.0121\n",
      "Epoch 25/25, Batch 1701/2250, Loss: 0.2648\n",
      "Epoch 25/25, Batch 1711/2250, Loss: 0.0397\n",
      "Epoch 25/25, Batch 1721/2250, Loss: 0.0021\n",
      "Epoch 25/25, Batch 1731/2250, Loss: 0.0064\n",
      "Epoch 25/25, Batch 1741/2250, Loss: 0.0150\n",
      "Epoch 25/25, Batch 1751/2250, Loss: 0.0291\n",
      "Epoch 25/25, Batch 1761/2250, Loss: 0.1001\n",
      "Epoch 25/25, Batch 1771/2250, Loss: 0.0016\n",
      "Epoch 25/25, Batch 1781/2250, Loss: 0.0027\n",
      "Epoch 25/25, Batch 1791/2250, Loss: 0.2049\n",
      "Epoch 25/25, Batch 1801/2250, Loss: 0.0180\n",
      "Epoch 25/25, Batch 1811/2250, Loss: 0.0044\n",
      "Epoch 25/25, Batch 1821/2250, Loss: 0.0463\n",
      "Epoch 25/25, Batch 1831/2250, Loss: 0.0121\n",
      "Epoch 25/25, Batch 1841/2250, Loss: 0.0100\n",
      "Epoch 25/25, Batch 1851/2250, Loss: 0.0021\n",
      "Epoch 25/25, Batch 1861/2250, Loss: 0.0101\n",
      "Epoch 25/25, Batch 1871/2250, Loss: 0.0357\n",
      "Epoch 25/25, Batch 1881/2250, Loss: 0.0332\n",
      "Epoch 25/25, Batch 1891/2250, Loss: 0.1138\n",
      "Epoch 25/25, Batch 1901/2250, Loss: 0.0119\n",
      "Epoch 25/25, Batch 1911/2250, Loss: 0.0285\n",
      "Epoch 25/25, Batch 1921/2250, Loss: 0.0016\n",
      "Epoch 25/25, Batch 1931/2250, Loss: 0.0108\n",
      "Epoch 25/25, Batch 1941/2250, Loss: 0.0055\n",
      "Epoch 25/25, Batch 1951/2250, Loss: 0.0310\n",
      "Epoch 25/25, Batch 1961/2250, Loss: 0.0080\n",
      "Epoch 25/25, Batch 1971/2250, Loss: 0.0175\n",
      "Epoch 25/25, Batch 1981/2250, Loss: 0.0058\n",
      "Epoch 25/25, Batch 1991/2250, Loss: 0.0074\n",
      "Epoch 25/25, Batch 2001/2250, Loss: 0.0379\n",
      "Epoch 25/25, Batch 2011/2250, Loss: 0.0039\n",
      "Epoch 25/25, Batch 2021/2250, Loss: 0.0059\n",
      "Epoch 25/25, Batch 2031/2250, Loss: 0.0018\n",
      "Epoch 25/25, Batch 2041/2250, Loss: 0.0047\n",
      "Epoch 25/25, Batch 2051/2250, Loss: 0.0135\n",
      "Epoch 25/25, Batch 2061/2250, Loss: 0.0018\n",
      "Epoch 25/25, Batch 2071/2250, Loss: 0.0222\n",
      "Epoch 25/25, Batch 2081/2250, Loss: 0.1596\n",
      "Epoch 25/25, Batch 2091/2250, Loss: 0.0412\n",
      "Epoch 25/25, Batch 2101/2250, Loss: 0.1986\n",
      "Epoch 25/25, Batch 2111/2250, Loss: 0.0160\n",
      "Epoch 25/25, Batch 2121/2250, Loss: 0.0730\n",
      "Epoch 25/25, Batch 2131/2250, Loss: 0.0128\n",
      "Epoch 25/25, Batch 2141/2250, Loss: 0.0681\n",
      "Epoch 25/25, Batch 2151/2250, Loss: 0.0298\n",
      "Epoch 25/25, Batch 2161/2250, Loss: 0.1685\n",
      "Epoch 25/25, Batch 2171/2250, Loss: 0.0007\n",
      "Epoch 25/25, Batch 2181/2250, Loss: 0.0163\n",
      "Epoch 25/25, Batch 2191/2250, Loss: 0.0164\n",
      "Epoch 25/25, Batch 2201/2250, Loss: 0.0172\n",
      "Epoch 25/25, Batch 2211/2250, Loss: 0.0326\n",
      "Epoch 25/25, Batch 2221/2250, Loss: 0.0024\n",
      "Epoch 25/25, Batch 2231/2250, Loss: 0.0080\n",
      "Epoch 25/25, Batch 2241/2250, Loss: 0.0004\n",
      "Epoch 25/25:\n",
      "Train Loss: 0.0374, Train Acc: 98.67%\n",
      "Val Loss: 0.0544, Val Acc: 98.14%\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1500x500 with 2 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading best model for evaluation...\n",
      "Test Accuracy: 0.9887\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Normal       0.99      0.99      0.99     12000\n",
      "      Stroke       0.99      0.99      0.99     12000\n",
      "\n",
      "    accuracy                           0.99     24000\n",
      "   macro avg       0.99      0.99      0.99     24000\n",
      "weighted avg       0.99      0.99      0.99     24000\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 800x600 with 2 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Test Accuracy: 0.9887\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-09T12:03:56.904949Z",
     "start_time": "2025-06-08T20:47:52.172993Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms, models\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from sklearn.metrics import classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "# -----------------------------\n",
    "# Settings & Hyperparameters\n",
    "# -----------------------------\n",
    "data_dir = \"Aug_dataset\"\n",
    "batch_size = 32  # reduce if using GPU with <6GB memory\n",
    "img_size = 224\n",
    "val_split = 0.2\n",
    "test_split = 0.2\n",
    "seed = 42\n",
    "num_workers = 4  # increase if your CPU can handle it\n",
    "epochs = 25\n",
    "\n",
    "# -----------------------------\n",
    "# Transforms\n",
    "# -----------------------------\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((img_size, img_size)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# -----------------------------\n",
    "# Dataset Loading & Splitting\n",
    "# -----------------------------\n",
    "dataset = datasets.ImageFolder(root=data_dir, transform=transform)\n",
    "class_names = dataset.classes\n",
    "\n",
    "total_size = len(dataset)\n",
    "val_size = int(val_split * total_size)\n",
    "test_size = int(test_split * total_size)\n",
    "train_size = total_size - val_size - test_size\n",
    "\n",
    "print(f\"Total samples: {total_size}\")\n",
    "print(f\"Train: {train_size}, Val: {val_size}, Test: {test_size}\")\n",
    "print(f\"Class names: {class_names}\")\n",
    "\n",
    "train_set, val_set, test_set = random_split(\n",
    "    dataset, [train_size, val_size, test_size],\n",
    "    generator=torch.Generator().manual_seed(seed)\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
    "val_loader = DataLoader(val_set, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
    "test_loader = DataLoader(test_set, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
    "\n",
    "# -----------------------------\n",
    "# Device Setup\n",
    "# -----------------------------\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# -----------------------------\n",
    "# Model: VGG16 + Fine-tuned Classifier\n",
    "# -----------------------------\n",
    "model = models.vgg16(pretrained=True)\n",
    "for param in model.features.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "model.classifier[6] = nn.Linear(4096, 2)  # 2 classes: Stroke and Normal\n",
    "model = model.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.classifier.parameters(), lr=1e-4)\n",
    "\n",
    "# -----------------------------\n",
    "# Evaluation Function\n",
    "# -----------------------------\n",
    "def evaluate_model(model, dataloader, silent=False):\n",
    "    model.eval()\n",
    "    correct, total = 0, 0\n",
    "    y_true, y_pred = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in dataloader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "            y_true.extend(labels.cpu().numpy())\n",
    "            y_pred.extend(preds.cpu().numpy())\n",
    "\n",
    "    acc = correct / total\n",
    "    if not silent:\n",
    "        print(classification_report(y_true, y_pred, target_names=class_names))\n",
    "    return acc\n",
    "\n",
    "# -----------------------------\n",
    "# Training Function\n",
    "# -----------------------------\n",
    "def train_model(model, epochs=10):\n",
    "    best_acc = 0\n",
    "    for epoch in range(epochs):\n",
    "        start = time.time()\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        correct, total = 0, 0\n",
    "\n",
    "        for batch_idx, (images, labels) in enumerate(train_loader):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "            # Batch progress logging\n",
    "            if batch_idx % 15 == 0:\n",
    "                print(f\"Epoch [{epoch+1}/{epochs}] \"\n",
    "                      f\"Step [{batch_idx}/{len(train_loader)}] \"\n",
    "                      f\"Loss: {loss.item():.4f}\")\n",
    "\n",
    "        # Epoch summary\n",
    "        train_acc = correct / total\n",
    "        val_acc = evaluate_model(model, val_loader, silent=True)\n",
    "        print(f\"Epoch [{epoch+1}/{epochs}] completed in {time.time() - start:.2f}s\")\n",
    "        print(f\"Train Accuracy: {train_acc:.4f}, Validation Accuracy: {val_acc:.4f}\\n\")\n",
    "\n",
    "        if val_acc > best_acc:\n",
    "            best_acc = val_acc\n",
    "            torch.save(model.state_dict(), \"Aug_dataset_vgg16_stroke.pth\")\n",
    "            print(f\"âœ… Best model updated and saved with Val Acc: {val_acc:.4f}\\n\")\n",
    "\n",
    "# -----------------------------\n",
    "# Run Training\n",
    "# -----------------------------\n",
    "train_model(model, epochs=epochs)\n",
    "# Load and Test the Best Model\n",
    "# -----------------------------\n",
    "model.load_state_dict(torch.load(\"Aug_dataset_vgg16_stroke.pth\"))\n",
    "print(\"\\nðŸ“Š Final Test Set Evaluation:\")\n",
    "evaluate_model(model, test_loader)"
   ],
   "id": "36b060a0b2aaf5a0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total samples: 117000\n",
      "Train: 70200, Val: 23400, Test: 23400\n",
      "Class names: ['Normal', 'Stroke']\n",
      "Using device: cuda\n",
      "Epoch [1/25] Step [0/2194] Loss: 0.9279\n",
      "Epoch [1/25] Step [15/2194] Loss: 0.7384\n",
      "Epoch [1/25] Step [30/2194] Loss: 0.5932\n",
      "Epoch [1/25] Step [45/2194] Loss: 0.6098\n",
      "Epoch [1/25] Step [60/2194] Loss: 0.5120\n",
      "Epoch [1/25] Step [75/2194] Loss: 0.4761\n",
      "Epoch [1/25] Step [90/2194] Loss: 0.5898\n",
      "Epoch [1/25] Step [105/2194] Loss: 0.6871\n",
      "Epoch [1/25] Step [120/2194] Loss: 0.5656\n",
      "Epoch [1/25] Step [135/2194] Loss: 0.6564\n",
      "Epoch [1/25] Step [150/2194] Loss: 0.6179\n",
      "Epoch [1/25] Step [165/2194] Loss: 0.5614\n",
      "Epoch [1/25] Step [180/2194] Loss: 0.5424\n",
      "Epoch [1/25] Step [195/2194] Loss: 0.5936\n",
      "Epoch [1/25] Step [210/2194] Loss: 0.3963\n",
      "Epoch [1/25] Step [225/2194] Loss: 0.6791\n",
      "Epoch [1/25] Step [240/2194] Loss: 0.5000\n",
      "Epoch [1/25] Step [255/2194] Loss: 0.5550\n",
      "Epoch [1/25] Step [270/2194] Loss: 0.6386\n",
      "Epoch [1/25] Step [285/2194] Loss: 0.5601\n",
      "Epoch [1/25] Step [300/2194] Loss: 0.5851\n",
      "Epoch [1/25] Step [315/2194] Loss: 0.4533\n",
      "Epoch [1/25] Step [330/2194] Loss: 0.5658\n",
      "Epoch [1/25] Step [345/2194] Loss: 0.4340\n",
      "Epoch [1/25] Step [360/2194] Loss: 0.5442\n",
      "Epoch [1/25] Step [375/2194] Loss: 0.4970\n",
      "Epoch [1/25] Step [390/2194] Loss: 0.6433\n",
      "Epoch [1/25] Step [405/2194] Loss: 0.5730\n",
      "Epoch [1/25] Step [420/2194] Loss: 0.6699\n",
      "Epoch [1/25] Step [435/2194] Loss: 0.5261\n",
      "Epoch [1/25] Step [450/2194] Loss: 0.4915\n",
      "Epoch [1/25] Step [465/2194] Loss: 0.4382\n",
      "Epoch [1/25] Step [480/2194] Loss: 0.4854\n",
      "Epoch [1/25] Step [495/2194] Loss: 0.6054\n",
      "Epoch [1/25] Step [510/2194] Loss: 0.3710\n",
      "Epoch [1/25] Step [525/2194] Loss: 0.4900\n",
      "Epoch [1/25] Step [540/2194] Loss: 0.5781\n",
      "Epoch [1/25] Step [555/2194] Loss: 0.4489\n",
      "Epoch [1/25] Step [570/2194] Loss: 0.5989\n",
      "Epoch [1/25] Step [585/2194] Loss: 0.4805\n",
      "Epoch [1/25] Step [600/2194] Loss: 0.6018\n",
      "Epoch [1/25] Step [615/2194] Loss: 0.7405\n",
      "Epoch [1/25] Step [630/2194] Loss: 0.4228\n",
      "Epoch [1/25] Step [645/2194] Loss: 0.5248\n",
      "Epoch [1/25] Step [660/2194] Loss: 0.5109\n",
      "Epoch [1/25] Step [675/2194] Loss: 0.5237\n",
      "Epoch [1/25] Step [690/2194] Loss: 0.5956\n",
      "Epoch [1/25] Step [705/2194] Loss: 0.4791\n",
      "Epoch [1/25] Step [720/2194] Loss: 0.3683\n",
      "Epoch [1/25] Step [735/2194] Loss: 0.4518\n",
      "Epoch [1/25] Step [750/2194] Loss: 0.3520\n",
      "Epoch [1/25] Step [765/2194] Loss: 0.3969\n",
      "Epoch [1/25] Step [780/2194] Loss: 0.5687\n",
      "Epoch [1/25] Step [795/2194] Loss: 0.5300\n",
      "Epoch [1/25] Step [810/2194] Loss: 0.4012\n",
      "Epoch [1/25] Step [825/2194] Loss: 0.5234\n",
      "Epoch [1/25] Step [840/2194] Loss: 0.5207\n",
      "Epoch [1/25] Step [855/2194] Loss: 0.4180\n",
      "Epoch [1/25] Step [870/2194] Loss: 0.4976\n",
      "Epoch [1/25] Step [885/2194] Loss: 0.4820\n",
      "Epoch [1/25] Step [900/2194] Loss: 0.5097\n",
      "Epoch [1/25] Step [915/2194] Loss: 0.4234\n",
      "Epoch [1/25] Step [930/2194] Loss: 0.4572\n",
      "Epoch [1/25] Step [945/2194] Loss: 0.5210\n",
      "Epoch [1/25] Step [960/2194] Loss: 0.5627\n",
      "Epoch [1/25] Step [975/2194] Loss: 0.5536\n",
      "Epoch [1/25] Step [990/2194] Loss: 0.3810\n",
      "Epoch [1/25] Step [1005/2194] Loss: 0.4040\n",
      "Epoch [1/25] Step [1020/2194] Loss: 0.5075\n",
      "Epoch [1/25] Step [1035/2194] Loss: 0.3844\n",
      "Epoch [1/25] Step [1050/2194] Loss: 0.6224\n",
      "Epoch [1/25] Step [1065/2194] Loss: 0.6266\n",
      "Epoch [1/25] Step [1080/2194] Loss: 0.3503\n",
      "Epoch [1/25] Step [1095/2194] Loss: 0.4648\n",
      "Epoch [1/25] Step [1110/2194] Loss: 0.4232\n",
      "Epoch [1/25] Step [1125/2194] Loss: 0.6333\n",
      "Epoch [1/25] Step [1140/2194] Loss: 0.3676\n",
      "Epoch [1/25] Step [1155/2194] Loss: 0.3072\n",
      "Epoch [1/25] Step [1170/2194] Loss: 0.5088\n",
      "Epoch [1/25] Step [1185/2194] Loss: 0.4975\n",
      "Epoch [1/25] Step [1200/2194] Loss: 0.3827\n",
      "Epoch [1/25] Step [1215/2194] Loss: 0.5675\n",
      "Epoch [1/25] Step [1230/2194] Loss: 0.6253\n",
      "Epoch [1/25] Step [1245/2194] Loss: 0.5676\n",
      "Epoch [1/25] Step [1260/2194] Loss: 0.6540\n",
      "Epoch [1/25] Step [1275/2194] Loss: 0.3638\n",
      "Epoch [1/25] Step [1290/2194] Loss: 0.4057\n",
      "Epoch [1/25] Step [1305/2194] Loss: 0.5296\n",
      "Epoch [1/25] Step [1320/2194] Loss: 0.3280\n",
      "Epoch [1/25] Step [1335/2194] Loss: 0.7240\n",
      "Epoch [1/25] Step [1350/2194] Loss: 0.4179\n",
      "Epoch [1/25] Step [1365/2194] Loss: 0.4017\n",
      "Epoch [1/25] Step [1380/2194] Loss: 0.5778\n",
      "Epoch [1/25] Step [1395/2194] Loss: 0.4785\n",
      "Epoch [1/25] Step [1410/2194] Loss: 0.3487\n",
      "Epoch [1/25] Step [1425/2194] Loss: 0.4285\n",
      "Epoch [1/25] Step [1440/2194] Loss: 0.4640\n",
      "Epoch [1/25] Step [1455/2194] Loss: 0.3934\n",
      "Epoch [1/25] Step [1470/2194] Loss: 0.3800\n",
      "Epoch [1/25] Step [1485/2194] Loss: 0.3552\n",
      "Epoch [1/25] Step [1500/2194] Loss: 0.3303\n",
      "Epoch [1/25] Step [1515/2194] Loss: 0.4588\n",
      "Epoch [1/25] Step [1530/2194] Loss: 0.3305\n",
      "Epoch [1/25] Step [1545/2194] Loss: 0.2526\n",
      "Epoch [1/25] Step [1560/2194] Loss: 0.4280\n",
      "Epoch [1/25] Step [1575/2194] Loss: 0.4371\n",
      "Epoch [1/25] Step [1590/2194] Loss: 0.4808\n",
      "Epoch [1/25] Step [1605/2194] Loss: 0.4752\n",
      "Epoch [1/25] Step [1620/2194] Loss: 0.4466\n",
      "Epoch [1/25] Step [1635/2194] Loss: 0.4825\n",
      "Epoch [1/25] Step [1650/2194] Loss: 0.5147\n",
      "Epoch [1/25] Step [1665/2194] Loss: 0.4328\n",
      "Epoch [1/25] Step [1680/2194] Loss: 0.6314\n",
      "Epoch [1/25] Step [1695/2194] Loss: 0.4153\n",
      "Epoch [1/25] Step [1710/2194] Loss: 0.3424\n",
      "Epoch [1/25] Step [1725/2194] Loss: 0.3933\n",
      "Epoch [1/25] Step [1740/2194] Loss: 0.4441\n",
      "Epoch [1/25] Step [1755/2194] Loss: 0.4616\n",
      "Epoch [1/25] Step [1770/2194] Loss: 0.5080\n",
      "Epoch [1/25] Step [1785/2194] Loss: 0.5342\n",
      "Epoch [1/25] Step [1800/2194] Loss: 0.3838\n",
      "Epoch [1/25] Step [1815/2194] Loss: 0.4937\n",
      "Epoch [1/25] Step [1830/2194] Loss: 0.4101\n",
      "Epoch [1/25] Step [1845/2194] Loss: 0.3750\n",
      "Epoch [1/25] Step [1860/2194] Loss: 0.3629\n",
      "Epoch [1/25] Step [1875/2194] Loss: 0.3539\n",
      "Epoch [1/25] Step [1890/2194] Loss: 0.4122\n",
      "Epoch [1/25] Step [1905/2194] Loss: 0.4451\n",
      "Epoch [1/25] Step [1920/2194] Loss: 0.3528\n",
      "Epoch [1/25] Step [1935/2194] Loss: 0.4862\n",
      "Epoch [1/25] Step [1950/2194] Loss: 0.2872\n",
      "Epoch [1/25] Step [1965/2194] Loss: 0.6281\n",
      "Epoch [1/25] Step [1980/2194] Loss: 0.4735\n",
      "Epoch [1/25] Step [1995/2194] Loss: 0.2544\n",
      "Epoch [1/25] Step [2010/2194] Loss: 0.3645\n",
      "Epoch [1/25] Step [2025/2194] Loss: 0.4271\n",
      "Epoch [1/25] Step [2040/2194] Loss: 0.5673\n",
      "Epoch [1/25] Step [2055/2194] Loss: 0.4789\n",
      "Epoch [1/25] Step [2070/2194] Loss: 0.3670\n",
      "Epoch [1/25] Step [2085/2194] Loss: 0.8382\n",
      "Epoch [1/25] Step [2100/2194] Loss: 0.3788\n",
      "Epoch [1/25] Step [2115/2194] Loss: 0.3369\n",
      "Epoch [1/25] Step [2130/2194] Loss: 0.3503\n",
      "Epoch [1/25] Step [2145/2194] Loss: 0.4938\n",
      "Epoch [1/25] Step [2160/2194] Loss: 0.3927\n",
      "Epoch [1/25] Step [2175/2194] Loss: 0.4108\n",
      "Epoch [1/25] Step [2190/2194] Loss: 0.3164\n",
      "Epoch [1/25] completed in 904.57s\n",
      "Train Accuracy: 0.7726, Validation Accuracy: 0.8235\n",
      "\n",
      "âœ… Best model updated and saved with Val Acc: 0.8235\n",
      "\n",
      "Epoch [2/25] Step [0/2194] Loss: 0.4651\n",
      "Epoch [2/25] Step [15/2194] Loss: 0.3273\n",
      "Epoch [2/25] Step [30/2194] Loss: 0.3775\n",
      "Epoch [2/25] Step [45/2194] Loss: 0.3836\n",
      "Epoch [2/25] Step [60/2194] Loss: 0.5094\n",
      "Epoch [2/25] Step [75/2194] Loss: 0.3831\n",
      "Epoch [2/25] Step [90/2194] Loss: 0.3521\n",
      "Epoch [2/25] Step [105/2194] Loss: 0.6095\n",
      "Epoch [2/25] Step [120/2194] Loss: 0.2305\n",
      "Epoch [2/25] Step [135/2194] Loss: 0.2461\n",
      "Epoch [2/25] Step [150/2194] Loss: 0.2829\n",
      "Epoch [2/25] Step [165/2194] Loss: 0.1758\n",
      "Epoch [2/25] Step [180/2194] Loss: 0.3582\n",
      "Epoch [2/25] Step [195/2194] Loss: 0.1663\n",
      "Epoch [2/25] Step [210/2194] Loss: 0.3211\n",
      "Epoch [2/25] Step [225/2194] Loss: 0.2004\n",
      "Epoch [2/25] Step [240/2194] Loss: 0.4407\n",
      "Epoch [2/25] Step [255/2194] Loss: 0.3258\n",
      "Epoch [2/25] Step [270/2194] Loss: 0.3420\n",
      "Epoch [2/25] Step [285/2194] Loss: 0.4285\n",
      "Epoch [2/25] Step [300/2194] Loss: 0.4980\n",
      "Epoch [2/25] Step [315/2194] Loss: 0.3129\n",
      "Epoch [2/25] Step [330/2194] Loss: 0.3797\n",
      "Epoch [2/25] Step [345/2194] Loss: 0.2561\n",
      "Epoch [2/25] Step [360/2194] Loss: 0.2419\n",
      "Epoch [2/25] Step [375/2194] Loss: 0.3932\n",
      "Epoch [2/25] Step [390/2194] Loss: 0.4932\n",
      "Epoch [2/25] Step [405/2194] Loss: 0.3525\n",
      "Epoch [2/25] Step [420/2194] Loss: 0.3619\n",
      "Epoch [2/25] Step [435/2194] Loss: 0.7775\n",
      "Epoch [2/25] Step [450/2194] Loss: 0.4652\n",
      "Epoch [2/25] Step [465/2194] Loss: 0.2422\n",
      "Epoch [2/25] Step [480/2194] Loss: 0.1967\n",
      "Epoch [2/25] Step [495/2194] Loss: 0.4396\n",
      "Epoch [2/25] Step [510/2194] Loss: 0.3570\n",
      "Epoch [2/25] Step [525/2194] Loss: 0.2606\n",
      "Epoch [2/25] Step [540/2194] Loss: 0.4190\n",
      "Epoch [2/25] Step [555/2194] Loss: 0.2697\n",
      "Epoch [2/25] Step [570/2194] Loss: 0.4238\n",
      "Epoch [2/25] Step [585/2194] Loss: 0.3615\n",
      "Epoch [2/25] Step [600/2194] Loss: 0.4985\n",
      "Epoch [2/25] Step [615/2194] Loss: 0.4297\n",
      "Epoch [2/25] Step [630/2194] Loss: 0.3609\n",
      "Epoch [2/25] Step [645/2194] Loss: 0.2165\n",
      "Epoch [2/25] Step [660/2194] Loss: 0.6817\n",
      "Epoch [2/25] Step [675/2194] Loss: 0.2717\n",
      "Epoch [2/25] Step [690/2194] Loss: 0.3415\n",
      "Epoch [2/25] Step [705/2194] Loss: 0.3253\n",
      "Epoch [2/25] Step [720/2194] Loss: 0.2632\n",
      "Epoch [2/25] Step [735/2194] Loss: 0.5516\n",
      "Epoch [2/25] Step [750/2194] Loss: 0.2754\n",
      "Epoch [2/25] Step [765/2194] Loss: 0.2741\n",
      "Epoch [2/25] Step [780/2194] Loss: 0.3110\n",
      "Epoch [2/25] Step [795/2194] Loss: 0.3984\n",
      "Epoch [2/25] Step [810/2194] Loss: 0.2828\n",
      "Epoch [2/25] Step [825/2194] Loss: 0.3620\n",
      "Epoch [2/25] Step [840/2194] Loss: 0.3521\n",
      "Epoch [2/25] Step [855/2194] Loss: 0.5543\n",
      "Epoch [2/25] Step [870/2194] Loss: 0.4164\n",
      "Epoch [2/25] Step [885/2194] Loss: 0.4890\n",
      "Epoch [2/25] Step [900/2194] Loss: 0.3108\n",
      "Epoch [2/25] Step [915/2194] Loss: 0.4461\n",
      "Epoch [2/25] Step [930/2194] Loss: 0.4313\n",
      "Epoch [2/25] Step [945/2194] Loss: 0.3029\n",
      "Epoch [2/25] Step [960/2194] Loss: 0.3041\n",
      "Epoch [2/25] Step [975/2194] Loss: 0.2200\n",
      "Epoch [2/25] Step [990/2194] Loss: 0.3351\n",
      "Epoch [2/25] Step [1005/2194] Loss: 0.5968\n",
      "Epoch [2/25] Step [1020/2194] Loss: 0.2620\n",
      "Epoch [2/25] Step [1035/2194] Loss: 0.5039\n",
      "Epoch [2/25] Step [1050/2194] Loss: 0.4991\n",
      "Epoch [2/25] Step [1065/2194] Loss: 0.2602\n",
      "Epoch [2/25] Step [1080/2194] Loss: 0.3615\n",
      "Epoch [2/25] Step [1095/2194] Loss: 0.3962\n",
      "Epoch [2/25] Step [1110/2194] Loss: 0.4414\n",
      "Epoch [2/25] Step [1125/2194] Loss: 0.3583\n",
      "Epoch [2/25] Step [1140/2194] Loss: 0.4014\n",
      "Epoch [2/25] Step [1155/2194] Loss: 0.3803\n",
      "Epoch [2/25] Step [1170/2194] Loss: 0.4352\n",
      "Epoch [2/25] Step [1185/2194] Loss: 0.3263\n",
      "Epoch [2/25] Step [1200/2194] Loss: 0.4722\n",
      "Epoch [2/25] Step [1215/2194] Loss: 0.2649\n",
      "Epoch [2/25] Step [1230/2194] Loss: 0.2799\n",
      "Epoch [2/25] Step [1245/2194] Loss: 0.3539\n",
      "Epoch [2/25] Step [1260/2194] Loss: 0.5232\n",
      "Epoch [2/25] Step [1275/2194] Loss: 0.3359\n",
      "Epoch [2/25] Step [1290/2194] Loss: 0.2413\n",
      "Epoch [2/25] Step [1305/2194] Loss: 0.4487\n",
      "Epoch [2/25] Step [1320/2194] Loss: 0.2732\n",
      "Epoch [2/25] Step [1335/2194] Loss: 0.2547\n",
      "Epoch [2/25] Step [1350/2194] Loss: 0.3888\n",
      "Epoch [2/25] Step [1365/2194] Loss: 0.3301\n",
      "Epoch [2/25] Step [1380/2194] Loss: 0.3394\n",
      "Epoch [2/25] Step [1395/2194] Loss: 0.4302\n",
      "Epoch [2/25] Step [1410/2194] Loss: 0.5622\n",
      "Epoch [2/25] Step [1425/2194] Loss: 0.3121\n",
      "Epoch [2/25] Step [1440/2194] Loss: 0.4283\n",
      "Epoch [2/25] Step [1455/2194] Loss: 0.3675\n",
      "Epoch [2/25] Step [1470/2194] Loss: 0.3703\n",
      "Epoch [2/25] Step [1485/2194] Loss: 0.3658\n",
      "Epoch [2/25] Step [1500/2194] Loss: 0.4016\n",
      "Epoch [2/25] Step [1515/2194] Loss: 0.5678\n",
      "Epoch [2/25] Step [1530/2194] Loss: 0.2375\n",
      "Epoch [2/25] Step [1545/2194] Loss: 0.2287\n",
      "Epoch [2/25] Step [1560/2194] Loss: 0.3146\n",
      "Epoch [2/25] Step [1575/2194] Loss: 0.3161\n",
      "Epoch [2/25] Step [1590/2194] Loss: 0.3049\n",
      "Epoch [2/25] Step [1605/2194] Loss: 0.4640\n",
      "Epoch [2/25] Step [1620/2194] Loss: 0.5779\n",
      "Epoch [2/25] Step [1635/2194] Loss: 0.3401\n",
      "Epoch [2/25] Step [1650/2194] Loss: 0.4467\n",
      "Epoch [2/25] Step [1665/2194] Loss: 0.3510\n",
      "Epoch [2/25] Step [1680/2194] Loss: 0.3384\n",
      "Epoch [2/25] Step [1695/2194] Loss: 0.3807\n",
      "Epoch [2/25] Step [1710/2194] Loss: 0.4342\n",
      "Epoch [2/25] Step [1725/2194] Loss: 0.2124\n",
      "Epoch [2/25] Step [1740/2194] Loss: 0.4090\n",
      "Epoch [2/25] Step [1755/2194] Loss: 0.2671\n",
      "Epoch [2/25] Step [1770/2194] Loss: 0.5567\n",
      "Epoch [2/25] Step [1785/2194] Loss: 0.2799\n",
      "Epoch [2/25] Step [1800/2194] Loss: 0.4786\n",
      "Epoch [2/25] Step [1815/2194] Loss: 0.3742\n",
      "Epoch [2/25] Step [1830/2194] Loss: 0.3172\n",
      "Epoch [2/25] Step [1845/2194] Loss: 0.2898\n",
      "Epoch [2/25] Step [1860/2194] Loss: 0.4249\n",
      "Epoch [2/25] Step [1875/2194] Loss: 0.2996\n",
      "Epoch [2/25] Step [1890/2194] Loss: 0.1955\n",
      "Epoch [2/25] Step [1905/2194] Loss: 0.3781\n",
      "Epoch [2/25] Step [1920/2194] Loss: 0.5902\n",
      "Epoch [2/25] Step [1935/2194] Loss: 0.4858\n",
      "Epoch [2/25] Step [1950/2194] Loss: 0.5113\n",
      "Epoch [2/25] Step [1965/2194] Loss: 0.2720\n",
      "Epoch [2/25] Step [1980/2194] Loss: 0.3837\n",
      "Epoch [2/25] Step [1995/2194] Loss: 0.2367\n",
      "Epoch [2/25] Step [2010/2194] Loss: 0.4434\n",
      "Epoch [2/25] Step [2025/2194] Loss: 0.3042\n",
      "Epoch [2/25] Step [2040/2194] Loss: 0.2473\n",
      "Epoch [2/25] Step [2055/2194] Loss: 0.4712\n",
      "Epoch [2/25] Step [2070/2194] Loss: 0.4099\n",
      "Epoch [2/25] Step [2085/2194] Loss: 0.3851\n",
      "Epoch [2/25] Step [2100/2194] Loss: 0.2824\n",
      "Epoch [2/25] Step [2115/2194] Loss: 0.3110\n",
      "Epoch [2/25] Step [2130/2194] Loss: 0.5674\n",
      "Epoch [2/25] Step [2145/2194] Loss: 0.4046\n",
      "Epoch [2/25] Step [2160/2194] Loss: 0.7078\n",
      "Epoch [2/25] Step [2175/2194] Loss: 0.3921\n",
      "Epoch [2/25] Step [2190/2194] Loss: 0.2412\n",
      "Epoch [2/25] completed in 894.59s\n",
      "Train Accuracy: 0.8374, Validation Accuracy: 0.8542\n",
      "\n",
      "âœ… Best model updated and saved with Val Acc: 0.8542\n",
      "\n",
      "Epoch [3/25] Step [0/2194] Loss: 0.3049\n",
      "Epoch [3/25] Step [15/2194] Loss: 0.3342\n",
      "Epoch [3/25] Step [30/2194] Loss: 0.3045\n",
      "Epoch [3/25] Step [45/2194] Loss: 0.1970\n",
      "Epoch [3/25] Step [60/2194] Loss: 0.4382\n",
      "Epoch [3/25] Step [75/2194] Loss: 0.1991\n",
      "Epoch [3/25] Step [90/2194] Loss: 0.2684\n",
      "Epoch [3/25] Step [105/2194] Loss: 0.3404\n",
      "Epoch [3/25] Step [120/2194] Loss: 0.2121\n",
      "Epoch [3/25] Step [135/2194] Loss: 0.3780\n",
      "Epoch [3/25] Step [150/2194] Loss: 0.1990\n",
      "Epoch [3/25] Step [165/2194] Loss: 0.1624\n",
      "Epoch [3/25] Step [180/2194] Loss: 0.5192\n",
      "Epoch [3/25] Step [195/2194] Loss: 0.3231\n",
      "Epoch [3/25] Step [210/2194] Loss: 0.3874\n",
      "Epoch [3/25] Step [225/2194] Loss: 0.3548\n",
      "Epoch [3/25] Step [240/2194] Loss: 0.2098\n",
      "Epoch [3/25] Step [255/2194] Loss: 0.2009\n",
      "Epoch [3/25] Step [270/2194] Loss: 0.3119\n",
      "Epoch [3/25] Step [285/2194] Loss: 0.1805\n",
      "Epoch [3/25] Step [300/2194] Loss: 0.2532\n",
      "Epoch [3/25] Step [315/2194] Loss: 0.2250\n",
      "Epoch [3/25] Step [330/2194] Loss: 0.1326\n",
      "Epoch [3/25] Step [345/2194] Loss: 0.3598\n",
      "Epoch [3/25] Step [360/2194] Loss: 0.3523\n",
      "Epoch [3/25] Step [375/2194] Loss: 0.2694\n",
      "Epoch [3/25] Step [390/2194] Loss: 0.1997\n",
      "Epoch [3/25] Step [405/2194] Loss: 0.2662\n",
      "Epoch [3/25] Step [420/2194] Loss: 0.1836\n",
      "Epoch [3/25] Step [435/2194] Loss: 0.2937\n",
      "Epoch [3/25] Step [450/2194] Loss: 0.2739\n",
      "Epoch [3/25] Step [465/2194] Loss: 0.4156\n",
      "Epoch [3/25] Step [480/2194] Loss: 0.2194\n",
      "Epoch [3/25] Step [495/2194] Loss: 0.3105\n",
      "Epoch [3/25] Step [510/2194] Loss: 0.0916\n",
      "Epoch [3/25] Step [525/2194] Loss: 0.3001\n",
      "Epoch [3/25] Step [540/2194] Loss: 0.4958\n",
      "Epoch [3/25] Step [555/2194] Loss: 0.3214\n",
      "Epoch [3/25] Step [570/2194] Loss: 0.4600\n",
      "Epoch [3/25] Step [585/2194] Loss: 0.2082\n",
      "Epoch [3/25] Step [600/2194] Loss: 0.3441\n",
      "Epoch [3/25] Step [615/2194] Loss: 0.3365\n",
      "Epoch [3/25] Step [630/2194] Loss: 0.2066\n",
      "Epoch [3/25] Step [645/2194] Loss: 0.1913\n",
      "Epoch [3/25] Step [660/2194] Loss: 0.4137\n",
      "Epoch [3/25] Step [675/2194] Loss: 0.3268\n",
      "Epoch [3/25] Step [690/2194] Loss: 0.3677\n",
      "Epoch [3/25] Step [705/2194] Loss: 0.2329\n",
      "Epoch [3/25] Step [720/2194] Loss: 0.3067\n",
      "Epoch [3/25] Step [735/2194] Loss: 0.2588\n",
      "Epoch [3/25] Step [750/2194] Loss: 0.3271\n",
      "Epoch [3/25] Step [765/2194] Loss: 0.3537\n",
      "Epoch [3/25] Step [780/2194] Loss: 0.2156\n",
      "Epoch [3/25] Step [795/2194] Loss: 0.2624\n",
      "Epoch [3/25] Step [810/2194] Loss: 0.3059\n",
      "Epoch [3/25] Step [825/2194] Loss: 0.5406\n",
      "Epoch [3/25] Step [840/2194] Loss: 0.3997\n",
      "Epoch [3/25] Step [855/2194] Loss: 0.4061\n",
      "Epoch [3/25] Step [870/2194] Loss: 0.1431\n",
      "Epoch [3/25] Step [885/2194] Loss: 0.1955\n",
      "Epoch [3/25] Step [900/2194] Loss: 0.3618\n",
      "Epoch [3/25] Step [915/2194] Loss: 0.4195\n",
      "Epoch [3/25] Step [930/2194] Loss: 0.2702\n",
      "Epoch [3/25] Step [945/2194] Loss: 0.4184\n",
      "Epoch [3/25] Step [960/2194] Loss: 0.2231\n",
      "Epoch [3/25] Step [975/2194] Loss: 0.3428\n",
      "Epoch [3/25] Step [990/2194] Loss: 0.5919\n",
      "Epoch [3/25] Step [1005/2194] Loss: 0.2912\n",
      "Epoch [3/25] Step [1020/2194] Loss: 0.2789\n",
      "Epoch [3/25] Step [1035/2194] Loss: 0.2177\n",
      "Epoch [3/25] Step [1050/2194] Loss: 0.0965\n",
      "Epoch [3/25] Step [1065/2194] Loss: 0.3295\n",
      "Epoch [3/25] Step [1080/2194] Loss: 0.2645\n",
      "Epoch [3/25] Step [1095/2194] Loss: 0.1527\n",
      "Epoch [3/25] Step [1110/2194] Loss: 0.2492\n",
      "Epoch [3/25] Step [1125/2194] Loss: 0.1546\n",
      "Epoch [3/25] Step [1140/2194] Loss: 0.3834\n",
      "Epoch [3/25] Step [1155/2194] Loss: 0.3240\n",
      "Epoch [3/25] Step [1170/2194] Loss: 0.3672\n",
      "Epoch [3/25] Step [1185/2194] Loss: 0.2934\n",
      "Epoch [3/25] Step [1200/2194] Loss: 0.3508\n",
      "Epoch [3/25] Step [1215/2194] Loss: 0.4606\n",
      "Epoch [3/25] Step [1230/2194] Loss: 0.3123\n",
      "Epoch [3/25] Step [1245/2194] Loss: 0.1906\n",
      "Epoch [3/25] Step [1260/2194] Loss: 0.3732\n",
      "Epoch [3/25] Step [1275/2194] Loss: 0.2690\n",
      "Epoch [3/25] Step [1290/2194] Loss: 0.3012\n",
      "Epoch [3/25] Step [1305/2194] Loss: 0.3079\n",
      "Epoch [3/25] Step [1320/2194] Loss: 0.3116\n",
      "Epoch [3/25] Step [1335/2194] Loss: 0.3193\n",
      "Epoch [3/25] Step [1350/2194] Loss: 0.2430\n",
      "Epoch [3/25] Step [1365/2194] Loss: 0.3128\n",
      "Epoch [3/25] Step [1380/2194] Loss: 0.2425\n",
      "Epoch [3/25] Step [1395/2194] Loss: 0.2911\n",
      "Epoch [3/25] Step [1410/2194] Loss: 0.1487\n",
      "Epoch [3/25] Step [1425/2194] Loss: 0.4147\n",
      "Epoch [3/25] Step [1440/2194] Loss: 0.4452\n",
      "Epoch [3/25] Step [1455/2194] Loss: 0.3758\n",
      "Epoch [3/25] Step [1470/2194] Loss: 0.3564\n",
      "Epoch [3/25] Step [1485/2194] Loss: 0.3394\n",
      "Epoch [3/25] Step [1500/2194] Loss: 0.2039\n",
      "Epoch [3/25] Step [1515/2194] Loss: 0.4028\n",
      "Epoch [3/25] Step [1530/2194] Loss: 0.2012\n",
      "Epoch [3/25] Step [1545/2194] Loss: 0.1441\n",
      "Epoch [3/25] Step [1560/2194] Loss: 0.2024\n",
      "Epoch [3/25] Step [1575/2194] Loss: 0.6748\n",
      "Epoch [3/25] Step [1590/2194] Loss: 0.2296\n",
      "Epoch [3/25] Step [1605/2194] Loss: 0.1960\n",
      "Epoch [3/25] Step [1620/2194] Loss: 0.1704\n",
      "Epoch [3/25] Step [1635/2194] Loss: 0.3437\n",
      "Epoch [3/25] Step [1650/2194] Loss: 0.3251\n",
      "Epoch [3/25] Step [1665/2194] Loss: 0.1917\n",
      "Epoch [3/25] Step [1680/2194] Loss: 0.2803\n",
      "Epoch [3/25] Step [1695/2194] Loss: 0.5898\n",
      "Epoch [3/25] Step [1710/2194] Loss: 0.2360\n",
      "Epoch [3/25] Step [1725/2194] Loss: 0.4134\n",
      "Epoch [3/25] Step [1740/2194] Loss: 0.1426\n",
      "Epoch [3/25] Step [1755/2194] Loss: 0.4078\n",
      "Epoch [3/25] Step [1770/2194] Loss: 0.1584\n",
      "Epoch [3/25] Step [1785/2194] Loss: 0.2662\n",
      "Epoch [3/25] Step [1800/2194] Loss: 0.2090\n",
      "Epoch [3/25] Step [1815/2194] Loss: 0.1956\n",
      "Epoch [3/25] Step [1830/2194] Loss: 0.3221\n",
      "Epoch [3/25] Step [1845/2194] Loss: 0.3075\n",
      "Epoch [3/25] Step [1860/2194] Loss: 0.1987\n",
      "Epoch [3/25] Step [1875/2194] Loss: 0.2301\n",
      "Epoch [3/25] Step [1890/2194] Loss: 0.2900\n",
      "Epoch [3/25] Step [1905/2194] Loss: 0.1382\n",
      "Epoch [3/25] Step [1920/2194] Loss: 0.2542\n",
      "Epoch [3/25] Step [1935/2194] Loss: 0.2941\n",
      "Epoch [3/25] Step [1950/2194] Loss: 0.2368\n",
      "Epoch [3/25] Step [1965/2194] Loss: 0.4928\n",
      "Epoch [3/25] Step [1980/2194] Loss: 0.3702\n",
      "Epoch [3/25] Step [1995/2194] Loss: 0.1916\n",
      "Epoch [3/25] Step [2010/2194] Loss: 0.3177\n",
      "Epoch [3/25] Step [2025/2194] Loss: 0.1694\n",
      "Epoch [3/25] Step [2040/2194] Loss: 0.3032\n",
      "Epoch [3/25] Step [2055/2194] Loss: 0.1484\n",
      "Epoch [3/25] Step [2070/2194] Loss: 0.1990\n",
      "Epoch [3/25] Step [2085/2194] Loss: 0.3467\n",
      "Epoch [3/25] Step [2100/2194] Loss: 0.3581\n",
      "Epoch [3/25] Step [2115/2194] Loss: 0.2974\n",
      "Epoch [3/25] Step [2130/2194] Loss: 0.3376\n",
      "Epoch [3/25] Step [2145/2194] Loss: 0.4164\n",
      "Epoch [3/25] Step [2160/2194] Loss: 0.2092\n",
      "Epoch [3/25] Step [2175/2194] Loss: 0.2340\n",
      "Epoch [3/25] Step [2190/2194] Loss: 0.2463\n",
      "Epoch [3/25] completed in 892.96s\n",
      "Train Accuracy: 0.8752, Validation Accuracy: 0.8691\n",
      "\n",
      "âœ… Best model updated and saved with Val Acc: 0.8691\n",
      "\n",
      "Epoch [4/25] Step [0/2194] Loss: 0.2165\n",
      "Epoch [4/25] Step [15/2194] Loss: 0.1712\n",
      "Epoch [4/25] Step [30/2194] Loss: 0.3821\n",
      "Epoch [4/25] Step [45/2194] Loss: 0.2726\n",
      "Epoch [4/25] Step [60/2194] Loss: 0.0608\n",
      "Epoch [4/25] Step [75/2194] Loss: 0.2062\n",
      "Epoch [4/25] Step [90/2194] Loss: 0.2514\n",
      "Epoch [4/25] Step [105/2194] Loss: 0.1767\n",
      "Epoch [4/25] Step [120/2194] Loss: 0.2718\n",
      "Epoch [4/25] Step [135/2194] Loss: 0.1829\n",
      "Epoch [4/25] Step [150/2194] Loss: 0.2390\n",
      "Epoch [4/25] Step [165/2194] Loss: 0.1091\n",
      "Epoch [4/25] Step [180/2194] Loss: 0.1946\n",
      "Epoch [4/25] Step [195/2194] Loss: 0.3557\n",
      "Epoch [4/25] Step [210/2194] Loss: 0.2630\n",
      "Epoch [4/25] Step [225/2194] Loss: 0.2860\n",
      "Epoch [4/25] Step [240/2194] Loss: 0.1570\n",
      "Epoch [4/25] Step [255/2194] Loss: 0.3062\n",
      "Epoch [4/25] Step [270/2194] Loss: 0.1782\n",
      "Epoch [4/25] Step [285/2194] Loss: 0.2868\n",
      "Epoch [4/25] Step [300/2194] Loss: 0.2251\n",
      "Epoch [4/25] Step [315/2194] Loss: 0.2868\n",
      "Epoch [4/25] Step [330/2194] Loss: 0.2332\n",
      "Epoch [4/25] Step [345/2194] Loss: 0.2114\n",
      "Epoch [4/25] Step [360/2194] Loss: 0.0800\n",
      "Epoch [4/25] Step [375/2194] Loss: 0.2813\n",
      "Epoch [4/25] Step [390/2194] Loss: 0.2218\n",
      "Epoch [4/25] Step [405/2194] Loss: 0.1374\n",
      "Epoch [4/25] Step [420/2194] Loss: 0.1469\n",
      "Epoch [4/25] Step [435/2194] Loss: 0.1533\n",
      "Epoch [4/25] Step [450/2194] Loss: 0.5099\n",
      "Epoch [4/25] Step [465/2194] Loss: 0.2141\n",
      "Epoch [4/25] Step [480/2194] Loss: 0.2135\n",
      "Epoch [4/25] Step [495/2194] Loss: 0.4151\n",
      "Epoch [4/25] Step [510/2194] Loss: 0.2858\n",
      "Epoch [4/25] Step [525/2194] Loss: 0.2392\n",
      "Epoch [4/25] Step [540/2194] Loss: 0.4362\n",
      "Epoch [4/25] Step [555/2194] Loss: 0.0488\n",
      "Epoch [4/25] Step [570/2194] Loss: 0.0733\n",
      "Epoch [4/25] Step [585/2194] Loss: 0.1300\n",
      "Epoch [4/25] Step [600/2194] Loss: 0.1563\n",
      "Epoch [4/25] Step [615/2194] Loss: 0.2267\n",
      "Epoch [4/25] Step [630/2194] Loss: 0.2897\n",
      "Epoch [4/25] Step [645/2194] Loss: 0.2390\n",
      "Epoch [4/25] Step [660/2194] Loss: 0.1888\n",
      "Epoch [4/25] Step [675/2194] Loss: 0.3398\n",
      "Epoch [4/25] Step [690/2194] Loss: 0.2480\n",
      "Epoch [4/25] Step [705/2194] Loss: 0.2870\n",
      "Epoch [4/25] Step [720/2194] Loss: 0.2540\n",
      "Epoch [4/25] Step [735/2194] Loss: 0.3114\n",
      "Epoch [4/25] Step [750/2194] Loss: 0.2354\n",
      "Epoch [4/25] Step [765/2194] Loss: 0.2058\n",
      "Epoch [4/25] Step [780/2194] Loss: 0.1394\n",
      "Epoch [4/25] Step [795/2194] Loss: 0.3499\n",
      "Epoch [4/25] Step [810/2194] Loss: 0.2333\n",
      "Epoch [4/25] Step [825/2194] Loss: 0.1004\n",
      "Epoch [4/25] Step [840/2194] Loss: 0.1798\n",
      "Epoch [4/25] Step [855/2194] Loss: 0.1710\n",
      "Epoch [4/25] Step [870/2194] Loss: 0.3486\n",
      "Epoch [4/25] Step [885/2194] Loss: 0.2775\n",
      "Epoch [4/25] Step [900/2194] Loss: 0.4307\n",
      "Epoch [4/25] Step [915/2194] Loss: 0.3929\n",
      "Epoch [4/25] Step [930/2194] Loss: 0.1922\n",
      "Epoch [4/25] Step [945/2194] Loss: 0.1966\n",
      "Epoch [4/25] Step [960/2194] Loss: 0.2828\n",
      "Epoch [4/25] Step [975/2194] Loss: 0.1419\n",
      "Epoch [4/25] Step [990/2194] Loss: 0.4318\n",
      "Epoch [4/25] Step [1005/2194] Loss: 0.0816\n",
      "Epoch [4/25] Step [1020/2194] Loss: 0.2561\n",
      "Epoch [4/25] Step [1035/2194] Loss: 0.3192\n",
      "Epoch [4/25] Step [1050/2194] Loss: 0.2743\n",
      "Epoch [4/25] Step [1065/2194] Loss: 0.0802\n",
      "Epoch [4/25] Step [1080/2194] Loss: 0.2012\n",
      "Epoch [4/25] Step [1095/2194] Loss: 0.1322\n",
      "Epoch [4/25] Step [1110/2194] Loss: 0.0657\n",
      "Epoch [4/25] Step [1125/2194] Loss: 0.2014\n",
      "Epoch [4/25] Step [1140/2194] Loss: 0.1154\n",
      "Epoch [4/25] Step [1155/2194] Loss: 0.1658\n",
      "Epoch [4/25] Step [1170/2194] Loss: 0.2958\n",
      "Epoch [4/25] Step [1185/2194] Loss: 0.2483\n",
      "Epoch [4/25] Step [1200/2194] Loss: 0.1570\n",
      "Epoch [4/25] Step [1215/2194] Loss: 0.1228\n",
      "Epoch [4/25] Step [1230/2194] Loss: 0.1375\n",
      "Epoch [4/25] Step [1245/2194] Loss: 0.1253\n",
      "Epoch [4/25] Step [1260/2194] Loss: 0.1582\n",
      "Epoch [4/25] Step [1275/2194] Loss: 0.1037\n",
      "Epoch [4/25] Step [1290/2194] Loss: 0.3397\n",
      "Epoch [4/25] Step [1305/2194] Loss: 0.3276\n",
      "Epoch [4/25] Step [1320/2194] Loss: 0.1383\n",
      "Epoch [4/25] Step [1335/2194] Loss: 0.1298\n",
      "Epoch [4/25] Step [1350/2194] Loss: 0.0682\n",
      "Epoch [4/25] Step [1365/2194] Loss: 0.1347\n",
      "Epoch [4/25] Step [1380/2194] Loss: 0.1097\n",
      "Epoch [4/25] Step [1395/2194] Loss: 0.3245\n",
      "Epoch [4/25] Step [1410/2194] Loss: 0.1938\n",
      "Epoch [4/25] Step [1425/2194] Loss: 0.1170\n",
      "Epoch [4/25] Step [1440/2194] Loss: 0.1422\n",
      "Epoch [4/25] Step [1455/2194] Loss: 0.2646\n",
      "Epoch [4/25] Step [1470/2194] Loss: 0.2018\n",
      "Epoch [4/25] Step [1485/2194] Loss: 0.1863\n",
      "Epoch [4/25] Step [1500/2194] Loss: 0.1449\n",
      "Epoch [4/25] Step [1515/2194] Loss: 0.1562\n",
      "Epoch [4/25] Step [1530/2194] Loss: 0.3132\n",
      "Epoch [4/25] Step [1545/2194] Loss: 0.2674\n",
      "Epoch [4/25] Step [1560/2194] Loss: 0.1732\n",
      "Epoch [4/25] Step [1575/2194] Loss: 0.2042\n",
      "Epoch [4/25] Step [1590/2194] Loss: 0.0864\n",
      "Epoch [4/25] Step [1605/2194] Loss: 0.3080\n",
      "Epoch [4/25] Step [1620/2194] Loss: 0.4721\n",
      "Epoch [4/25] Step [1635/2194] Loss: 0.2011\n",
      "Epoch [4/25] Step [1650/2194] Loss: 0.2665\n",
      "Epoch [4/25] Step [1665/2194] Loss: 0.2920\n",
      "Epoch [4/25] Step [1680/2194] Loss: 0.1739\n",
      "Epoch [4/25] Step [1695/2194] Loss: 0.1651\n",
      "Epoch [4/25] Step [1710/2194] Loss: 0.2823\n",
      "Epoch [4/25] Step [1725/2194] Loss: 0.1973\n",
      "Epoch [4/25] Step [1740/2194] Loss: 0.3586\n",
      "Epoch [4/25] Step [1755/2194] Loss: 0.3028\n",
      "Epoch [4/25] Step [1770/2194] Loss: 0.1598\n",
      "Epoch [4/25] Step [1785/2194] Loss: 0.1296\n",
      "Epoch [4/25] Step [1800/2194] Loss: 0.2721\n",
      "Epoch [4/25] Step [1815/2194] Loss: 0.2346\n",
      "Epoch [4/25] Step [1830/2194] Loss: 0.3191\n",
      "Epoch [4/25] Step [1845/2194] Loss: 0.2527\n",
      "Epoch [4/25] Step [1860/2194] Loss: 0.1872\n",
      "Epoch [4/25] Step [1875/2194] Loss: 0.3631\n",
      "Epoch [4/25] Step [1890/2194] Loss: 0.2418\n",
      "Epoch [4/25] Step [1905/2194] Loss: 0.2876\n",
      "Epoch [4/25] Step [1920/2194] Loss: 0.2452\n",
      "Epoch [4/25] Step [1935/2194] Loss: 0.1793\n",
      "Epoch [4/25] Step [1950/2194] Loss: 0.2877\n",
      "Epoch [4/25] Step [1965/2194] Loss: 0.1074\n",
      "Epoch [4/25] Step [1980/2194] Loss: 0.1771\n",
      "Epoch [4/25] Step [1995/2194] Loss: 0.1082\n",
      "Epoch [4/25] Step [2010/2194] Loss: 0.3076\n",
      "Epoch [4/25] Step [2025/2194] Loss: 0.2679\n",
      "Epoch [4/25] Step [2040/2194] Loss: 0.1569\n",
      "Epoch [4/25] Step [2055/2194] Loss: 0.1545\n",
      "Epoch [4/25] Step [2070/2194] Loss: 0.2863\n",
      "Epoch [4/25] Step [2085/2194] Loss: 0.2275\n",
      "Epoch [4/25] Step [2100/2194] Loss: 0.0901\n",
      "Epoch [4/25] Step [2115/2194] Loss: 0.4780\n",
      "Epoch [4/25] Step [2130/2194] Loss: 0.2120\n",
      "Epoch [4/25] Step [2145/2194] Loss: 0.3850\n",
      "Epoch [4/25] Step [2160/2194] Loss: 0.2943\n",
      "Epoch [4/25] Step [2175/2194] Loss: 0.2167\n",
      "Epoch [4/25] Step [2190/2194] Loss: 0.2032\n",
      "Epoch [4/25] completed in 892.97s\n",
      "Train Accuracy: 0.9029, Validation Accuracy: 0.8744\n",
      "\n",
      "âœ… Best model updated and saved with Val Acc: 0.8744\n",
      "\n",
      "Epoch [5/25] Step [0/2194] Loss: 0.1691\n",
      "Epoch [5/25] Step [15/2194] Loss: 0.1209\n",
      "Epoch [5/25] Step [30/2194] Loss: 0.3115\n",
      "Epoch [5/25] Step [45/2194] Loss: 0.2542\n",
      "Epoch [5/25] Step [60/2194] Loss: 0.1221\n",
      "Epoch [5/25] Step [75/2194] Loss: 0.0460\n",
      "Epoch [5/25] Step [90/2194] Loss: 0.1081\n",
      "Epoch [5/25] Step [105/2194] Loss: 0.1431\n",
      "Epoch [5/25] Step [120/2194] Loss: 0.2136\n",
      "Epoch [5/25] Step [135/2194] Loss: 0.3651\n",
      "Epoch [5/25] Step [150/2194] Loss: 0.0764\n",
      "Epoch [5/25] Step [165/2194] Loss: 0.2905\n",
      "Epoch [5/25] Step [180/2194] Loss: 0.2211\n",
      "Epoch [5/25] Step [195/2194] Loss: 0.1216\n",
      "Epoch [5/25] Step [210/2194] Loss: 0.1504\n",
      "Epoch [5/25] Step [225/2194] Loss: 0.1814\n",
      "Epoch [5/25] Step [240/2194] Loss: 0.1908\n",
      "Epoch [5/25] Step [255/2194] Loss: 0.2556\n",
      "Epoch [5/25] Step [270/2194] Loss: 0.2747\n",
      "Epoch [5/25] Step [285/2194] Loss: 0.1110\n",
      "Epoch [5/25] Step [300/2194] Loss: 0.1586\n",
      "Epoch [5/25] Step [315/2194] Loss: 0.2318\n",
      "Epoch [5/25] Step [330/2194] Loss: 0.3656\n",
      "Epoch [5/25] Step [345/2194] Loss: 0.2227\n",
      "Epoch [5/25] Step [360/2194] Loss: 0.1623\n",
      "Epoch [5/25] Step [375/2194] Loss: 0.2578\n",
      "Epoch [5/25] Step [390/2194] Loss: 0.1706\n",
      "Epoch [5/25] Step [405/2194] Loss: 0.0843\n",
      "Epoch [5/25] Step [420/2194] Loss: 0.0899\n",
      "Epoch [5/25] Step [435/2194] Loss: 0.0870\n",
      "Epoch [5/25] Step [450/2194] Loss: 0.0776\n",
      "Epoch [5/25] Step [465/2194] Loss: 0.0575\n",
      "Epoch [5/25] Step [480/2194] Loss: 0.2525\n",
      "Epoch [5/25] Step [495/2194] Loss: 0.3569\n",
      "Epoch [5/25] Step [510/2194] Loss: 0.2195\n",
      "Epoch [5/25] Step [525/2194] Loss: 0.2273\n",
      "Epoch [5/25] Step [540/2194] Loss: 0.1511\n",
      "Epoch [5/25] Step [555/2194] Loss: 0.2732\n",
      "Epoch [5/25] Step [570/2194] Loss: 0.1634\n",
      "Epoch [5/25] Step [585/2194] Loss: 0.1227\n",
      "Epoch [5/25] Step [600/2194] Loss: 0.1327\n",
      "Epoch [5/25] Step [615/2194] Loss: 0.3035\n",
      "Epoch [5/25] Step [630/2194] Loss: 0.1844\n",
      "Epoch [5/25] Step [645/2194] Loss: 0.2689\n",
      "Epoch [5/25] Step [660/2194] Loss: 0.2814\n",
      "Epoch [5/25] Step [675/2194] Loss: 0.2000\n",
      "Epoch [5/25] Step [690/2194] Loss: 0.2148\n",
      "Epoch [5/25] Step [705/2194] Loss: 0.0760\n",
      "Epoch [5/25] Step [720/2194] Loss: 0.3882\n",
      "Epoch [5/25] Step [735/2194] Loss: 0.1281\n",
      "Epoch [5/25] Step [750/2194] Loss: 0.1151\n",
      "Epoch [5/25] Step [765/2194] Loss: 0.3716\n",
      "Epoch [5/25] Step [780/2194] Loss: 0.1691\n",
      "Epoch [5/25] Step [795/2194] Loss: 0.0644\n",
      "Epoch [5/25] Step [810/2194] Loss: 0.1905\n",
      "Epoch [5/25] Step [825/2194] Loss: 0.1372\n",
      "Epoch [5/25] Step [840/2194] Loss: 0.0813\n",
      "Epoch [5/25] Step [855/2194] Loss: 0.3639\n",
      "Epoch [5/25] Step [870/2194] Loss: 0.0866\n",
      "Epoch [5/25] Step [885/2194] Loss: 0.2341\n",
      "Epoch [5/25] Step [900/2194] Loss: 0.1100\n",
      "Epoch [5/25] Step [915/2194] Loss: 0.2402\n",
      "Epoch [5/25] Step [930/2194] Loss: 0.1257\n",
      "Epoch [5/25] Step [945/2194] Loss: 0.1793\n",
      "Epoch [5/25] Step [960/2194] Loss: 0.3122\n",
      "Epoch [5/25] Step [975/2194] Loss: 0.1597\n",
      "Epoch [5/25] Step [990/2194] Loss: 0.3207\n",
      "Epoch [5/25] Step [1005/2194] Loss: 0.1246\n",
      "Epoch [5/25] Step [1020/2194] Loss: 0.1759\n",
      "Epoch [5/25] Step [1035/2194] Loss: 0.2016\n",
      "Epoch [5/25] Step [1050/2194] Loss: 0.1965\n",
      "Epoch [5/25] Step [1065/2194] Loss: 0.1106\n",
      "Epoch [5/25] Step [1080/2194] Loss: 0.2074\n",
      "Epoch [5/25] Step [1095/2194] Loss: 0.2227\n",
      "Epoch [5/25] Step [1110/2194] Loss: 0.0647\n",
      "Epoch [5/25] Step [1125/2194] Loss: 0.2480\n",
      "Epoch [5/25] Step [1140/2194] Loss: 0.0182\n",
      "Epoch [5/25] Step [1155/2194] Loss: 0.2698\n",
      "Epoch [5/25] Step [1170/2194] Loss: 0.2760\n",
      "Epoch [5/25] Step [1185/2194] Loss: 0.2490\n",
      "Epoch [5/25] Step [1200/2194] Loss: 0.1287\n",
      "Epoch [5/25] Step [1215/2194] Loss: 0.1672\n",
      "Epoch [5/25] Step [1230/2194] Loss: 0.0864\n",
      "Epoch [5/25] Step [1245/2194] Loss: 0.1521\n",
      "Epoch [5/25] Step [1260/2194] Loss: 0.2188\n",
      "Epoch [5/25] Step [1275/2194] Loss: 0.2096\n",
      "Epoch [5/25] Step [1290/2194] Loss: 0.1640\n",
      "Epoch [5/25] Step [1305/2194] Loss: 0.1427\n",
      "Epoch [5/25] Step [1320/2194] Loss: 0.0826\n",
      "Epoch [5/25] Step [1335/2194] Loss: 0.3236\n",
      "Epoch [5/25] Step [1350/2194] Loss: 0.2116\n",
      "Epoch [5/25] Step [1365/2194] Loss: 0.2095\n",
      "Epoch [5/25] Step [1380/2194] Loss: 0.1370\n",
      "Epoch [5/25] Step [1395/2194] Loss: 0.1193\n",
      "Epoch [5/25] Step [1410/2194] Loss: 0.4153\n",
      "Epoch [5/25] Step [1425/2194] Loss: 0.1842\n",
      "Epoch [5/25] Step [1440/2194] Loss: 0.0957\n",
      "Epoch [5/25] Step [1455/2194] Loss: 0.3008\n",
      "Epoch [5/25] Step [1470/2194] Loss: 0.1249\n",
      "Epoch [5/25] Step [1485/2194] Loss: 0.2454\n",
      "Epoch [5/25] Step [1500/2194] Loss: 0.1134\n",
      "Epoch [5/25] Step [1515/2194] Loss: 0.0886\n",
      "Epoch [5/25] Step [1530/2194] Loss: 0.2002\n",
      "Epoch [5/25] Step [1545/2194] Loss: 0.0684\n",
      "Epoch [5/25] Step [1560/2194] Loss: 0.1592\n",
      "Epoch [5/25] Step [1575/2194] Loss: 0.0718\n",
      "Epoch [5/25] Step [1590/2194] Loss: 0.2603\n",
      "Epoch [5/25] Step [1605/2194] Loss: 0.1758\n",
      "Epoch [5/25] Step [1620/2194] Loss: 0.2826\n",
      "Epoch [5/25] Step [1635/2194] Loss: 0.1959\n",
      "Epoch [5/25] Step [1650/2194] Loss: 0.1819\n",
      "Epoch [5/25] Step [1665/2194] Loss: 0.0783\n",
      "Epoch [5/25] Step [1680/2194] Loss: 0.1542\n",
      "Epoch [5/25] Step [1695/2194] Loss: 0.1066\n",
      "Epoch [5/25] Step [1710/2194] Loss: 0.1768\n",
      "Epoch [5/25] Step [1725/2194] Loss: 0.1850\n",
      "Epoch [5/25] Step [1740/2194] Loss: 0.2864\n",
      "Epoch [5/25] Step [1755/2194] Loss: 0.2257\n",
      "Epoch [5/25] Step [1770/2194] Loss: 0.2485\n",
      "Epoch [5/25] Step [1785/2194] Loss: 0.0597\n",
      "Epoch [5/25] Step [1800/2194] Loss: 0.2910\n",
      "Epoch [5/25] Step [1815/2194] Loss: 0.1901\n",
      "Epoch [5/25] Step [1830/2194] Loss: 0.1484\n",
      "Epoch [5/25] Step [1845/2194] Loss: 0.1410\n",
      "Epoch [5/25] Step [1860/2194] Loss: 0.3097\n",
      "Epoch [5/25] Step [1875/2194] Loss: 0.0775\n",
      "Epoch [5/25] Step [1890/2194] Loss: 0.4183\n",
      "Epoch [5/25] Step [1905/2194] Loss: 0.3328\n",
      "Epoch [5/25] Step [1920/2194] Loss: 0.3728\n",
      "Epoch [5/25] Step [1935/2194] Loss: 0.1215\n",
      "Epoch [5/25] Step [1950/2194] Loss: 0.1508\n",
      "Epoch [5/25] Step [1965/2194] Loss: 0.0499\n",
      "Epoch [5/25] Step [1980/2194] Loss: 0.1548\n",
      "Epoch [5/25] Step [1995/2194] Loss: 0.2629\n",
      "Epoch [5/25] Step [2010/2194] Loss: 0.0878\n",
      "Epoch [5/25] Step [2025/2194] Loss: 0.2675\n",
      "Epoch [5/25] Step [2040/2194] Loss: 0.3353\n",
      "Epoch [5/25] Step [2055/2194] Loss: 0.2153\n",
      "Epoch [5/25] Step [2070/2194] Loss: 0.1062\n",
      "Epoch [5/25] Step [2085/2194] Loss: 0.0925\n",
      "Epoch [5/25] Step [2100/2194] Loss: 0.4784\n",
      "Epoch [5/25] Step [2115/2194] Loss: 0.1634\n",
      "Epoch [5/25] Step [2130/2194] Loss: 0.2218\n",
      "Epoch [5/25] Step [2145/2194] Loss: 0.1996\n",
      "Epoch [5/25] Step [2160/2194] Loss: 0.2807\n",
      "Epoch [5/25] Step [2175/2194] Loss: 0.1193\n",
      "Epoch [5/25] Step [2190/2194] Loss: 0.2107\n",
      "Epoch [5/25] completed in 989.50s\n",
      "Train Accuracy: 0.9223, Validation Accuracy: 0.8903\n",
      "\n",
      "âœ… Best model updated and saved with Val Acc: 0.8903\n",
      "\n",
      "Epoch [6/25] Step [0/2194] Loss: 0.0207\n",
      "Epoch [6/25] Step [15/2194] Loss: 0.1234\n",
      "Epoch [6/25] Step [30/2194] Loss: 0.2051\n",
      "Epoch [6/25] Step [45/2194] Loss: 0.1677\n",
      "Epoch [6/25] Step [60/2194] Loss: 0.1162\n",
      "Epoch [6/25] Step [75/2194] Loss: 0.1392\n",
      "Epoch [6/25] Step [90/2194] Loss: 0.1539\n",
      "Epoch [6/25] Step [105/2194] Loss: 0.1103\n",
      "Epoch [6/25] Step [120/2194] Loss: 0.2744\n",
      "Epoch [6/25] Step [135/2194] Loss: 0.1200\n",
      "Epoch [6/25] Step [150/2194] Loss: 0.0555\n",
      "Epoch [6/25] Step [165/2194] Loss: 0.2814\n",
      "Epoch [6/25] Step [180/2194] Loss: 0.1993\n",
      "Epoch [6/25] Step [195/2194] Loss: 0.2417\n",
      "Epoch [6/25] Step [210/2194] Loss: 0.1543\n",
      "Epoch [6/25] Step [225/2194] Loss: 0.0941\n",
      "Epoch [6/25] Step [240/2194] Loss: 0.1234\n",
      "Epoch [6/25] Step [255/2194] Loss: 0.4709\n",
      "Epoch [6/25] Step [270/2194] Loss: 0.0222\n",
      "Epoch [6/25] Step [285/2194] Loss: 0.2097\n",
      "Epoch [6/25] Step [300/2194] Loss: 0.0817\n",
      "Epoch [6/25] Step [315/2194] Loss: 0.0579\n",
      "Epoch [6/25] Step [330/2194] Loss: 0.0903\n",
      "Epoch [6/25] Step [345/2194] Loss: 0.1660\n",
      "Epoch [6/25] Step [360/2194] Loss: 0.0642\n",
      "Epoch [6/25] Step [375/2194] Loss: 0.0797\n",
      "Epoch [6/25] Step [390/2194] Loss: 0.1900\n",
      "Epoch [6/25] Step [405/2194] Loss: 0.0491\n",
      "Epoch [6/25] Step [420/2194] Loss: 0.1744\n",
      "Epoch [6/25] Step [435/2194] Loss: 0.1627\n",
      "Epoch [6/25] Step [450/2194] Loss: 0.1956\n",
      "Epoch [6/25] Step [465/2194] Loss: 0.1826\n",
      "Epoch [6/25] Step [480/2194] Loss: 0.3400\n",
      "Epoch [6/25] Step [495/2194] Loss: 0.0860\n",
      "Epoch [6/25] Step [510/2194] Loss: 0.1352\n",
      "Epoch [6/25] Step [525/2194] Loss: 0.1685\n",
      "Epoch [6/25] Step [540/2194] Loss: 0.3000\n",
      "Epoch [6/25] Step [555/2194] Loss: 0.1384\n",
      "Epoch [6/25] Step [570/2194] Loss: 0.1255\n",
      "Epoch [6/25] Step [585/2194] Loss: 0.2288\n",
      "Epoch [6/25] Step [600/2194] Loss: 0.1886\n",
      "Epoch [6/25] Step [615/2194] Loss: 0.2513\n",
      "Epoch [6/25] Step [630/2194] Loss: 0.1785\n",
      "Epoch [6/25] Step [645/2194] Loss: 0.0913\n",
      "Epoch [6/25] Step [660/2194] Loss: 0.2417\n",
      "Epoch [6/25] Step [675/2194] Loss: 0.2047\n",
      "Epoch [6/25] Step [690/2194] Loss: 0.0670\n",
      "Epoch [6/25] Step [705/2194] Loss: 0.2768\n",
      "Epoch [6/25] Step [720/2194] Loss: 0.2623\n",
      "Epoch [6/25] Step [735/2194] Loss: 0.1956\n",
      "Epoch [6/25] Step [750/2194] Loss: 0.1136\n",
      "Epoch [6/25] Step [765/2194] Loss: 0.2042\n",
      "Epoch [6/25] Step [780/2194] Loss: 0.0796\n",
      "Epoch [6/25] Step [795/2194] Loss: 0.3192\n",
      "Epoch [6/25] Step [810/2194] Loss: 0.2544\n",
      "Epoch [6/25] Step [825/2194] Loss: 0.2436\n",
      "Epoch [6/25] Step [840/2194] Loss: 0.1290\n",
      "Epoch [6/25] Step [855/2194] Loss: 0.1523\n",
      "Epoch [6/25] Step [870/2194] Loss: 0.2857\n",
      "Epoch [6/25] Step [885/2194] Loss: 0.1200\n",
      "Epoch [6/25] Step [900/2194] Loss: 0.1688\n",
      "Epoch [6/25] Step [915/2194] Loss: 0.1749\n",
      "Epoch [6/25] Step [930/2194] Loss: 0.2295\n",
      "Epoch [6/25] Step [945/2194] Loss: 0.0393\n",
      "Epoch [6/25] Step [960/2194] Loss: 0.0958\n",
      "Epoch [6/25] Step [975/2194] Loss: 0.2038\n",
      "Epoch [6/25] Step [990/2194] Loss: 0.2560\n",
      "Epoch [6/25] Step [1005/2194] Loss: 0.3224\n",
      "Epoch [6/25] Step [1020/2194] Loss: 0.1235\n",
      "Epoch [6/25] Step [1035/2194] Loss: 0.1721\n",
      "Epoch [6/25] Step [1050/2194] Loss: 0.0884\n",
      "Epoch [6/25] Step [1065/2194] Loss: 0.0766\n",
      "Epoch [6/25] Step [1080/2194] Loss: 0.2729\n",
      "Epoch [6/25] Step [1095/2194] Loss: 0.1214\n",
      "Epoch [6/25] Step [1110/2194] Loss: 0.0532\n",
      "Epoch [6/25] Step [1125/2194] Loss: 0.2625\n",
      "Epoch [6/25] Step [1140/2194] Loss: 0.1151\n",
      "Epoch [6/25] Step [1155/2194] Loss: 0.1602\n",
      "Epoch [6/25] Step [1170/2194] Loss: 0.0467\n",
      "Epoch [6/25] Step [1185/2194] Loss: 0.1537\n",
      "Epoch [6/25] Step [1200/2194] Loss: 0.0811\n",
      "Epoch [6/25] Step [1215/2194] Loss: 0.0630\n",
      "Epoch [6/25] Step [1230/2194] Loss: 0.2723\n",
      "Epoch [6/25] Step [1245/2194] Loss: 0.0407\n",
      "Epoch [6/25] Step [1260/2194] Loss: 0.1381\n",
      "Epoch [6/25] Step [1275/2194] Loss: 0.1284\n",
      "Epoch [6/25] Step [1290/2194] Loss: 0.1361\n",
      "Epoch [6/25] Step [1305/2194] Loss: 0.1976\n",
      "Epoch [6/25] Step [1320/2194] Loss: 0.4735\n",
      "Epoch [6/25] Step [1335/2194] Loss: 0.1528\n",
      "Epoch [6/25] Step [1350/2194] Loss: 0.2174\n",
      "Epoch [6/25] Step [1365/2194] Loss: 0.1240\n",
      "Epoch [6/25] Step [1380/2194] Loss: 0.0607\n",
      "Epoch [6/25] Step [1395/2194] Loss: 0.1094\n",
      "Epoch [6/25] Step [1410/2194] Loss: 0.0897\n",
      "Epoch [6/25] Step [1425/2194] Loss: 0.2288\n",
      "Epoch [6/25] Step [1440/2194] Loss: 0.1831\n",
      "Epoch [6/25] Step [1455/2194] Loss: 0.2241\n",
      "Epoch [6/25] Step [1470/2194] Loss: 0.1479\n",
      "Epoch [6/25] Step [1485/2194] Loss: 0.2719\n",
      "Epoch [6/25] Step [1500/2194] Loss: 0.1279\n",
      "Epoch [6/25] Step [1515/2194] Loss: 0.1171\n",
      "Epoch [6/25] Step [1530/2194] Loss: 0.0645\n",
      "Epoch [6/25] Step [1545/2194] Loss: 0.2451\n",
      "Epoch [6/25] Step [1560/2194] Loss: 0.1217\n",
      "Epoch [6/25] Step [1575/2194] Loss: 0.2619\n",
      "Epoch [6/25] Step [1590/2194] Loss: 0.1249\n",
      "Epoch [6/25] Step [1605/2194] Loss: 0.1544\n",
      "Epoch [6/25] Step [1620/2194] Loss: 0.0933\n",
      "Epoch [6/25] Step [1635/2194] Loss: 0.0860\n",
      "Epoch [6/25] Step [1650/2194] Loss: 0.1500\n",
      "Epoch [6/25] Step [1665/2194] Loss: 0.1364\n",
      "Epoch [6/25] Step [1680/2194] Loss: 0.1899\n",
      "Epoch [6/25] Step [1695/2194] Loss: 0.0979\n",
      "Epoch [6/25] Step [1710/2194] Loss: 0.0859\n",
      "Epoch [6/25] Step [1725/2194] Loss: 0.2316\n",
      "Epoch [6/25] Step [1740/2194] Loss: 0.1491\n",
      "Epoch [6/25] Step [1755/2194] Loss: 0.2196\n",
      "Epoch [6/25] Step [1770/2194] Loss: 0.1992\n",
      "Epoch [6/25] Step [1785/2194] Loss: 0.1691\n",
      "Epoch [6/25] Step [1800/2194] Loss: 0.1550\n",
      "Epoch [6/25] Step [1815/2194] Loss: 0.1075\n",
      "Epoch [6/25] Step [1830/2194] Loss: 0.0650\n",
      "Epoch [6/25] Step [1845/2194] Loss: 0.1275\n",
      "Epoch [6/25] Step [1860/2194] Loss: 0.1460\n",
      "Epoch [6/25] Step [1875/2194] Loss: 0.1864\n",
      "Epoch [6/25] Step [1890/2194] Loss: 0.1126\n",
      "Epoch [6/25] Step [1905/2194] Loss: 0.0808\n",
      "Epoch [6/25] Step [1920/2194] Loss: 0.1715\n",
      "Epoch [6/25] Step [1935/2194] Loss: 0.1716\n",
      "Epoch [6/25] Step [1950/2194] Loss: 0.2372\n",
      "Epoch [6/25] Step [1965/2194] Loss: 0.1885\n",
      "Epoch [6/25] Step [1980/2194] Loss: 0.0439\n",
      "Epoch [6/25] Step [1995/2194] Loss: 0.2618\n",
      "Epoch [6/25] Step [2010/2194] Loss: 0.1196\n",
      "Epoch [6/25] Step [2025/2194] Loss: 0.2153\n",
      "Epoch [6/25] Step [2040/2194] Loss: 0.0884\n",
      "Epoch [6/25] Step [2055/2194] Loss: 0.2358\n",
      "Epoch [6/25] Step [2070/2194] Loss: 0.1117\n",
      "Epoch [6/25] Step [2085/2194] Loss: 0.2527\n",
      "Epoch [6/25] Step [2100/2194] Loss: 0.2087\n",
      "Epoch [6/25] Step [2115/2194] Loss: 0.0409\n",
      "Epoch [6/25] Step [2130/2194] Loss: 0.1788\n",
      "Epoch [6/25] Step [2145/2194] Loss: 0.3726\n",
      "Epoch [6/25] Step [2160/2194] Loss: 0.1113\n",
      "Epoch [6/25] Step [2175/2194] Loss: 0.1821\n",
      "Epoch [6/25] Step [2190/2194] Loss: 0.1742\n",
      "Epoch [6/25] completed in 2040.40s\n",
      "Train Accuracy: 0.9379, Validation Accuracy: 0.8764\n",
      "\n",
      "Epoch [7/25] Step [0/2194] Loss: 0.1238\n",
      "Epoch [7/25] Step [15/2194] Loss: 0.0577\n",
      "Epoch [7/25] Step [30/2194] Loss: 0.0408\n",
      "Epoch [7/25] Step [45/2194] Loss: 0.1219\n",
      "Epoch [7/25] Step [60/2194] Loss: 0.0860\n",
      "Epoch [7/25] Step [75/2194] Loss: 0.2257\n",
      "Epoch [7/25] Step [90/2194] Loss: 0.1433\n",
      "Epoch [7/25] Step [105/2194] Loss: 0.1279\n",
      "Epoch [7/25] Step [120/2194] Loss: 0.0756\n",
      "Epoch [7/25] Step [135/2194] Loss: 0.0409\n",
      "Epoch [7/25] Step [150/2194] Loss: 0.0623\n",
      "Epoch [7/25] Step [165/2194] Loss: 0.1480\n",
      "Epoch [7/25] Step [180/2194] Loss: 0.0586\n",
      "Epoch [7/25] Step [195/2194] Loss: 0.0772\n",
      "Epoch [7/25] Step [210/2194] Loss: 0.0413\n",
      "Epoch [7/25] Step [225/2194] Loss: 0.1855\n",
      "Epoch [7/25] Step [240/2194] Loss: 0.0793\n",
      "Epoch [7/25] Step [255/2194] Loss: 0.2013\n",
      "Epoch [7/25] Step [270/2194] Loss: 0.0448\n",
      "Epoch [7/25] Step [285/2194] Loss: 0.1166\n",
      "Epoch [7/25] Step [300/2194] Loss: 0.1469\n",
      "Epoch [7/25] Step [315/2194] Loss: 0.1521\n",
      "Epoch [7/25] Step [330/2194] Loss: 0.2346\n",
      "Epoch [7/25] Step [345/2194] Loss: 0.1280\n",
      "Epoch [7/25] Step [360/2194] Loss: 0.3303\n",
      "Epoch [7/25] Step [375/2194] Loss: 0.0862\n",
      "Epoch [7/25] Step [390/2194] Loss: 0.0130\n",
      "Epoch [7/25] Step [405/2194] Loss: 0.0551\n",
      "Epoch [7/25] Step [420/2194] Loss: 0.0901\n",
      "Epoch [7/25] Step [435/2194] Loss: 0.0633\n",
      "Epoch [7/25] Step [450/2194] Loss: 0.1802\n",
      "Epoch [7/25] Step [465/2194] Loss: 0.2412\n",
      "Epoch [7/25] Step [480/2194] Loss: 0.1584\n",
      "Epoch [7/25] Step [495/2194] Loss: 0.0458\n",
      "Epoch [7/25] Step [510/2194] Loss: 0.0932\n",
      "Epoch [7/25] Step [525/2194] Loss: 0.0427\n",
      "Epoch [7/25] Step [540/2194] Loss: 0.1164\n",
      "Epoch [7/25] Step [555/2194] Loss: 0.3255\n",
      "Epoch [7/25] Step [570/2194] Loss: 0.2362\n",
      "Epoch [7/25] Step [585/2194] Loss: 0.2154\n",
      "Epoch [7/25] Step [600/2194] Loss: 0.1007\n",
      "Epoch [7/25] Step [615/2194] Loss: 0.1052\n",
      "Epoch [7/25] Step [630/2194] Loss: 0.0341\n",
      "Epoch [7/25] Step [645/2194] Loss: 0.1509\n",
      "Epoch [7/25] Step [660/2194] Loss: 0.0457\n",
      "Epoch [7/25] Step [675/2194] Loss: 0.0788\n",
      "Epoch [7/25] Step [690/2194] Loss: 0.2012\n",
      "Epoch [7/25] Step [705/2194] Loss: 0.3919\n",
      "Epoch [7/25] Step [720/2194] Loss: 0.1236\n",
      "Epoch [7/25] Step [735/2194] Loss: 0.3083\n",
      "Epoch [7/25] Step [750/2194] Loss: 0.1550\n",
      "Epoch [7/25] Step [765/2194] Loss: 0.1419\n",
      "Epoch [7/25] Step [780/2194] Loss: 0.0737\n",
      "Epoch [7/25] Step [795/2194] Loss: 0.0747\n",
      "Epoch [7/25] Step [810/2194] Loss: 0.0763\n",
      "Epoch [7/25] Step [825/2194] Loss: 0.1554\n",
      "Epoch [7/25] Step [840/2194] Loss: 0.3007\n",
      "Epoch [7/25] Step [855/2194] Loss: 0.1214\n",
      "Epoch [7/25] Step [870/2194] Loss: 0.1082\n",
      "Epoch [7/25] Step [885/2194] Loss: 0.1590\n",
      "Epoch [7/25] Step [900/2194] Loss: 0.0803\n",
      "Epoch [7/25] Step [915/2194] Loss: 0.1356\n",
      "Epoch [7/25] Step [930/2194] Loss: 0.1365\n",
      "Epoch [7/25] Step [945/2194] Loss: 0.2650\n",
      "Epoch [7/25] Step [960/2194] Loss: 0.1979\n",
      "Epoch [7/25] Step [975/2194] Loss: 0.1171\n",
      "Epoch [7/25] Step [990/2194] Loss: 0.1021\n",
      "Epoch [7/25] Step [1005/2194] Loss: 0.0457\n",
      "Epoch [7/25] Step [1020/2194] Loss: 0.0857\n",
      "Epoch [7/25] Step [1035/2194] Loss: 0.0452\n",
      "Epoch [7/25] Step [1050/2194] Loss: 0.1511\n",
      "Epoch [7/25] Step [1065/2194] Loss: 0.1082\n",
      "Epoch [7/25] Step [1080/2194] Loss: 0.1268\n",
      "Epoch [7/25] Step [1095/2194] Loss: 0.2107\n",
      "Epoch [7/25] Step [1110/2194] Loss: 0.0699\n",
      "Epoch [7/25] Step [1125/2194] Loss: 0.1744\n",
      "Epoch [7/25] Step [1140/2194] Loss: 0.0548\n",
      "Epoch [7/25] Step [1155/2194] Loss: 0.1137\n",
      "Epoch [7/25] Step [1170/2194] Loss: 0.0384\n",
      "Epoch [7/25] Step [1185/2194] Loss: 0.0450\n",
      "Epoch [7/25] Step [1200/2194] Loss: 0.1481\n",
      "Epoch [7/25] Step [1215/2194] Loss: 0.1519\n",
      "Epoch [7/25] Step [1230/2194] Loss: 0.1371\n",
      "Epoch [7/25] Step [1245/2194] Loss: 0.0756\n",
      "Epoch [7/25] Step [1260/2194] Loss: 0.0823\n",
      "Epoch [7/25] Step [1275/2194] Loss: 0.0847\n",
      "Epoch [7/25] Step [1290/2194] Loss: 0.0590\n",
      "Epoch [7/25] Step [1305/2194] Loss: 0.1084\n",
      "Epoch [7/25] Step [1320/2194] Loss: 0.1516\n",
      "Epoch [7/25] Step [1335/2194] Loss: 0.2228\n",
      "Epoch [7/25] Step [1350/2194] Loss: 0.2020\n",
      "Epoch [7/25] Step [1365/2194] Loss: 0.0906\n",
      "Epoch [7/25] Step [1380/2194] Loss: 0.1419\n",
      "Epoch [7/25] Step [1395/2194] Loss: 0.3108\n",
      "Epoch [7/25] Step [1410/2194] Loss: 0.0753\n",
      "Epoch [7/25] Step [1425/2194] Loss: 0.0644\n",
      "Epoch [7/25] Step [1440/2194] Loss: 0.0786\n",
      "Epoch [7/25] Step [1455/2194] Loss: 0.1693\n",
      "Epoch [7/25] Step [1470/2194] Loss: 0.0724\n",
      "Epoch [7/25] Step [1485/2194] Loss: 0.0766\n",
      "Epoch [7/25] Step [1500/2194] Loss: 0.1954\n",
      "Epoch [7/25] Step [1515/2194] Loss: 0.0945\n",
      "Epoch [7/25] Step [1530/2194] Loss: 0.0943\n",
      "Epoch [7/25] Step [1545/2194] Loss: 0.4391\n",
      "Epoch [7/25] Step [1560/2194] Loss: 0.0959\n",
      "Epoch [7/25] Step [1575/2194] Loss: 0.1011\n",
      "Epoch [7/25] Step [1590/2194] Loss: 0.2002\n",
      "Epoch [7/25] Step [1605/2194] Loss: 0.0625\n",
      "Epoch [7/25] Step [1620/2194] Loss: 0.0989\n",
      "Epoch [7/25] Step [1635/2194] Loss: 0.2287\n",
      "Epoch [7/25] Step [1650/2194] Loss: 0.0509\n",
      "Epoch [7/25] Step [1665/2194] Loss: 0.0964\n",
      "Epoch [7/25] Step [1680/2194] Loss: 0.0367\n",
      "Epoch [7/25] Step [1695/2194] Loss: 0.2484\n",
      "Epoch [7/25] Step [1710/2194] Loss: 0.0482\n",
      "Epoch [7/25] Step [1725/2194] Loss: 0.0567\n",
      "Epoch [7/25] Step [1740/2194] Loss: 0.0639\n",
      "Epoch [7/25] Step [1755/2194] Loss: 0.1125\n",
      "Epoch [7/25] Step [1770/2194] Loss: 0.1692\n",
      "Epoch [7/25] Step [1785/2194] Loss: 0.0373\n",
      "Epoch [7/25] Step [1800/2194] Loss: 0.1752\n",
      "Epoch [7/25] Step [1815/2194] Loss: 0.4967\n",
      "Epoch [7/25] Step [1830/2194] Loss: 0.2003\n",
      "Epoch [7/25] Step [1845/2194] Loss: 0.0677\n",
      "Epoch [7/25] Step [1860/2194] Loss: 0.1216\n",
      "Epoch [7/25] Step [1875/2194] Loss: 0.3342\n",
      "Epoch [7/25] Step [1890/2194] Loss: 0.1389\n",
      "Epoch [7/25] Step [1905/2194] Loss: 0.0810\n",
      "Epoch [7/25] Step [1920/2194] Loss: 0.0176\n",
      "Epoch [7/25] Step [1935/2194] Loss: 0.0660\n",
      "Epoch [7/25] Step [1950/2194] Loss: 0.0186\n",
      "Epoch [7/25] Step [1965/2194] Loss: 0.0749\n",
      "Epoch [7/25] Step [1980/2194] Loss: 0.1866\n",
      "Epoch [7/25] Step [1995/2194] Loss: 0.0455\n",
      "Epoch [7/25] Step [2010/2194] Loss: 0.1101\n",
      "Epoch [7/25] Step [2025/2194] Loss: 0.0587\n",
      "Epoch [7/25] Step [2040/2194] Loss: 0.2932\n",
      "Epoch [7/25] Step [2055/2194] Loss: 0.1118\n",
      "Epoch [7/25] Step [2070/2194] Loss: 0.1347\n",
      "Epoch [7/25] Step [2085/2194] Loss: 0.0943\n",
      "Epoch [7/25] Step [2100/2194] Loss: 0.1868\n",
      "Epoch [7/25] Step [2115/2194] Loss: 0.0464\n",
      "Epoch [7/25] Step [2130/2194] Loss: 0.1246\n",
      "Epoch [7/25] Step [2145/2194] Loss: 0.0127\n",
      "Epoch [7/25] Step [2160/2194] Loss: 0.1588\n",
      "Epoch [7/25] Step [2175/2194] Loss: 0.0896\n",
      "Epoch [7/25] Step [2190/2194] Loss: 0.0815\n",
      "Epoch [7/25] completed in 2040.22s\n",
      "Train Accuracy: 0.9471, Validation Accuracy: 0.8931\n",
      "\n",
      "âœ… Best model updated and saved with Val Acc: 0.8931\n",
      "\n",
      "Epoch [8/25] Step [0/2194] Loss: 0.1323\n",
      "Epoch [8/25] Step [15/2194] Loss: 0.0318\n",
      "Epoch [8/25] Step [30/2194] Loss: 0.0709\n",
      "Epoch [8/25] Step [45/2194] Loss: 0.1749\n",
      "Epoch [8/25] Step [60/2194] Loss: 0.0765\n",
      "Epoch [8/25] Step [75/2194] Loss: 0.0969\n",
      "Epoch [8/25] Step [90/2194] Loss: 0.0443\n",
      "Epoch [8/25] Step [105/2194] Loss: 0.0348\n",
      "Epoch [8/25] Step [120/2194] Loss: 0.0701\n",
      "Epoch [8/25] Step [135/2194] Loss: 0.0809\n",
      "Epoch [8/25] Step [150/2194] Loss: 0.2411\n",
      "Epoch [8/25] Step [165/2194] Loss: 0.3281\n",
      "Epoch [8/25] Step [180/2194] Loss: 0.1874\n",
      "Epoch [8/25] Step [195/2194] Loss: 0.1195\n",
      "Epoch [8/25] Step [210/2194] Loss: 0.3481\n",
      "Epoch [8/25] Step [225/2194] Loss: 0.1287\n",
      "Epoch [8/25] Step [240/2194] Loss: 0.0248\n",
      "Epoch [8/25] Step [255/2194] Loss: 0.0619\n",
      "Epoch [8/25] Step [270/2194] Loss: 0.1532\n",
      "Epoch [8/25] Step [285/2194] Loss: 0.0880\n",
      "Epoch [8/25] Step [300/2194] Loss: 0.0370\n",
      "Epoch [8/25] Step [315/2194] Loss: 0.0667\n",
      "Epoch [8/25] Step [330/2194] Loss: 0.0422\n",
      "Epoch [8/25] Step [345/2194] Loss: 0.0899\n",
      "Epoch [8/25] Step [360/2194] Loss: 0.0305\n",
      "Epoch [8/25] Step [375/2194] Loss: 0.0310\n",
      "Epoch [8/25] Step [390/2194] Loss: 0.1631\n",
      "Epoch [8/25] Step [405/2194] Loss: 0.0575\n",
      "Epoch [8/25] Step [420/2194] Loss: 0.1334\n",
      "Epoch [8/25] Step [435/2194] Loss: 0.1310\n",
      "Epoch [8/25] Step [450/2194] Loss: 0.1867\n",
      "Epoch [8/25] Step [465/2194] Loss: 0.0668\n",
      "Epoch [8/25] Step [480/2194] Loss: 0.1487\n",
      "Epoch [8/25] Step [495/2194] Loss: 0.0608\n",
      "Epoch [8/25] Step [510/2194] Loss: 0.3669\n",
      "Epoch [8/25] Step [525/2194] Loss: 0.0685\n",
      "Epoch [8/25] Step [540/2194] Loss: 0.0581\n",
      "Epoch [8/25] Step [555/2194] Loss: 0.2789\n",
      "Epoch [8/25] Step [570/2194] Loss: 0.1176\n",
      "Epoch [8/25] Step [585/2194] Loss: 0.0670\n",
      "Epoch [8/25] Step [600/2194] Loss: 0.1299\n",
      "Epoch [8/25] Step [615/2194] Loss: 0.0721\n",
      "Epoch [8/25] Step [630/2194] Loss: 0.1989\n",
      "Epoch [8/25] Step [645/2194] Loss: 0.0199\n",
      "Epoch [8/25] Step [660/2194] Loss: 0.1263\n",
      "Epoch [8/25] Step [675/2194] Loss: 0.0594\n",
      "Epoch [8/25] Step [690/2194] Loss: 0.0276\n",
      "Epoch [8/25] Step [705/2194] Loss: 0.0066\n",
      "Epoch [8/25] Step [720/2194] Loss: 0.1783\n",
      "Epoch [8/25] Step [735/2194] Loss: 0.1406\n",
      "Epoch [8/25] Step [750/2194] Loss: 0.1305\n",
      "Epoch [8/25] Step [765/2194] Loss: 0.0613\n",
      "Epoch [8/25] Step [780/2194] Loss: 0.1161\n",
      "Epoch [8/25] Step [795/2194] Loss: 0.0831\n",
      "Epoch [8/25] Step [810/2194] Loss: 0.0664\n",
      "Epoch [8/25] Step [825/2194] Loss: 0.0438\n",
      "Epoch [8/25] Step [840/2194] Loss: 0.0666\n",
      "Epoch [8/25] Step [855/2194] Loss: 0.0630\n",
      "Epoch [8/25] Step [870/2194] Loss: 0.0903\n",
      "Epoch [8/25] Step [885/2194] Loss: 0.1800\n",
      "Epoch [8/25] Step [900/2194] Loss: 0.1775\n",
      "Epoch [8/25] Step [915/2194] Loss: 0.0601\n",
      "Epoch [8/25] Step [930/2194] Loss: 0.0272\n",
      "Epoch [8/25] Step [945/2194] Loss: 0.0635\n",
      "Epoch [8/25] Step [960/2194] Loss: 0.2618\n",
      "Epoch [8/25] Step [975/2194] Loss: 0.1453\n",
      "Epoch [8/25] Step [990/2194] Loss: 0.1151\n",
      "Epoch [8/25] Step [1005/2194] Loss: 0.0282\n",
      "Epoch [8/25] Step [1020/2194] Loss: 0.1292\n",
      "Epoch [8/25] Step [1035/2194] Loss: 0.0211\n",
      "Epoch [8/25] Step [1050/2194] Loss: 0.1023\n",
      "Epoch [8/25] Step [1065/2194] Loss: 0.1309\n",
      "Epoch [8/25] Step [1080/2194] Loss: 0.1186\n",
      "Epoch [8/25] Step [1095/2194] Loss: 0.0280\n",
      "Epoch [8/25] Step [1110/2194] Loss: 0.2012\n",
      "Epoch [8/25] Step [1125/2194] Loss: 0.1290\n",
      "Epoch [8/25] Step [1140/2194] Loss: 0.1128\n",
      "Epoch [8/25] Step [1155/2194] Loss: 0.0698\n",
      "Epoch [8/25] Step [1170/2194] Loss: 0.3174\n",
      "Epoch [8/25] Step [1185/2194] Loss: 0.0507\n",
      "Epoch [8/25] Step [1200/2194] Loss: 0.0238\n",
      "Epoch [8/25] Step [1215/2194] Loss: 0.0887\n",
      "Epoch [8/25] Step [1230/2194] Loss: 0.0416\n",
      "Epoch [8/25] Step [1245/2194] Loss: 0.0880\n",
      "Epoch [8/25] Step [1260/2194] Loss: 0.0051\n",
      "Epoch [8/25] Step [1275/2194] Loss: 0.1113\n",
      "Epoch [8/25] Step [1290/2194] Loss: 0.0902\n",
      "Epoch [8/25] Step [1305/2194] Loss: 0.2162\n",
      "Epoch [8/25] Step [1320/2194] Loss: 0.1924\n",
      "Epoch [8/25] Step [1335/2194] Loss: 0.0909\n",
      "Epoch [8/25] Step [1350/2194] Loss: 0.0541\n",
      "Epoch [8/25] Step [1365/2194] Loss: 0.0461\n",
      "Epoch [8/25] Step [1380/2194] Loss: 0.0414\n",
      "Epoch [8/25] Step [1395/2194] Loss: 0.1551\n",
      "Epoch [8/25] Step [1410/2194] Loss: 0.1072\n",
      "Epoch [8/25] Step [1425/2194] Loss: 0.0423\n",
      "Epoch [8/25] Step [1440/2194] Loss: 0.1392\n",
      "Epoch [8/25] Step [1455/2194] Loss: 0.0294\n",
      "Epoch [8/25] Step [1470/2194] Loss: 0.2443\n",
      "Epoch [8/25] Step [1485/2194] Loss: 0.0496\n",
      "Epoch [8/25] Step [1500/2194] Loss: 0.0996\n",
      "Epoch [8/25] Step [1515/2194] Loss: 0.1288\n",
      "Epoch [8/25] Step [1530/2194] Loss: 0.0815\n",
      "Epoch [8/25] Step [1545/2194] Loss: 0.0836\n",
      "Epoch [8/25] Step [1560/2194] Loss: 0.2459\n",
      "Epoch [8/25] Step [1575/2194] Loss: 0.0216\n",
      "Epoch [8/25] Step [1590/2194] Loss: 0.0667\n",
      "Epoch [8/25] Step [1605/2194] Loss: 0.0593\n",
      "Epoch [8/25] Step [1620/2194] Loss: 0.0295\n",
      "Epoch [8/25] Step [1635/2194] Loss: 0.0483\n",
      "Epoch [8/25] Step [1650/2194] Loss: 0.0855\n",
      "Epoch [8/25] Step [1665/2194] Loss: 0.0822\n",
      "Epoch [8/25] Step [1680/2194] Loss: 0.1369\n",
      "Epoch [8/25] Step [1695/2194] Loss: 0.1707\n",
      "Epoch [8/25] Step [1710/2194] Loss: 0.0571\n",
      "Epoch [8/25] Step [1725/2194] Loss: 0.0364\n",
      "Epoch [8/25] Step [1740/2194] Loss: 0.1151\n",
      "Epoch [8/25] Step [1755/2194] Loss: 0.1488\n",
      "Epoch [8/25] Step [1770/2194] Loss: 0.0371\n",
      "Epoch [8/25] Step [1785/2194] Loss: 0.1299\n",
      "Epoch [8/25] Step [1800/2194] Loss: 0.1542\n",
      "Epoch [8/25] Step [1815/2194] Loss: 0.0850\n",
      "Epoch [8/25] Step [1830/2194] Loss: 0.0708\n",
      "Epoch [8/25] Step [1845/2194] Loss: 0.0877\n",
      "Epoch [8/25] Step [1860/2194] Loss: 0.0414\n",
      "Epoch [8/25] Step [1875/2194] Loss: 0.0977\n",
      "Epoch [8/25] Step [1890/2194] Loss: 0.0833\n",
      "Epoch [8/25] Step [1905/2194] Loss: 0.0426\n",
      "Epoch [8/25] Step [1920/2194] Loss: 0.2105\n",
      "Epoch [8/25] Step [1935/2194] Loss: 0.1385\n",
      "Epoch [8/25] Step [1950/2194] Loss: 0.1522\n",
      "Epoch [8/25] Step [1965/2194] Loss: 0.1161\n",
      "Epoch [8/25] Step [1980/2194] Loss: 0.0850\n",
      "Epoch [8/25] Step [1995/2194] Loss: 0.0515\n",
      "Epoch [8/25] Step [2010/2194] Loss: 0.0681\n",
      "Epoch [8/25] Step [2025/2194] Loss: 0.2351\n",
      "Epoch [8/25] Step [2040/2194] Loss: 0.0737\n",
      "Epoch [8/25] Step [2055/2194] Loss: 0.1360\n",
      "Epoch [8/25] Step [2070/2194] Loss: 0.1901\n",
      "Epoch [8/25] Step [2085/2194] Loss: 0.1247\n",
      "Epoch [8/25] Step [2100/2194] Loss: 0.1637\n",
      "Epoch [8/25] Step [2115/2194] Loss: 0.1003\n",
      "Epoch [8/25] Step [2130/2194] Loss: 0.0314\n",
      "Epoch [8/25] Step [2145/2194] Loss: 0.1698\n",
      "Epoch [8/25] Step [2160/2194] Loss: 0.0781\n",
      "Epoch [8/25] Step [2175/2194] Loss: 0.0572\n",
      "Epoch [8/25] Step [2190/2194] Loss: 0.0858\n",
      "Epoch [8/25] completed in 2043.98s\n",
      "Train Accuracy: 0.9568, Validation Accuracy: 0.8911\n",
      "\n",
      "Epoch [9/25] Step [0/2194] Loss: 0.2235\n",
      "Epoch [9/25] Step [15/2194] Loss: 0.0913\n",
      "Epoch [9/25] Step [30/2194] Loss: 0.0632\n",
      "Epoch [9/25] Step [45/2194] Loss: 0.0594\n",
      "Epoch [9/25] Step [60/2194] Loss: 0.0148\n",
      "Epoch [9/25] Step [75/2194] Loss: 0.1143\n",
      "Epoch [9/25] Step [90/2194] Loss: 0.0439\n",
      "Epoch [9/25] Step [105/2194] Loss: 0.0356\n",
      "Epoch [9/25] Step [120/2194] Loss: 0.0129\n",
      "Epoch [9/25] Step [135/2194] Loss: 0.1546\n",
      "Epoch [9/25] Step [150/2194] Loss: 0.0076\n",
      "Epoch [9/25] Step [165/2194] Loss: 0.0660\n",
      "Epoch [9/25] Step [180/2194] Loss: 0.0281\n",
      "Epoch [9/25] Step [195/2194] Loss: 0.0217\n",
      "Epoch [9/25] Step [210/2194] Loss: 0.1195\n",
      "Epoch [9/25] Step [225/2194] Loss: 0.1145\n",
      "Epoch [9/25] Step [240/2194] Loss: 0.0802\n",
      "Epoch [9/25] Step [255/2194] Loss: 0.0414\n",
      "Epoch [9/25] Step [270/2194] Loss: 0.1878\n",
      "Epoch [9/25] Step [285/2194] Loss: 0.0428\n",
      "Epoch [9/25] Step [300/2194] Loss: 0.2220\n",
      "Epoch [9/25] Step [315/2194] Loss: 0.1526\n",
      "Epoch [9/25] Step [330/2194] Loss: 0.0920\n",
      "Epoch [9/25] Step [345/2194] Loss: 0.0652\n",
      "Epoch [9/25] Step [360/2194] Loss: 0.1477\n",
      "Epoch [9/25] Step [375/2194] Loss: 0.0969\n",
      "Epoch [9/25] Step [390/2194] Loss: 0.0612\n",
      "Epoch [9/25] Step [405/2194] Loss: 0.0280\n",
      "Epoch [9/25] Step [420/2194] Loss: 0.0334\n",
      "Epoch [9/25] Step [435/2194] Loss: 0.1423\n",
      "Epoch [9/25] Step [450/2194] Loss: 0.2301\n",
      "Epoch [9/25] Step [465/2194] Loss: 0.0192\n",
      "Epoch [9/25] Step [480/2194] Loss: 0.1686\n",
      "Epoch [9/25] Step [495/2194] Loss: 0.0122\n",
      "Epoch [9/25] Step [510/2194] Loss: 0.0719\n",
      "Epoch [9/25] Step [525/2194] Loss: 0.0606\n",
      "Epoch [9/25] Step [540/2194] Loss: 0.1447\n",
      "Epoch [9/25] Step [555/2194] Loss: 0.1153\n",
      "Epoch [9/25] Step [570/2194] Loss: 0.1177\n",
      "Epoch [9/25] Step [585/2194] Loss: 0.1328\n",
      "Epoch [9/25] Step [600/2194] Loss: 0.0446\n",
      "Epoch [9/25] Step [615/2194] Loss: 0.1143\n",
      "Epoch [9/25] Step [630/2194] Loss: 0.1372\n",
      "Epoch [9/25] Step [645/2194] Loss: 0.0992\n",
      "Epoch [9/25] Step [660/2194] Loss: 0.0915\n",
      "Epoch [9/25] Step [675/2194] Loss: 0.1089\n",
      "Epoch [9/25] Step [690/2194] Loss: 0.0267\n",
      "Epoch [9/25] Step [705/2194] Loss: 0.0738\n",
      "Epoch [9/25] Step [720/2194] Loss: 0.0426\n",
      "Epoch [9/25] Step [735/2194] Loss: 0.0237\n",
      "Epoch [9/25] Step [750/2194] Loss: 0.1717\n",
      "Epoch [9/25] Step [765/2194] Loss: 0.1161\n",
      "Epoch [9/25] Step [780/2194] Loss: 0.1691\n",
      "Epoch [9/25] Step [795/2194] Loss: 0.0292\n",
      "Epoch [9/25] Step [810/2194] Loss: 0.1260\n",
      "Epoch [9/25] Step [825/2194] Loss: 0.0289\n",
      "Epoch [9/25] Step [840/2194] Loss: 0.0419\n",
      "Epoch [9/25] Step [855/2194] Loss: 0.0441\n",
      "Epoch [9/25] Step [870/2194] Loss: 0.0329\n",
      "Epoch [9/25] Step [885/2194] Loss: 0.0507\n",
      "Epoch [9/25] Step [900/2194] Loss: 0.1355\n",
      "Epoch [9/25] Step [915/2194] Loss: 0.0341\n",
      "Epoch [9/25] Step [930/2194] Loss: 0.0990\n",
      "Epoch [9/25] Step [945/2194] Loss: 0.1841\n",
      "Epoch [9/25] Step [960/2194] Loss: 0.0510\n",
      "Epoch [9/25] Step [975/2194] Loss: 0.0563\n",
      "Epoch [9/25] Step [990/2194] Loss: 0.1071\n",
      "Epoch [9/25] Step [1005/2194] Loss: 0.0577\n",
      "Epoch [9/25] Step [1020/2194] Loss: 0.0602\n",
      "Epoch [9/25] Step [1035/2194] Loss: 0.0124\n",
      "Epoch [9/25] Step [1050/2194] Loss: 0.0732\n",
      "Epoch [9/25] Step [1065/2194] Loss: 0.1814\n",
      "Epoch [9/25] Step [1080/2194] Loss: 0.0418\n",
      "Epoch [9/25] Step [1095/2194] Loss: 0.0689\n",
      "Epoch [9/25] Step [1110/2194] Loss: 0.0796\n",
      "Epoch [9/25] Step [1125/2194] Loss: 0.0386\n",
      "Epoch [9/25] Step [1140/2194] Loss: 0.0229\n",
      "Epoch [9/25] Step [1155/2194] Loss: 0.0364\n",
      "Epoch [9/25] Step [1170/2194] Loss: 0.1152\n",
      "Epoch [9/25] Step [1185/2194] Loss: 0.1731\n",
      "Epoch [9/25] Step [1200/2194] Loss: 0.0395\n",
      "Epoch [9/25] Step [1215/2194] Loss: 0.1070\n",
      "Epoch [9/25] Step [1230/2194] Loss: 0.1159\n",
      "Epoch [9/25] Step [1245/2194] Loss: 0.0569\n",
      "Epoch [9/25] Step [1260/2194] Loss: 0.0816\n",
      "Epoch [9/25] Step [1275/2194] Loss: 0.0273\n",
      "Epoch [9/25] Step [1290/2194] Loss: 0.0566\n",
      "Epoch [9/25] Step [1305/2194] Loss: 0.1062\n",
      "Epoch [9/25] Step [1320/2194] Loss: 0.0987\n",
      "Epoch [9/25] Step [1335/2194] Loss: 0.0585\n",
      "Epoch [9/25] Step [1350/2194] Loss: 0.0740\n",
      "Epoch [9/25] Step [1365/2194] Loss: 0.0273\n",
      "Epoch [9/25] Step [1380/2194] Loss: 0.2385\n",
      "Epoch [9/25] Step [1395/2194] Loss: 0.2042\n",
      "Epoch [9/25] Step [1410/2194] Loss: 0.0182\n",
      "Epoch [9/25] Step [1425/2194] Loss: 0.0930\n",
      "Epoch [9/25] Step [1440/2194] Loss: 0.0615\n",
      "Epoch [9/25] Step [1455/2194] Loss: 0.1900\n",
      "Epoch [9/25] Step [1470/2194] Loss: 0.1605\n",
      "Epoch [9/25] Step [1485/2194] Loss: 0.1449\n",
      "Epoch [9/25] Step [1500/2194] Loss: 0.1619\n",
      "Epoch [9/25] Step [1515/2194] Loss: 0.2279\n",
      "Epoch [9/25] Step [1530/2194] Loss: 0.3531\n",
      "Epoch [9/25] Step [1545/2194] Loss: 0.0272\n",
      "Epoch [9/25] Step [1560/2194] Loss: 0.0401\n",
      "Epoch [9/25] Step [1575/2194] Loss: 0.0345\n",
      "Epoch [9/25] Step [1590/2194] Loss: 0.1637\n",
      "Epoch [9/25] Step [1605/2194] Loss: 0.0344\n",
      "Epoch [9/25] Step [1620/2194] Loss: 0.1150\n",
      "Epoch [9/25] Step [1635/2194] Loss: 0.0984\n",
      "Epoch [9/25] Step [1650/2194] Loss: 0.0517\n",
      "Epoch [9/25] Step [1665/2194] Loss: 0.0383\n",
      "Epoch [9/25] Step [1680/2194] Loss: 0.0579\n",
      "Epoch [9/25] Step [1695/2194] Loss: 0.0821\n",
      "Epoch [9/25] Step [1710/2194] Loss: 0.3241\n",
      "Epoch [9/25] Step [1725/2194] Loss: 0.0995\n",
      "Epoch [9/25] Step [1740/2194] Loss: 0.0747\n",
      "Epoch [9/25] Step [1755/2194] Loss: 0.1527\n",
      "Epoch [9/25] Step [1770/2194] Loss: 0.0495\n",
      "Epoch [9/25] Step [1785/2194] Loss: 0.0062\n",
      "Epoch [9/25] Step [1800/2194] Loss: 0.0607\n",
      "Epoch [9/25] Step [1815/2194] Loss: 0.0603\n",
      "Epoch [9/25] Step [1830/2194] Loss: 0.0485\n",
      "Epoch [9/25] Step [1845/2194] Loss: 0.0786\n",
      "Epoch [9/25] Step [1860/2194] Loss: 0.0382\n",
      "Epoch [9/25] Step [1875/2194] Loss: 0.1155\n",
      "Epoch [9/25] Step [1890/2194] Loss: 0.0285\n",
      "Epoch [9/25] Step [1905/2194] Loss: 0.1860\n",
      "Epoch [9/25] Step [1920/2194] Loss: 0.1143\n",
      "Epoch [9/25] Step [1935/2194] Loss: 0.0901\n",
      "Epoch [9/25] Step [1950/2194] Loss: 0.0619\n",
      "Epoch [9/25] Step [1965/2194] Loss: 0.0638\n",
      "Epoch [9/25] Step [1980/2194] Loss: 0.0822\n",
      "Epoch [9/25] Step [1995/2194] Loss: 0.0306\n",
      "Epoch [9/25] Step [2010/2194] Loss: 0.0514\n",
      "Epoch [9/25] Step [2025/2194] Loss: 0.0365\n",
      "Epoch [9/25] Step [2040/2194] Loss: 0.0567\n",
      "Epoch [9/25] Step [2055/2194] Loss: 0.1633\n",
      "Epoch [9/25] Step [2070/2194] Loss: 0.2992\n",
      "Epoch [9/25] Step [2085/2194] Loss: 0.0490\n",
      "Epoch [9/25] Step [2100/2194] Loss: 0.0561\n",
      "Epoch [9/25] Step [2115/2194] Loss: 0.1280\n",
      "Epoch [9/25] Step [2130/2194] Loss: 0.0442\n",
      "Epoch [9/25] Step [2145/2194] Loss: 0.0734\n",
      "Epoch [9/25] Step [2160/2194] Loss: 0.0259\n",
      "Epoch [9/25] Step [2175/2194] Loss: 0.2174\n",
      "Epoch [9/25] Step [2190/2194] Loss: 0.1214\n",
      "Epoch [9/25] completed in 2043.92s\n",
      "Train Accuracy: 0.9637, Validation Accuracy: 0.8987\n",
      "\n",
      "âœ… Best model updated and saved with Val Acc: 0.8987\n",
      "\n",
      "Epoch [10/25] Step [0/2194] Loss: 0.0689\n",
      "Epoch [10/25] Step [15/2194] Loss: 0.0147\n",
      "Epoch [10/25] Step [30/2194] Loss: 0.0387\n",
      "Epoch [10/25] Step [45/2194] Loss: 0.0297\n",
      "Epoch [10/25] Step [60/2194] Loss: 0.0240\n",
      "Epoch [10/25] Step [75/2194] Loss: 0.1859\n",
      "Epoch [10/25] Step [90/2194] Loss: 0.0489\n",
      "Epoch [10/25] Step [105/2194] Loss: 0.1339\n",
      "Epoch [10/25] Step [120/2194] Loss: 0.0324\n",
      "Epoch [10/25] Step [135/2194] Loss: 0.1765\n",
      "Epoch [10/25] Step [150/2194] Loss: 0.0236\n",
      "Epoch [10/25] Step [165/2194] Loss: 0.0817\n",
      "Epoch [10/25] Step [180/2194] Loss: 0.2206\n",
      "Epoch [10/25] Step [195/2194] Loss: 0.0208\n",
      "Epoch [10/25] Step [210/2194] Loss: 0.0660\n",
      "Epoch [10/25] Step [225/2194] Loss: 0.0125\n",
      "Epoch [10/25] Step [240/2194] Loss: 0.0405\n",
      "Epoch [10/25] Step [255/2194] Loss: 0.3333\n",
      "Epoch [10/25] Step [270/2194] Loss: 0.0185\n",
      "Epoch [10/25] Step [285/2194] Loss: 0.0626\n",
      "Epoch [10/25] Step [300/2194] Loss: 0.0769\n",
      "Epoch [10/25] Step [315/2194] Loss: 0.0390\n",
      "Epoch [10/25] Step [330/2194] Loss: 0.0523\n",
      "Epoch [10/25] Step [345/2194] Loss: 0.0657\n",
      "Epoch [10/25] Step [360/2194] Loss: 0.0347\n",
      "Epoch [10/25] Step [375/2194] Loss: 0.2660\n",
      "Epoch [10/25] Step [390/2194] Loss: 0.1344\n",
      "Epoch [10/25] Step [405/2194] Loss: 0.2180\n",
      "Epoch [10/25] Step [420/2194] Loss: 0.0615\n",
      "Epoch [10/25] Step [435/2194] Loss: 0.1264\n",
      "Epoch [10/25] Step [450/2194] Loss: 0.1131\n",
      "Epoch [10/25] Step [465/2194] Loss: 0.1002\n",
      "Epoch [10/25] Step [480/2194] Loss: 0.1317\n",
      "Epoch [10/25] Step [495/2194] Loss: 0.1927\n",
      "Epoch [10/25] Step [510/2194] Loss: 0.0510\n",
      "Epoch [10/25] Step [525/2194] Loss: 0.0451\n",
      "Epoch [10/25] Step [540/2194] Loss: 0.0576\n",
      "Epoch [10/25] Step [555/2194] Loss: 0.1450\n",
      "Epoch [10/25] Step [570/2194] Loss: 0.1035\n",
      "Epoch [10/25] Step [585/2194] Loss: 0.0345\n",
      "Epoch [10/25] Step [600/2194] Loss: 0.0597\n",
      "Epoch [10/25] Step [615/2194] Loss: 0.0252\n",
      "Epoch [10/25] Step [630/2194] Loss: 0.0797\n",
      "Epoch [10/25] Step [645/2194] Loss: 0.0848\n",
      "Epoch [10/25] Step [660/2194] Loss: 0.0279\n",
      "Epoch [10/25] Step [675/2194] Loss: 0.0277\n",
      "Epoch [10/25] Step [690/2194] Loss: 0.2564\n",
      "Epoch [10/25] Step [705/2194] Loss: 0.0252\n",
      "Epoch [10/25] Step [720/2194] Loss: 0.1051\n",
      "Epoch [10/25] Step [735/2194] Loss: 0.0361\n",
      "Epoch [10/25] Step [750/2194] Loss: 0.0722\n",
      "Epoch [10/25] Step [765/2194] Loss: 0.1131\n",
      "Epoch [10/25] Step [780/2194] Loss: 0.0673\n",
      "Epoch [10/25] Step [795/2194] Loss: 0.0098\n",
      "Epoch [10/25] Step [810/2194] Loss: 0.0680\n",
      "Epoch [10/25] Step [825/2194] Loss: 0.1273\n",
      "Epoch [10/25] Step [840/2194] Loss: 0.1555\n",
      "Epoch [10/25] Step [855/2194] Loss: 0.0652\n",
      "Epoch [10/25] Step [870/2194] Loss: 0.0706\n",
      "Epoch [10/25] Step [885/2194] Loss: 0.0367\n",
      "Epoch [10/25] Step [900/2194] Loss: 0.0255\n",
      "Epoch [10/25] Step [915/2194] Loss: 0.1695\n",
      "Epoch [10/25] Step [930/2194] Loss: 0.0728\n",
      "Epoch [10/25] Step [945/2194] Loss: 0.0186\n",
      "Epoch [10/25] Step [960/2194] Loss: 0.0147\n",
      "Epoch [10/25] Step [975/2194] Loss: 0.0719\n",
      "Epoch [10/25] Step [990/2194] Loss: 0.1102\n",
      "Epoch [10/25] Step [1005/2194] Loss: 0.0972\n",
      "Epoch [10/25] Step [1020/2194] Loss: 0.0896\n",
      "Epoch [10/25] Step [1035/2194] Loss: 0.1045\n",
      "Epoch [10/25] Step [1050/2194] Loss: 0.0362\n",
      "Epoch [10/25] Step [1065/2194] Loss: 0.0716\n",
      "Epoch [10/25] Step [1080/2194] Loss: 0.2914\n",
      "Epoch [10/25] Step [1095/2194] Loss: 0.1513\n",
      "Epoch [10/25] Step [1110/2194] Loss: 0.0614\n",
      "Epoch [10/25] Step [1125/2194] Loss: 0.1339\n",
      "Epoch [10/25] Step [1140/2194] Loss: 0.0319\n",
      "Epoch [10/25] Step [1155/2194] Loss: 0.0262\n",
      "Epoch [10/25] Step [1170/2194] Loss: 0.0153\n",
      "Epoch [10/25] Step [1185/2194] Loss: 0.0499\n",
      "Epoch [10/25] Step [1200/2194] Loss: 0.1289\n",
      "Epoch [10/25] Step [1215/2194] Loss: 0.0840\n",
      "Epoch [10/25] Step [1230/2194] Loss: 0.0129\n",
      "Epoch [10/25] Step [1245/2194] Loss: 0.0849\n",
      "Epoch [10/25] Step [1260/2194] Loss: 0.0678\n",
      "Epoch [10/25] Step [1275/2194] Loss: 0.0568\n",
      "Epoch [10/25] Step [1290/2194] Loss: 0.0377\n",
      "Epoch [10/25] Step [1305/2194] Loss: 0.0092\n",
      "Epoch [10/25] Step [1320/2194] Loss: 0.0339\n",
      "Epoch [10/25] Step [1335/2194] Loss: 0.1647\n",
      "Epoch [10/25] Step [1350/2194] Loss: 0.1995\n",
      "Epoch [10/25] Step [1365/2194] Loss: 0.2881\n",
      "Epoch [10/25] Step [1380/2194] Loss: 0.0134\n",
      "Epoch [10/25] Step [1395/2194] Loss: 0.0639\n",
      "Epoch [10/25] Step [1410/2194] Loss: 0.1875\n",
      "Epoch [10/25] Step [1425/2194] Loss: 0.1299\n",
      "Epoch [10/25] Step [1440/2194] Loss: 0.0482\n",
      "Epoch [10/25] Step [1455/2194] Loss: 0.0917\n",
      "Epoch [10/25] Step [1470/2194] Loss: 0.0355\n",
      "Epoch [10/25] Step [1485/2194] Loss: 0.0547\n",
      "Epoch [10/25] Step [1500/2194] Loss: 0.0402\n",
      "Epoch [10/25] Step [1515/2194] Loss: 0.0085\n",
      "Epoch [10/25] Step [1530/2194] Loss: 0.0140\n",
      "Epoch [10/25] Step [1545/2194] Loss: 0.0934\n",
      "Epoch [10/25] Step [1560/2194] Loss: 0.0402\n",
      "Epoch [10/25] Step [1575/2194] Loss: 0.0223\n",
      "Epoch [10/25] Step [1590/2194] Loss: 0.0314\n",
      "Epoch [10/25] Step [1605/2194] Loss: 0.1246\n",
      "Epoch [10/25] Step [1620/2194] Loss: 0.1690\n",
      "Epoch [10/25] Step [1635/2194] Loss: 0.0446\n",
      "Epoch [10/25] Step [1650/2194] Loss: 0.1037\n",
      "Epoch [10/25] Step [1665/2194] Loss: 0.0346\n",
      "Epoch [10/25] Step [1680/2194] Loss: 0.1538\n",
      "Epoch [10/25] Step [1695/2194] Loss: 0.0302\n",
      "Epoch [10/25] Step [1710/2194] Loss: 0.0401\n",
      "Epoch [10/25] Step [1725/2194] Loss: 0.0559\n",
      "Epoch [10/25] Step [1740/2194] Loss: 0.0835\n",
      "Epoch [10/25] Step [1755/2194] Loss: 0.0637\n",
      "Epoch [10/25] Step [1770/2194] Loss: 0.1255\n",
      "Epoch [10/25] Step [1785/2194] Loss: 0.1184\n",
      "Epoch [10/25] Step [1800/2194] Loss: 0.0405\n",
      "Epoch [10/25] Step [1815/2194] Loss: 0.0946\n",
      "Epoch [10/25] Step [1830/2194] Loss: 0.1437\n",
      "Epoch [10/25] Step [1845/2194] Loss: 0.2223\n",
      "Epoch [10/25] Step [1860/2194] Loss: 0.0174\n",
      "Epoch [10/25] Step [1875/2194] Loss: 0.0133\n",
      "Epoch [10/25] Step [1890/2194] Loss: 0.0449\n",
      "Epoch [10/25] Step [1905/2194] Loss: 0.0691\n",
      "Epoch [10/25] Step [1920/2194] Loss: 0.1266\n",
      "Epoch [10/25] Step [1935/2194] Loss: 0.1805\n",
      "Epoch [10/25] Step [1950/2194] Loss: 0.0291\n",
      "Epoch [10/25] Step [1965/2194] Loss: 0.0612\n",
      "Epoch [10/25] Step [1980/2194] Loss: 0.0284\n",
      "Epoch [10/25] Step [1995/2194] Loss: 0.1262\n",
      "Epoch [10/25] Step [2010/2194] Loss: 0.0671\n",
      "Epoch [10/25] Step [2025/2194] Loss: 0.2276\n",
      "Epoch [10/25] Step [2040/2194] Loss: 0.0911\n",
      "Epoch [10/25] Step [2055/2194] Loss: 0.0785\n",
      "Epoch [10/25] Step [2070/2194] Loss: 0.0244\n",
      "Epoch [10/25] Step [2085/2194] Loss: 0.1412\n",
      "Epoch [10/25] Step [2100/2194] Loss: 0.1155\n",
      "Epoch [10/25] Step [2115/2194] Loss: 0.1524\n",
      "Epoch [10/25] Step [2130/2194] Loss: 0.2263\n",
      "Epoch [10/25] Step [2145/2194] Loss: 0.2057\n",
      "Epoch [10/25] Step [2160/2194] Loss: 0.0567\n",
      "Epoch [10/25] Step [2175/2194] Loss: 0.0793\n",
      "Epoch [10/25] Step [2190/2194] Loss: 0.0472\n",
      "Epoch [10/25] completed in 2042.81s\n",
      "Train Accuracy: 0.9681, Validation Accuracy: 0.8961\n",
      "\n",
      "Epoch [11/25] Step [0/2194] Loss: 0.1550\n",
      "Epoch [11/25] Step [15/2194] Loss: 0.1500\n",
      "Epoch [11/25] Step [30/2194] Loss: 0.0635\n",
      "Epoch [11/25] Step [45/2194] Loss: 0.0075\n",
      "Epoch [11/25] Step [60/2194] Loss: 0.0789\n",
      "Epoch [11/25] Step [75/2194] Loss: 0.0157\n",
      "Epoch [11/25] Step [90/2194] Loss: 0.1175\n",
      "Epoch [11/25] Step [105/2194] Loss: 0.1743\n",
      "Epoch [11/25] Step [120/2194] Loss: 0.0576\n",
      "Epoch [11/25] Step [135/2194] Loss: 0.0344\n",
      "Epoch [11/25] Step [150/2194] Loss: 0.4013\n",
      "Epoch [11/25] Step [165/2194] Loss: 0.0400\n",
      "Epoch [11/25] Step [180/2194] Loss: 0.1018\n",
      "Epoch [11/25] Step [195/2194] Loss: 0.0251\n",
      "Epoch [11/25] Step [210/2194] Loss: 0.0601\n",
      "Epoch [11/25] Step [225/2194] Loss: 0.0514\n",
      "Epoch [11/25] Step [240/2194] Loss: 0.0477\n",
      "Epoch [11/25] Step [255/2194] Loss: 0.0246\n",
      "Epoch [11/25] Step [270/2194] Loss: 0.0200\n",
      "Epoch [11/25] Step [285/2194] Loss: 0.0660\n",
      "Epoch [11/25] Step [300/2194] Loss: 0.0040\n",
      "Epoch [11/25] Step [315/2194] Loss: 0.0199\n",
      "Epoch [11/25] Step [330/2194] Loss: 0.0176\n",
      "Epoch [11/25] Step [345/2194] Loss: 0.0135\n",
      "Epoch [11/25] Step [360/2194] Loss: 0.0397\n",
      "Epoch [11/25] Step [375/2194] Loss: 0.1046\n",
      "Epoch [11/25] Step [390/2194] Loss: 0.0486\n",
      "Epoch [11/25] Step [405/2194] Loss: 0.1011\n",
      "Epoch [11/25] Step [420/2194] Loss: 0.0980\n",
      "Epoch [11/25] Step [435/2194] Loss: 0.2172\n",
      "Epoch [11/25] Step [450/2194] Loss: 0.0276\n",
      "Epoch [11/25] Step [465/2194] Loss: 0.0600\n",
      "Epoch [11/25] Step [480/2194] Loss: 0.1401\n",
      "Epoch [11/25] Step [495/2194] Loss: 0.1155\n",
      "Epoch [11/25] Step [510/2194] Loss: 0.0472\n",
      "Epoch [11/25] Step [525/2194] Loss: 0.0772\n",
      "Epoch [11/25] Step [540/2194] Loss: 0.1731\n",
      "Epoch [11/25] Step [555/2194] Loss: 0.0268\n",
      "Epoch [11/25] Step [570/2194] Loss: 0.0214\n",
      "Epoch [11/25] Step [585/2194] Loss: 0.1578\n",
      "Epoch [11/25] Step [600/2194] Loss: 0.0739\n",
      "Epoch [11/25] Step [615/2194] Loss: 0.0187\n",
      "Epoch [11/25] Step [630/2194] Loss: 0.2843\n",
      "Epoch [11/25] Step [645/2194] Loss: 0.0676\n",
      "Epoch [11/25] Step [660/2194] Loss: 0.2558\n",
      "Epoch [11/25] Step [675/2194] Loss: 0.0927\n",
      "Epoch [11/25] Step [690/2194] Loss: 0.0716\n",
      "Epoch [11/25] Step [705/2194] Loss: 0.2356\n",
      "Epoch [11/25] Step [720/2194] Loss: 0.0359\n",
      "Epoch [11/25] Step [735/2194] Loss: 0.1326\n",
      "Epoch [11/25] Step [750/2194] Loss: 0.0483\n",
      "Epoch [11/25] Step [765/2194] Loss: 0.0520\n",
      "Epoch [11/25] Step [780/2194] Loss: 0.1717\n",
      "Epoch [11/25] Step [795/2194] Loss: 0.1154\n",
      "Epoch [11/25] Step [810/2194] Loss: 0.1268\n",
      "Epoch [11/25] Step [825/2194] Loss: 0.0878\n",
      "Epoch [11/25] Step [840/2194] Loss: 0.0109\n",
      "Epoch [11/25] Step [855/2194] Loss: 0.0494\n",
      "Epoch [11/25] Step [870/2194] Loss: 0.1020\n",
      "Epoch [11/25] Step [885/2194] Loss: 0.3381\n",
      "Epoch [11/25] Step [900/2194] Loss: 0.0714\n",
      "Epoch [11/25] Step [915/2194] Loss: 0.0670\n",
      "Epoch [11/25] Step [930/2194] Loss: 0.0239\n",
      "Epoch [11/25] Step [945/2194] Loss: 0.0861\n",
      "Epoch [11/25] Step [960/2194] Loss: 0.1096\n",
      "Epoch [11/25] Step [975/2194] Loss: 0.0207\n",
      "Epoch [11/25] Step [990/2194] Loss: 0.0378\n",
      "Epoch [11/25] Step [1005/2194] Loss: 0.0263\n",
      "Epoch [11/25] Step [1020/2194] Loss: 0.0713\n",
      "Epoch [11/25] Step [1035/2194] Loss: 0.0677\n",
      "Epoch [11/25] Step [1050/2194] Loss: 0.1502\n",
      "Epoch [11/25] Step [1065/2194] Loss: 0.0869\n",
      "Epoch [11/25] Step [1080/2194] Loss: 0.0709\n",
      "Epoch [11/25] Step [1095/2194] Loss: 0.0183\n",
      "Epoch [11/25] Step [1110/2194] Loss: 0.1646\n",
      "Epoch [11/25] Step [1125/2194] Loss: 0.1544\n",
      "Epoch [11/25] Step [1140/2194] Loss: 0.0138\n",
      "Epoch [11/25] Step [1155/2194] Loss: 0.0271\n",
      "Epoch [11/25] Step [1170/2194] Loss: 0.1051\n",
      "Epoch [11/25] Step [1185/2194] Loss: 0.0475\n",
      "Epoch [11/25] Step [1200/2194] Loss: 0.1345\n",
      "Epoch [11/25] Step [1215/2194] Loss: 0.0879\n",
      "Epoch [11/25] Step [1230/2194] Loss: 0.0452\n",
      "Epoch [11/25] Step [1245/2194] Loss: 0.0041\n",
      "Epoch [11/25] Step [1260/2194] Loss: 0.0911\n",
      "Epoch [11/25] Step [1275/2194] Loss: 0.2073\n",
      "Epoch [11/25] Step [1290/2194] Loss: 0.0279\n",
      "Epoch [11/25] Step [1305/2194] Loss: 0.0258\n",
      "Epoch [11/25] Step [1320/2194] Loss: 0.0168\n",
      "Epoch [11/25] Step [1335/2194] Loss: 0.0388\n",
      "Epoch [11/25] Step [1350/2194] Loss: 0.0201\n",
      "Epoch [11/25] Step [1365/2194] Loss: 0.0163\n",
      "Epoch [11/25] Step [1380/2194] Loss: 0.1063\n",
      "Epoch [11/25] Step [1395/2194] Loss: 0.0013\n",
      "Epoch [11/25] Step [1410/2194] Loss: 0.0188\n",
      "Epoch [11/25] Step [1425/2194] Loss: 0.0716\n",
      "Epoch [11/25] Step [1440/2194] Loss: 0.0124\n",
      "Epoch [11/25] Step [1455/2194] Loss: 0.1731\n",
      "Epoch [11/25] Step [1470/2194] Loss: 0.0515\n",
      "Epoch [11/25] Step [1485/2194] Loss: 0.0180\n",
      "Epoch [11/25] Step [1500/2194] Loss: 0.1075\n",
      "Epoch [11/25] Step [1515/2194] Loss: 0.3945\n",
      "Epoch [11/25] Step [1530/2194] Loss: 0.0720\n",
      "Epoch [11/25] Step [1545/2194] Loss: 0.0847\n",
      "Epoch [11/25] Step [1560/2194] Loss: 0.0130\n",
      "Epoch [11/25] Step [1575/2194] Loss: 0.1131\n",
      "Epoch [11/25] Step [1590/2194] Loss: 0.0896\n",
      "Epoch [11/25] Step [1605/2194] Loss: 0.0111\n",
      "Epoch [11/25] Step [1620/2194] Loss: 0.1934\n",
      "Epoch [11/25] Step [1635/2194] Loss: 0.0761\n",
      "Epoch [11/25] Step [1650/2194] Loss: 0.0366\n",
      "Epoch [11/25] Step [1665/2194] Loss: 0.0228\n",
      "Epoch [11/25] Step [1680/2194] Loss: 0.1335\n",
      "Epoch [11/25] Step [1695/2194] Loss: 0.1389\n",
      "Epoch [11/25] Step [1710/2194] Loss: 0.0209\n",
      "Epoch [11/25] Step [1725/2194] Loss: 0.0804\n",
      "Epoch [11/25] Step [1740/2194] Loss: 0.1963\n",
      "Epoch [11/25] Step [1755/2194] Loss: 0.1271\n",
      "Epoch [11/25] Step [1770/2194] Loss: 0.0604\n",
      "Epoch [11/25] Step [1785/2194] Loss: 0.0775\n",
      "Epoch [11/25] Step [1800/2194] Loss: 0.0256\n",
      "Epoch [11/25] Step [1815/2194] Loss: 0.0683\n",
      "Epoch [11/25] Step [1830/2194] Loss: 0.0909\n",
      "Epoch [11/25] Step [1845/2194] Loss: 0.0702\n",
      "Epoch [11/25] Step [1860/2194] Loss: 0.1147\n",
      "Epoch [11/25] Step [1875/2194] Loss: 0.0614\n",
      "Epoch [11/25] Step [1890/2194] Loss: 0.0718\n",
      "Epoch [11/25] Step [1905/2194] Loss: 0.0164\n",
      "Epoch [11/25] Step [1920/2194] Loss: 0.0049\n",
      "Epoch [11/25] Step [1935/2194] Loss: 0.0038\n",
      "Epoch [11/25] Step [1950/2194] Loss: 0.0267\n",
      "Epoch [11/25] Step [1965/2194] Loss: 0.0118\n",
      "Epoch [11/25] Step [1980/2194] Loss: 0.0247\n",
      "Epoch [11/25] Step [1995/2194] Loss: 0.0374\n",
      "Epoch [11/25] Step [2010/2194] Loss: 0.0204\n",
      "Epoch [11/25] Step [2025/2194] Loss: 0.1663\n",
      "Epoch [11/25] Step [2040/2194] Loss: 0.1244\n",
      "Epoch [11/25] Step [2055/2194] Loss: 0.0480\n",
      "Epoch [11/25] Step [2070/2194] Loss: 0.0496\n",
      "Epoch [11/25] Step [2085/2194] Loss: 0.0341\n",
      "Epoch [11/25] Step [2100/2194] Loss: 0.0151\n",
      "Epoch [11/25] Step [2115/2194] Loss: 0.0718\n",
      "Epoch [11/25] Step [2130/2194] Loss: 0.0594\n",
      "Epoch [11/25] Step [2145/2194] Loss: 0.0112\n",
      "Epoch [11/25] Step [2160/2194] Loss: 0.1214\n",
      "Epoch [11/25] Step [2175/2194] Loss: 0.1501\n",
      "Epoch [11/25] Step [2190/2194] Loss: 0.1595\n",
      "Epoch [11/25] completed in 2041.01s\n",
      "Train Accuracy: 0.9729, Validation Accuracy: 0.8930\n",
      "\n",
      "Epoch [12/25] Step [0/2194] Loss: 0.0619\n",
      "Epoch [12/25] Step [15/2194] Loss: 0.0861\n",
      "Epoch [12/25] Step [30/2194] Loss: 0.0059\n",
      "Epoch [12/25] Step [45/2194] Loss: 0.0498\n",
      "Epoch [12/25] Step [60/2194] Loss: 0.0632\n",
      "Epoch [12/25] Step [75/2194] Loss: 0.0850\n",
      "Epoch [12/25] Step [90/2194] Loss: 0.0075\n",
      "Epoch [12/25] Step [105/2194] Loss: 0.0319\n",
      "Epoch [12/25] Step [120/2194] Loss: 0.0378\n",
      "Epoch [12/25] Step [135/2194] Loss: 0.1843\n",
      "Epoch [12/25] Step [150/2194] Loss: 0.0778\n",
      "Epoch [12/25] Step [165/2194] Loss: 0.1369\n",
      "Epoch [12/25] Step [180/2194] Loss: 0.0214\n",
      "Epoch [12/25] Step [195/2194] Loss: 0.2105\n",
      "Epoch [12/25] Step [210/2194] Loss: 0.0470\n",
      "Epoch [12/25] Step [225/2194] Loss: 0.0218\n",
      "Epoch [12/25] Step [240/2194] Loss: 0.0238\n",
      "Epoch [12/25] Step [255/2194] Loss: 0.0285\n",
      "Epoch [12/25] Step [270/2194] Loss: 0.0135\n",
      "Epoch [12/25] Step [285/2194] Loss: 0.0366\n",
      "Epoch [12/25] Step [300/2194] Loss: 0.0194\n",
      "Epoch [12/25] Step [315/2194] Loss: 0.0061\n",
      "Epoch [12/25] Step [330/2194] Loss: 0.0273\n",
      "Epoch [12/25] Step [345/2194] Loss: 0.0674\n",
      "Epoch [12/25] Step [360/2194] Loss: 0.1153\n",
      "Epoch [12/25] Step [375/2194] Loss: 0.0190\n",
      "Epoch [12/25] Step [390/2194] Loss: 0.2513\n",
      "Epoch [12/25] Step [405/2194] Loss: 0.0235\n",
      "Epoch [12/25] Step [420/2194] Loss: 0.0760\n",
      "Epoch [12/25] Step [435/2194] Loss: 0.0976\n",
      "Epoch [12/25] Step [450/2194] Loss: 0.0048\n",
      "Epoch [12/25] Step [465/2194] Loss: 0.0254\n",
      "Epoch [12/25] Step [480/2194] Loss: 0.0499\n",
      "Epoch [12/25] Step [495/2194] Loss: 0.0204\n",
      "Epoch [12/25] Step [510/2194] Loss: 0.0022\n",
      "Epoch [12/25] Step [525/2194] Loss: 0.0381\n",
      "Epoch [12/25] Step [540/2194] Loss: 0.0506\n",
      "Epoch [12/25] Step [555/2194] Loss: 0.0523\n",
      "Epoch [12/25] Step [570/2194] Loss: 0.0221\n",
      "Epoch [12/25] Step [585/2194] Loss: 0.0199\n",
      "Epoch [12/25] Step [600/2194] Loss: 0.0953\n",
      "Epoch [12/25] Step [615/2194] Loss: 0.0061\n",
      "Epoch [12/25] Step [630/2194] Loss: 0.0409\n",
      "Epoch [12/25] Step [645/2194] Loss: 0.0085\n",
      "Epoch [12/25] Step [660/2194] Loss: 0.1411\n",
      "Epoch [12/25] Step [675/2194] Loss: 0.3466\n",
      "Epoch [12/25] Step [690/2194] Loss: 0.0349\n",
      "Epoch [12/25] Step [705/2194] Loss: 0.0040\n",
      "Epoch [12/25] Step [720/2194] Loss: 0.0775\n",
      "Epoch [12/25] Step [735/2194] Loss: 0.0119\n",
      "Epoch [12/25] Step [750/2194] Loss: 0.0146\n",
      "Epoch [12/25] Step [765/2194] Loss: 0.1095\n",
      "Epoch [12/25] Step [780/2194] Loss: 0.0659\n",
      "Epoch [12/25] Step [795/2194] Loss: 0.0543\n",
      "Epoch [12/25] Step [810/2194] Loss: 0.0322\n",
      "Epoch [12/25] Step [825/2194] Loss: 0.0808\n",
      "Epoch [12/25] Step [840/2194] Loss: 0.0231\n",
      "Epoch [12/25] Step [855/2194] Loss: 0.0208\n",
      "Epoch [12/25] Step [870/2194] Loss: 0.0023\n",
      "Epoch [12/25] Step [885/2194] Loss: 0.0326\n",
      "Epoch [12/25] Step [900/2194] Loss: 0.0583\n",
      "Epoch [12/25] Step [915/2194] Loss: 0.0906\n",
      "Epoch [12/25] Step [930/2194] Loss: 0.3267\n",
      "Epoch [12/25] Step [945/2194] Loss: 0.1124\n",
      "Epoch [12/25] Step [960/2194] Loss: 0.1296\n",
      "Epoch [12/25] Step [975/2194] Loss: 0.1222\n",
      "Epoch [12/25] Step [990/2194] Loss: 0.0611\n",
      "Epoch [12/25] Step [1005/2194] Loss: 0.0233\n",
      "Epoch [12/25] Step [1020/2194] Loss: 0.0344\n",
      "Epoch [12/25] Step [1035/2194] Loss: 0.0388\n",
      "Epoch [12/25] Step [1050/2194] Loss: 0.0369\n",
      "Epoch [12/25] Step [1065/2194] Loss: 0.0613\n",
      "Epoch [12/25] Step [1080/2194] Loss: 0.0557\n",
      "Epoch [12/25] Step [1095/2194] Loss: 0.0397\n",
      "Epoch [12/25] Step [1110/2194] Loss: 0.0309\n",
      "Epoch [12/25] Step [1125/2194] Loss: 0.0181\n",
      "Epoch [12/25] Step [1140/2194] Loss: 0.0498\n",
      "Epoch [12/25] Step [1155/2194] Loss: 0.3095\n",
      "Epoch [12/25] Step [1170/2194] Loss: 0.0416\n",
      "Epoch [12/25] Step [1185/2194] Loss: 0.0802\n",
      "Epoch [12/25] Step [1200/2194] Loss: 0.0211\n",
      "Epoch [12/25] Step [1215/2194] Loss: 0.0261\n",
      "Epoch [12/25] Step [1230/2194] Loss: 0.0338\n",
      "Epoch [12/25] Step [1245/2194] Loss: 0.1154\n",
      "Epoch [12/25] Step [1260/2194] Loss: 0.0678\n",
      "Epoch [12/25] Step [1275/2194] Loss: 0.0409\n",
      "Epoch [12/25] Step [1290/2194] Loss: 0.1382\n",
      "Epoch [12/25] Step [1305/2194] Loss: 0.0202\n",
      "Epoch [12/25] Step [1320/2194] Loss: 0.0212\n",
      "Epoch [12/25] Step [1335/2194] Loss: 0.1572\n",
      "Epoch [12/25] Step [1350/2194] Loss: 0.0394\n",
      "Epoch [12/25] Step [1365/2194] Loss: 0.0424\n",
      "Epoch [12/25] Step [1380/2194] Loss: 0.0271\n",
      "Epoch [12/25] Step [1395/2194] Loss: 0.0513\n",
      "Epoch [12/25] Step [1410/2194] Loss: 0.0475\n",
      "Epoch [12/25] Step [1425/2194] Loss: 0.1044\n",
      "Epoch [12/25] Step [1440/2194] Loss: 0.0076\n",
      "Epoch [12/25] Step [1455/2194] Loss: 0.0009\n",
      "Epoch [12/25] Step [1470/2194] Loss: 0.0531\n",
      "Epoch [12/25] Step [1485/2194] Loss: 0.1532\n",
      "Epoch [12/25] Step [1500/2194] Loss: 0.0208\n",
      "Epoch [12/25] Step [1515/2194] Loss: 0.3152\n",
      "Epoch [12/25] Step [1530/2194] Loss: 0.1120\n",
      "Epoch [12/25] Step [1545/2194] Loss: 0.2031\n",
      "Epoch [12/25] Step [1560/2194] Loss: 0.0797\n",
      "Epoch [12/25] Step [1575/2194] Loss: 0.0711\n",
      "Epoch [12/25] Step [1590/2194] Loss: 0.0620\n",
      "Epoch [12/25] Step [1605/2194] Loss: 0.0942\n",
      "Epoch [12/25] Step [1620/2194] Loss: 0.0746\n",
      "Epoch [12/25] Step [1635/2194] Loss: 0.0124\n",
      "Epoch [12/25] Step [1650/2194] Loss: 0.1329\n",
      "Epoch [12/25] Step [1665/2194] Loss: 0.0128\n",
      "Epoch [12/25] Step [1680/2194] Loss: 0.0885\n",
      "Epoch [12/25] Step [1695/2194] Loss: 0.0165\n",
      "Epoch [12/25] Step [1710/2194] Loss: 0.0182\n",
      "Epoch [12/25] Step [1725/2194] Loss: 0.0094\n",
      "Epoch [12/25] Step [1740/2194] Loss: 0.0913\n",
      "Epoch [12/25] Step [1755/2194] Loss: 0.0741\n",
      "Epoch [12/25] Step [1770/2194] Loss: 0.0125\n",
      "Epoch [12/25] Step [1785/2194] Loss: 0.0401\n",
      "Epoch [12/25] Step [1800/2194] Loss: 0.0607\n",
      "Epoch [12/25] Step [1815/2194] Loss: 0.0738\n",
      "Epoch [12/25] Step [1830/2194] Loss: 0.0455\n",
      "Epoch [12/25] Step [1845/2194] Loss: 0.0233\n",
      "Epoch [12/25] Step [1860/2194] Loss: 0.0593\n",
      "Epoch [12/25] Step [1875/2194] Loss: 0.0372\n",
      "Epoch [12/25] Step [1890/2194] Loss: 0.1067\n",
      "Epoch [12/25] Step [1905/2194] Loss: 0.1128\n",
      "Epoch [12/25] Step [1920/2194] Loss: 0.1770\n",
      "Epoch [12/25] Step [1935/2194] Loss: 0.0457\n",
      "Epoch [12/25] Step [1950/2194] Loss: 0.1213\n",
      "Epoch [12/25] Step [1965/2194] Loss: 0.0027\n",
      "Epoch [12/25] Step [1980/2194] Loss: 0.1277\n",
      "Epoch [12/25] Step [1995/2194] Loss: 0.1496\n",
      "Epoch [12/25] Step [2010/2194] Loss: 0.0047\n",
      "Epoch [12/25] Step [2025/2194] Loss: 0.1407\n",
      "Epoch [12/25] Step [2040/2194] Loss: 0.0430\n",
      "Epoch [12/25] Step [2055/2194] Loss: 0.1404\n",
      "Epoch [12/25] Step [2070/2194] Loss: 0.0271\n",
      "Epoch [12/25] Step [2085/2194] Loss: 0.0494\n",
      "Epoch [12/25] Step [2100/2194] Loss: 0.1559\n",
      "Epoch [12/25] Step [2115/2194] Loss: 0.0343\n",
      "Epoch [12/25] Step [2130/2194] Loss: 0.0474\n",
      "Epoch [12/25] Step [2145/2194] Loss: 0.0833\n",
      "Epoch [12/25] Step [2160/2194] Loss: 0.0169\n",
      "Epoch [12/25] Step [2175/2194] Loss: 0.0213\n",
      "Epoch [12/25] Step [2190/2194] Loss: 0.0306\n",
      "Epoch [12/25] completed in 2040.88s\n",
      "Train Accuracy: 0.9752, Validation Accuracy: 0.8964\n",
      "\n",
      "Epoch [13/25] Step [0/2194] Loss: 0.0202\n",
      "Epoch [13/25] Step [15/2194] Loss: 0.0084\n",
      "Epoch [13/25] Step [30/2194] Loss: 0.0161\n",
      "Epoch [13/25] Step [45/2194] Loss: 0.0252\n",
      "Epoch [13/25] Step [60/2194] Loss: 0.0200\n",
      "Epoch [13/25] Step [75/2194] Loss: 0.0245\n",
      "Epoch [13/25] Step [90/2194] Loss: 0.0652\n",
      "Epoch [13/25] Step [105/2194] Loss: 0.0530\n",
      "Epoch [13/25] Step [120/2194] Loss: 0.0129\n",
      "Epoch [13/25] Step [135/2194] Loss: 0.0367\n",
      "Epoch [13/25] Step [150/2194] Loss: 0.0327\n",
      "Epoch [13/25] Step [165/2194] Loss: 0.0076\n",
      "Epoch [13/25] Step [180/2194] Loss: 0.0582\n",
      "Epoch [13/25] Step [195/2194] Loss: 0.0097\n",
      "Epoch [13/25] Step [210/2194] Loss: 0.1722\n",
      "Epoch [13/25] Step [225/2194] Loss: 0.0120\n",
      "Epoch [13/25] Step [240/2194] Loss: 0.1395\n",
      "Epoch [13/25] Step [255/2194] Loss: 0.0034\n",
      "Epoch [13/25] Step [270/2194] Loss: 0.0215\n",
      "Epoch [13/25] Step [285/2194] Loss: 0.0374\n",
      "Epoch [13/25] Step [300/2194] Loss: 0.1337\n",
      "Epoch [13/25] Step [315/2194] Loss: 0.1320\n",
      "Epoch [13/25] Step [330/2194] Loss: 0.0041\n",
      "Epoch [13/25] Step [345/2194] Loss: 0.0219\n",
      "Epoch [13/25] Step [360/2194] Loss: 0.0926\n",
      "Epoch [13/25] Step [375/2194] Loss: 0.0153\n",
      "Epoch [13/25] Step [390/2194] Loss: 0.0284\n",
      "Epoch [13/25] Step [405/2194] Loss: 0.0378\n",
      "Epoch [13/25] Step [420/2194] Loss: 0.0085\n",
      "Epoch [13/25] Step [435/2194] Loss: 0.1406\n",
      "Epoch [13/25] Step [450/2194] Loss: 0.2166\n",
      "Epoch [13/25] Step [465/2194] Loss: 0.0299\n",
      "Epoch [13/25] Step [480/2194] Loss: 0.0481\n",
      "Epoch [13/25] Step [495/2194] Loss: 0.0010\n",
      "Epoch [13/25] Step [510/2194] Loss: 0.0389\n",
      "Epoch [13/25] Step [525/2194] Loss: 0.0108\n",
      "Epoch [13/25] Step [540/2194] Loss: 0.0740\n",
      "Epoch [13/25] Step [555/2194] Loss: 0.1197\n",
      "Epoch [13/25] Step [570/2194] Loss: 0.0277\n",
      "Epoch [13/25] Step [585/2194] Loss: 0.1337\n",
      "Epoch [13/25] Step [600/2194] Loss: 0.0478\n",
      "Epoch [13/25] Step [615/2194] Loss: 0.0480\n",
      "Epoch [13/25] Step [630/2194] Loss: 0.0198\n",
      "Epoch [13/25] Step [645/2194] Loss: 0.0240\n",
      "Epoch [13/25] Step [660/2194] Loss: 0.0243\n",
      "Epoch [13/25] Step [675/2194] Loss: 0.0861\n",
      "Epoch [13/25] Step [690/2194] Loss: 0.0056\n",
      "Epoch [13/25] Step [705/2194] Loss: 0.0968\n",
      "Epoch [13/25] Step [720/2194] Loss: 0.0311\n",
      "Epoch [13/25] Step [735/2194] Loss: 0.0411\n",
      "Epoch [13/25] Step [750/2194] Loss: 0.0182\n",
      "Epoch [13/25] Step [765/2194] Loss: 0.0262\n",
      "Epoch [13/25] Step [780/2194] Loss: 0.0031\n",
      "Epoch [13/25] Step [795/2194] Loss: 0.0095\n",
      "Epoch [13/25] Step [810/2194] Loss: 0.0025\n",
      "Epoch [13/25] Step [825/2194] Loss: 0.0290\n",
      "Epoch [13/25] Step [840/2194] Loss: 0.0173\n",
      "Epoch [13/25] Step [855/2194] Loss: 0.0086\n",
      "Epoch [13/25] Step [870/2194] Loss: 0.0682\n",
      "Epoch [13/25] Step [885/2194] Loss: 0.0127\n",
      "Epoch [13/25] Step [900/2194] Loss: 0.0296\n",
      "Epoch [13/25] Step [915/2194] Loss: 0.0140\n",
      "Epoch [13/25] Step [930/2194] Loss: 0.0212\n",
      "Epoch [13/25] Step [945/2194] Loss: 0.0053\n",
      "Epoch [13/25] Step [960/2194] Loss: 0.4056\n",
      "Epoch [13/25] Step [975/2194] Loss: 0.0199\n",
      "Epoch [13/25] Step [990/2194] Loss: 0.0028\n",
      "Epoch [13/25] Step [1005/2194] Loss: 0.0044\n",
      "Epoch [13/25] Step [1020/2194] Loss: 0.0062\n",
      "Epoch [13/25] Step [1035/2194] Loss: 0.0314\n",
      "Epoch [13/25] Step [1050/2194] Loss: 0.0198\n",
      "Epoch [13/25] Step [1065/2194] Loss: 0.0797\n",
      "Epoch [13/25] Step [1080/2194] Loss: 0.1365\n",
      "Epoch [13/25] Step [1095/2194] Loss: 0.0902\n",
      "Epoch [13/25] Step [1110/2194] Loss: 0.0706\n",
      "Epoch [13/25] Step [1125/2194] Loss: 0.0379\n",
      "Epoch [13/25] Step [1140/2194] Loss: 0.0322\n",
      "Epoch [13/25] Step [1155/2194] Loss: 0.0141\n",
      "Epoch [13/25] Step [1170/2194] Loss: 0.1691\n",
      "Epoch [13/25] Step [1185/2194] Loss: 0.1121\n",
      "Epoch [13/25] Step [1200/2194] Loss: 0.0022\n",
      "Epoch [13/25] Step [1215/2194] Loss: 0.0689\n",
      "Epoch [13/25] Step [1230/2194] Loss: 0.0111\n",
      "Epoch [13/25] Step [1245/2194] Loss: 0.0338\n",
      "Epoch [13/25] Step [1260/2194] Loss: 0.0294\n",
      "Epoch [13/25] Step [1275/2194] Loss: 0.0041\n",
      "Epoch [13/25] Step [1290/2194] Loss: 0.0583\n",
      "Epoch [13/25] Step [1305/2194] Loss: 0.2282\n",
      "Epoch [13/25] Step [1320/2194] Loss: 0.2176\n",
      "Epoch [13/25] Step [1335/2194] Loss: 0.0225\n",
      "Epoch [13/25] Step [1350/2194] Loss: 0.1553\n",
      "Epoch [13/25] Step [1365/2194] Loss: 0.1943\n",
      "Epoch [13/25] Step [1380/2194] Loss: 0.0362\n",
      "Epoch [13/25] Step [1395/2194] Loss: 0.0610\n",
      "Epoch [13/25] Step [1410/2194] Loss: 0.0425\n",
      "Epoch [13/25] Step [1425/2194] Loss: 0.1387\n",
      "Epoch [13/25] Step [1440/2194] Loss: 0.0652\n",
      "Epoch [13/25] Step [1455/2194] Loss: 0.1187\n",
      "Epoch [13/25] Step [1470/2194] Loss: 0.1291\n",
      "Epoch [13/25] Step [1485/2194] Loss: 0.0042\n",
      "Epoch [13/25] Step [1500/2194] Loss: 0.0210\n",
      "Epoch [13/25] Step [1515/2194] Loss: 0.0073\n",
      "Epoch [13/25] Step [1530/2194] Loss: 0.0156\n",
      "Epoch [13/25] Step [1545/2194] Loss: 0.0121\n",
      "Epoch [13/25] Step [1560/2194] Loss: 0.0639\n",
      "Epoch [13/25] Step [1575/2194] Loss: 0.0371\n",
      "Epoch [13/25] Step [1590/2194] Loss: 0.0177\n",
      "Epoch [13/25] Step [1605/2194] Loss: 0.0111\n",
      "Epoch [13/25] Step [1620/2194] Loss: 0.0056\n",
      "Epoch [13/25] Step [1635/2194] Loss: 0.0409\n",
      "Epoch [13/25] Step [1650/2194] Loss: 0.0122\n",
      "Epoch [13/25] Step [1665/2194] Loss: 0.1967\n",
      "Epoch [13/25] Step [1680/2194] Loss: 0.2351\n",
      "Epoch [13/25] Step [1695/2194] Loss: 0.1102\n",
      "Epoch [13/25] Step [1710/2194] Loss: 0.0246\n",
      "Epoch [13/25] Step [1725/2194] Loss: 0.0226\n",
      "Epoch [13/25] Step [1740/2194] Loss: 0.0470\n",
      "Epoch [13/25] Step [1755/2194] Loss: 0.0141\n",
      "Epoch [13/25] Step [1770/2194] Loss: 0.0305\n",
      "Epoch [13/25] Step [1785/2194] Loss: 0.0060\n",
      "Epoch [13/25] Step [1800/2194] Loss: 0.0195\n",
      "Epoch [13/25] Step [1815/2194] Loss: 0.0449\n",
      "Epoch [13/25] Step [1830/2194] Loss: 0.0224\n",
      "Epoch [13/25] Step [1845/2194] Loss: 0.0311\n",
      "Epoch [13/25] Step [1860/2194] Loss: 0.1890\n",
      "Epoch [13/25] Step [1875/2194] Loss: 0.0375\n",
      "Epoch [13/25] Step [1890/2194] Loss: 0.0155\n",
      "Epoch [13/25] Step [1905/2194] Loss: 0.0099\n",
      "Epoch [13/25] Step [1920/2194] Loss: 0.0349\n",
      "Epoch [13/25] Step [1935/2194] Loss: 0.0032\n",
      "Epoch [13/25] Step [1950/2194] Loss: 0.1031\n",
      "Epoch [13/25] Step [1965/2194] Loss: 0.0318\n",
      "Epoch [13/25] Step [1980/2194] Loss: 0.0099\n",
      "Epoch [13/25] Step [1995/2194] Loss: 0.3256\n",
      "Epoch [13/25] Step [2010/2194] Loss: 0.0042\n",
      "Epoch [13/25] Step [2025/2194] Loss: 0.0109\n",
      "Epoch [13/25] Step [2040/2194] Loss: 0.0999\n",
      "Epoch [13/25] Step [2055/2194] Loss: 0.0458\n",
      "Epoch [13/25] Step [2070/2194] Loss: 0.0650\n",
      "Epoch [13/25] Step [2085/2194] Loss: 0.0337\n",
      "Epoch [13/25] Step [2100/2194] Loss: 0.0022\n",
      "Epoch [13/25] Step [2115/2194] Loss: 0.1109\n",
      "Epoch [13/25] Step [2130/2194] Loss: 0.0604\n",
      "Epoch [13/25] Step [2145/2194] Loss: 0.0231\n",
      "Epoch [13/25] Step [2160/2194] Loss: 0.0552\n",
      "Epoch [13/25] Step [2175/2194] Loss: 0.0254\n",
      "Epoch [13/25] Step [2190/2194] Loss: 0.0593\n",
      "Epoch [13/25] completed in 2040.66s\n",
      "Train Accuracy: 0.9785, Validation Accuracy: 0.8940\n",
      "\n",
      "Epoch [14/25] Step [0/2194] Loss: 0.1035\n",
      "Epoch [14/25] Step [15/2194] Loss: 0.0243\n",
      "Epoch [14/25] Step [30/2194] Loss: 0.0522\n",
      "Epoch [14/25] Step [45/2194] Loss: 0.0091\n",
      "Epoch [14/25] Step [60/2194] Loss: 0.1041\n",
      "Epoch [14/25] Step [75/2194] Loss: 0.0164\n",
      "Epoch [14/25] Step [90/2194] Loss: 0.0545\n",
      "Epoch [14/25] Step [105/2194] Loss: 0.0322\n",
      "Epoch [14/25] Step [120/2194] Loss: 0.0530\n",
      "Epoch [14/25] Step [135/2194] Loss: 0.3234\n",
      "Epoch [14/25] Step [150/2194] Loss: 0.0425\n",
      "Epoch [14/25] Step [165/2194] Loss: 0.0081\n",
      "Epoch [14/25] Step [180/2194] Loss: 0.0754\n",
      "Epoch [14/25] Step [195/2194] Loss: 0.0648\n",
      "Epoch [14/25] Step [210/2194] Loss: 0.0025\n",
      "Epoch [14/25] Step [225/2194] Loss: 0.0268\n",
      "Epoch [14/25] Step [240/2194] Loss: 0.0123\n",
      "Epoch [14/25] Step [255/2194] Loss: 0.0673\n",
      "Epoch [14/25] Step [270/2194] Loss: 0.1517\n",
      "Epoch [14/25] Step [285/2194] Loss: 0.0042\n",
      "Epoch [14/25] Step [300/2194] Loss: 0.0007\n",
      "Epoch [14/25] Step [315/2194] Loss: 0.0013\n",
      "Epoch [14/25] Step [330/2194] Loss: 0.0520\n",
      "Epoch [14/25] Step [345/2194] Loss: 0.0812\n",
      "Epoch [14/25] Step [360/2194] Loss: 0.0701\n",
      "Epoch [14/25] Step [375/2194] Loss: 0.0402\n",
      "Epoch [14/25] Step [390/2194] Loss: 0.0297\n",
      "Epoch [14/25] Step [405/2194] Loss: 0.0097\n",
      "Epoch [14/25] Step [420/2194] Loss: 0.0153\n",
      "Epoch [14/25] Step [435/2194] Loss: 0.0664\n",
      "Epoch [14/25] Step [450/2194] Loss: 0.0120\n",
      "Epoch [14/25] Step [465/2194] Loss: 0.1358\n",
      "Epoch [14/25] Step [480/2194] Loss: 0.0527\n",
      "Epoch [14/25] Step [495/2194] Loss: 0.1322\n",
      "Epoch [14/25] Step [510/2194] Loss: 0.0452\n",
      "Epoch [14/25] Step [525/2194] Loss: 0.0586\n",
      "Epoch [14/25] Step [540/2194] Loss: 0.0987\n",
      "Epoch [14/25] Step [555/2194] Loss: 0.0088\n",
      "Epoch [14/25] Step [570/2194] Loss: 0.0080\n",
      "Epoch [14/25] Step [585/2194] Loss: 0.0478\n",
      "Epoch [14/25] Step [600/2194] Loss: 0.0147\n",
      "Epoch [14/25] Step [615/2194] Loss: 0.0239\n",
      "Epoch [14/25] Step [630/2194] Loss: 0.1341\n",
      "Epoch [14/25] Step [645/2194] Loss: 0.0036\n",
      "Epoch [14/25] Step [660/2194] Loss: 0.0807\n",
      "Epoch [14/25] Step [675/2194] Loss: 0.0894\n",
      "Epoch [14/25] Step [690/2194] Loss: 0.1380\n",
      "Epoch [14/25] Step [705/2194] Loss: 0.2490\n",
      "Epoch [14/25] Step [720/2194] Loss: 0.0343\n",
      "Epoch [14/25] Step [735/2194] Loss: 0.0152\n",
      "Epoch [14/25] Step [750/2194] Loss: 0.0103\n",
      "Epoch [14/25] Step [765/2194] Loss: 0.0263\n",
      "Epoch [14/25] Step [780/2194] Loss: 0.1203\n",
      "Epoch [14/25] Step [795/2194] Loss: 0.0496\n",
      "Epoch [14/25] Step [810/2194] Loss: 0.0346\n",
      "Epoch [14/25] Step [825/2194] Loss: 0.0301\n",
      "Epoch [14/25] Step [840/2194] Loss: 0.0109\n",
      "Epoch [14/25] Step [855/2194] Loss: 0.1605\n",
      "Epoch [14/25] Step [870/2194] Loss: 0.0603\n",
      "Epoch [14/25] Step [885/2194] Loss: 0.0623\n",
      "Epoch [14/25] Step [900/2194] Loss: 0.0453\n",
      "Epoch [14/25] Step [915/2194] Loss: 0.1999\n",
      "Epoch [14/25] Step [930/2194] Loss: 0.0023\n",
      "Epoch [14/25] Step [945/2194] Loss: 0.1166\n",
      "Epoch [14/25] Step [960/2194] Loss: 0.0348\n",
      "Epoch [14/25] Step [975/2194] Loss: 0.0296\n",
      "Epoch [14/25] Step [990/2194] Loss: 0.0685\n",
      "Epoch [14/25] Step [1005/2194] Loss: 0.0310\n",
      "Epoch [14/25] Step [1020/2194] Loss: 0.0108\n",
      "Epoch [14/25] Step [1035/2194] Loss: 0.0183\n",
      "Epoch [14/25] Step [1050/2194] Loss: 0.0481\n",
      "Epoch [14/25] Step [1065/2194] Loss: 0.0218\n",
      "Epoch [14/25] Step [1080/2194] Loss: 0.1570\n",
      "Epoch [14/25] Step [1095/2194] Loss: 0.0091\n",
      "Epoch [14/25] Step [1110/2194] Loss: 0.1144\n",
      "Epoch [14/25] Step [1125/2194] Loss: 0.0329\n",
      "Epoch [14/25] Step [1140/2194] Loss: 0.0561\n",
      "Epoch [14/25] Step [1155/2194] Loss: 0.0211\n",
      "Epoch [14/25] Step [1170/2194] Loss: 0.0123\n",
      "Epoch [14/25] Step [1185/2194] Loss: 0.0209\n",
      "Epoch [14/25] Step [1200/2194] Loss: 0.0130\n",
      "Epoch [14/25] Step [1215/2194] Loss: 0.0628\n",
      "Epoch [14/25] Step [1230/2194] Loss: 0.1241\n",
      "Epoch [14/25] Step [1245/2194] Loss: 0.0689\n",
      "Epoch [14/25] Step [1260/2194] Loss: 0.0157\n",
      "Epoch [14/25] Step [1275/2194] Loss: 0.0106\n",
      "Epoch [14/25] Step [1290/2194] Loss: 0.0225\n",
      "Epoch [14/25] Step [1305/2194] Loss: 0.4584\n",
      "Epoch [14/25] Step [1320/2194] Loss: 0.0742\n",
      "Epoch [14/25] Step [1335/2194] Loss: 0.1426\n",
      "Epoch [14/25] Step [1350/2194] Loss: 0.0261\n",
      "Epoch [14/25] Step [1365/2194] Loss: 0.0298\n",
      "Epoch [14/25] Step [1380/2194] Loss: 0.0059\n",
      "Epoch [14/25] Step [1395/2194] Loss: 0.0714\n",
      "Epoch [14/25] Step [1410/2194] Loss: 0.0331\n",
      "Epoch [14/25] Step [1425/2194] Loss: 0.0302\n",
      "Epoch [14/25] Step [1440/2194] Loss: 0.0175\n",
      "Epoch [14/25] Step [1455/2194] Loss: 0.0031\n",
      "Epoch [14/25] Step [1470/2194] Loss: 0.1783\n",
      "Epoch [14/25] Step [1485/2194] Loss: 0.0039\n",
      "Epoch [14/25] Step [1500/2194] Loss: 0.0029\n",
      "Epoch [14/25] Step [1515/2194] Loss: 0.0037\n",
      "Epoch [14/25] Step [1530/2194] Loss: 0.0191\n",
      "Epoch [14/25] Step [1545/2194] Loss: 0.1535\n",
      "Epoch [14/25] Step [1560/2194] Loss: 0.0079\n",
      "Epoch [14/25] Step [1575/2194] Loss: 0.0488\n",
      "Epoch [14/25] Step [1590/2194] Loss: 0.0772\n",
      "Epoch [14/25] Step [1605/2194] Loss: 0.0406\n",
      "Epoch [14/25] Step [1620/2194] Loss: 0.1247\n",
      "Epoch [14/25] Step [1635/2194] Loss: 0.0320\n",
      "Epoch [14/25] Step [1650/2194] Loss: 0.0124\n",
      "Epoch [14/25] Step [1665/2194] Loss: 0.0221\n",
      "Epoch [14/25] Step [1680/2194] Loss: 0.0076\n",
      "Epoch [14/25] Step [1695/2194] Loss: 0.0334\n",
      "Epoch [14/25] Step [1710/2194] Loss: 0.1514\n",
      "Epoch [14/25] Step [1725/2194] Loss: 0.1803\n",
      "Epoch [14/25] Step [1740/2194] Loss: 0.0032\n",
      "Epoch [14/25] Step [1755/2194] Loss: 0.0414\n",
      "Epoch [14/25] Step [1770/2194] Loss: 0.0316\n",
      "Epoch [14/25] Step [1785/2194] Loss: 0.0278\n",
      "Epoch [14/25] Step [1800/2194] Loss: 0.0453\n",
      "Epoch [14/25] Step [1815/2194] Loss: 0.0038\n",
      "Epoch [14/25] Step [1830/2194] Loss: 0.1856\n",
      "Epoch [14/25] Step [1845/2194] Loss: 0.2956\n",
      "Epoch [14/25] Step [1860/2194] Loss: 0.0653\n",
      "Epoch [14/25] Step [1875/2194] Loss: 0.0213\n",
      "Epoch [14/25] Step [1890/2194] Loss: 0.0488\n",
      "Epoch [14/25] Step [1905/2194] Loss: 0.0359\n",
      "Epoch [14/25] Step [1920/2194] Loss: 0.0061\n",
      "Epoch [14/25] Step [1935/2194] Loss: 0.0415\n",
      "Epoch [14/25] Step [1950/2194] Loss: 0.1732\n",
      "Epoch [14/25] Step [1965/2194] Loss: 0.0366\n",
      "Epoch [14/25] Step [1980/2194] Loss: 0.1054\n",
      "Epoch [14/25] Step [1995/2194] Loss: 0.0567\n",
      "Epoch [14/25] Step [2010/2194] Loss: 0.0109\n",
      "Epoch [14/25] Step [2025/2194] Loss: 0.0936\n",
      "Epoch [14/25] Step [2040/2194] Loss: 0.0090\n",
      "Epoch [14/25] Step [2055/2194] Loss: 0.0124\n",
      "Epoch [14/25] Step [2070/2194] Loss: 0.0491\n",
      "Epoch [14/25] Step [2085/2194] Loss: 0.0129\n",
      "Epoch [14/25] Step [2100/2194] Loss: 0.0116\n",
      "Epoch [14/25] Step [2115/2194] Loss: 0.0217\n",
      "Epoch [14/25] Step [2130/2194] Loss: 0.1175\n",
      "Epoch [14/25] Step [2145/2194] Loss: 0.0217\n",
      "Epoch [14/25] Step [2160/2194] Loss: 0.2770\n",
      "Epoch [14/25] Step [2175/2194] Loss: 0.0726\n",
      "Epoch [14/25] Step [2190/2194] Loss: 0.0597\n",
      "Epoch [14/25] completed in 2607.87s\n",
      "Train Accuracy: 0.9791, Validation Accuracy: 0.8877\n",
      "\n",
      "Epoch [15/25] Step [0/2194] Loss: 0.0202\n",
      "Epoch [15/25] Step [15/2194] Loss: 0.1772\n",
      "Epoch [15/25] Step [30/2194] Loss: 0.0117\n",
      "Epoch [15/25] Step [45/2194] Loss: 0.0023\n",
      "Epoch [15/25] Step [60/2194] Loss: 0.0248\n",
      "Epoch [15/25] Step [75/2194] Loss: 0.0707\n",
      "Epoch [15/25] Step [90/2194] Loss: 0.1575\n",
      "Epoch [15/25] Step [105/2194] Loss: 0.0040\n",
      "Epoch [15/25] Step [120/2194] Loss: 0.1505\n",
      "Epoch [15/25] Step [135/2194] Loss: 0.0081\n",
      "Epoch [15/25] Step [150/2194] Loss: 0.0086\n",
      "Epoch [15/25] Step [165/2194] Loss: 0.0710\n",
      "Epoch [15/25] Step [180/2194] Loss: 0.0214\n",
      "Epoch [15/25] Step [195/2194] Loss: 0.1509\n",
      "Epoch [15/25] Step [210/2194] Loss: 0.0091\n",
      "Epoch [15/25] Step [225/2194] Loss: 0.0329\n",
      "Epoch [15/25] Step [240/2194] Loss: 0.1586\n",
      "Epoch [15/25] Step [255/2194] Loss: 0.0261\n",
      "Epoch [15/25] Step [270/2194] Loss: 0.0465\n",
      "Epoch [15/25] Step [285/2194] Loss: 0.1320\n",
      "Epoch [15/25] Step [300/2194] Loss: 0.0151\n",
      "Epoch [15/25] Step [315/2194] Loss: 0.0051\n",
      "Epoch [15/25] Step [330/2194] Loss: 0.0999\n",
      "Epoch [15/25] Step [345/2194] Loss: 0.0266\n",
      "Epoch [15/25] Step [360/2194] Loss: 0.1043\n",
      "Epoch [15/25] Step [375/2194] Loss: 0.0662\n",
      "Epoch [15/25] Step [390/2194] Loss: 0.0100\n",
      "Epoch [15/25] Step [405/2194] Loss: 0.0143\n",
      "Epoch [15/25] Step [420/2194] Loss: 0.0015\n",
      "Epoch [15/25] Step [435/2194] Loss: 0.0091\n",
      "Epoch [15/25] Step [450/2194] Loss: 0.0076\n",
      "Epoch [15/25] Step [465/2194] Loss: 0.0167\n",
      "Epoch [15/25] Step [480/2194] Loss: 0.0209\n",
      "Epoch [15/25] Step [495/2194] Loss: 0.0185\n",
      "Epoch [15/25] Step [510/2194] Loss: 0.0054\n",
      "Epoch [15/25] Step [525/2194] Loss: 0.0087\n",
      "Epoch [15/25] Step [540/2194] Loss: 0.0747\n",
      "Epoch [15/25] Step [555/2194] Loss: 0.0883\n",
      "Epoch [15/25] Step [570/2194] Loss: 0.1117\n",
      "Epoch [15/25] Step [585/2194] Loss: 0.0387\n",
      "Epoch [15/25] Step [600/2194] Loss: 0.0715\n",
      "Epoch [15/25] Step [615/2194] Loss: 0.0198\n",
      "Epoch [15/25] Step [630/2194] Loss: 0.0020\n",
      "Epoch [15/25] Step [645/2194] Loss: 0.0345\n",
      "Epoch [15/25] Step [660/2194] Loss: 0.0117\n",
      "Epoch [15/25] Step [675/2194] Loss: 0.0036\n",
      "Epoch [15/25] Step [690/2194] Loss: 0.0073\n",
      "Epoch [15/25] Step [705/2194] Loss: 0.0122\n",
      "Epoch [15/25] Step [720/2194] Loss: 0.0737\n",
      "Epoch [15/25] Step [735/2194] Loss: 0.0265\n",
      "Epoch [15/25] Step [750/2194] Loss: 0.0263\n",
      "Epoch [15/25] Step [765/2194] Loss: 0.0831\n",
      "Epoch [15/25] Step [780/2194] Loss: 0.0520\n",
      "Epoch [15/25] Step [795/2194] Loss: 0.0020\n",
      "Epoch [15/25] Step [810/2194] Loss: 0.0298\n",
      "Epoch [15/25] Step [825/2194] Loss: 0.0552\n",
      "Epoch [15/25] Step [840/2194] Loss: 0.1020\n",
      "Epoch [15/25] Step [855/2194] Loss: 0.0215\n",
      "Epoch [15/25] Step [870/2194] Loss: 0.0349\n",
      "Epoch [15/25] Step [885/2194] Loss: 0.0020\n",
      "Epoch [15/25] Step [900/2194] Loss: 0.0318\n",
      "Epoch [15/25] Step [915/2194] Loss: 0.0256\n",
      "Epoch [15/25] Step [930/2194] Loss: 0.0441\n",
      "Epoch [15/25] Step [945/2194] Loss: 0.1098\n",
      "Epoch [15/25] Step [960/2194] Loss: 0.0203\n",
      "Epoch [15/25] Step [975/2194] Loss: 0.0537\n",
      "Epoch [15/25] Step [990/2194] Loss: 0.0800\n",
      "Epoch [15/25] Step [1005/2194] Loss: 0.0138\n",
      "Epoch [15/25] Step [1020/2194] Loss: 0.0596\n",
      "Epoch [15/25] Step [1035/2194] Loss: 0.0098\n",
      "Epoch [15/25] Step [1050/2194] Loss: 0.0018\n",
      "Epoch [15/25] Step [1065/2194] Loss: 0.0657\n",
      "Epoch [15/25] Step [1080/2194] Loss: 0.0706\n",
      "Epoch [15/25] Step [1095/2194] Loss: 0.0161\n",
      "Epoch [15/25] Step [1110/2194] Loss: 0.1185\n",
      "Epoch [15/25] Step [1125/2194] Loss: 0.0121\n",
      "Epoch [15/25] Step [1140/2194] Loss: 0.1337\n",
      "Epoch [15/25] Step [1155/2194] Loss: 0.2075\n",
      "Epoch [15/25] Step [1170/2194] Loss: 0.0382\n",
      "Epoch [15/25] Step [1185/2194] Loss: 0.2808\n",
      "Epoch [15/25] Step [1200/2194] Loss: 0.0148\n",
      "Epoch [15/25] Step [1215/2194] Loss: 0.0541\n",
      "Epoch [15/25] Step [1230/2194] Loss: 0.0484\n",
      "Epoch [15/25] Step [1245/2194] Loss: 0.2056\n",
      "Epoch [15/25] Step [1260/2194] Loss: 0.0088\n",
      "Epoch [15/25] Step [1275/2194] Loss: 0.0088\n",
      "Epoch [15/25] Step [1290/2194] Loss: 0.0490\n",
      "Epoch [15/25] Step [1305/2194] Loss: 0.1241\n",
      "Epoch [15/25] Step [1320/2194] Loss: 0.0445\n",
      "Epoch [15/25] Step [1335/2194] Loss: 0.0165\n",
      "Epoch [15/25] Step [1350/2194] Loss: 0.0073\n",
      "Epoch [15/25] Step [1365/2194] Loss: 0.0365\n",
      "Epoch [15/25] Step [1380/2194] Loss: 0.0160\n",
      "Epoch [15/25] Step [1395/2194] Loss: 0.2461\n",
      "Epoch [15/25] Step [1410/2194] Loss: 0.0497\n",
      "Epoch [15/25] Step [1425/2194] Loss: 0.1071\n",
      "Epoch [15/25] Step [1440/2194] Loss: 0.0207\n",
      "Epoch [15/25] Step [1455/2194] Loss: 0.0459\n",
      "Epoch [15/25] Step [1470/2194] Loss: 0.0411\n",
      "Epoch [15/25] Step [1485/2194] Loss: 0.0824\n",
      "Epoch [15/25] Step [1500/2194] Loss: 0.0155\n",
      "Epoch [15/25] Step [1515/2194] Loss: 0.0285\n",
      "Epoch [15/25] Step [1530/2194] Loss: 0.0172\n",
      "Epoch [15/25] Step [1545/2194] Loss: 0.0123\n",
      "Epoch [15/25] Step [1560/2194] Loss: 0.0207\n",
      "Epoch [15/25] Step [1575/2194] Loss: 0.0022\n",
      "Epoch [15/25] Step [1590/2194] Loss: 0.0234\n",
      "Epoch [15/25] Step [1605/2194] Loss: 0.0075\n",
      "Epoch [15/25] Step [1620/2194] Loss: 0.0012\n",
      "Epoch [15/25] Step [1635/2194] Loss: 0.0516\n",
      "Epoch [15/25] Step [1650/2194] Loss: 0.0375\n",
      "Epoch [15/25] Step [1665/2194] Loss: 0.0254\n",
      "Epoch [15/25] Step [1680/2194] Loss: 0.0104\n",
      "Epoch [15/25] Step [1695/2194] Loss: 0.0148\n",
      "Epoch [15/25] Step [1710/2194] Loss: 0.0101\n",
      "Epoch [15/25] Step [1725/2194] Loss: 0.0307\n",
      "Epoch [15/25] Step [1740/2194] Loss: 0.0049\n",
      "Epoch [15/25] Step [1755/2194] Loss: 0.0017\n",
      "Epoch [15/25] Step [1770/2194] Loss: 0.0033\n",
      "Epoch [15/25] Step [1785/2194] Loss: 0.2539\n",
      "Epoch [15/25] Step [1800/2194] Loss: 0.0022\n",
      "Epoch [15/25] Step [1815/2194] Loss: 0.0092\n",
      "Epoch [15/25] Step [1830/2194] Loss: 0.0175\n",
      "Epoch [15/25] Step [1845/2194] Loss: 0.0287\n",
      "Epoch [15/25] Step [1860/2194] Loss: 0.0911\n",
      "Epoch [15/25] Step [1875/2194] Loss: 0.0870\n",
      "Epoch [15/25] Step [1890/2194] Loss: 0.4845\n",
      "Epoch [15/25] Step [1905/2194] Loss: 0.1632\n",
      "Epoch [15/25] Step [1920/2194] Loss: 0.0630\n",
      "Epoch [15/25] Step [1935/2194] Loss: 0.0307\n",
      "Epoch [15/25] Step [1950/2194] Loss: 0.0494\n",
      "Epoch [15/25] Step [1965/2194] Loss: 0.0106\n",
      "Epoch [15/25] Step [1980/2194] Loss: 0.0136\n",
      "Epoch [15/25] Step [1995/2194] Loss: 0.1300\n",
      "Epoch [15/25] Step [2010/2194] Loss: 0.0551\n",
      "Epoch [15/25] Step [2025/2194] Loss: 0.0177\n",
      "Epoch [15/25] Step [2040/2194] Loss: 0.0294\n",
      "Epoch [15/25] Step [2055/2194] Loss: 0.0438\n",
      "Epoch [15/25] Step [2070/2194] Loss: 0.0117\n",
      "Epoch [15/25] Step [2085/2194] Loss: 0.0117\n",
      "Epoch [15/25] Step [2100/2194] Loss: 0.0188\n",
      "Epoch [15/25] Step [2115/2194] Loss: 0.0038\n",
      "Epoch [15/25] Step [2130/2194] Loss: 0.0219\n",
      "Epoch [15/25] Step [2145/2194] Loss: 0.1441\n",
      "Epoch [15/25] Step [2160/2194] Loss: 0.0601\n",
      "Epoch [15/25] Step [2175/2194] Loss: 0.1148\n",
      "Epoch [15/25] Step [2190/2194] Loss: 0.0277\n",
      "Epoch [15/25] completed in 2781.89s\n",
      "Train Accuracy: 0.9810, Validation Accuracy: 0.9009\n",
      "\n",
      "âœ… Best model updated and saved with Val Acc: 0.9009\n",
      "\n",
      "Epoch [16/25] Step [0/2194] Loss: 0.0073\n",
      "Epoch [16/25] Step [15/2194] Loss: 0.1726\n",
      "Epoch [16/25] Step [30/2194] Loss: 0.1031\n",
      "Epoch [16/25] Step [45/2194] Loss: 0.0228\n",
      "Epoch [16/25] Step [60/2194] Loss: 0.0346\n",
      "Epoch [16/25] Step [75/2194] Loss: 0.0176\n",
      "Epoch [16/25] Step [90/2194] Loss: 0.0137\n",
      "Epoch [16/25] Step [105/2194] Loss: 0.0250\n",
      "Epoch [16/25] Step [120/2194] Loss: 0.0512\n",
      "Epoch [16/25] Step [135/2194] Loss: 0.0117\n",
      "Epoch [16/25] Step [150/2194] Loss: 0.1045\n",
      "Epoch [16/25] Step [165/2194] Loss: 0.0082\n",
      "Epoch [16/25] Step [180/2194] Loss: 0.0076\n",
      "Epoch [16/25] Step [195/2194] Loss: 0.0691\n",
      "Epoch [16/25] Step [210/2194] Loss: 0.0167\n",
      "Epoch [16/25] Step [225/2194] Loss: 0.0384\n",
      "Epoch [16/25] Step [240/2194] Loss: 0.0214\n",
      "Epoch [16/25] Step [255/2194] Loss: 0.0337\n",
      "Epoch [16/25] Step [270/2194] Loss: 0.0342\n",
      "Epoch [16/25] Step [285/2194] Loss: 0.0045\n",
      "Epoch [16/25] Step [300/2194] Loss: 0.0053\n",
      "Epoch [16/25] Step [315/2194] Loss: 0.0060\n",
      "Epoch [16/25] Step [330/2194] Loss: 0.0151\n",
      "Epoch [16/25] Step [345/2194] Loss: 0.0096\n",
      "Epoch [16/25] Step [360/2194] Loss: 0.0103\n",
      "Epoch [16/25] Step [375/2194] Loss: 0.0688\n",
      "Epoch [16/25] Step [390/2194] Loss: 0.0221\n",
      "Epoch [16/25] Step [405/2194] Loss: 0.0152\n",
      "Epoch [16/25] Step [420/2194] Loss: 0.0235\n",
      "Epoch [16/25] Step [435/2194] Loss: 0.0115\n",
      "Epoch [16/25] Step [450/2194] Loss: 0.0099\n",
      "Epoch [16/25] Step [465/2194] Loss: 0.0024\n",
      "Epoch [16/25] Step [480/2194] Loss: 0.0034\n",
      "Epoch [16/25] Step [495/2194] Loss: 0.2630\n",
      "Epoch [16/25] Step [510/2194] Loss: 0.0033\n",
      "Epoch [16/25] Step [525/2194] Loss: 0.0242\n",
      "Epoch [16/25] Step [540/2194] Loss: 0.0751\n",
      "Epoch [16/25] Step [555/2194] Loss: 0.0112\n",
      "Epoch [16/25] Step [570/2194] Loss: 0.0257\n",
      "Epoch [16/25] Step [585/2194] Loss: 0.0015\n",
      "Epoch [16/25] Step [600/2194] Loss: 0.0230\n",
      "Epoch [16/25] Step [615/2194] Loss: 0.1036\n",
      "Epoch [16/25] Step [630/2194] Loss: 0.1130\n",
      "Epoch [16/25] Step [645/2194] Loss: 0.0053\n",
      "Epoch [16/25] Step [660/2194] Loss: 0.0015\n",
      "Epoch [16/25] Step [675/2194] Loss: 0.0062\n",
      "Epoch [16/25] Step [690/2194] Loss: 0.0278\n",
      "Epoch [16/25] Step [705/2194] Loss: 0.0685\n",
      "Epoch [16/25] Step [720/2194] Loss: 0.1407\n",
      "Epoch [16/25] Step [735/2194] Loss: 0.0947\n",
      "Epoch [16/25] Step [750/2194] Loss: 0.0143\n",
      "Epoch [16/25] Step [765/2194] Loss: 0.0571\n",
      "Epoch [16/25] Step [780/2194] Loss: 0.0614\n",
      "Epoch [16/25] Step [795/2194] Loss: 0.0467\n",
      "Epoch [16/25] Step [810/2194] Loss: 0.0035\n",
      "Epoch [16/25] Step [825/2194] Loss: 0.0073\n",
      "Epoch [16/25] Step [840/2194] Loss: 0.0029\n",
      "Epoch [16/25] Step [855/2194] Loss: 0.0084\n",
      "Epoch [16/25] Step [870/2194] Loss: 0.0828\n",
      "Epoch [16/25] Step [885/2194] Loss: 0.1522\n",
      "Epoch [16/25] Step [900/2194] Loss: 0.0018\n",
      "Epoch [16/25] Step [915/2194] Loss: 0.2744\n",
      "Epoch [16/25] Step [930/2194] Loss: 0.0053\n",
      "Epoch [16/25] Step [945/2194] Loss: 0.0819\n",
      "Epoch [16/25] Step [960/2194] Loss: 0.2012\n",
      "Epoch [16/25] Step [975/2194] Loss: 0.0095\n",
      "Epoch [16/25] Step [990/2194] Loss: 0.0167\n",
      "Epoch [16/25] Step [1005/2194] Loss: 0.1433\n",
      "Epoch [16/25] Step [1020/2194] Loss: 0.0027\n",
      "Epoch [16/25] Step [1035/2194] Loss: 0.0515\n",
      "Epoch [16/25] Step [1050/2194] Loss: 0.0013\n",
      "Epoch [16/25] Step [1065/2194] Loss: 0.0805\n",
      "Epoch [16/25] Step [1080/2194] Loss: 0.0264\n",
      "Epoch [16/25] Step [1095/2194] Loss: 0.0742\n",
      "Epoch [16/25] Step [1110/2194] Loss: 0.0316\n",
      "Epoch [16/25] Step [1125/2194] Loss: 0.0108\n",
      "Epoch [16/25] Step [1140/2194] Loss: 0.0105\n",
      "Epoch [16/25] Step [1155/2194] Loss: 0.1194\n",
      "Epoch [16/25] Step [1170/2194] Loss: 0.0132\n",
      "Epoch [16/25] Step [1185/2194] Loss: 0.0559\n",
      "Epoch [16/25] Step [1200/2194] Loss: 0.0076\n",
      "Epoch [16/25] Step [1215/2194] Loss: 0.0027\n",
      "Epoch [16/25] Step [1230/2194] Loss: 0.0113\n",
      "Epoch [16/25] Step [1245/2194] Loss: 0.0292\n",
      "Epoch [16/25] Step [1260/2194] Loss: 0.0706\n",
      "Epoch [16/25] Step [1275/2194] Loss: 0.0299\n",
      "Epoch [16/25] Step [1290/2194] Loss: 0.0069\n",
      "Epoch [16/25] Step [1305/2194] Loss: 0.0823\n",
      "Epoch [16/25] Step [1320/2194] Loss: 0.0114\n",
      "Epoch [16/25] Step [1335/2194] Loss: 0.0060\n",
      "Epoch [16/25] Step [1350/2194] Loss: 0.0154\n",
      "Epoch [16/25] Step [1365/2194] Loss: 0.0112\n",
      "Epoch [16/25] Step [1380/2194] Loss: 0.0468\n",
      "Epoch [16/25] Step [1395/2194] Loss: 0.0038\n",
      "Epoch [16/25] Step [1410/2194] Loss: 0.0976\n",
      "Epoch [16/25] Step [1425/2194] Loss: 0.0036\n",
      "Epoch [16/25] Step [1440/2194] Loss: 0.0031\n",
      "Epoch [16/25] Step [1455/2194] Loss: 0.0662\n",
      "Epoch [16/25] Step [1470/2194] Loss: 0.0152\n",
      "Epoch [16/25] Step [1485/2194] Loss: 0.0101\n",
      "Epoch [16/25] Step [1500/2194] Loss: 0.0470\n",
      "Epoch [16/25] Step [1515/2194] Loss: 0.1180\n",
      "Epoch [16/25] Step [1530/2194] Loss: 0.0064\n",
      "Epoch [16/25] Step [1545/2194] Loss: 0.0402\n",
      "Epoch [16/25] Step [1560/2194] Loss: 0.0474\n",
      "Epoch [16/25] Step [1575/2194] Loss: 0.0247\n",
      "Epoch [16/25] Step [1590/2194] Loss: 0.1128\n",
      "Epoch [16/25] Step [1605/2194] Loss: 0.1628\n",
      "Epoch [16/25] Step [1620/2194] Loss: 0.0039\n",
      "Epoch [16/25] Step [1635/2194] Loss: 0.1459\n",
      "Epoch [16/25] Step [1650/2194] Loss: 0.1003\n",
      "Epoch [16/25] Step [1665/2194] Loss: 0.0300\n",
      "Epoch [16/25] Step [1680/2194] Loss: 0.0250\n",
      "Epoch [16/25] Step [1695/2194] Loss: 0.0300\n",
      "Epoch [16/25] Step [1710/2194] Loss: 0.0029\n",
      "Epoch [16/25] Step [1725/2194] Loss: 0.0512\n",
      "Epoch [16/25] Step [1740/2194] Loss: 0.0150\n",
      "Epoch [16/25] Step [1755/2194] Loss: 0.0425\n",
      "Epoch [16/25] Step [1770/2194] Loss: 0.0054\n",
      "Epoch [16/25] Step [1785/2194] Loss: 0.0021\n",
      "Epoch [16/25] Step [1800/2194] Loss: 0.1790\n",
      "Epoch [16/25] Step [1815/2194] Loss: 0.0319\n",
      "Epoch [16/25] Step [1830/2194] Loss: 0.0015\n",
      "Epoch [16/25] Step [1845/2194] Loss: 0.0310\n",
      "Epoch [16/25] Step [1860/2194] Loss: 0.0062\n",
      "Epoch [16/25] Step [1875/2194] Loss: 0.0060\n",
      "Epoch [16/25] Step [1890/2194] Loss: 0.0125\n",
      "Epoch [16/25] Step [1905/2194] Loss: 0.0065\n",
      "Epoch [16/25] Step [1920/2194] Loss: 0.0064\n",
      "Epoch [16/25] Step [1935/2194] Loss: 0.0173\n",
      "Epoch [16/25] Step [1950/2194] Loss: 0.0744\n",
      "Epoch [16/25] Step [1965/2194] Loss: 0.0029\n",
      "Epoch [16/25] Step [1980/2194] Loss: 0.0257\n",
      "Epoch [16/25] Step [1995/2194] Loss: 0.0733\n",
      "Epoch [16/25] Step [2010/2194] Loss: 0.0328\n",
      "Epoch [16/25] Step [2025/2194] Loss: 0.0455\n",
      "Epoch [16/25] Step [2040/2194] Loss: 0.0408\n",
      "Epoch [16/25] Step [2055/2194] Loss: 0.0049\n",
      "Epoch [16/25] Step [2070/2194] Loss: 0.0062\n",
      "Epoch [16/25] Step [2085/2194] Loss: 0.0967\n",
      "Epoch [16/25] Step [2100/2194] Loss: 0.1129\n",
      "Epoch [16/25] Step [2115/2194] Loss: 0.0073\n",
      "Epoch [16/25] Step [2130/2194] Loss: 0.0260\n",
      "Epoch [16/25] Step [2145/2194] Loss: 0.0109\n",
      "Epoch [16/25] Step [2160/2194] Loss: 0.0180\n",
      "Epoch [16/25] Step [2175/2194] Loss: 0.2824\n",
      "Epoch [16/25] Step [2190/2194] Loss: 0.0039\n",
      "Epoch [16/25] completed in 2785.83s\n",
      "Train Accuracy: 0.9825, Validation Accuracy: 0.9024\n",
      "\n",
      "âœ… Best model updated and saved with Val Acc: 0.9024\n",
      "\n",
      "Epoch [17/25] Step [0/2194] Loss: 0.0169\n",
      "Epoch [17/25] Step [15/2194] Loss: 0.0357\n",
      "Epoch [17/25] Step [30/2194] Loss: 0.0534\n",
      "Epoch [17/25] Step [45/2194] Loss: 0.0076\n",
      "Epoch [17/25] Step [60/2194] Loss: 0.0224\n",
      "Epoch [17/25] Step [75/2194] Loss: 0.0915\n",
      "Epoch [17/25] Step [90/2194] Loss: 0.0621\n",
      "Epoch [17/25] Step [105/2194] Loss: 0.0015\n",
      "Epoch [17/25] Step [120/2194] Loss: 0.0373\n",
      "Epoch [17/25] Step [135/2194] Loss: 0.0416\n",
      "Epoch [17/25] Step [150/2194] Loss: 0.1966\n",
      "Epoch [17/25] Step [165/2194] Loss: 0.0119\n",
      "Epoch [17/25] Step [180/2194] Loss: 0.0052\n",
      "Epoch [17/25] Step [195/2194] Loss: 0.0065\n",
      "Epoch [17/25] Step [210/2194] Loss: 0.0131\n",
      "Epoch [17/25] Step [225/2194] Loss: 0.0183\n",
      "Epoch [17/25] Step [240/2194] Loss: 0.0023\n",
      "Epoch [17/25] Step [255/2194] Loss: 0.1383\n",
      "Epoch [17/25] Step [270/2194] Loss: 0.0779\n",
      "Epoch [17/25] Step [285/2194] Loss: 0.0009\n",
      "Epoch [17/25] Step [300/2194] Loss: 0.0283\n",
      "Epoch [17/25] Step [315/2194] Loss: 0.0648\n",
      "Epoch [17/25] Step [330/2194] Loss: 0.0542\n",
      "Epoch [17/25] Step [345/2194] Loss: 0.1310\n",
      "Epoch [17/25] Step [360/2194] Loss: 0.0273\n",
      "Epoch [17/25] Step [375/2194] Loss: 0.0238\n",
      "Epoch [17/25] Step [390/2194] Loss: 0.0294\n",
      "Epoch [17/25] Step [405/2194] Loss: 0.0604\n",
      "Epoch [17/25] Step [420/2194] Loss: 0.0059\n",
      "Epoch [17/25] Step [435/2194] Loss: 0.0647\n",
      "Epoch [17/25] Step [450/2194] Loss: 0.0063\n",
      "Epoch [17/25] Step [465/2194] Loss: 0.0744\n",
      "Epoch [17/25] Step [480/2194] Loss: 0.0356\n",
      "Epoch [17/25] Step [495/2194] Loss: 0.0330\n",
      "Epoch [17/25] Step [510/2194] Loss: 0.0207\n",
      "Epoch [17/25] Step [525/2194] Loss: 0.0501\n",
      "Epoch [17/25] Step [540/2194] Loss: 0.0566\n",
      "Epoch [17/25] Step [555/2194] Loss: 0.0303\n",
      "Epoch [17/25] Step [570/2194] Loss: 0.0109\n",
      "Epoch [17/25] Step [585/2194] Loss: 0.0893\n",
      "Epoch [17/25] Step [600/2194] Loss: 0.0416\n",
      "Epoch [17/25] Step [615/2194] Loss: 0.0341\n",
      "Epoch [17/25] Step [630/2194] Loss: 0.0076\n",
      "Epoch [17/25] Step [645/2194] Loss: 0.0692\n",
      "Epoch [17/25] Step [660/2194] Loss: 0.0060\n",
      "Epoch [17/25] Step [675/2194] Loss: 0.0076\n",
      "Epoch [17/25] Step [690/2194] Loss: 0.0041\n",
      "Epoch [17/25] Step [705/2194] Loss: 0.0160\n",
      "Epoch [17/25] Step [720/2194] Loss: 0.0074\n",
      "Epoch [17/25] Step [735/2194] Loss: 0.0013\n",
      "Epoch [17/25] Step [750/2194] Loss: 0.0112\n",
      "Epoch [17/25] Step [765/2194] Loss: 0.0690\n",
      "Epoch [17/25] Step [780/2194] Loss: 0.0565\n",
      "Epoch [17/25] Step [795/2194] Loss: 0.0011\n",
      "Epoch [17/25] Step [810/2194] Loss: 0.1815\n",
      "Epoch [17/25] Step [825/2194] Loss: 0.0318\n",
      "Epoch [17/25] Step [840/2194] Loss: 0.0168\n",
      "Epoch [17/25] Step [855/2194] Loss: 0.0064\n",
      "Epoch [17/25] Step [870/2194] Loss: 0.0089\n",
      "Epoch [17/25] Step [885/2194] Loss: 0.0068\n",
      "Epoch [17/25] Step [900/2194] Loss: 0.0162\n",
      "Epoch [17/25] Step [915/2194] Loss: 0.0373\n",
      "Epoch [17/25] Step [930/2194] Loss: 0.0065\n",
      "Epoch [17/25] Step [945/2194] Loss: 0.0055\n",
      "Epoch [17/25] Step [960/2194] Loss: 0.0784\n",
      "Epoch [17/25] Step [975/2194] Loss: 0.0045\n",
      "Epoch [17/25] Step [990/2194] Loss: 0.0051\n",
      "Epoch [17/25] Step [1005/2194] Loss: 0.0012\n",
      "Epoch [17/25] Step [1020/2194] Loss: 0.0315\n",
      "Epoch [17/25] Step [1035/2194] Loss: 0.0225\n",
      "Epoch [17/25] Step [1050/2194] Loss: 0.0597\n",
      "Epoch [17/25] Step [1065/2194] Loss: 0.0423\n",
      "Epoch [17/25] Step [1080/2194] Loss: 0.0108\n",
      "Epoch [17/25] Step [1095/2194] Loss: 0.0051\n",
      "Epoch [17/25] Step [1110/2194] Loss: 0.0613\n",
      "Epoch [17/25] Step [1125/2194] Loss: 0.0112\n",
      "Epoch [17/25] Step [1140/2194] Loss: 0.0098\n",
      "Epoch [17/25] Step [1155/2194] Loss: 0.1037\n",
      "Epoch [17/25] Step [1170/2194] Loss: 0.1350\n",
      "Epoch [17/25] Step [1185/2194] Loss: 0.0079\n",
      "Epoch [17/25] Step [1200/2194] Loss: 0.0016\n",
      "Epoch [17/25] Step [1215/2194] Loss: 0.0652\n",
      "Epoch [17/25] Step [1230/2194] Loss: 0.0044\n",
      "Epoch [17/25] Step [1245/2194] Loss: 0.1196\n",
      "Epoch [17/25] Step [1260/2194] Loss: 0.1413\n",
      "Epoch [17/25] Step [1275/2194] Loss: 0.0040\n",
      "Epoch [17/25] Step [1290/2194] Loss: 0.0640\n",
      "Epoch [17/25] Step [1305/2194] Loss: 0.0832\n",
      "Epoch [17/25] Step [1320/2194] Loss: 0.0289\n",
      "Epoch [17/25] Step [1335/2194] Loss: 0.0356\n",
      "Epoch [17/25] Step [1350/2194] Loss: 0.0068\n",
      "Epoch [17/25] Step [1365/2194] Loss: 0.1150\n",
      "Epoch [17/25] Step [1380/2194] Loss: 0.0147\n",
      "Epoch [17/25] Step [1395/2194] Loss: 0.0632\n",
      "Epoch [17/25] Step [1410/2194] Loss: 0.1841\n",
      "Epoch [17/25] Step [1425/2194] Loss: 0.0184\n",
      "Epoch [17/25] Step [1440/2194] Loss: 0.0065\n",
      "Epoch [17/25] Step [1455/2194] Loss: 0.0663\n",
      "Epoch [17/25] Step [1470/2194] Loss: 0.0128\n",
      "Epoch [17/25] Step [1485/2194] Loss: 0.2490\n",
      "Epoch [17/25] Step [1500/2194] Loss: 0.2142\n",
      "Epoch [17/25] Step [1515/2194] Loss: 0.0298\n",
      "Epoch [17/25] Step [1530/2194] Loss: 0.0555\n",
      "Epoch [17/25] Step [1545/2194] Loss: 0.0477\n",
      "Epoch [17/25] Step [1560/2194] Loss: 0.0048\n",
      "Epoch [17/25] Step [1575/2194] Loss: 0.0827\n",
      "Epoch [17/25] Step [1590/2194] Loss: 0.0024\n",
      "Epoch [17/25] Step [1605/2194] Loss: 0.0246\n",
      "Epoch [17/25] Step [1620/2194] Loss: 0.0165\n",
      "Epoch [17/25] Step [1635/2194] Loss: 0.0004\n",
      "Epoch [17/25] Step [1650/2194] Loss: 0.0111\n",
      "Epoch [17/25] Step [1665/2194] Loss: 0.1131\n",
      "Epoch [17/25] Step [1680/2194] Loss: 0.1714\n",
      "Epoch [17/25] Step [1695/2194] Loss: 0.0027\n",
      "Epoch [17/25] Step [1710/2194] Loss: 0.0016\n",
      "Epoch [17/25] Step [1725/2194] Loss: 0.0478\n",
      "Epoch [17/25] Step [1740/2194] Loss: 0.2072\n",
      "Epoch [17/25] Step [1755/2194] Loss: 0.1556\n",
      "Epoch [17/25] Step [1770/2194] Loss: 0.0165\n",
      "Epoch [17/25] Step [1785/2194] Loss: 0.0103\n",
      "Epoch [17/25] Step [1800/2194] Loss: 0.0139\n",
      "Epoch [17/25] Step [1815/2194] Loss: 0.0627\n",
      "Epoch [17/25] Step [1830/2194] Loss: 0.0143\n",
      "Epoch [17/25] Step [1845/2194] Loss: 0.0501\n",
      "Epoch [17/25] Step [1860/2194] Loss: 0.0420\n",
      "Epoch [17/25] Step [1875/2194] Loss: 0.0049\n",
      "Epoch [17/25] Step [1890/2194] Loss: 0.0027\n",
      "Epoch [17/25] Step [1905/2194] Loss: 0.0061\n",
      "Epoch [17/25] Step [1920/2194] Loss: 0.0012\n",
      "Epoch [17/25] Step [1935/2194] Loss: 0.0400\n",
      "Epoch [17/25] Step [1950/2194] Loss: 0.1028\n",
      "Epoch [17/25] Step [1965/2194] Loss: 0.0194\n",
      "Epoch [17/25] Step [1980/2194] Loss: 0.0619\n",
      "Epoch [17/25] Step [1995/2194] Loss: 0.0472\n",
      "Epoch [17/25] Step [2010/2194] Loss: 0.0118\n",
      "Epoch [17/25] Step [2025/2194] Loss: 0.0169\n",
      "Epoch [17/25] Step [2040/2194] Loss: 0.0019\n",
      "Epoch [17/25] Step [2055/2194] Loss: 0.0067\n",
      "Epoch [17/25] Step [2070/2194] Loss: 0.0539\n",
      "Epoch [17/25] Step [2085/2194] Loss: 0.0599\n",
      "Epoch [17/25] Step [2100/2194] Loss: 0.1438\n",
      "Epoch [17/25] Step [2115/2194] Loss: 0.0159\n",
      "Epoch [17/25] Step [2130/2194] Loss: 0.0029\n",
      "Epoch [17/25] Step [2145/2194] Loss: 0.1106\n",
      "Epoch [17/25] Step [2160/2194] Loss: 0.0072\n",
      "Epoch [17/25] Step [2175/2194] Loss: 0.0210\n",
      "Epoch [17/25] Step [2190/2194] Loss: 0.0022\n",
      "Epoch [17/25] completed in 2782.93s\n",
      "Train Accuracy: 0.9830, Validation Accuracy: 0.9031\n",
      "\n",
      "âœ… Best model updated and saved with Val Acc: 0.9031\n",
      "\n",
      "Epoch [18/25] Step [0/2194] Loss: 0.0196\n",
      "Epoch [18/25] Step [15/2194] Loss: 0.0029\n",
      "Epoch [18/25] Step [30/2194] Loss: 0.0449\n",
      "Epoch [18/25] Step [45/2194] Loss: 0.0121\n",
      "Epoch [18/25] Step [60/2194] Loss: 0.0023\n",
      "Epoch [18/25] Step [75/2194] Loss: 0.0324\n",
      "Epoch [18/25] Step [90/2194] Loss: 0.0458\n",
      "Epoch [18/25] Step [105/2194] Loss: 0.0895\n",
      "Epoch [18/25] Step [120/2194] Loss: 0.0254\n",
      "Epoch [18/25] Step [135/2194] Loss: 0.1078\n",
      "Epoch [18/25] Step [150/2194] Loss: 0.0634\n",
      "Epoch [18/25] Step [165/2194] Loss: 0.1703\n",
      "Epoch [18/25] Step [180/2194] Loss: 0.0079\n",
      "Epoch [18/25] Step [195/2194] Loss: 0.0054\n",
      "Epoch [18/25] Step [210/2194] Loss: 0.1014\n",
      "Epoch [18/25] Step [225/2194] Loss: 0.0107\n",
      "Epoch [18/25] Step [240/2194] Loss: 0.0012\n",
      "Epoch [18/25] Step [255/2194] Loss: 0.0153\n",
      "Epoch [18/25] Step [270/2194] Loss: 0.0034\n",
      "Epoch [18/25] Step [285/2194] Loss: 0.0565\n",
      "Epoch [18/25] Step [300/2194] Loss: 0.0199\n",
      "Epoch [18/25] Step [315/2194] Loss: 0.0542\n",
      "Epoch [18/25] Step [330/2194] Loss: 0.0049\n",
      "Epoch [18/25] Step [345/2194] Loss: 0.0076\n",
      "Epoch [18/25] Step [360/2194] Loss: 0.0720\n",
      "Epoch [18/25] Step [375/2194] Loss: 0.0014\n",
      "Epoch [18/25] Step [390/2194] Loss: 0.0021\n",
      "Epoch [18/25] Step [405/2194] Loss: 0.0413\n",
      "Epoch [18/25] Step [420/2194] Loss: 0.0122\n",
      "Epoch [18/25] Step [435/2194] Loss: 0.0259\n",
      "Epoch [18/25] Step [450/2194] Loss: 0.0421\n",
      "Epoch [18/25] Step [465/2194] Loss: 0.0008\n",
      "Epoch [18/25] Step [480/2194] Loss: 0.0257\n",
      "Epoch [18/25] Step [495/2194] Loss: 0.0082\n",
      "Epoch [18/25] Step [510/2194] Loss: 0.0679\n",
      "Epoch [18/25] Step [525/2194] Loss: 0.0162\n",
      "Epoch [18/25] Step [540/2194] Loss: 0.0069\n",
      "Epoch [18/25] Step [555/2194] Loss: 0.0090\n",
      "Epoch [18/25] Step [570/2194] Loss: 0.0285\n",
      "Epoch [18/25] Step [585/2194] Loss: 0.0906\n",
      "Epoch [18/25] Step [600/2194] Loss: 0.0310\n",
      "Epoch [18/25] Step [615/2194] Loss: 0.0178\n",
      "Epoch [18/25] Step [630/2194] Loss: 0.0042\n",
      "Epoch [18/25] Step [645/2194] Loss: 0.0342\n",
      "Epoch [18/25] Step [660/2194] Loss: 0.0074\n",
      "Epoch [18/25] Step [675/2194] Loss: 0.0024\n",
      "Epoch [18/25] Step [690/2194] Loss: 0.0919\n",
      "Epoch [18/25] Step [705/2194] Loss: 0.0579\n",
      "Epoch [18/25] Step [720/2194] Loss: 0.0403\n",
      "Epoch [18/25] Step [735/2194] Loss: 0.0284\n",
      "Epoch [18/25] Step [750/2194] Loss: 0.0138\n",
      "Epoch [18/25] Step [765/2194] Loss: 0.0231\n",
      "Epoch [18/25] Step [780/2194] Loss: 0.0547\n",
      "Epoch [18/25] Step [795/2194] Loss: 0.0598\n",
      "Epoch [18/25] Step [810/2194] Loss: 0.0144\n",
      "Epoch [18/25] Step [825/2194] Loss: 0.1626\n",
      "Epoch [18/25] Step [840/2194] Loss: 0.0075\n",
      "Epoch [18/25] Step [855/2194] Loss: 0.0075\n",
      "Epoch [18/25] Step [870/2194] Loss: 0.0127\n",
      "Epoch [18/25] Step [885/2194] Loss: 0.0605\n",
      "Epoch [18/25] Step [900/2194] Loss: 0.0112\n",
      "Epoch [18/25] Step [915/2194] Loss: 0.0050\n",
      "Epoch [18/25] Step [930/2194] Loss: 0.0027\n",
      "Epoch [18/25] Step [945/2194] Loss: 0.0185\n",
      "Epoch [18/25] Step [960/2194] Loss: 0.0067\n",
      "Epoch [18/25] Step [975/2194] Loss: 0.1202\n",
      "Epoch [18/25] Step [990/2194] Loss: 0.0241\n",
      "Epoch [18/25] Step [1005/2194] Loss: 0.0061\n",
      "Epoch [18/25] Step [1020/2194] Loss: 0.0642\n",
      "Epoch [18/25] Step [1035/2194] Loss: 0.0097\n",
      "Epoch [18/25] Step [1050/2194] Loss: 0.0101\n",
      "Epoch [18/25] Step [1065/2194] Loss: 0.1701\n",
      "Epoch [18/25] Step [1080/2194] Loss: 0.0502\n",
      "Epoch [18/25] Step [1095/2194] Loss: 0.1890\n",
      "Epoch [18/25] Step [1110/2194] Loss: 0.0010\n",
      "Epoch [18/25] Step [1125/2194] Loss: 0.0070\n",
      "Epoch [18/25] Step [1140/2194] Loss: 0.0069\n",
      "Epoch [18/25] Step [1155/2194] Loss: 0.0805\n",
      "Epoch [18/25] Step [1170/2194] Loss: 0.0174\n",
      "Epoch [18/25] Step [1185/2194] Loss: 0.0472\n",
      "Epoch [18/25] Step [1200/2194] Loss: 0.0152\n",
      "Epoch [18/25] Step [1215/2194] Loss: 0.0244\n",
      "Epoch [18/25] Step [1230/2194] Loss: 0.1040\n",
      "Epoch [18/25] Step [1245/2194] Loss: 0.0721\n",
      "Epoch [18/25] Step [1260/2194] Loss: 0.0203\n",
      "Epoch [18/25] Step [1275/2194] Loss: 0.0060\n",
      "Epoch [18/25] Step [1290/2194] Loss: 0.0320\n",
      "Epoch [18/25] Step [1305/2194] Loss: 0.0022\n",
      "Epoch [18/25] Step [1320/2194] Loss: 0.0623\n",
      "Epoch [18/25] Step [1335/2194] Loss: 0.0144\n",
      "Epoch [18/25] Step [1350/2194] Loss: 0.1475\n",
      "Epoch [18/25] Step [1365/2194] Loss: 0.0421\n",
      "Epoch [18/25] Step [1380/2194] Loss: 0.0401\n",
      "Epoch [18/25] Step [1395/2194] Loss: 0.0436\n",
      "Epoch [18/25] Step [1410/2194] Loss: 0.0438\n",
      "Epoch [18/25] Step [1425/2194] Loss: 0.0799\n",
      "Epoch [18/25] Step [1440/2194] Loss: 0.0015\n",
      "Epoch [18/25] Step [1455/2194] Loss: 0.0472\n",
      "Epoch [18/25] Step [1470/2194] Loss: 0.0019\n",
      "Epoch [18/25] Step [1485/2194] Loss: 0.1150\n",
      "Epoch [18/25] Step [1500/2194] Loss: 0.0055\n",
      "Epoch [18/25] Step [1515/2194] Loss: 0.0198\n",
      "Epoch [18/25] Step [1530/2194] Loss: 0.0162\n",
      "Epoch [18/25] Step [1545/2194] Loss: 0.0825\n",
      "Epoch [18/25] Step [1560/2194] Loss: 0.0206\n",
      "Epoch [18/25] Step [1575/2194] Loss: 0.0026\n",
      "Epoch [18/25] Step [1590/2194] Loss: 0.0349\n",
      "Epoch [18/25] Step [1605/2194] Loss: 0.0035\n",
      "Epoch [18/25] Step [1620/2194] Loss: 0.1915\n",
      "Epoch [18/25] Step [1635/2194] Loss: 0.0381\n",
      "Epoch [18/25] Step [1650/2194] Loss: 0.0020\n",
      "Epoch [18/25] Step [1665/2194] Loss: 0.1265\n",
      "Epoch [18/25] Step [1680/2194] Loss: 0.0113\n",
      "Epoch [18/25] Step [1695/2194] Loss: 0.0047\n",
      "Epoch [18/25] Step [1710/2194] Loss: 0.0904\n",
      "Epoch [18/25] Step [1725/2194] Loss: 0.0097\n",
      "Epoch [18/25] Step [1740/2194] Loss: 0.0675\n",
      "Epoch [18/25] Step [1755/2194] Loss: 0.0048\n",
      "Epoch [18/25] Step [1770/2194] Loss: 0.0083\n",
      "Epoch [18/25] Step [1785/2194] Loss: 0.0350\n",
      "Epoch [18/25] Step [1800/2194] Loss: 0.0247\n",
      "Epoch [18/25] Step [1815/2194] Loss: 0.0300\n",
      "Epoch [18/25] Step [1830/2194] Loss: 0.0063\n",
      "Epoch [18/25] Step [1845/2194] Loss: 0.0069\n",
      "Epoch [18/25] Step [1860/2194] Loss: 0.0534\n",
      "Epoch [18/25] Step [1875/2194] Loss: 0.0175\n",
      "Epoch [18/25] Step [1890/2194] Loss: 0.0802\n",
      "Epoch [18/25] Step [1905/2194] Loss: 0.0794\n",
      "Epoch [18/25] Step [1920/2194] Loss: 0.0247\n",
      "Epoch [18/25] Step [1935/2194] Loss: 0.0060\n",
      "Epoch [18/25] Step [1950/2194] Loss: 0.0035\n",
      "Epoch [18/25] Step [1965/2194] Loss: 0.0056\n",
      "Epoch [18/25] Step [1980/2194] Loss: 0.0573\n",
      "Epoch [18/25] Step [1995/2194] Loss: 0.0020\n",
      "Epoch [18/25] Step [2010/2194] Loss: 0.0015\n",
      "Epoch [18/25] Step [2025/2194] Loss: 0.0293\n",
      "Epoch [18/25] Step [2040/2194] Loss: 0.0505\n",
      "Epoch [18/25] Step [2055/2194] Loss: 0.0369\n",
      "Epoch [18/25] Step [2070/2194] Loss: 0.0288\n",
      "Epoch [18/25] Step [2085/2194] Loss: 0.0006\n",
      "Epoch [18/25] Step [2100/2194] Loss: 0.3807\n",
      "Epoch [18/25] Step [2115/2194] Loss: 0.0561\n",
      "Epoch [18/25] Step [2130/2194] Loss: 0.0682\n",
      "Epoch [18/25] Step [2145/2194] Loss: 0.0720\n",
      "Epoch [18/25] Step [2160/2194] Loss: 0.0187\n",
      "Epoch [18/25] Step [2175/2194] Loss: 0.0115\n",
      "Epoch [18/25] Step [2190/2194] Loss: 0.0661\n",
      "Epoch [18/25] completed in 2781.23s\n",
      "Train Accuracy: 0.9843, Validation Accuracy: 0.9015\n",
      "\n",
      "Epoch [19/25] Step [0/2194] Loss: 0.0512\n",
      "Epoch [19/25] Step [15/2194] Loss: 0.0026\n",
      "Epoch [19/25] Step [30/2194] Loss: 0.0113\n",
      "Epoch [19/25] Step [45/2194] Loss: 0.0132\n",
      "Epoch [19/25] Step [60/2194] Loss: 0.0240\n",
      "Epoch [19/25] Step [75/2194] Loss: 0.0121\n",
      "Epoch [19/25] Step [90/2194] Loss: 0.0719\n",
      "Epoch [19/25] Step [105/2194] Loss: 0.2171\n",
      "Epoch [19/25] Step [120/2194] Loss: 0.0141\n",
      "Epoch [19/25] Step [135/2194] Loss: 0.0051\n",
      "Epoch [19/25] Step [150/2194] Loss: 0.0043\n",
      "Epoch [19/25] Step [165/2194] Loss: 0.0011\n",
      "Epoch [19/25] Step [180/2194] Loss: 0.0618\n",
      "Epoch [19/25] Step [195/2194] Loss: 0.1258\n",
      "Epoch [19/25] Step [210/2194] Loss: 0.2396\n",
      "Epoch [19/25] Step [225/2194] Loss: 0.0009\n",
      "Epoch [19/25] Step [240/2194] Loss: 0.0804\n",
      "Epoch [19/25] Step [255/2194] Loss: 0.0007\n",
      "Epoch [19/25] Step [270/2194] Loss: 0.0124\n",
      "Epoch [19/25] Step [285/2194] Loss: 0.0248\n",
      "Epoch [19/25] Step [300/2194] Loss: 0.1113\n",
      "Epoch [19/25] Step [315/2194] Loss: 0.1614\n",
      "Epoch [19/25] Step [330/2194] Loss: 0.0323\n",
      "Epoch [19/25] Step [345/2194] Loss: 0.1004\n",
      "Epoch [19/25] Step [360/2194] Loss: 0.0130\n",
      "Epoch [19/25] Step [375/2194] Loss: 0.0064\n",
      "Epoch [19/25] Step [390/2194] Loss: 0.1652\n",
      "Epoch [19/25] Step [405/2194] Loss: 0.0036\n",
      "Epoch [19/25] Step [420/2194] Loss: 0.0308\n",
      "Epoch [19/25] Step [435/2194] Loss: 0.0006\n",
      "Epoch [19/25] Step [450/2194] Loss: 0.1281\n",
      "Epoch [19/25] Step [465/2194] Loss: 0.0182\n",
      "Epoch [19/25] Step [480/2194] Loss: 0.0009\n",
      "Epoch [19/25] Step [495/2194] Loss: 0.0023\n",
      "Epoch [19/25] Step [510/2194] Loss: 0.0356\n",
      "Epoch [19/25] Step [525/2194] Loss: 0.0015\n",
      "Epoch [19/25] Step [540/2194] Loss: 0.0139\n",
      "Epoch [19/25] Step [555/2194] Loss: 0.1011\n",
      "Epoch [19/25] Step [570/2194] Loss: 0.0562\n",
      "Epoch [19/25] Step [585/2194] Loss: 0.0404\n",
      "Epoch [19/25] Step [600/2194] Loss: 0.0570\n",
      "Epoch [19/25] Step [615/2194] Loss: 0.0099\n",
      "Epoch [19/25] Step [630/2194] Loss: 0.0595\n",
      "Epoch [19/25] Step [645/2194] Loss: 0.0724\n",
      "Epoch [19/25] Step [660/2194] Loss: 0.0340\n",
      "Epoch [19/25] Step [675/2194] Loss: 0.0093\n",
      "Epoch [19/25] Step [690/2194] Loss: 0.0238\n",
      "Epoch [19/25] Step [705/2194] Loss: 0.1302\n",
      "Epoch [19/25] Step [720/2194] Loss: 0.0742\n",
      "Epoch [19/25] Step [735/2194] Loss: 0.0139\n",
      "Epoch [19/25] Step [750/2194] Loss: 0.0204\n",
      "Epoch [19/25] Step [765/2194] Loss: 0.1955\n",
      "Epoch [19/25] Step [780/2194] Loss: 0.0927\n",
      "Epoch [19/25] Step [795/2194] Loss: 0.0054\n",
      "Epoch [19/25] Step [810/2194] Loss: 0.0795\n",
      "Epoch [19/25] Step [825/2194] Loss: 0.0234\n",
      "Epoch [19/25] Step [840/2194] Loss: 0.0487\n",
      "Epoch [19/25] Step [855/2194] Loss: 0.0092\n",
      "Epoch [19/25] Step [870/2194] Loss: 0.1591\n",
      "Epoch [19/25] Step [885/2194] Loss: 0.0216\n",
      "Epoch [19/25] Step [900/2194] Loss: 0.0199\n",
      "Epoch [19/25] Step [915/2194] Loss: 0.1129\n",
      "Epoch [19/25] Step [930/2194] Loss: 0.0076\n",
      "Epoch [19/25] Step [945/2194] Loss: 0.0280\n",
      "Epoch [19/25] Step [960/2194] Loss: 0.0887\n",
      "Epoch [19/25] Step [975/2194] Loss: 0.0499\n",
      "Epoch [19/25] Step [990/2194] Loss: 0.0062\n",
      "Epoch [19/25] Step [1005/2194] Loss: 0.0039\n",
      "Epoch [19/25] Step [1020/2194] Loss: 0.0005\n",
      "Epoch [19/25] Step [1035/2194] Loss: 0.1044\n",
      "Epoch [19/25] Step [1050/2194] Loss: 0.0053\n",
      "Epoch [19/25] Step [1065/2194] Loss: 0.0358\n",
      "Epoch [19/25] Step [1080/2194] Loss: 0.0180\n",
      "Epoch [19/25] Step [1095/2194] Loss: 0.0568\n",
      "Epoch [19/25] Step [1110/2194] Loss: 0.1079\n",
      "Epoch [19/25] Step [1125/2194] Loss: 0.0372\n",
      "Epoch [19/25] Step [1140/2194] Loss: 0.0292\n",
      "Epoch [19/25] Step [1155/2194] Loss: 0.0141\n",
      "Epoch [19/25] Step [1170/2194] Loss: 0.0099\n",
      "Epoch [19/25] Step [1185/2194] Loss: 0.0055\n",
      "Epoch [19/25] Step [1200/2194] Loss: 0.0119\n",
      "Epoch [19/25] Step [1215/2194] Loss: 0.0433\n",
      "Epoch [19/25] Step [1230/2194] Loss: 0.0053\n",
      "Epoch [19/25] Step [1245/2194] Loss: 0.1160\n",
      "Epoch [19/25] Step [1260/2194] Loss: 0.0150\n",
      "Epoch [19/25] Step [1275/2194] Loss: 0.0185\n",
      "Epoch [19/25] Step [1290/2194] Loss: 0.0343\n",
      "Epoch [19/25] Step [1305/2194] Loss: 0.0050\n",
      "Epoch [19/25] Step [1320/2194] Loss: 0.0047\n",
      "Epoch [19/25] Step [1335/2194] Loss: 0.0070\n",
      "Epoch [19/25] Step [1350/2194] Loss: 0.0559\n",
      "Epoch [19/25] Step [1365/2194] Loss: 0.0055\n",
      "Epoch [19/25] Step [1380/2194] Loss: 0.0247\n",
      "Epoch [19/25] Step [1395/2194] Loss: 0.0102\n",
      "Epoch [19/25] Step [1410/2194] Loss: 0.0417\n",
      "Epoch [19/25] Step [1425/2194] Loss: 0.0336\n",
      "Epoch [19/25] Step [1440/2194] Loss: 0.0123\n",
      "Epoch [19/25] Step [1455/2194] Loss: 0.0125\n",
      "Epoch [19/25] Step [1470/2194] Loss: 0.0101\n",
      "Epoch [19/25] Step [1485/2194] Loss: 0.0625\n",
      "Epoch [19/25] Step [1500/2194] Loss: 0.0456\n",
      "Epoch [19/25] Step [1515/2194] Loss: 0.0004\n",
      "Epoch [19/25] Step [1530/2194] Loss: 0.0479\n",
      "Epoch [19/25] Step [1545/2194] Loss: 0.0009\n",
      "Epoch [19/25] Step [1560/2194] Loss: 0.0754\n",
      "Epoch [19/25] Step [1575/2194] Loss: 0.0402\n",
      "Epoch [19/25] Step [1590/2194] Loss: 0.0159\n",
      "Epoch [19/25] Step [1605/2194] Loss: 0.1206\n",
      "Epoch [19/25] Step [1620/2194] Loss: 0.0019\n",
      "Epoch [19/25] Step [1635/2194] Loss: 0.0017\n",
      "Epoch [19/25] Step [1650/2194] Loss: 0.2029\n",
      "Epoch [19/25] Step [1665/2194] Loss: 0.0139\n",
      "Epoch [19/25] Step [1680/2194] Loss: 0.0003\n",
      "Epoch [19/25] Step [1695/2194] Loss: 0.0311\n",
      "Epoch [19/25] Step [1710/2194] Loss: 0.0485\n",
      "Epoch [19/25] Step [1725/2194] Loss: 0.0030\n",
      "Epoch [19/25] Step [1740/2194] Loss: 0.0787\n",
      "Epoch [19/25] Step [1755/2194] Loss: 0.0155\n",
      "Epoch [19/25] Step [1770/2194] Loss: 0.0381\n",
      "Epoch [19/25] Step [1785/2194] Loss: 0.0047\n",
      "Epoch [19/25] Step [1800/2194] Loss: 0.0126\n",
      "Epoch [19/25] Step [1815/2194] Loss: 0.1422\n",
      "Epoch [19/25] Step [1830/2194] Loss: 0.0121\n",
      "Epoch [19/25] Step [1845/2194] Loss: 0.0023\n",
      "Epoch [19/25] Step [1860/2194] Loss: 0.0034\n",
      "Epoch [19/25] Step [1875/2194] Loss: 0.0272\n",
      "Epoch [19/25] Step [1890/2194] Loss: 0.0452\n",
      "Epoch [19/25] Step [1905/2194] Loss: 0.0951\n",
      "Epoch [19/25] Step [1920/2194] Loss: 0.1257\n",
      "Epoch [19/25] Step [1935/2194] Loss: 0.0103\n",
      "Epoch [19/25] Step [1950/2194] Loss: 0.0399\n",
      "Epoch [19/25] Step [1965/2194] Loss: 0.0649\n",
      "Epoch [19/25] Step [1980/2194] Loss: 0.0150\n",
      "Epoch [19/25] Step [1995/2194] Loss: 0.0011\n",
      "Epoch [19/25] Step [2010/2194] Loss: 0.0148\n",
      "Epoch [19/25] Step [2025/2194] Loss: 0.0684\n",
      "Epoch [19/25] Step [2040/2194] Loss: 0.1162\n",
      "Epoch [19/25] Step [2055/2194] Loss: 0.0189\n",
      "Epoch [19/25] Step [2070/2194] Loss: 0.0021\n",
      "Epoch [19/25] Step [2085/2194] Loss: 0.0387\n",
      "Epoch [19/25] Step [2100/2194] Loss: 0.0003\n",
      "Epoch [19/25] Step [2115/2194] Loss: 0.0052\n",
      "Epoch [19/25] Step [2130/2194] Loss: 0.0087\n",
      "Epoch [19/25] Step [2145/2194] Loss: 0.0009\n",
      "Epoch [19/25] Step [2160/2194] Loss: 0.0061\n",
      "Epoch [19/25] Step [2175/2194] Loss: 0.0364\n",
      "Epoch [19/25] Step [2190/2194] Loss: 0.0019\n",
      "Epoch [19/25] completed in 2780.66s\n",
      "Train Accuracy: 0.9854, Validation Accuracy: 0.8957\n",
      "\n",
      "Epoch [20/25] Step [0/2194] Loss: 0.0508\n",
      "Epoch [20/25] Step [15/2194] Loss: 0.0027\n",
      "Epoch [20/25] Step [30/2194] Loss: 0.2056\n",
      "Epoch [20/25] Step [45/2194] Loss: 0.0081\n",
      "Epoch [20/25] Step [60/2194] Loss: 0.0517\n",
      "Epoch [20/25] Step [75/2194] Loss: 0.0175\n",
      "Epoch [20/25] Step [90/2194] Loss: 0.0400\n",
      "Epoch [20/25] Step [105/2194] Loss: 0.0009\n",
      "Epoch [20/25] Step [120/2194] Loss: 0.1673\n",
      "Epoch [20/25] Step [135/2194] Loss: 0.0305\n",
      "Epoch [20/25] Step [150/2194] Loss: 0.0255\n",
      "Epoch [20/25] Step [165/2194] Loss: 0.0020\n",
      "Epoch [20/25] Step [180/2194] Loss: 0.0245\n",
      "Epoch [20/25] Step [195/2194] Loss: 0.0564\n",
      "Epoch [20/25] Step [210/2194] Loss: 0.0030\n",
      "Epoch [20/25] Step [225/2194] Loss: 0.0125\n",
      "Epoch [20/25] Step [240/2194] Loss: 0.0709\n",
      "Epoch [20/25] Step [255/2194] Loss: 0.0002\n",
      "Epoch [20/25] Step [270/2194] Loss: 0.0106\n",
      "Epoch [20/25] Step [285/2194] Loss: 0.0215\n",
      "Epoch [20/25] Step [300/2194] Loss: 0.1492\n",
      "Epoch [20/25] Step [315/2194] Loss: 0.4899\n",
      "Epoch [20/25] Step [330/2194] Loss: 0.0804\n",
      "Epoch [20/25] Step [345/2194] Loss: 0.0108\n",
      "Epoch [20/25] Step [360/2194] Loss: 0.0253\n",
      "Epoch [20/25] Step [375/2194] Loss: 0.0636\n",
      "Epoch [20/25] Step [390/2194] Loss: 0.0018\n",
      "Epoch [20/25] Step [405/2194] Loss: 0.0020\n",
      "Epoch [20/25] Step [420/2194] Loss: 0.0105\n",
      "Epoch [20/25] Step [435/2194] Loss: 0.0803\n",
      "Epoch [20/25] Step [450/2194] Loss: 0.0101\n",
      "Epoch [20/25] Step [465/2194] Loss: 0.0146\n",
      "Epoch [20/25] Step [480/2194] Loss: 0.0191\n",
      "Epoch [20/25] Step [495/2194] Loss: 0.0099\n",
      "Epoch [20/25] Step [510/2194] Loss: 0.0148\n",
      "Epoch [20/25] Step [525/2194] Loss: 0.0036\n",
      "Epoch [20/25] Step [540/2194] Loss: 0.0215\n",
      "Epoch [20/25] Step [555/2194] Loss: 0.0177\n",
      "Epoch [20/25] Step [570/2194] Loss: 0.0315\n",
      "Epoch [20/25] Step [585/2194] Loss: 0.0129\n",
      "Epoch [20/25] Step [600/2194] Loss: 0.0535\n",
      "Epoch [20/25] Step [615/2194] Loss: 0.0274\n",
      "Epoch [20/25] Step [630/2194] Loss: 0.1379\n",
      "Epoch [20/25] Step [645/2194] Loss: 0.1232\n",
      "Epoch [20/25] Step [660/2194] Loss: 0.0004\n",
      "Epoch [20/25] Step [675/2194] Loss: 0.0096\n",
      "Epoch [20/25] Step [690/2194] Loss: 0.0433\n",
      "Epoch [20/25] Step [705/2194] Loss: 0.0024\n",
      "Epoch [20/25] Step [720/2194] Loss: 0.0038\n",
      "Epoch [20/25] Step [735/2194] Loss: 0.0958\n",
      "Epoch [20/25] Step [750/2194] Loss: 0.0372\n",
      "Epoch [20/25] Step [765/2194] Loss: 0.0167\n",
      "Epoch [20/25] Step [780/2194] Loss: 0.0533\n",
      "Epoch [20/25] Step [795/2194] Loss: 0.0068\n",
      "Epoch [20/25] Step [810/2194] Loss: 0.0296\n",
      "Epoch [20/25] Step [825/2194] Loss: 0.1231\n",
      "Epoch [20/25] Step [840/2194] Loss: 0.0088\n",
      "Epoch [20/25] Step [855/2194] Loss: 0.0280\n",
      "Epoch [20/25] Step [870/2194] Loss: 0.0087\n",
      "Epoch [20/25] Step [885/2194] Loss: 0.0017\n",
      "Epoch [20/25] Step [900/2194] Loss: 0.0197\n",
      "Epoch [20/25] Step [915/2194] Loss: 0.0798\n",
      "Epoch [20/25] Step [930/2194] Loss: 0.0222\n",
      "Epoch [20/25] Step [945/2194] Loss: 0.0559\n",
      "Epoch [20/25] Step [960/2194] Loss: 0.0128\n",
      "Epoch [20/25] Step [975/2194] Loss: 0.1006\n",
      "Epoch [20/25] Step [990/2194] Loss: 0.0023\n",
      "Epoch [20/25] Step [1005/2194] Loss: 0.0086\n",
      "Epoch [20/25] Step [1020/2194] Loss: 0.0071\n",
      "Epoch [20/25] Step [1035/2194] Loss: 0.0016\n",
      "Epoch [20/25] Step [1050/2194] Loss: 0.0017\n",
      "Epoch [20/25] Step [1065/2194] Loss: 0.0262\n",
      "Epoch [20/25] Step [1080/2194] Loss: 0.0021\n",
      "Epoch [20/25] Step [1095/2194] Loss: 0.0568\n",
      "Epoch [20/25] Step [1110/2194] Loss: 0.0026\n",
      "Epoch [20/25] Step [1125/2194] Loss: 0.0372\n",
      "Epoch [20/25] Step [1140/2194] Loss: 0.0042\n",
      "Epoch [20/25] Step [1155/2194] Loss: 0.0171\n",
      "Epoch [20/25] Step [1170/2194] Loss: 0.0108\n",
      "Epoch [20/25] Step [1185/2194] Loss: 0.1077\n",
      "Epoch [20/25] Step [1200/2194] Loss: 0.0571\n",
      "Epoch [20/25] Step [1215/2194] Loss: 0.0124\n",
      "Epoch [20/25] Step [1230/2194] Loss: 0.0815\n",
      "Epoch [20/25] Step [1245/2194] Loss: 0.0158\n",
      "Epoch [20/25] Step [1260/2194] Loss: 0.0208\n",
      "Epoch [20/25] Step [1275/2194] Loss: 0.0075\n",
      "Epoch [20/25] Step [1290/2194] Loss: 0.0007\n",
      "Epoch [20/25] Step [1305/2194] Loss: 0.0051\n",
      "Epoch [20/25] Step [1320/2194] Loss: 0.0878\n",
      "Epoch [20/25] Step [1335/2194] Loss: 0.2177\n",
      "Epoch [20/25] Step [1350/2194] Loss: 0.0365\n",
      "Epoch [20/25] Step [1365/2194] Loss: 0.1119\n",
      "Epoch [20/25] Step [1380/2194] Loss: 0.0143\n",
      "Epoch [20/25] Step [1395/2194] Loss: 0.0285\n",
      "Epoch [20/25] Step [1410/2194] Loss: 0.0179\n",
      "Epoch [20/25] Step [1425/2194] Loss: 0.0023\n",
      "Epoch [20/25] Step [1440/2194] Loss: 0.0536\n",
      "Epoch [20/25] Step [1455/2194] Loss: 0.0561\n",
      "Epoch [20/25] Step [1470/2194] Loss: 0.0468\n",
      "Epoch [20/25] Step [1485/2194] Loss: 0.0478\n",
      "Epoch [20/25] Step [1500/2194] Loss: 0.0132\n",
      "Epoch [20/25] Step [1515/2194] Loss: 0.2052\n",
      "Epoch [20/25] Step [1530/2194] Loss: 0.0900\n",
      "Epoch [20/25] Step [1545/2194] Loss: 0.0051\n",
      "Epoch [20/25] Step [1560/2194] Loss: 0.0472\n",
      "Epoch [20/25] Step [1575/2194] Loss: 0.0096\n",
      "Epoch [20/25] Step [1590/2194] Loss: 0.0149\n",
      "Epoch [20/25] Step [1605/2194] Loss: 0.0022\n",
      "Epoch [20/25] Step [1620/2194] Loss: 0.0007\n",
      "Epoch [20/25] Step [1635/2194] Loss: 0.0051\n",
      "Epoch [20/25] Step [1650/2194] Loss: 0.0164\n",
      "Epoch [20/25] Step [1665/2194] Loss: 0.0019\n",
      "Epoch [20/25] Step [1680/2194] Loss: 0.0025\n",
      "Epoch [20/25] Step [1695/2194] Loss: 0.0006\n",
      "Epoch [20/25] Step [1710/2194] Loss: 0.0008\n",
      "Epoch [20/25] Step [1725/2194] Loss: 0.0705\n",
      "Epoch [20/25] Step [1740/2194] Loss: 0.1969\n",
      "Epoch [20/25] Step [1755/2194] Loss: 0.0203\n",
      "Epoch [20/25] Step [1770/2194] Loss: 0.0785\n",
      "Epoch [20/25] Step [1785/2194] Loss: 0.0762\n",
      "Epoch [20/25] Step [1800/2194] Loss: 0.0077\n",
      "Epoch [20/25] Step [1815/2194] Loss: 0.0184\n",
      "Epoch [20/25] Step [1830/2194] Loss: 0.0026\n",
      "Epoch [20/25] Step [1845/2194] Loss: 0.0361\n",
      "Epoch [20/25] Step [1860/2194] Loss: 0.0462\n",
      "Epoch [20/25] Step [1875/2194] Loss: 0.0102\n",
      "Epoch [20/25] Step [1890/2194] Loss: 0.0277\n",
      "Epoch [20/25] Step [1905/2194] Loss: 0.0516\n",
      "Epoch [20/25] Step [1920/2194] Loss: 0.1218\n",
      "Epoch [20/25] Step [1935/2194] Loss: 0.0028\n",
      "Epoch [20/25] Step [1950/2194] Loss: 0.0085\n",
      "Epoch [20/25] Step [1965/2194] Loss: 0.0039\n",
      "Epoch [20/25] Step [1980/2194] Loss: 0.0212\n",
      "Epoch [20/25] Step [1995/2194] Loss: 0.0563\n",
      "Epoch [20/25] Step [2010/2194] Loss: 0.0293\n",
      "Epoch [20/25] Step [2025/2194] Loss: 0.3413\n",
      "Epoch [20/25] Step [2040/2194] Loss: 0.0095\n",
      "Epoch [20/25] Step [2055/2194] Loss: 0.0159\n",
      "Epoch [20/25] Step [2070/2194] Loss: 0.0210\n",
      "Epoch [20/25] Step [2085/2194] Loss: 0.0169\n",
      "Epoch [20/25] Step [2100/2194] Loss: 0.0076\n",
      "Epoch [20/25] Step [2115/2194] Loss: 0.0012\n",
      "Epoch [20/25] Step [2130/2194] Loss: 0.0138\n",
      "Epoch [20/25] Step [2145/2194] Loss: 0.0019\n",
      "Epoch [20/25] Step [2160/2194] Loss: 0.1156\n",
      "Epoch [20/25] Step [2175/2194] Loss: 0.0032\n",
      "Epoch [20/25] Step [2190/2194] Loss: 0.0746\n",
      "Epoch [20/25] completed in 2781.13s\n",
      "Train Accuracy: 0.9864, Validation Accuracy: 0.9012\n",
      "\n",
      "Epoch [21/25] Step [0/2194] Loss: 0.0082\n",
      "Epoch [21/25] Step [15/2194] Loss: 0.0075\n",
      "Epoch [21/25] Step [30/2194] Loss: 0.0005\n",
      "Epoch [21/25] Step [45/2194] Loss: 0.0184\n",
      "Epoch [21/25] Step [60/2194] Loss: 0.0027\n",
      "Epoch [21/25] Step [75/2194] Loss: 0.0222\n",
      "Epoch [21/25] Step [90/2194] Loss: 0.0042\n",
      "Epoch [21/25] Step [105/2194] Loss: 0.0574\n",
      "Epoch [21/25] Step [120/2194] Loss: 0.3006\n",
      "Epoch [21/25] Step [135/2194] Loss: 0.0069\n",
      "Epoch [21/25] Step [150/2194] Loss: 0.0520\n",
      "Epoch [21/25] Step [165/2194] Loss: 0.0586\n",
      "Epoch [21/25] Step [180/2194] Loss: 0.2856\n",
      "Epoch [21/25] Step [195/2194] Loss: 0.0092\n",
      "Epoch [21/25] Step [210/2194] Loss: 0.0290\n",
      "Epoch [21/25] Step [225/2194] Loss: 0.0007\n",
      "Epoch [21/25] Step [240/2194] Loss: 0.0022\n",
      "Epoch [21/25] Step [255/2194] Loss: 0.0033\n",
      "Epoch [21/25] Step [270/2194] Loss: 0.0069\n",
      "Epoch [21/25] Step [285/2194] Loss: 0.0081\n",
      "Epoch [21/25] Step [300/2194] Loss: 0.0043\n",
      "Epoch [21/25] Step [315/2194] Loss: 0.0110\n",
      "Epoch [21/25] Step [330/2194] Loss: 0.0062\n",
      "Epoch [21/25] Step [345/2194] Loss: 0.0033\n",
      "Epoch [21/25] Step [360/2194] Loss: 0.0296\n",
      "Epoch [21/25] Step [375/2194] Loss: 0.0099\n",
      "Epoch [21/25] Step [390/2194] Loss: 0.0423\n",
      "Epoch [21/25] Step [405/2194] Loss: 0.0076\n",
      "Epoch [21/25] Step [420/2194] Loss: 0.1271\n",
      "Epoch [21/25] Step [435/2194] Loss: 0.0642\n",
      "Epoch [21/25] Step [450/2194] Loss: 0.1535\n",
      "Epoch [21/25] Step [465/2194] Loss: 0.0541\n",
      "Epoch [21/25] Step [480/2194] Loss: 0.0163\n",
      "Epoch [21/25] Step [495/2194] Loss: 0.0117\n",
      "Epoch [21/25] Step [510/2194] Loss: 0.0049\n",
      "Epoch [21/25] Step [525/2194] Loss: 0.0126\n",
      "Epoch [21/25] Step [540/2194] Loss: 0.0035\n",
      "Epoch [21/25] Step [555/2194] Loss: 0.0458\n",
      "Epoch [21/25] Step [570/2194] Loss: 0.0248\n",
      "Epoch [21/25] Step [585/2194] Loss: 0.1529\n",
      "Epoch [21/25] Step [600/2194] Loss: 0.0932\n",
      "Epoch [21/25] Step [615/2194] Loss: 0.0774\n",
      "Epoch [21/25] Step [630/2194] Loss: 0.0197\n",
      "Epoch [21/25] Step [645/2194] Loss: 0.0547\n",
      "Epoch [21/25] Step [660/2194] Loss: 0.0097\n",
      "Epoch [21/25] Step [675/2194] Loss: 0.0017\n",
      "Epoch [21/25] Step [690/2194] Loss: 0.0008\n",
      "Epoch [21/25] Step [705/2194] Loss: 0.0041\n",
      "Epoch [21/25] Step [720/2194] Loss: 0.0317\n",
      "Epoch [21/25] Step [735/2194] Loss: 0.2609\n",
      "Epoch [21/25] Step [750/2194] Loss: 0.0496\n",
      "Epoch [21/25] Step [765/2194] Loss: 0.0061\n",
      "Epoch [21/25] Step [780/2194] Loss: 0.0037\n",
      "Epoch [21/25] Step [795/2194] Loss: 0.0062\n",
      "Epoch [21/25] Step [810/2194] Loss: 0.0542\n",
      "Epoch [21/25] Step [825/2194] Loss: 0.0052\n",
      "Epoch [21/25] Step [840/2194] Loss: 0.0029\n",
      "Epoch [21/25] Step [855/2194] Loss: 0.0145\n",
      "Epoch [21/25] Step [870/2194] Loss: 0.1870\n",
      "Epoch [21/25] Step [885/2194] Loss: 0.0700\n",
      "Epoch [21/25] Step [900/2194] Loss: 0.0022\n",
      "Epoch [21/25] Step [915/2194] Loss: 0.0007\n",
      "Epoch [21/25] Step [930/2194] Loss: 0.0880\n",
      "Epoch [21/25] Step [945/2194] Loss: 0.0020\n",
      "Epoch [21/25] Step [960/2194] Loss: 0.1228\n",
      "Epoch [21/25] Step [975/2194] Loss: 0.0091\n",
      "Epoch [21/25] Step [990/2194] Loss: 0.1824\n",
      "Epoch [21/25] Step [1005/2194] Loss: 0.0081\n",
      "Epoch [21/25] Step [1020/2194] Loss: 0.0099\n",
      "Epoch [21/25] Step [1035/2194] Loss: 0.1090\n",
      "Epoch [21/25] Step [1050/2194] Loss: 0.1103\n",
      "Epoch [21/25] Step [1065/2194] Loss: 0.0016\n",
      "Epoch [21/25] Step [1080/2194] Loss: 0.0213\n",
      "Epoch [21/25] Step [1095/2194] Loss: 0.0012\n",
      "Epoch [21/25] Step [1110/2194] Loss: 0.0865\n",
      "Epoch [21/25] Step [1125/2194] Loss: 0.1061\n",
      "Epoch [21/25] Step [1140/2194] Loss: 0.0039\n",
      "Epoch [21/25] Step [1155/2194] Loss: 0.0857\n",
      "Epoch [21/25] Step [1170/2194] Loss: 0.0460\n",
      "Epoch [21/25] Step [1185/2194] Loss: 0.0042\n",
      "Epoch [21/25] Step [1200/2194] Loss: 0.0216\n",
      "Epoch [21/25] Step [1215/2194] Loss: 0.1497\n",
      "Epoch [21/25] Step [1230/2194] Loss: 0.0998\n",
      "Epoch [21/25] Step [1245/2194] Loss: 0.0044\n",
      "Epoch [21/25] Step [1260/2194] Loss: 0.0286\n",
      "Epoch [21/25] Step [1275/2194] Loss: 0.0239\n",
      "Epoch [21/25] Step [1290/2194] Loss: 0.0517\n",
      "Epoch [21/25] Step [1305/2194] Loss: 0.1357\n",
      "Epoch [21/25] Step [1320/2194] Loss: 0.0102\n",
      "Epoch [21/25] Step [1335/2194] Loss: 0.0090\n",
      "Epoch [21/25] Step [1350/2194] Loss: 0.0073\n",
      "Epoch [21/25] Step [1365/2194] Loss: 0.0005\n",
      "Epoch [21/25] Step [1380/2194] Loss: 0.1416\n",
      "Epoch [21/25] Step [1395/2194] Loss: 0.0336\n",
      "Epoch [21/25] Step [1410/2194] Loss: 0.0511\n",
      "Epoch [21/25] Step [1425/2194] Loss: 0.0151\n",
      "Epoch [21/25] Step [1440/2194] Loss: 0.0119\n",
      "Epoch [21/25] Step [1455/2194] Loss: 0.0014\n",
      "Epoch [21/25] Step [1470/2194] Loss: 0.0208\n",
      "Epoch [21/25] Step [1485/2194] Loss: 0.0136\n",
      "Epoch [21/25] Step [1500/2194] Loss: 0.2129\n",
      "Epoch [21/25] Step [1515/2194] Loss: 0.0020\n",
      "Epoch [21/25] Step [1530/2194] Loss: 0.0050\n",
      "Epoch [21/25] Step [1545/2194] Loss: 0.0131\n",
      "Epoch [21/25] Step [1560/2194] Loss: 0.1134\n",
      "Epoch [21/25] Step [1575/2194] Loss: 0.0456\n",
      "Epoch [21/25] Step [1590/2194] Loss: 0.1303\n",
      "Epoch [21/25] Step [1605/2194] Loss: 0.0106\n",
      "Epoch [21/25] Step [1620/2194] Loss: 0.0143\n",
      "Epoch [21/25] Step [1635/2194] Loss: 0.0069\n",
      "Epoch [21/25] Step [1650/2194] Loss: 0.0079\n",
      "Epoch [21/25] Step [1665/2194] Loss: 0.0161\n",
      "Epoch [21/25] Step [1680/2194] Loss: 0.0014\n",
      "Epoch [21/25] Step [1695/2194] Loss: 0.0352\n",
      "Epoch [21/25] Step [1710/2194] Loss: 0.0008\n",
      "Epoch [21/25] Step [1725/2194] Loss: 0.0007\n",
      "Epoch [21/25] Step [1740/2194] Loss: 0.1155\n",
      "Epoch [21/25] Step [1755/2194] Loss: 0.1407\n",
      "Epoch [21/25] Step [1770/2194] Loss: 0.0138\n",
      "Epoch [21/25] Step [1785/2194] Loss: 0.0929\n",
      "Epoch [21/25] Step [1800/2194] Loss: 0.0014\n",
      "Epoch [21/25] Step [1815/2194] Loss: 0.0126\n",
      "Epoch [21/25] Step [1830/2194] Loss: 0.0860\n",
      "Epoch [21/25] Step [1845/2194] Loss: 0.0089\n",
      "Epoch [21/25] Step [1860/2194] Loss: 0.1282\n",
      "Epoch [21/25] Step [1875/2194] Loss: 0.0025\n",
      "Epoch [21/25] Step [1890/2194] Loss: 0.0574\n",
      "Epoch [21/25] Step [1905/2194] Loss: 0.0637\n",
      "Epoch [21/25] Step [1920/2194] Loss: 0.0062\n",
      "Epoch [21/25] Step [1935/2194] Loss: 0.0901\n",
      "Epoch [21/25] Step [1950/2194] Loss: 0.0698\n",
      "Epoch [21/25] Step [1965/2194] Loss: 0.0090\n",
      "Epoch [21/25] Step [1980/2194] Loss: 0.0187\n",
      "Epoch [21/25] Step [1995/2194] Loss: 0.0097\n",
      "Epoch [21/25] Step [2010/2194] Loss: 0.0222\n",
      "Epoch [21/25] Step [2025/2194] Loss: 0.0323\n",
      "Epoch [21/25] Step [2040/2194] Loss: 0.0089\n",
      "Epoch [21/25] Step [2055/2194] Loss: 0.0574\n",
      "Epoch [21/25] Step [2070/2194] Loss: 0.0271\n",
      "Epoch [21/25] Step [2085/2194] Loss: 0.1551\n",
      "Epoch [21/25] Step [2100/2194] Loss: 0.0008\n",
      "Epoch [21/25] Step [2115/2194] Loss: 0.0045\n",
      "Epoch [21/25] Step [2130/2194] Loss: 0.0117\n",
      "Epoch [21/25] Step [2145/2194] Loss: 0.0094\n",
      "Epoch [21/25] Step [2160/2194] Loss: 0.0109\n",
      "Epoch [21/25] Step [2175/2194] Loss: 0.0271\n",
      "Epoch [21/25] Step [2190/2194] Loss: 0.2022\n",
      "Epoch [21/25] completed in 2780.50s\n",
      "Train Accuracy: 0.9864, Validation Accuracy: 0.9023\n",
      "\n",
      "Epoch [22/25] Step [0/2194] Loss: 0.0031\n",
      "Epoch [22/25] Step [15/2194] Loss: 0.0135\n",
      "Epoch [22/25] Step [30/2194] Loss: 0.0080\n",
      "Epoch [22/25] Step [45/2194] Loss: 0.0046\n",
      "Epoch [22/25] Step [60/2194] Loss: 0.0109\n",
      "Epoch [22/25] Step [75/2194] Loss: 0.1090\n",
      "Epoch [22/25] Step [90/2194] Loss: 0.0060\n",
      "Epoch [22/25] Step [105/2194] Loss: 0.0104\n",
      "Epoch [22/25] Step [120/2194] Loss: 0.0355\n",
      "Epoch [22/25] Step [135/2194] Loss: 0.0081\n",
      "Epoch [22/25] Step [150/2194] Loss: 0.0031\n",
      "Epoch [22/25] Step [165/2194] Loss: 0.0448\n",
      "Epoch [22/25] Step [180/2194] Loss: 0.0173\n",
      "Epoch [22/25] Step [195/2194] Loss: 0.0050\n",
      "Epoch [22/25] Step [210/2194] Loss: 0.0204\n",
      "Epoch [22/25] Step [225/2194] Loss: 0.0333\n",
      "Epoch [22/25] Step [240/2194] Loss: 0.0101\n",
      "Epoch [22/25] Step [255/2194] Loss: 0.0029\n",
      "Epoch [22/25] Step [270/2194] Loss: 0.0145\n",
      "Epoch [22/25] Step [285/2194] Loss: 0.0006\n",
      "Epoch [22/25] Step [300/2194] Loss: 0.0453\n",
      "Epoch [22/25] Step [315/2194] Loss: 0.2483\n",
      "Epoch [22/25] Step [330/2194] Loss: 0.0067\n",
      "Epoch [22/25] Step [345/2194] Loss: 0.1046\n",
      "Epoch [22/25] Step [360/2194] Loss: 0.0109\n",
      "Epoch [22/25] Step [375/2194] Loss: 0.0076\n",
      "Epoch [22/25] Step [390/2194] Loss: 0.0005\n",
      "Epoch [22/25] Step [405/2194] Loss: 0.0121\n",
      "Epoch [22/25] Step [420/2194] Loss: 0.0037\n",
      "Epoch [22/25] Step [435/2194] Loss: 0.0064\n",
      "Epoch [22/25] Step [450/2194] Loss: 0.0047\n",
      "Epoch [22/25] Step [465/2194] Loss: 0.0001\n",
      "Epoch [22/25] Step [480/2194] Loss: 0.1439\n",
      "Epoch [22/25] Step [495/2194] Loss: 0.0112\n",
      "Epoch [22/25] Step [510/2194] Loss: 0.0043\n",
      "Epoch [22/25] Step [525/2194] Loss: 0.0021\n",
      "Epoch [22/25] Step [540/2194] Loss: 0.0167\n",
      "Epoch [22/25] Step [555/2194] Loss: 0.0260\n",
      "Epoch [22/25] Step [570/2194] Loss: 0.0018\n",
      "Epoch [22/25] Step [585/2194] Loss: 0.0244\n",
      "Epoch [22/25] Step [600/2194] Loss: 0.0594\n",
      "Epoch [22/25] Step [615/2194] Loss: 0.0047\n",
      "Epoch [22/25] Step [630/2194] Loss: 0.0014\n",
      "Epoch [22/25] Step [645/2194] Loss: 0.0088\n",
      "Epoch [22/25] Step [660/2194] Loss: 0.0022\n",
      "Epoch [22/25] Step [675/2194] Loss: 0.0177\n",
      "Epoch [22/25] Step [690/2194] Loss: 0.1297\n",
      "Epoch [22/25] Step [705/2194] Loss: 0.0190\n",
      "Epoch [22/25] Step [720/2194] Loss: 0.0031\n",
      "Epoch [22/25] Step [735/2194] Loss: 0.1397\n",
      "Epoch [22/25] Step [750/2194] Loss: 0.0120\n",
      "Epoch [22/25] Step [765/2194] Loss: 0.0638\n",
      "Epoch [22/25] Step [780/2194] Loss: 0.0032\n",
      "Epoch [22/25] Step [795/2194] Loss: 0.1827\n",
      "Epoch [22/25] Step [810/2194] Loss: 0.0374\n",
      "Epoch [22/25] Step [825/2194] Loss: 0.0051\n",
      "Epoch [22/25] Step [840/2194] Loss: 0.0251\n",
      "Epoch [22/25] Step [855/2194] Loss: 0.1059\n",
      "Epoch [22/25] Step [870/2194] Loss: 0.0847\n",
      "Epoch [22/25] Step [885/2194] Loss: 0.0037\n",
      "Epoch [22/25] Step [900/2194] Loss: 0.1766\n",
      "Epoch [22/25] Step [915/2194] Loss: 0.0948\n",
      "Epoch [22/25] Step [930/2194] Loss: 0.1089\n",
      "Epoch [22/25] Step [945/2194] Loss: 0.0031\n",
      "Epoch [22/25] Step [960/2194] Loss: 0.0093\n",
      "Epoch [22/25] Step [975/2194] Loss: 0.0038\n",
      "Epoch [22/25] Step [990/2194] Loss: 0.0122\n",
      "Epoch [22/25] Step [1005/2194] Loss: 0.0021\n",
      "Epoch [22/25] Step [1020/2194] Loss: 0.1464\n",
      "Epoch [22/25] Step [1035/2194] Loss: 0.0841\n",
      "Epoch [22/25] Step [1050/2194] Loss: 0.0139\n",
      "Epoch [22/25] Step [1065/2194] Loss: 0.0102\n",
      "Epoch [22/25] Step [1080/2194] Loss: 0.0551\n",
      "Epoch [22/25] Step [1095/2194] Loss: 0.0574\n",
      "Epoch [22/25] Step [1110/2194] Loss: 0.0510\n",
      "Epoch [22/25] Step [1125/2194] Loss: 0.0092\n",
      "Epoch [22/25] Step [1140/2194] Loss: 0.0004\n",
      "Epoch [22/25] Step [1155/2194] Loss: 0.1019\n",
      "Epoch [22/25] Step [1170/2194] Loss: 0.1008\n",
      "Epoch [22/25] Step [1185/2194] Loss: 0.0014\n",
      "Epoch [22/25] Step [1200/2194] Loss: 0.0183\n",
      "Epoch [22/25] Step [1215/2194] Loss: 0.0137\n",
      "Epoch [22/25] Step [1230/2194] Loss: 0.0085\n",
      "Epoch [22/25] Step [1245/2194] Loss: 0.0035\n",
      "Epoch [22/25] Step [1260/2194] Loss: 0.0060\n",
      "Epoch [22/25] Step [1275/2194] Loss: 0.0559\n",
      "Epoch [22/25] Step [1290/2194] Loss: 0.0047\n",
      "Epoch [22/25] Step [1305/2194] Loss: 0.0495\n",
      "Epoch [22/25] Step [1320/2194] Loss: 0.0155\n",
      "Epoch [22/25] Step [1335/2194] Loss: 0.1456\n",
      "Epoch [22/25] Step [1350/2194] Loss: 0.0032\n",
      "Epoch [22/25] Step [1365/2194] Loss: 0.0039\n",
      "Epoch [22/25] Step [1380/2194] Loss: 0.0007\n",
      "Epoch [22/25] Step [1395/2194] Loss: 0.2538\n",
      "Epoch [22/25] Step [1410/2194] Loss: 0.0756\n",
      "Epoch [22/25] Step [1425/2194] Loss: 0.0106\n",
      "Epoch [22/25] Step [1440/2194] Loss: 0.1620\n",
      "Epoch [22/25] Step [1455/2194] Loss: 0.0008\n",
      "Epoch [22/25] Step [1470/2194] Loss: 0.0622\n",
      "Epoch [22/25] Step [1485/2194] Loss: 0.0019\n",
      "Epoch [22/25] Step [1500/2194] Loss: 0.0125\n",
      "Epoch [22/25] Step [1515/2194] Loss: 0.1143\n",
      "Epoch [22/25] Step [1530/2194] Loss: 0.1426\n",
      "Epoch [22/25] Step [1545/2194] Loss: 0.0286\n",
      "Epoch [22/25] Step [1560/2194] Loss: 0.0509\n",
      "Epoch [22/25] Step [1575/2194] Loss: 0.1000\n",
      "Epoch [22/25] Step [1590/2194] Loss: 0.0047\n",
      "Epoch [22/25] Step [1605/2194] Loss: 0.0095\n",
      "Epoch [22/25] Step [1620/2194] Loss: 0.0312\n",
      "Epoch [22/25] Step [1635/2194] Loss: 0.0513\n",
      "Epoch [22/25] Step [1650/2194] Loss: 0.0820\n",
      "Epoch [22/25] Step [1665/2194] Loss: 0.0183\n",
      "Epoch [22/25] Step [1680/2194] Loss: 0.0183\n",
      "Epoch [22/25] Step [1695/2194] Loss: 0.0128\n",
      "Epoch [22/25] Step [1710/2194] Loss: 0.0317\n",
      "Epoch [22/25] Step [1725/2194] Loss: 0.0043\n",
      "Epoch [22/25] Step [1740/2194] Loss: 0.1545\n",
      "Epoch [22/25] Step [1755/2194] Loss: 0.0278\n",
      "Epoch [22/25] Step [1770/2194] Loss: 0.0558\n",
      "Epoch [22/25] Step [1785/2194] Loss: 0.1897\n",
      "Epoch [22/25] Step [1800/2194] Loss: 0.0194\n",
      "Epoch [22/25] Step [1815/2194] Loss: 0.0004\n",
      "Epoch [22/25] Step [1830/2194] Loss: 0.0187\n",
      "Epoch [22/25] Step [1845/2194] Loss: 0.0030\n",
      "Epoch [22/25] Step [1860/2194] Loss: 0.1676\n",
      "Epoch [22/25] Step [1875/2194] Loss: 0.0016\n",
      "Epoch [22/25] Step [1890/2194] Loss: 0.0019\n",
      "Epoch [22/25] Step [1905/2194] Loss: 0.0169\n",
      "Epoch [22/25] Step [1920/2194] Loss: 0.0081\n",
      "Epoch [22/25] Step [1935/2194] Loss: 0.0092\n",
      "Epoch [22/25] Step [1950/2194] Loss: 0.0098\n",
      "Epoch [22/25] Step [1965/2194] Loss: 0.0094\n",
      "Epoch [22/25] Step [1980/2194] Loss: 0.0073\n",
      "Epoch [22/25] Step [1995/2194] Loss: 0.0157\n",
      "Epoch [22/25] Step [2010/2194] Loss: 0.1117\n",
      "Epoch [22/25] Step [2025/2194] Loss: 0.0273\n",
      "Epoch [22/25] Step [2040/2194] Loss: 0.0270\n",
      "Epoch [22/25] Step [2055/2194] Loss: 0.0065\n",
      "Epoch [22/25] Step [2070/2194] Loss: 0.0100\n",
      "Epoch [22/25] Step [2085/2194] Loss: 0.0053\n",
      "Epoch [22/25] Step [2100/2194] Loss: 0.0687\n",
      "Epoch [22/25] Step [2115/2194] Loss: 0.0553\n",
      "Epoch [22/25] Step [2130/2194] Loss: 0.0005\n",
      "Epoch [22/25] Step [2145/2194] Loss: 0.0835\n",
      "Epoch [22/25] Step [2160/2194] Loss: 0.0166\n",
      "Epoch [22/25] Step [2175/2194] Loss: 0.0541\n",
      "Epoch [22/25] Step [2190/2194] Loss: 0.0054\n",
      "Epoch [22/25] completed in 2780.27s\n",
      "Train Accuracy: 0.9874, Validation Accuracy: 0.9005\n",
      "\n",
      "Epoch [23/25] Step [0/2194] Loss: 0.0018\n",
      "Epoch [23/25] Step [15/2194] Loss: 0.0030\n",
      "Epoch [23/25] Step [30/2194] Loss: 0.0702\n",
      "Epoch [23/25] Step [45/2194] Loss: 0.0001\n",
      "Epoch [23/25] Step [60/2194] Loss: 0.0008\n",
      "Epoch [23/25] Step [75/2194] Loss: 0.0005\n",
      "Epoch [23/25] Step [90/2194] Loss: 0.0124\n",
      "Epoch [23/25] Step [105/2194] Loss: 0.0024\n",
      "Epoch [23/25] Step [120/2194] Loss: 0.0029\n",
      "Epoch [23/25] Step [135/2194] Loss: 0.0135\n",
      "Epoch [23/25] Step [150/2194] Loss: 0.1097\n",
      "Epoch [23/25] Step [165/2194] Loss: 0.0242\n",
      "Epoch [23/25] Step [180/2194] Loss: 0.0055\n",
      "Epoch [23/25] Step [195/2194] Loss: 0.0639\n",
      "Epoch [23/25] Step [210/2194] Loss: 0.0215\n",
      "Epoch [23/25] Step [225/2194] Loss: 0.2073\n",
      "Epoch [23/25] Step [240/2194] Loss: 0.0004\n",
      "Epoch [23/25] Step [255/2194] Loss: 0.0397\n",
      "Epoch [23/25] Step [270/2194] Loss: 0.0048\n",
      "Epoch [23/25] Step [285/2194] Loss: 0.0044\n",
      "Epoch [23/25] Step [300/2194] Loss: 0.0008\n",
      "Epoch [23/25] Step [315/2194] Loss: 0.0111\n",
      "Epoch [23/25] Step [330/2194] Loss: 0.1381\n",
      "Epoch [23/25] Step [345/2194] Loss: 0.0131\n",
      "Epoch [23/25] Step [360/2194] Loss: 0.0092\n",
      "Epoch [23/25] Step [375/2194] Loss: 0.0103\n",
      "Epoch [23/25] Step [390/2194] Loss: 0.0066\n",
      "Epoch [23/25] Step [405/2194] Loss: 0.0225\n",
      "Epoch [23/25] Step [420/2194] Loss: 0.1107\n",
      "Epoch [23/25] Step [435/2194] Loss: 0.0532\n",
      "Epoch [23/25] Step [450/2194] Loss: 0.0959\n",
      "Epoch [23/25] Step [465/2194] Loss: 0.1744\n",
      "Epoch [23/25] Step [480/2194] Loss: 0.0224\n",
      "Epoch [23/25] Step [495/2194] Loss: 0.0114\n",
      "Epoch [23/25] Step [510/2194] Loss: 0.0231\n",
      "Epoch [23/25] Step [525/2194] Loss: 0.0204\n",
      "Epoch [23/25] Step [540/2194] Loss: 0.0057\n",
      "Epoch [23/25] Step [555/2194] Loss: 0.0007\n",
      "Epoch [23/25] Step [570/2194] Loss: 0.0086\n",
      "Epoch [23/25] Step [585/2194] Loss: 0.0089\n",
      "Epoch [23/25] Step [600/2194] Loss: 0.0124\n",
      "Epoch [23/25] Step [615/2194] Loss: 0.0095\n",
      "Epoch [23/25] Step [630/2194] Loss: 0.0010\n",
      "Epoch [23/25] Step [645/2194] Loss: 0.0002\n",
      "Epoch [23/25] Step [660/2194] Loss: 0.0195\n",
      "Epoch [23/25] Step [675/2194] Loss: 0.1123\n",
      "Epoch [23/25] Step [690/2194] Loss: 0.0787\n",
      "Epoch [23/25] Step [705/2194] Loss: 0.1085\n",
      "Epoch [23/25] Step [720/2194] Loss: 0.0005\n",
      "Epoch [23/25] Step [735/2194] Loss: 0.0123\n",
      "Epoch [23/25] Step [750/2194] Loss: 0.0005\n",
      "Epoch [23/25] Step [765/2194] Loss: 0.0037\n",
      "Epoch [23/25] Step [780/2194] Loss: 0.0192\n",
      "Epoch [23/25] Step [795/2194] Loss: 0.0315\n",
      "Epoch [23/25] Step [810/2194] Loss: 0.0037\n",
      "Epoch [23/25] Step [825/2194] Loss: 0.0016\n",
      "Epoch [23/25] Step [840/2194] Loss: 0.0006\n",
      "Epoch [23/25] Step [855/2194] Loss: 0.0082\n",
      "Epoch [23/25] Step [870/2194] Loss: 0.0006\n",
      "Epoch [23/25] Step [885/2194] Loss: 0.0378\n",
      "Epoch [23/25] Step [900/2194] Loss: 0.0084\n",
      "Epoch [23/25] Step [915/2194] Loss: 0.0083\n",
      "Epoch [23/25] Step [930/2194] Loss: 0.0056\n",
      "Epoch [23/25] Step [945/2194] Loss: 0.0133\n",
      "Epoch [23/25] Step [960/2194] Loss: 0.0008\n",
      "Epoch [23/25] Step [975/2194] Loss: 0.0008\n",
      "Epoch [23/25] Step [990/2194] Loss: 0.0207\n",
      "Epoch [23/25] Step [1005/2194] Loss: 0.0379\n",
      "Epoch [23/25] Step [1020/2194] Loss: 0.0286\n",
      "Epoch [23/25] Step [1035/2194] Loss: 0.0808\n",
      "Epoch [23/25] Step [1050/2194] Loss: 0.0039\n",
      "Epoch [23/25] Step [1065/2194] Loss: 0.0233\n",
      "Epoch [23/25] Step [1080/2194] Loss: 0.0017\n",
      "Epoch [23/25] Step [1095/2194] Loss: 0.1428\n",
      "Epoch [23/25] Step [1110/2194] Loss: 0.1765\n",
      "Epoch [23/25] Step [1125/2194] Loss: 0.0032\n",
      "Epoch [23/25] Step [1140/2194] Loss: 0.0304\n",
      "Epoch [23/25] Step [1155/2194] Loss: 0.0004\n",
      "Epoch [23/25] Step [1170/2194] Loss: 0.0301\n",
      "Epoch [23/25] Step [1185/2194] Loss: 0.1256\n",
      "Epoch [23/25] Step [1200/2194] Loss: 0.0020\n",
      "Epoch [23/25] Step [1215/2194] Loss: 0.0605\n",
      "Epoch [23/25] Step [1230/2194] Loss: 0.0647\n",
      "Epoch [23/25] Step [1245/2194] Loss: 0.0179\n",
      "Epoch [23/25] Step [1260/2194] Loss: 0.0728\n",
      "Epoch [23/25] Step [1275/2194] Loss: 0.1056\n",
      "Epoch [23/25] Step [1290/2194] Loss: 0.0626\n",
      "Epoch [23/25] Step [1305/2194] Loss: 0.0053\n",
      "Epoch [23/25] Step [1320/2194] Loss: 0.0037\n",
      "Epoch [23/25] Step [1335/2194] Loss: 0.0038\n",
      "Epoch [23/25] Step [1350/2194] Loss: 0.0233\n",
      "Epoch [23/25] Step [1365/2194] Loss: 0.0057\n",
      "Epoch [23/25] Step [1380/2194] Loss: 0.0016\n",
      "Epoch [23/25] Step [1395/2194] Loss: 0.0059\n",
      "Epoch [23/25] Step [1410/2194] Loss: 0.0531\n",
      "Epoch [23/25] Step [1425/2194] Loss: 0.0401\n",
      "Epoch [23/25] Step [1440/2194] Loss: 0.0021\n",
      "Epoch [23/25] Step [1455/2194] Loss: 0.0617\n",
      "Epoch [23/25] Step [1470/2194] Loss: 0.0710\n",
      "Epoch [23/25] Step [1485/2194] Loss: 0.0033\n",
      "Epoch [23/25] Step [1500/2194] Loss: 0.0298\n",
      "Epoch [23/25] Step [1515/2194] Loss: 0.0452\n",
      "Epoch [23/25] Step [1530/2194] Loss: 0.0467\n",
      "Epoch [23/25] Step [1545/2194] Loss: 0.0261\n",
      "Epoch [23/25] Step [1560/2194] Loss: 0.0076\n",
      "Epoch [23/25] Step [1575/2194] Loss: 0.0258\n",
      "Epoch [23/25] Step [1590/2194] Loss: 0.0024\n",
      "Epoch [23/25] Step [1605/2194] Loss: 0.0023\n",
      "Epoch [23/25] Step [1620/2194] Loss: 0.0226\n",
      "Epoch [23/25] Step [1635/2194] Loss: 0.0545\n",
      "Epoch [23/25] Step [1650/2194] Loss: 0.0491\n",
      "Epoch [23/25] Step [1665/2194] Loss: 0.0155\n",
      "Epoch [23/25] Step [1680/2194] Loss: 0.0234\n",
      "Epoch [23/25] Step [1695/2194] Loss: 0.0040\n",
      "Epoch [23/25] Step [1710/2194] Loss: 0.0019\n",
      "Epoch [23/25] Step [1725/2194] Loss: 0.0091\n",
      "Epoch [23/25] Step [1740/2194] Loss: 0.0061\n",
      "Epoch [23/25] Step [1755/2194] Loss: 0.1991\n",
      "Epoch [23/25] Step [1770/2194] Loss: 0.0546\n",
      "Epoch [23/25] Step [1785/2194] Loss: 0.0751\n",
      "Epoch [23/25] Step [1800/2194] Loss: 0.1317\n",
      "Epoch [23/25] Step [1815/2194] Loss: 0.0089\n",
      "Epoch [23/25] Step [1830/2194] Loss: 0.0493\n",
      "Epoch [23/25] Step [1845/2194] Loss: 0.0351\n",
      "Epoch [23/25] Step [1860/2194] Loss: 0.0042\n",
      "Epoch [23/25] Step [1875/2194] Loss: 0.0170\n",
      "Epoch [23/25] Step [1890/2194] Loss: 0.0058\n",
      "Epoch [23/25] Step [1905/2194] Loss: 0.0231\n",
      "Epoch [23/25] Step [1920/2194] Loss: 0.0087\n",
      "Epoch [23/25] Step [1935/2194] Loss: 0.0386\n",
      "Epoch [23/25] Step [1950/2194] Loss: 0.1758\n",
      "Epoch [23/25] Step [1965/2194] Loss: 0.0147\n",
      "Epoch [23/25] Step [1980/2194] Loss: 0.0179\n",
      "Epoch [23/25] Step [1995/2194] Loss: 0.0059\n",
      "Epoch [23/25] Step [2010/2194] Loss: 0.0718\n",
      "Epoch [23/25] Step [2025/2194] Loss: 0.0205\n",
      "Epoch [23/25] Step [2040/2194] Loss: 0.0238\n",
      "Epoch [23/25] Step [2055/2194] Loss: 0.0046\n",
      "Epoch [23/25] Step [2070/2194] Loss: 0.0934\n",
      "Epoch [23/25] Step [2085/2194] Loss: 0.0035\n",
      "Epoch [23/25] Step [2100/2194] Loss: 0.0916\n",
      "Epoch [23/25] Step [2115/2194] Loss: 0.0101\n",
      "Epoch [23/25] Step [2130/2194] Loss: 0.0417\n",
      "Epoch [23/25] Step [2145/2194] Loss: 0.0670\n",
      "Epoch [23/25] Step [2160/2194] Loss: 0.0314\n",
      "Epoch [23/25] Step [2175/2194] Loss: 0.0150\n",
      "Epoch [23/25] Step [2190/2194] Loss: 0.0066\n",
      "Epoch [23/25] completed in 2782.37s\n",
      "Train Accuracy: 0.9877, Validation Accuracy: 0.9011\n",
      "\n",
      "Epoch [24/25] Step [0/2194] Loss: 0.0560\n",
      "Epoch [24/25] Step [15/2194] Loss: 0.0236\n",
      "Epoch [24/25] Step [30/2194] Loss: 0.0026\n",
      "Epoch [24/25] Step [45/2194] Loss: 0.0008\n",
      "Epoch [24/25] Step [60/2194] Loss: 0.0031\n",
      "Epoch [24/25] Step [75/2194] Loss: 0.0003\n",
      "Epoch [24/25] Step [90/2194] Loss: 0.0051\n",
      "Epoch [24/25] Step [105/2194] Loss: 0.0021\n",
      "Epoch [24/25] Step [120/2194] Loss: 0.0073\n",
      "Epoch [24/25] Step [135/2194] Loss: 0.0059\n",
      "Epoch [24/25] Step [150/2194] Loss: 0.0273\n",
      "Epoch [24/25] Step [165/2194] Loss: 0.0189\n",
      "Epoch [24/25] Step [180/2194] Loss: 0.0900\n",
      "Epoch [24/25] Step [195/2194] Loss: 0.0005\n",
      "Epoch [24/25] Step [210/2194] Loss: 0.1263\n",
      "Epoch [24/25] Step [225/2194] Loss: 0.0024\n",
      "Epoch [24/25] Step [240/2194] Loss: 0.0277\n",
      "Epoch [24/25] Step [255/2194] Loss: 0.0042\n",
      "Epoch [24/25] Step [270/2194] Loss: 0.0244\n",
      "Epoch [24/25] Step [285/2194] Loss: 0.0037\n",
      "Epoch [24/25] Step [300/2194] Loss: 0.0043\n",
      "Epoch [24/25] Step [315/2194] Loss: 0.0023\n",
      "Epoch [24/25] Step [330/2194] Loss: 0.0385\n",
      "Epoch [24/25] Step [345/2194] Loss: 0.0012\n",
      "Epoch [24/25] Step [360/2194] Loss: 0.0609\n",
      "Epoch [24/25] Step [375/2194] Loss: 0.0937\n",
      "Epoch [24/25] Step [390/2194] Loss: 0.0365\n",
      "Epoch [24/25] Step [405/2194] Loss: 0.0016\n",
      "Epoch [24/25] Step [420/2194] Loss: 0.0030\n",
      "Epoch [24/25] Step [435/2194] Loss: 0.1554\n",
      "Epoch [24/25] Step [450/2194] Loss: 0.0003\n",
      "Epoch [24/25] Step [465/2194] Loss: 0.0139\n",
      "Epoch [24/25] Step [480/2194] Loss: 0.0158\n",
      "Epoch [24/25] Step [495/2194] Loss: 0.0073\n",
      "Epoch [24/25] Step [510/2194] Loss: 0.0017\n",
      "Epoch [24/25] Step [525/2194] Loss: 0.0116\n",
      "Epoch [24/25] Step [540/2194] Loss: 0.0009\n",
      "Epoch [24/25] Step [555/2194] Loss: 0.0055\n",
      "Epoch [24/25] Step [570/2194] Loss: 0.0245\n",
      "Epoch [24/25] Step [585/2194] Loss: 0.0099\n",
      "Epoch [24/25] Step [600/2194] Loss: 0.0232\n",
      "Epoch [24/25] Step [615/2194] Loss: 0.0110\n",
      "Epoch [24/25] Step [630/2194] Loss: 0.0481\n",
      "Epoch [24/25] Step [645/2194] Loss: 0.0106\n",
      "Epoch [24/25] Step [660/2194] Loss: 0.0205\n",
      "Epoch [24/25] Step [675/2194] Loss: 0.0057\n",
      "Epoch [24/25] Step [690/2194] Loss: 0.0101\n",
      "Epoch [24/25] Step [705/2194] Loss: 0.0451\n",
      "Epoch [24/25] Step [720/2194] Loss: 0.0327\n",
      "Epoch [24/25] Step [735/2194] Loss: 0.0297\n",
      "Epoch [24/25] Step [750/2194] Loss: 0.0048\n",
      "Epoch [24/25] Step [765/2194] Loss: 0.0070\n",
      "Epoch [24/25] Step [780/2194] Loss: 0.0263\n",
      "Epoch [24/25] Step [795/2194] Loss: 0.0488\n",
      "Epoch [24/25] Step [810/2194] Loss: 0.0105\n",
      "Epoch [24/25] Step [825/2194] Loss: 0.0376\n",
      "Epoch [24/25] Step [840/2194] Loss: 0.0593\n",
      "Epoch [24/25] Step [855/2194] Loss: 0.0281\n",
      "Epoch [24/25] Step [870/2194] Loss: 0.0344\n",
      "Epoch [24/25] Step [885/2194] Loss: 0.0014\n",
      "Epoch [24/25] Step [900/2194] Loss: 0.0111\n",
      "Epoch [24/25] Step [915/2194] Loss: 0.0035\n",
      "Epoch [24/25] Step [930/2194] Loss: 0.0143\n",
      "Epoch [24/25] Step [945/2194] Loss: 0.0060\n",
      "Epoch [24/25] Step [960/2194] Loss: 0.0199\n",
      "Epoch [24/25] Step [975/2194] Loss: 0.0059\n",
      "Epoch [24/25] Step [990/2194] Loss: 0.0483\n",
      "Epoch [24/25] Step [1005/2194] Loss: 0.0615\n",
      "Epoch [24/25] Step [1020/2194] Loss: 0.0040\n",
      "Epoch [24/25] Step [1035/2194] Loss: 0.0156\n",
      "Epoch [24/25] Step [1050/2194] Loss: 0.0164\n",
      "Epoch [24/25] Step [1065/2194] Loss: 0.0411\n",
      "Epoch [24/25] Step [1080/2194] Loss: 0.0170\n",
      "Epoch [24/25] Step [1095/2194] Loss: 0.1150\n",
      "Epoch [24/25] Step [1110/2194] Loss: 0.0299\n",
      "Epoch [24/25] Step [1125/2194] Loss: 0.0088\n",
      "Epoch [24/25] Step [1140/2194] Loss: 0.0044\n",
      "Epoch [24/25] Step [1155/2194] Loss: 0.0341\n",
      "Epoch [24/25] Step [1170/2194] Loss: 0.0337\n",
      "Epoch [24/25] Step [1185/2194] Loss: 0.0052\n",
      "Epoch [24/25] Step [1200/2194] Loss: 0.0865\n",
      "Epoch [24/25] Step [1215/2194] Loss: 0.0229\n",
      "Epoch [24/25] Step [1230/2194] Loss: 0.0044\n",
      "Epoch [24/25] Step [1245/2194] Loss: 0.1359\n",
      "Epoch [24/25] Step [1260/2194] Loss: 0.0149\n",
      "Epoch [24/25] Step [1275/2194] Loss: 0.0496\n",
      "Epoch [24/25] Step [1290/2194] Loss: 0.0193\n",
      "Epoch [24/25] Step [1305/2194] Loss: 0.0125\n",
      "Epoch [24/25] Step [1320/2194] Loss: 0.0554\n",
      "Epoch [24/25] Step [1335/2194] Loss: 0.2825\n",
      "Epoch [24/25] Step [1350/2194] Loss: 0.0104\n",
      "Epoch [24/25] Step [1365/2194] Loss: 0.0023\n",
      "Epoch [24/25] Step [1380/2194] Loss: 0.2105\n",
      "Epoch [24/25] Step [1395/2194] Loss: 0.0025\n",
      "Epoch [24/25] Step [1410/2194] Loss: 0.0034\n",
      "Epoch [24/25] Step [1425/2194] Loss: 0.0291\n",
      "Epoch [24/25] Step [1440/2194] Loss: 0.0136\n",
      "Epoch [24/25] Step [1455/2194] Loss: 0.0651\n",
      "Epoch [24/25] Step [1470/2194] Loss: 0.0123\n",
      "Epoch [24/25] Step [1485/2194] Loss: 0.0390\n",
      "Epoch [24/25] Step [1500/2194] Loss: 0.0214\n",
      "Epoch [24/25] Step [1515/2194] Loss: 0.0039\n",
      "Epoch [24/25] Step [1530/2194] Loss: 0.0202\n",
      "Epoch [24/25] Step [1545/2194] Loss: 0.0031\n",
      "Epoch [24/25] Step [1560/2194] Loss: 0.0869\n",
      "Epoch [24/25] Step [1575/2194] Loss: 0.0456\n",
      "Epoch [24/25] Step [1590/2194] Loss: 0.0076\n",
      "Epoch [24/25] Step [1605/2194] Loss: 0.0003\n",
      "Epoch [24/25] Step [1620/2194] Loss: 0.0027\n",
      "Epoch [24/25] Step [1635/2194] Loss: 0.0043\n",
      "Epoch [24/25] Step [1650/2194] Loss: 0.0036\n",
      "Epoch [24/25] Step [1665/2194] Loss: 0.0112\n",
      "Epoch [24/25] Step [1680/2194] Loss: 0.0723\n",
      "Epoch [24/25] Step [1695/2194] Loss: 0.0319\n",
      "Epoch [24/25] Step [1710/2194] Loss: 0.1195\n",
      "Epoch [24/25] Step [1725/2194] Loss: 0.0049\n",
      "Epoch [24/25] Step [1740/2194] Loss: 0.0319\n",
      "Epoch [24/25] Step [1755/2194] Loss: 0.0005\n",
      "Epoch [24/25] Step [1770/2194] Loss: 0.0096\n",
      "Epoch [24/25] Step [1785/2194] Loss: 0.0210\n",
      "Epoch [24/25] Step [1800/2194] Loss: 0.0007\n",
      "Epoch [24/25] Step [1815/2194] Loss: 0.0015\n",
      "Epoch [24/25] Step [1830/2194] Loss: 0.2253\n",
      "Epoch [24/25] Step [1845/2194] Loss: 0.0016\n",
      "Epoch [24/25] Step [1860/2194] Loss: 0.0021\n",
      "Epoch [24/25] Step [1875/2194] Loss: 0.0148\n",
      "Epoch [24/25] Step [1890/2194] Loss: 0.0595\n",
      "Epoch [24/25] Step [1905/2194] Loss: 0.0042\n",
      "Epoch [24/25] Step [1920/2194] Loss: 0.0034\n",
      "Epoch [24/25] Step [1935/2194] Loss: 0.1048\n",
      "Epoch [24/25] Step [1950/2194] Loss: 0.0180\n",
      "Epoch [24/25] Step [1965/2194] Loss: 0.0192\n",
      "Epoch [24/25] Step [1980/2194] Loss: 0.0325\n",
      "Epoch [24/25] Step [1995/2194] Loss: 0.0004\n",
      "Epoch [24/25] Step [2010/2194] Loss: 0.0846\n",
      "Epoch [24/25] Step [2025/2194] Loss: 0.0017\n",
      "Epoch [24/25] Step [2040/2194] Loss: 0.1451\n",
      "Epoch [24/25] Step [2055/2194] Loss: 0.0473\n",
      "Epoch [24/25] Step [2070/2194] Loss: 0.0010\n",
      "Epoch [24/25] Step [2085/2194] Loss: 0.0974\n",
      "Epoch [24/25] Step [2100/2194] Loss: 0.1002\n",
      "Epoch [24/25] Step [2115/2194] Loss: 0.0012\n",
      "Epoch [24/25] Step [2130/2194] Loss: 0.0188\n",
      "Epoch [24/25] Step [2145/2194] Loss: 0.0034\n",
      "Epoch [24/25] Step [2160/2194] Loss: 0.0041\n",
      "Epoch [24/25] Step [2175/2194] Loss: 0.0026\n",
      "Epoch [24/25] Step [2190/2194] Loss: 0.1253\n",
      "Epoch [24/25] completed in 2783.34s\n",
      "Train Accuracy: 0.9879, Validation Accuracy: 0.9020\n",
      "\n",
      "Epoch [25/25] Step [0/2194] Loss: 0.0017\n",
      "Epoch [25/25] Step [15/2194] Loss: 0.0910\n",
      "Epoch [25/25] Step [30/2194] Loss: 0.0329\n",
      "Epoch [25/25] Step [45/2194] Loss: 0.0097\n",
      "Epoch [25/25] Step [60/2194] Loss: 0.0021\n",
      "Epoch [25/25] Step [75/2194] Loss: 0.0031\n",
      "Epoch [25/25] Step [90/2194] Loss: 0.0489\n",
      "Epoch [25/25] Step [105/2194] Loss: 0.0713\n",
      "Epoch [25/25] Step [120/2194] Loss: 0.0119\n",
      "Epoch [25/25] Step [135/2194] Loss: 0.0174\n",
      "Epoch [25/25] Step [150/2194] Loss: 0.2048\n",
      "Epoch [25/25] Step [165/2194] Loss: 0.0038\n",
      "Epoch [25/25] Step [180/2194] Loss: 0.0030\n",
      "Epoch [25/25] Step [195/2194] Loss: 0.0465\n",
      "Epoch [25/25] Step [210/2194] Loss: 0.0075\n",
      "Epoch [25/25] Step [225/2194] Loss: 0.0517\n",
      "Epoch [25/25] Step [240/2194] Loss: 0.0140\n",
      "Epoch [25/25] Step [255/2194] Loss: 0.0062\n",
      "Epoch [25/25] Step [270/2194] Loss: 0.1035\n",
      "Epoch [25/25] Step [285/2194] Loss: 0.0028\n",
      "Epoch [25/25] Step [300/2194] Loss: 0.0020\n",
      "Epoch [25/25] Step [315/2194] Loss: 0.0603\n",
      "Epoch [25/25] Step [330/2194] Loss: 0.0028\n",
      "Epoch [25/25] Step [345/2194] Loss: 0.0805\n",
      "Epoch [25/25] Step [360/2194] Loss: 0.0071\n",
      "Epoch [25/25] Step [375/2194] Loss: 0.0063\n",
      "Epoch [25/25] Step [390/2194] Loss: 0.0015\n",
      "Epoch [25/25] Step [405/2194] Loss: 0.0005\n",
      "Epoch [25/25] Step [420/2194] Loss: 0.0887\n",
      "Epoch [25/25] Step [435/2194] Loss: 0.0039\n",
      "Epoch [25/25] Step [450/2194] Loss: 0.0626\n",
      "Epoch [25/25] Step [465/2194] Loss: 0.0019\n",
      "Epoch [25/25] Step [480/2194] Loss: 0.0007\n",
      "Epoch [25/25] Step [495/2194] Loss: 0.0105\n",
      "Epoch [25/25] Step [510/2194] Loss: 0.0111\n",
      "Epoch [25/25] Step [525/2194] Loss: 0.0002\n",
      "Epoch [25/25] Step [540/2194] Loss: 0.0010\n",
      "Epoch [25/25] Step [555/2194] Loss: 0.0023\n",
      "Epoch [25/25] Step [570/2194] Loss: 0.0004\n",
      "Epoch [25/25] Step [585/2194] Loss: 0.0389\n",
      "Epoch [25/25] Step [600/2194] Loss: 0.0089\n",
      "Epoch [25/25] Step [615/2194] Loss: 0.1831\n",
      "Epoch [25/25] Step [630/2194] Loss: 0.0009\n",
      "Epoch [25/25] Step [645/2194] Loss: 0.0146\n",
      "Epoch [25/25] Step [660/2194] Loss: 0.0003\n",
      "Epoch [25/25] Step [675/2194] Loss: 0.0008\n",
      "Epoch [25/25] Step [690/2194] Loss: 0.0063\n",
      "Epoch [25/25] Step [705/2194] Loss: 0.0153\n",
      "Epoch [25/25] Step [720/2194] Loss: 0.0843\n",
      "Epoch [25/25] Step [735/2194] Loss: 0.0005\n",
      "Epoch [25/25] Step [750/2194] Loss: 0.0028\n",
      "Epoch [25/25] Step [765/2194] Loss: 0.0210\n",
      "Epoch [25/25] Step [780/2194] Loss: 0.0065\n",
      "Epoch [25/25] Step [795/2194] Loss: 0.0016\n",
      "Epoch [25/25] Step [810/2194] Loss: 0.0012\n",
      "Epoch [25/25] Step [825/2194] Loss: 0.0414\n",
      "Epoch [25/25] Step [840/2194] Loss: 0.1616\n",
      "Epoch [25/25] Step [855/2194] Loss: 0.0005\n",
      "Epoch [25/25] Step [870/2194] Loss: 0.0080\n",
      "Epoch [25/25] Step [885/2194] Loss: 0.0198\n",
      "Epoch [25/25] Step [900/2194] Loss: 0.0997\n",
      "Epoch [25/25] Step [915/2194] Loss: 0.0468\n",
      "Epoch [25/25] Step [930/2194] Loss: 0.0016\n",
      "Epoch [25/25] Step [945/2194] Loss: 0.0850\n",
      "Epoch [25/25] Step [960/2194] Loss: 0.0185\n",
      "Epoch [25/25] Step [975/2194] Loss: 0.0026\n",
      "Epoch [25/25] Step [990/2194] Loss: 0.1605\n",
      "Epoch [25/25] Step [1005/2194] Loss: 0.0024\n",
      "Epoch [25/25] Step [1020/2194] Loss: 0.0025\n",
      "Epoch [25/25] Step [1035/2194] Loss: 0.0005\n",
      "Epoch [25/25] Step [1050/2194] Loss: 0.1720\n",
      "Epoch [25/25] Step [1065/2194] Loss: 0.0447\n",
      "Epoch [25/25] Step [1080/2194] Loss: 0.0167\n",
      "Epoch [25/25] Step [1095/2194] Loss: 0.0077\n",
      "Epoch [25/25] Step [1110/2194] Loss: 0.0070\n",
      "Epoch [25/25] Step [1125/2194] Loss: 0.0070\n",
      "Epoch [25/25] Step [1140/2194] Loss: 0.0732\n",
      "Epoch [25/25] Step [1155/2194] Loss: 0.0050\n",
      "Epoch [25/25] Step [1170/2194] Loss: 0.0040\n",
      "Epoch [25/25] Step [1185/2194] Loss: 0.0539\n",
      "Epoch [25/25] Step [1200/2194] Loss: 0.0270\n",
      "Epoch [25/25] Step [1215/2194] Loss: 0.0013\n",
      "Epoch [25/25] Step [1230/2194] Loss: 0.0557\n",
      "Epoch [25/25] Step [1245/2194] Loss: 0.0101\n",
      "Epoch [25/25] Step [1260/2194] Loss: 0.0629\n",
      "Epoch [25/25] Step [1275/2194] Loss: 0.0026\n",
      "Epoch [25/25] Step [1290/2194] Loss: 0.0227\n",
      "Epoch [25/25] Step [1305/2194] Loss: 0.0024\n",
      "Epoch [25/25] Step [1320/2194] Loss: 0.0009\n",
      "Epoch [25/25] Step [1335/2194] Loss: 0.0012\n",
      "Epoch [25/25] Step [1350/2194] Loss: 0.0054\n",
      "Epoch [25/25] Step [1365/2194] Loss: 0.0008\n",
      "Epoch [25/25] Step [1380/2194] Loss: 0.0466\n",
      "Epoch [25/25] Step [1395/2194] Loss: 0.0083\n",
      "Epoch [25/25] Step [1410/2194] Loss: 0.0201\n",
      "Epoch [25/25] Step [1425/2194] Loss: 0.0019\n",
      "Epoch [25/25] Step [1440/2194] Loss: 0.0098\n",
      "Epoch [25/25] Step [1455/2194] Loss: 0.0027\n",
      "Epoch [25/25] Step [1470/2194] Loss: 0.0058\n",
      "Epoch [25/25] Step [1485/2194] Loss: 0.0010\n",
      "Epoch [25/25] Step [1500/2194] Loss: 0.0149\n",
      "Epoch [25/25] Step [1515/2194] Loss: 0.2557\n",
      "Epoch [25/25] Step [1530/2194] Loss: 0.0197\n",
      "Epoch [25/25] Step [1545/2194] Loss: 0.1698\n",
      "Epoch [25/25] Step [1560/2194] Loss: 0.0212\n",
      "Epoch [25/25] Step [1575/2194] Loss: 0.0366\n",
      "Epoch [25/25] Step [1590/2194] Loss: 0.0205\n",
      "Epoch [25/25] Step [1605/2194] Loss: 0.0017\n",
      "Epoch [25/25] Step [1620/2194] Loss: 0.0010\n",
      "Epoch [25/25] Step [1635/2194] Loss: 0.0167\n",
      "Epoch [25/25] Step [1650/2194] Loss: 0.0381\n",
      "Epoch [25/25] Step [1665/2194] Loss: 0.1374\n",
      "Epoch [25/25] Step [1680/2194] Loss: 0.0117\n",
      "Epoch [25/25] Step [1695/2194] Loss: 0.0006\n",
      "Epoch [25/25] Step [1710/2194] Loss: 0.0031\n",
      "Epoch [25/25] Step [1725/2194] Loss: 0.0800\n",
      "Epoch [25/25] Step [1740/2194] Loss: 0.0129\n",
      "Epoch [25/25] Step [1755/2194] Loss: 0.1192\n",
      "Epoch [25/25] Step [1770/2194] Loss: 0.0157\n",
      "Epoch [25/25] Step [1785/2194] Loss: 0.0009\n",
      "Epoch [25/25] Step [1800/2194] Loss: 0.2082\n",
      "Epoch [25/25] Step [1815/2194] Loss: 0.0842\n",
      "Epoch [25/25] Step [1830/2194] Loss: 0.0014\n",
      "Epoch [25/25] Step [1845/2194] Loss: 0.0024\n",
      "Epoch [25/25] Step [1860/2194] Loss: 0.2862\n",
      "Epoch [25/25] Step [1875/2194] Loss: 0.0328\n",
      "Epoch [25/25] Step [1890/2194] Loss: 0.0013\n",
      "Epoch [25/25] Step [1905/2194] Loss: 0.0983\n",
      "Epoch [25/25] Step [1920/2194] Loss: 0.0927\n",
      "Epoch [25/25] Step [1935/2194] Loss: 0.0024\n",
      "Epoch [25/25] Step [1950/2194] Loss: 0.0071\n",
      "Epoch [25/25] Step [1965/2194] Loss: 0.0061\n",
      "Epoch [25/25] Step [1980/2194] Loss: 0.0587\n",
      "Epoch [25/25] Step [1995/2194] Loss: 0.0011\n",
      "Epoch [25/25] Step [2010/2194] Loss: 0.0477\n",
      "Epoch [25/25] Step [2025/2194] Loss: 0.0345\n",
      "Epoch [25/25] Step [2040/2194] Loss: 0.0023\n",
      "Epoch [25/25] Step [2055/2194] Loss: 0.0854\n",
      "Epoch [25/25] Step [2070/2194] Loss: 0.1542\n",
      "Epoch [25/25] Step [2085/2194] Loss: 0.0166\n",
      "Epoch [25/25] Step [2100/2194] Loss: 0.0282\n",
      "Epoch [25/25] Step [2115/2194] Loss: 0.0036\n",
      "Epoch [25/25] Step [2130/2194] Loss: 0.0313\n",
      "Epoch [25/25] Step [2145/2194] Loss: 0.1093\n",
      "Epoch [25/25] Step [2160/2194] Loss: 0.0658\n",
      "Epoch [25/25] Step [2175/2194] Loss: 0.0030\n",
      "Epoch [25/25] Step [2190/2194] Loss: 0.0911\n",
      "Epoch [25/25] completed in 2786.20s\n",
      "Train Accuracy: 0.9897, Validation Accuracy: 0.8995\n",
      "\n",
      "\n",
      "ðŸ“Š Final Test Set Evaluation:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Normal       0.90      0.95      0.92     15576\n",
      "      Stroke       0.88      0.80      0.84      7824\n",
      "\n",
      "    accuracy                           0.90     23400\n",
      "   macro avg       0.89      0.87      0.88     23400\n",
      "weighted avg       0.90      0.90      0.90     23400\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8974786324786325"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-10T01:35:20.095953Z",
     "start_time": "2025-06-09T12:03:57.103080Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms, models\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from sklearn.metrics import classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "# -----------------------------\n",
    "# Settings & Hyperparameters\n",
    "# -----------------------------\n",
    "data_dir = \"Balanced_dataset\"\n",
    "batch_size = 32  # reduce if using GPU with <6GB memory\n",
    "img_size = 224\n",
    "val_split = 0.2\n",
    "test_split = 0.2\n",
    "seed = 42\n",
    "num_workers = 4  # increase if your CPU can handle it\n",
    "epochs = 25\n",
    "\n",
    "# -----------------------------\n",
    "# Transforms\n",
    "# -----------------------------\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((img_size, img_size)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# -----------------------------\n",
    "# Dataset Loading & Splitting\n",
    "# -----------------------------\n",
    "dataset = datasets.ImageFolder(root=data_dir, transform=transform)\n",
    "class_names = dataset.classes\n",
    "\n",
    "total_size = len(dataset)\n",
    "val_size = int(val_split * total_size)\n",
    "test_size = int(test_split * total_size)\n",
    "train_size = total_size - val_size - test_size\n",
    "\n",
    "print(f\"Total samples: {total_size}\")\n",
    "print(f\"Train: {train_size}, Val: {val_size}, Test: {test_size}\")\n",
    "print(f\"Class names: {class_names}\")\n",
    "\n",
    "train_set, val_set, test_set = random_split(\n",
    "    dataset, [train_size, val_size, test_size],\n",
    "    generator=torch.Generator().manual_seed(seed)\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
    "val_loader = DataLoader(val_set, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
    "test_loader = DataLoader(test_set, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
    "\n",
    "# -----------------------------\n",
    "# Device Setup\n",
    "# -----------------------------\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# -----------------------------\n",
    "# Model: VGG16 + Fine-tuned Classifier\n",
    "# -----------------------------\n",
    "model = models.vgg16(pretrained=True)\n",
    "for param in model.features.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "model.classifier[6] = nn.Linear(4096, 2)  # 2 classes: Stroke and Normal\n",
    "model = model.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.classifier.parameters(), lr=1e-4)\n",
    "\n",
    "# -----------------------------\n",
    "# Evaluation Function\n",
    "# -----------------------------\n",
    "def evaluate_model(model, dataloader, silent=False):\n",
    "    model.eval()\n",
    "    correct, total = 0, 0\n",
    "    y_true, y_pred = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in dataloader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "            y_true.extend(labels.cpu().numpy())\n",
    "            y_pred.extend(preds.cpu().numpy())\n",
    "\n",
    "    acc = correct / total\n",
    "    if not silent:\n",
    "        print(classification_report(y_true, y_pred, target_names=class_names))\n",
    "    return acc\n",
    "\n",
    "# -----------------------------\n",
    "# Training Function\n",
    "# -----------------------------\n",
    "def train_model(model, epochs=10):\n",
    "    best_acc = 0\n",
    "    for epoch in range(epochs):\n",
    "        start = time.time()\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        correct, total = 0, 0\n",
    "\n",
    "        for batch_idx, (images, labels) in enumerate(train_loader):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "            # Batch progress logging\n",
    "            if batch_idx % 15 == 0:\n",
    "                print(f\"Epoch [{epoch+1}/{epochs}] \"\n",
    "                      f\"Step [{batch_idx}/{len(train_loader)}] \"\n",
    "                      f\"Loss: {loss.item():.4f}\")\n",
    "\n",
    "        # Epoch summary\n",
    "        train_acc = correct / total\n",
    "        val_acc = evaluate_model(model, val_loader, silent=True)\n",
    "        print(f\"Epoch [{epoch+1}/{epochs}] completed in {time.time() - start:.2f}s\")\n",
    "        print(f\"Train Accuracy: {train_acc:.4f}, Validation Accuracy: {val_acc:.4f}\\n\")\n",
    "\n",
    "        if val_acc > best_acc:\n",
    "            best_acc = val_acc\n",
    "            torch.save(model.state_dict(), \"Balanced_dataset_vgg16_stroke.pth\")\n",
    "            print(f\"âœ… Best model updated and saved with Val Acc: {val_acc:.4f}\\n\")\n",
    "\n",
    "# -----------------------------\n",
    "# Run Training\n",
    "# -----------------------------\n",
    "train_model(model, epochs=epochs)\n",
    "# Load and Test the Best Model\n",
    "# -----------------------------\n",
    "model.load_state_dict(torch.load(\"Balanced_dataset_vgg16_stroke.pth\"))\n",
    "print(\"\\nðŸ“Š Final Test Set Evaluation:\")\n",
    "evaluate_model(model, test_loader)"
   ],
   "id": "74880ab99a615566",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total samples: 120000\n",
      "Train: 72000, Val: 24000, Test: 24000\n",
      "Class names: ['Normal', 'Stroke']\n",
      "Using device: cuda\n",
      "Epoch [1/25] Step [0/2250] Loss: 0.7692\n",
      "Epoch [1/25] Step [15/2250] Loss: 0.6561\n",
      "Epoch [1/25] Step [30/2250] Loss: 0.7342\n",
      "Epoch [1/25] Step [45/2250] Loss: 0.6003\n",
      "Epoch [1/25] Step [60/2250] Loss: 0.5656\n",
      "Epoch [1/25] Step [75/2250] Loss: 0.5169\n",
      "Epoch [1/25] Step [90/2250] Loss: 0.5492\n",
      "Epoch [1/25] Step [105/2250] Loss: 0.6450\n",
      "Epoch [1/25] Step [120/2250] Loss: 0.5099\n",
      "Epoch [1/25] Step [135/2250] Loss: 0.5714\n",
      "Epoch [1/25] Step [150/2250] Loss: 0.4540\n",
      "Epoch [1/25] Step [165/2250] Loss: 0.4783\n",
      "Epoch [1/25] Step [180/2250] Loss: 0.5054\n",
      "Epoch [1/25] Step [195/2250] Loss: 0.6689\n",
      "Epoch [1/25] Step [210/2250] Loss: 0.3816\n",
      "Epoch [1/25] Step [225/2250] Loss: 0.7756\n",
      "Epoch [1/25] Step [240/2250] Loss: 0.4676\n",
      "Epoch [1/25] Step [255/2250] Loss: 0.4071\n",
      "Epoch [1/25] Step [270/2250] Loss: 0.5913\n",
      "Epoch [1/25] Step [285/2250] Loss: 0.6049\n",
      "Epoch [1/25] Step [300/2250] Loss: 0.2978\n",
      "Epoch [1/25] Step [315/2250] Loss: 0.4765\n",
      "Epoch [1/25] Step [330/2250] Loss: 0.7074\n",
      "Epoch [1/25] Step [345/2250] Loss: 0.5040\n",
      "Epoch [1/25] Step [360/2250] Loss: 0.6457\n",
      "Epoch [1/25] Step [375/2250] Loss: 0.4985\n",
      "Epoch [1/25] Step [390/2250] Loss: 0.7101\n",
      "Epoch [1/25] Step [405/2250] Loss: 0.4565\n",
      "Epoch [1/25] Step [420/2250] Loss: 0.4960\n",
      "Epoch [1/25] Step [435/2250] Loss: 0.4401\n",
      "Epoch [1/25] Step [450/2250] Loss: 0.4449\n",
      "Epoch [1/25] Step [465/2250] Loss: 0.4842\n",
      "Epoch [1/25] Step [480/2250] Loss: 0.4337\n",
      "Epoch [1/25] Step [495/2250] Loss: 0.4195\n",
      "Epoch [1/25] Step [510/2250] Loss: 0.3553\n",
      "Epoch [1/25] Step [525/2250] Loss: 0.4689\n",
      "Epoch [1/25] Step [540/2250] Loss: 0.5133\n",
      "Epoch [1/25] Step [555/2250] Loss: 0.4765\n",
      "Epoch [1/25] Step [570/2250] Loss: 0.3755\n",
      "Epoch [1/25] Step [585/2250] Loss: 0.3945\n",
      "Epoch [1/25] Step [600/2250] Loss: 0.3556\n",
      "Epoch [1/25] Step [615/2250] Loss: 0.5136\n",
      "Epoch [1/25] Step [630/2250] Loss: 0.3633\n",
      "Epoch [1/25] Step [645/2250] Loss: 0.4935\n",
      "Epoch [1/25] Step [660/2250] Loss: 0.4349\n",
      "Epoch [1/25] Step [675/2250] Loss: 0.4593\n",
      "Epoch [1/25] Step [690/2250] Loss: 0.4881\n",
      "Epoch [1/25] Step [705/2250] Loss: 0.4790\n",
      "Epoch [1/25] Step [720/2250] Loss: 0.5565\n",
      "Epoch [1/25] Step [735/2250] Loss: 0.5021\n",
      "Epoch [1/25] Step [750/2250] Loss: 0.5315\n",
      "Epoch [1/25] Step [765/2250] Loss: 0.6581\n",
      "Epoch [1/25] Step [780/2250] Loss: 0.5270\n",
      "Epoch [1/25] Step [795/2250] Loss: 0.3818\n",
      "Epoch [1/25] Step [810/2250] Loss: 0.5177\n",
      "Epoch [1/25] Step [825/2250] Loss: 0.3522\n",
      "Epoch [1/25] Step [840/2250] Loss: 0.2769\n",
      "Epoch [1/25] Step [855/2250] Loss: 0.6466\n",
      "Epoch [1/25] Step [870/2250] Loss: 0.4531\n",
      "Epoch [1/25] Step [885/2250] Loss: 0.3761\n",
      "Epoch [1/25] Step [900/2250] Loss: 0.5051\n",
      "Epoch [1/25] Step [915/2250] Loss: 0.6236\n",
      "Epoch [1/25] Step [930/2250] Loss: 0.4472\n",
      "Epoch [1/25] Step [945/2250] Loss: 0.2732\n",
      "Epoch [1/25] Step [960/2250] Loss: 0.3010\n",
      "Epoch [1/25] Step [975/2250] Loss: 0.4852\n",
      "Epoch [1/25] Step [990/2250] Loss: 0.2838\n",
      "Epoch [1/25] Step [1005/2250] Loss: 0.3885\n",
      "Epoch [1/25] Step [1020/2250] Loss: 0.5945\n",
      "Epoch [1/25] Step [1035/2250] Loss: 0.5562\n",
      "Epoch [1/25] Step [1050/2250] Loss: 0.4233\n",
      "Epoch [1/25] Step [1065/2250] Loss: 0.2319\n",
      "Epoch [1/25] Step [1080/2250] Loss: 0.6047\n",
      "Epoch [1/25] Step [1095/2250] Loss: 0.4847\n",
      "Epoch [1/25] Step [1110/2250] Loss: 0.4588\n",
      "Epoch [1/25] Step [1125/2250] Loss: 0.2518\n",
      "Epoch [1/25] Step [1140/2250] Loss: 0.4215\n",
      "Epoch [1/25] Step [1155/2250] Loss: 0.4917\n",
      "Epoch [1/25] Step [1170/2250] Loss: 0.3742\n",
      "Epoch [1/25] Step [1185/2250] Loss: 0.3232\n",
      "Epoch [1/25] Step [1200/2250] Loss: 0.2968\n",
      "Epoch [1/25] Step [1215/2250] Loss: 0.3898\n",
      "Epoch [1/25] Step [1230/2250] Loss: 0.4477\n",
      "Epoch [1/25] Step [1245/2250] Loss: 0.3395\n",
      "Epoch [1/25] Step [1260/2250] Loss: 0.3633\n",
      "Epoch [1/25] Step [1275/2250] Loss: 0.3308\n",
      "Epoch [1/25] Step [1290/2250] Loss: 0.2405\n",
      "Epoch [1/25] Step [1305/2250] Loss: 0.4259\n",
      "Epoch [1/25] Step [1320/2250] Loss: 0.5603\n",
      "Epoch [1/25] Step [1335/2250] Loss: 0.3214\n",
      "Epoch [1/25] Step [1350/2250] Loss: 0.2424\n",
      "Epoch [1/25] Step [1365/2250] Loss: 0.3423\n",
      "Epoch [1/25] Step [1380/2250] Loss: 0.4928\n",
      "Epoch [1/25] Step [1395/2250] Loss: 0.3883\n",
      "Epoch [1/25] Step [1410/2250] Loss: 0.2218\n",
      "Epoch [1/25] Step [1425/2250] Loss: 0.3603\n",
      "Epoch [1/25] Step [1440/2250] Loss: 0.4332\n",
      "Epoch [1/25] Step [1455/2250] Loss: 0.2430\n",
      "Epoch [1/25] Step [1470/2250] Loss: 0.4244\n",
      "Epoch [1/25] Step [1485/2250] Loss: 0.3256\n",
      "Epoch [1/25] Step [1500/2250] Loss: 0.4021\n",
      "Epoch [1/25] Step [1515/2250] Loss: 0.1888\n",
      "Epoch [1/25] Step [1530/2250] Loss: 0.2951\n",
      "Epoch [1/25] Step [1545/2250] Loss: 0.2839\n",
      "Epoch [1/25] Step [1560/2250] Loss: 0.4408\n",
      "Epoch [1/25] Step [1575/2250] Loss: 0.4072\n",
      "Epoch [1/25] Step [1590/2250] Loss: 0.2293\n",
      "Epoch [1/25] Step [1605/2250] Loss: 0.7193\n",
      "Epoch [1/25] Step [1620/2250] Loss: 0.2662\n",
      "Epoch [1/25] Step [1635/2250] Loss: 0.3449\n",
      "Epoch [1/25] Step [1650/2250] Loss: 0.2912\n",
      "Epoch [1/25] Step [1665/2250] Loss: 0.2989\n",
      "Epoch [1/25] Step [1680/2250] Loss: 0.3752\n",
      "Epoch [1/25] Step [1695/2250] Loss: 0.3703\n",
      "Epoch [1/25] Step [1710/2250] Loss: 0.4690\n",
      "Epoch [1/25] Step [1725/2250] Loss: 0.2866\n",
      "Epoch [1/25] Step [1740/2250] Loss: 0.3563\n",
      "Epoch [1/25] Step [1755/2250] Loss: 0.3385\n",
      "Epoch [1/25] Step [1770/2250] Loss: 0.2828\n",
      "Epoch [1/25] Step [1785/2250] Loss: 0.3049\n",
      "Epoch [1/25] Step [1800/2250] Loss: 0.3309\n",
      "Epoch [1/25] Step [1815/2250] Loss: 0.5200\n",
      "Epoch [1/25] Step [1830/2250] Loss: 0.2936\n",
      "Epoch [1/25] Step [1845/2250] Loss: 0.4532\n",
      "Epoch [1/25] Step [1860/2250] Loss: 0.2279\n",
      "Epoch [1/25] Step [1875/2250] Loss: 0.3586\n",
      "Epoch [1/25] Step [1890/2250] Loss: 0.4044\n",
      "Epoch [1/25] Step [1905/2250] Loss: 0.3001\n",
      "Epoch [1/25] Step [1920/2250] Loss: 0.5054\n",
      "Epoch [1/25] Step [1935/2250] Loss: 0.4513\n",
      "Epoch [1/25] Step [1950/2250] Loss: 0.1952\n",
      "Epoch [1/25] Step [1965/2250] Loss: 0.4187\n",
      "Epoch [1/25] Step [1980/2250] Loss: 0.1962\n",
      "Epoch [1/25] Step [1995/2250] Loss: 0.3142\n",
      "Epoch [1/25] Step [2010/2250] Loss: 0.4070\n",
      "Epoch [1/25] Step [2025/2250] Loss: 0.4252\n",
      "Epoch [1/25] Step [2040/2250] Loss: 0.4290\n",
      "Epoch [1/25] Step [2055/2250] Loss: 0.2325\n",
      "Epoch [1/25] Step [2070/2250] Loss: 0.3712\n",
      "Epoch [1/25] Step [2085/2250] Loss: 0.3311\n",
      "Epoch [1/25] Step [2100/2250] Loss: 0.4446\n",
      "Epoch [1/25] Step [2115/2250] Loss: 0.3457\n",
      "Epoch [1/25] Step [2130/2250] Loss: 0.2799\n",
      "Epoch [1/25] Step [2145/2250] Loss: 0.2211\n",
      "Epoch [1/25] Step [2160/2250] Loss: 0.2261\n",
      "Epoch [1/25] Step [2175/2250] Loss: 0.3028\n",
      "Epoch [1/25] Step [2190/2250] Loss: 0.3760\n",
      "Epoch [1/25] Step [2205/2250] Loss: 0.3205\n",
      "Epoch [1/25] Step [2220/2250] Loss: 0.3756\n",
      "Epoch [1/25] Step [2235/2250] Loss: 0.2537\n",
      "Epoch [1/25] completed in 1766.65s\n",
      "Train Accuracy: 0.8047, Validation Accuracy: 0.8722\n",
      "\n",
      "âœ… Best model updated and saved with Val Acc: 0.8722\n",
      "\n",
      "Epoch [2/25] Step [0/2250] Loss: 0.2854\n",
      "Epoch [2/25] Step [15/2250] Loss: 0.4840\n",
      "Epoch [2/25] Step [30/2250] Loss: 0.4033\n",
      "Epoch [2/25] Step [45/2250] Loss: 0.3572\n",
      "Epoch [2/25] Step [60/2250] Loss: 0.2509\n",
      "Epoch [2/25] Step [75/2250] Loss: 0.2126\n",
      "Epoch [2/25] Step [90/2250] Loss: 0.3411\n",
      "Epoch [2/25] Step [105/2250] Loss: 0.2158\n",
      "Epoch [2/25] Step [120/2250] Loss: 0.4468\n",
      "Epoch [2/25] Step [135/2250] Loss: 0.3899\n",
      "Epoch [2/25] Step [150/2250] Loss: 0.3046\n",
      "Epoch [2/25] Step [165/2250] Loss: 0.2116\n",
      "Epoch [2/25] Step [180/2250] Loss: 0.5163\n",
      "Epoch [2/25] Step [195/2250] Loss: 0.2111\n",
      "Epoch [2/25] Step [210/2250] Loss: 0.1082\n",
      "Epoch [2/25] Step [225/2250] Loss: 0.3259\n",
      "Epoch [2/25] Step [240/2250] Loss: 0.3361\n",
      "Epoch [2/25] Step [255/2250] Loss: 0.0804\n",
      "Epoch [2/25] Step [270/2250] Loss: 0.7101\n",
      "Epoch [2/25] Step [285/2250] Loss: 0.1968\n",
      "Epoch [2/25] Step [300/2250] Loss: 0.1711\n",
      "Epoch [2/25] Step [315/2250] Loss: 0.3325\n",
      "Epoch [2/25] Step [330/2250] Loss: 0.1733\n",
      "Epoch [2/25] Step [345/2250] Loss: 0.1893\n",
      "Epoch [2/25] Step [360/2250] Loss: 0.4332\n",
      "Epoch [2/25] Step [375/2250] Loss: 0.3342\n",
      "Epoch [2/25] Step [390/2250] Loss: 0.2309\n",
      "Epoch [2/25] Step [405/2250] Loss: 0.2815\n",
      "Epoch [2/25] Step [420/2250] Loss: 0.2717\n",
      "Epoch [2/25] Step [435/2250] Loss: 0.3852\n",
      "Epoch [2/25] Step [450/2250] Loss: 0.1686\n",
      "Epoch [2/25] Step [465/2250] Loss: 0.2887\n",
      "Epoch [2/25] Step [480/2250] Loss: 0.2224\n",
      "Epoch [2/25] Step [495/2250] Loss: 0.1166\n",
      "Epoch [2/25] Step [510/2250] Loss: 0.3256\n",
      "Epoch [2/25] Step [525/2250] Loss: 0.1849\n",
      "Epoch [2/25] Step [540/2250] Loss: 0.2251\n",
      "Epoch [2/25] Step [555/2250] Loss: 0.1315\n",
      "Epoch [2/25] Step [570/2250] Loss: 0.1094\n",
      "Epoch [2/25] Step [585/2250] Loss: 0.2287\n",
      "Epoch [2/25] Step [600/2250] Loss: 0.2509\n",
      "Epoch [2/25] Step [615/2250] Loss: 0.2974\n",
      "Epoch [2/25] Step [630/2250] Loss: 0.1604\n",
      "Epoch [2/25] Step [645/2250] Loss: 0.5102\n",
      "Epoch [2/25] Step [660/2250] Loss: 0.3285\n",
      "Epoch [2/25] Step [675/2250] Loss: 0.1185\n",
      "Epoch [2/25] Step [690/2250] Loss: 0.2891\n",
      "Epoch [2/25] Step [705/2250] Loss: 0.3055\n",
      "Epoch [2/25] Step [720/2250] Loss: 0.2472\n",
      "Epoch [2/25] Step [735/2250] Loss: 0.3394\n",
      "Epoch [2/25] Step [750/2250] Loss: 0.3491\n",
      "Epoch [2/25] Step [765/2250] Loss: 0.2556\n",
      "Epoch [2/25] Step [780/2250] Loss: 0.4246\n",
      "Epoch [2/25] Step [795/2250] Loss: 0.4450\n",
      "Epoch [2/25] Step [810/2250] Loss: 0.3586\n",
      "Epoch [2/25] Step [825/2250] Loss: 0.2524\n",
      "Epoch [2/25] Step [840/2250] Loss: 0.1649\n",
      "Epoch [2/25] Step [855/2250] Loss: 0.1739\n",
      "Epoch [2/25] Step [870/2250] Loss: 0.1688\n",
      "Epoch [2/25] Step [885/2250] Loss: 0.2142\n",
      "Epoch [2/25] Step [900/2250] Loss: 0.2677\n",
      "Epoch [2/25] Step [915/2250] Loss: 0.2547\n",
      "Epoch [2/25] Step [930/2250] Loss: 0.3021\n",
      "Epoch [2/25] Step [945/2250] Loss: 0.3724\n",
      "Epoch [2/25] Step [960/2250] Loss: 0.3442\n",
      "Epoch [2/25] Step [975/2250] Loss: 0.3663\n",
      "Epoch [2/25] Step [990/2250] Loss: 0.2208\n",
      "Epoch [2/25] Step [1005/2250] Loss: 0.2685\n",
      "Epoch [2/25] Step [1020/2250] Loss: 0.3128\n",
      "Epoch [2/25] Step [1035/2250] Loss: 0.1830\n",
      "Epoch [2/25] Step [1050/2250] Loss: 0.2885\n",
      "Epoch [2/25] Step [1065/2250] Loss: 0.2288\n",
      "Epoch [2/25] Step [1080/2250] Loss: 0.2881\n",
      "Epoch [2/25] Step [1095/2250] Loss: 0.2888\n",
      "Epoch [2/25] Step [1110/2250] Loss: 0.2118\n",
      "Epoch [2/25] Step [1125/2250] Loss: 0.1482\n",
      "Epoch [2/25] Step [1140/2250] Loss: 0.1742\n",
      "Epoch [2/25] Step [1155/2250] Loss: 0.1440\n",
      "Epoch [2/25] Step [1170/2250] Loss: 0.2411\n",
      "Epoch [2/25] Step [1185/2250] Loss: 0.2024\n",
      "Epoch [2/25] Step [1200/2250] Loss: 0.3708\n",
      "Epoch [2/25] Step [1215/2250] Loss: 0.4062\n",
      "Epoch [2/25] Step [1230/2250] Loss: 0.1894\n",
      "Epoch [2/25] Step [1245/2250] Loss: 0.3216\n",
      "Epoch [2/25] Step [1260/2250] Loss: 0.2468\n",
      "Epoch [2/25] Step [1275/2250] Loss: 0.4004\n",
      "Epoch [2/25] Step [1290/2250] Loss: 0.3109\n",
      "Epoch [2/25] Step [1305/2250] Loss: 0.1838\n",
      "Epoch [2/25] Step [1320/2250] Loss: 0.2378\n",
      "Epoch [2/25] Step [1335/2250] Loss: 0.1152\n",
      "Epoch [2/25] Step [1350/2250] Loss: 0.2073\n",
      "Epoch [2/25] Step [1365/2250] Loss: 0.3847\n",
      "Epoch [2/25] Step [1380/2250] Loss: 0.2217\n",
      "Epoch [2/25] Step [1395/2250] Loss: 0.1329\n",
      "Epoch [2/25] Step [1410/2250] Loss: 0.2787\n",
      "Epoch [2/25] Step [1425/2250] Loss: 0.3480\n",
      "Epoch [2/25] Step [1440/2250] Loss: 0.1016\n",
      "Epoch [2/25] Step [1455/2250] Loss: 0.1786\n",
      "Epoch [2/25] Step [1470/2250] Loss: 0.1290\n",
      "Epoch [2/25] Step [1485/2250] Loss: 0.1107\n",
      "Epoch [2/25] Step [1500/2250] Loss: 0.3059\n",
      "Epoch [2/25] Step [1515/2250] Loss: 0.2988\n",
      "Epoch [2/25] Step [1530/2250] Loss: 0.2567\n",
      "Epoch [2/25] Step [1545/2250] Loss: 0.1603\n",
      "Epoch [2/25] Step [1560/2250] Loss: 0.3430\n",
      "Epoch [2/25] Step [1575/2250] Loss: 0.1275\n",
      "Epoch [2/25] Step [1590/2250] Loss: 0.2540\n",
      "Epoch [2/25] Step [1605/2250] Loss: 0.2386\n",
      "Epoch [2/25] Step [1620/2250] Loss: 0.3206\n",
      "Epoch [2/25] Step [1635/2250] Loss: 0.2074\n",
      "Epoch [2/25] Step [1650/2250] Loss: 0.1379\n",
      "Epoch [2/25] Step [1665/2250] Loss: 0.1726\n",
      "Epoch [2/25] Step [1680/2250] Loss: 0.1885\n",
      "Epoch [2/25] Step [1695/2250] Loss: 0.2653\n",
      "Epoch [2/25] Step [1710/2250] Loss: 0.0973\n",
      "Epoch [2/25] Step [1725/2250] Loss: 0.2754\n",
      "Epoch [2/25] Step [1740/2250] Loss: 0.2745\n",
      "Epoch [2/25] Step [1755/2250] Loss: 0.1854\n",
      "Epoch [2/25] Step [1770/2250] Loss: 0.3060\n",
      "Epoch [2/25] Step [1785/2250] Loss: 0.2724\n",
      "Epoch [2/25] Step [1800/2250] Loss: 0.3327\n",
      "Epoch [2/25] Step [1815/2250] Loss: 0.3378\n",
      "Epoch [2/25] Step [1830/2250] Loss: 0.3593\n",
      "Epoch [2/25] Step [1845/2250] Loss: 0.2899\n",
      "Epoch [2/25] Step [1860/2250] Loss: 0.3320\n",
      "Epoch [2/25] Step [1875/2250] Loss: 0.2557\n",
      "Epoch [2/25] Step [1890/2250] Loss: 0.2435\n",
      "Epoch [2/25] Step [1905/2250] Loss: 0.1916\n",
      "Epoch [2/25] Step [1920/2250] Loss: 0.1107\n",
      "Epoch [2/25] Step [1935/2250] Loss: 0.2504\n",
      "Epoch [2/25] Step [1950/2250] Loss: 0.2347\n",
      "Epoch [2/25] Step [1965/2250] Loss: 0.1931\n",
      "Epoch [2/25] Step [1980/2250] Loss: 0.3186\n",
      "Epoch [2/25] Step [1995/2250] Loss: 0.2974\n",
      "Epoch [2/25] Step [2010/2250] Loss: 0.3665\n",
      "Epoch [2/25] Step [2025/2250] Loss: 0.2153\n",
      "Epoch [2/25] Step [2040/2250] Loss: 0.3507\n",
      "Epoch [2/25] Step [2055/2250] Loss: 0.1919\n",
      "Epoch [2/25] Step [2070/2250] Loss: 0.2365\n",
      "Epoch [2/25] Step [2085/2250] Loss: 0.1059\n",
      "Epoch [2/25] Step [2100/2250] Loss: 0.1011\n",
      "Epoch [2/25] Step [2115/2250] Loss: 0.2381\n",
      "Epoch [2/25] Step [2130/2250] Loss: 0.2412\n",
      "Epoch [2/25] Step [2145/2250] Loss: 0.2068\n",
      "Epoch [2/25] Step [2160/2250] Loss: 0.1553\n",
      "Epoch [2/25] Step [2175/2250] Loss: 0.1981\n",
      "Epoch [2/25] Step [2190/2250] Loss: 0.2748\n",
      "Epoch [2/25] Step [2205/2250] Loss: 0.0936\n",
      "Epoch [2/25] Step [2220/2250] Loss: 0.2462\n",
      "Epoch [2/25] Step [2235/2250] Loss: 0.2710\n",
      "Epoch [2/25] completed in 1766.27s\n",
      "Train Accuracy: 0.8862, Validation Accuracy: 0.9002\n",
      "\n",
      "âœ… Best model updated and saved with Val Acc: 0.9002\n",
      "\n",
      "Epoch [3/25] Step [0/2250] Loss: 0.1890\n",
      "Epoch [3/25] Step [15/2250] Loss: 0.2727\n",
      "Epoch [3/25] Step [30/2250] Loss: 0.0546\n",
      "Epoch [3/25] Step [45/2250] Loss: 0.1309\n",
      "Epoch [3/25] Step [60/2250] Loss: 0.1252\n",
      "Epoch [3/25] Step [75/2250] Loss: 0.2085\n",
      "Epoch [3/25] Step [90/2250] Loss: 0.0691\n",
      "Epoch [3/25] Step [105/2250] Loss: 0.1663\n",
      "Epoch [3/25] Step [120/2250] Loss: 0.2287\n",
      "Epoch [3/25] Step [135/2250] Loss: 0.2300\n",
      "Epoch [3/25] Step [150/2250] Loss: 0.1812\n",
      "Epoch [3/25] Step [165/2250] Loss: 0.1539\n",
      "Epoch [3/25] Step [180/2250] Loss: 0.2014\n",
      "Epoch [3/25] Step [195/2250] Loss: 0.1050\n",
      "Epoch [3/25] Step [210/2250] Loss: 0.0948\n",
      "Epoch [3/25] Step [225/2250] Loss: 0.0892\n",
      "Epoch [3/25] Step [240/2250] Loss: 0.4084\n",
      "Epoch [3/25] Step [255/2250] Loss: 0.2017\n",
      "Epoch [3/25] Step [270/2250] Loss: 0.1308\n",
      "Epoch [3/25] Step [285/2250] Loss: 0.1095\n",
      "Epoch [3/25] Step [300/2250] Loss: 0.2375\n",
      "Epoch [3/25] Step [315/2250] Loss: 0.1818\n",
      "Epoch [3/25] Step [330/2250] Loss: 0.0464\n",
      "Epoch [3/25] Step [345/2250] Loss: 0.1681\n",
      "Epoch [3/25] Step [360/2250] Loss: 0.2269\n",
      "Epoch [3/25] Step [375/2250] Loss: 0.3154\n",
      "Epoch [3/25] Step [390/2250] Loss: 0.1410\n",
      "Epoch [3/25] Step [405/2250] Loss: 0.1041\n",
      "Epoch [3/25] Step [420/2250] Loss: 0.1038\n",
      "Epoch [3/25] Step [435/2250] Loss: 0.3371\n",
      "Epoch [3/25] Step [450/2250] Loss: 0.2127\n",
      "Epoch [3/25] Step [465/2250] Loss: 0.2604\n",
      "Epoch [3/25] Step [480/2250] Loss: 0.0349\n",
      "Epoch [3/25] Step [495/2250] Loss: 0.3663\n",
      "Epoch [3/25] Step [510/2250] Loss: 0.2019\n",
      "Epoch [3/25] Step [525/2250] Loss: 0.2231\n",
      "Epoch [3/25] Step [540/2250] Loss: 0.3155\n",
      "Epoch [3/25] Step [555/2250] Loss: 0.1969\n",
      "Epoch [3/25] Step [570/2250] Loss: 0.3129\n",
      "Epoch [3/25] Step [585/2250] Loss: 0.2807\n",
      "Epoch [3/25] Step [600/2250] Loss: 0.1141\n",
      "Epoch [3/25] Step [615/2250] Loss: 0.1741\n",
      "Epoch [3/25] Step [630/2250] Loss: 0.1075\n",
      "Epoch [3/25] Step [645/2250] Loss: 0.1608\n",
      "Epoch [3/25] Step [660/2250] Loss: 0.1945\n",
      "Epoch [3/25] Step [675/2250] Loss: 0.3272\n",
      "Epoch [3/25] Step [690/2250] Loss: 0.3213\n",
      "Epoch [3/25] Step [705/2250] Loss: 0.1512\n",
      "Epoch [3/25] Step [720/2250] Loss: 0.3011\n",
      "Epoch [3/25] Step [735/2250] Loss: 0.1335\n",
      "Epoch [3/25] Step [750/2250] Loss: 0.1729\n",
      "Epoch [3/25] Step [765/2250] Loss: 0.2206\n",
      "Epoch [3/25] Step [780/2250] Loss: 0.0980\n",
      "Epoch [3/25] Step [795/2250] Loss: 0.1361\n",
      "Epoch [3/25] Step [810/2250] Loss: 0.0770\n",
      "Epoch [3/25] Step [825/2250] Loss: 0.0864\n",
      "Epoch [3/25] Step [840/2250] Loss: 0.1231\n",
      "Epoch [3/25] Step [855/2250] Loss: 0.1058\n",
      "Epoch [3/25] Step [870/2250] Loss: 0.1368\n",
      "Epoch [3/25] Step [885/2250] Loss: 0.4075\n",
      "Epoch [3/25] Step [900/2250] Loss: 0.3252\n",
      "Epoch [3/25] Step [915/2250] Loss: 0.0781\n",
      "Epoch [3/25] Step [930/2250] Loss: 0.1548\n",
      "Epoch [3/25] Step [945/2250] Loss: 0.1025\n",
      "Epoch [3/25] Step [960/2250] Loss: 0.1971\n",
      "Epoch [3/25] Step [975/2250] Loss: 0.1849\n",
      "Epoch [3/25] Step [990/2250] Loss: 0.1348\n",
      "Epoch [3/25] Step [1005/2250] Loss: 0.3284\n",
      "Epoch [3/25] Step [1020/2250] Loss: 0.3198\n",
      "Epoch [3/25] Step [1035/2250] Loss: 0.2017\n",
      "Epoch [3/25] Step [1050/2250] Loss: 0.0912\n",
      "Epoch [3/25] Step [1065/2250] Loss: 0.1973\n",
      "Epoch [3/25] Step [1080/2250] Loss: 0.1922\n",
      "Epoch [3/25] Step [1095/2250] Loss: 0.1723\n",
      "Epoch [3/25] Step [1110/2250] Loss: 0.0535\n",
      "Epoch [3/25] Step [1125/2250] Loss: 0.0751\n",
      "Epoch [3/25] Step [1140/2250] Loss: 0.2094\n",
      "Epoch [3/25] Step [1155/2250] Loss: 0.0978\n",
      "Epoch [3/25] Step [1170/2250] Loss: 0.3250\n",
      "Epoch [3/25] Step [1185/2250] Loss: 0.2157\n",
      "Epoch [3/25] Step [1200/2250] Loss: 0.0744\n",
      "Epoch [3/25] Step [1215/2250] Loss: 0.1805\n",
      "Epoch [3/25] Step [1230/2250] Loss: 0.0391\n",
      "Epoch [3/25] Step [1245/2250] Loss: 0.1564\n",
      "Epoch [3/25] Step [1260/2250] Loss: 0.2100\n",
      "Epoch [3/25] Step [1275/2250] Loss: 0.1532\n",
      "Epoch [3/25] Step [1290/2250] Loss: 0.1814\n",
      "Epoch [3/25] Step [1305/2250] Loss: 0.3355\n",
      "Epoch [3/25] Step [1320/2250] Loss: 0.1632\n",
      "Epoch [3/25] Step [1335/2250] Loss: 0.1358\n",
      "Epoch [3/25] Step [1350/2250] Loss: 0.1779\n",
      "Epoch [3/25] Step [1365/2250] Loss: 0.0483\n",
      "Epoch [3/25] Step [1380/2250] Loss: 0.1408\n",
      "Epoch [3/25] Step [1395/2250] Loss: 0.1767\n",
      "Epoch [3/25] Step [1410/2250] Loss: 0.2341\n",
      "Epoch [3/25] Step [1425/2250] Loss: 0.1614\n",
      "Epoch [3/25] Step [1440/2250] Loss: 0.1385\n",
      "Epoch [3/25] Step [1455/2250] Loss: 0.1958\n",
      "Epoch [3/25] Step [1470/2250] Loss: 0.1075\n",
      "Epoch [3/25] Step [1485/2250] Loss: 0.1921\n",
      "Epoch [3/25] Step [1500/2250] Loss: 0.2876\n",
      "Epoch [3/25] Step [1515/2250] Loss: 0.3045\n",
      "Epoch [3/25] Step [1530/2250] Loss: 0.0936\n",
      "Epoch [3/25] Step [1545/2250] Loss: 0.1055\n",
      "Epoch [3/25] Step [1560/2250] Loss: 0.0543\n",
      "Epoch [3/25] Step [1575/2250] Loss: 0.1725\n",
      "Epoch [3/25] Step [1590/2250] Loss: 0.2080\n",
      "Epoch [3/25] Step [1605/2250] Loss: 0.2501\n",
      "Epoch [3/25] Step [1620/2250] Loss: 0.2228\n",
      "Epoch [3/25] Step [1635/2250] Loss: 0.1725\n",
      "Epoch [3/25] Step [1650/2250] Loss: 0.3300\n",
      "Epoch [3/25] Step [1665/2250] Loss: 0.1062\n",
      "Epoch [3/25] Step [1680/2250] Loss: 0.1713\n",
      "Epoch [3/25] Step [1695/2250] Loss: 0.1000\n",
      "Epoch [3/25] Step [1710/2250] Loss: 0.2320\n",
      "Epoch [3/25] Step [1725/2250] Loss: 0.4077\n",
      "Epoch [3/25] Step [1740/2250] Loss: 0.1316\n",
      "Epoch [3/25] Step [1755/2250] Loss: 0.1193\n",
      "Epoch [3/25] Step [1770/2250] Loss: 0.0857\n",
      "Epoch [3/25] Step [1785/2250] Loss: 0.0951\n",
      "Epoch [3/25] Step [1800/2250] Loss: 0.2582\n",
      "Epoch [3/25] Step [1815/2250] Loss: 0.1644\n",
      "Epoch [3/25] Step [1830/2250] Loss: 0.2802\n",
      "Epoch [3/25] Step [1845/2250] Loss: 0.2383\n",
      "Epoch [3/25] Step [1860/2250] Loss: 0.2165\n",
      "Epoch [3/25] Step [1875/2250] Loss: 0.2620\n",
      "Epoch [3/25] Step [1890/2250] Loss: 0.1192\n",
      "Epoch [3/25] Step [1905/2250] Loss: 0.1921\n",
      "Epoch [3/25] Step [1920/2250] Loss: 0.0623\n",
      "Epoch [3/25] Step [1935/2250] Loss: 0.1221\n",
      "Epoch [3/25] Step [1950/2250] Loss: 0.4520\n",
      "Epoch [3/25] Step [1965/2250] Loss: 0.2102\n",
      "Epoch [3/25] Step [1980/2250] Loss: 0.1283\n",
      "Epoch [3/25] Step [1995/2250] Loss: 0.0674\n",
      "Epoch [3/25] Step [2010/2250] Loss: 0.2156\n",
      "Epoch [3/25] Step [2025/2250] Loss: 0.3863\n",
      "Epoch [3/25] Step [2040/2250] Loss: 0.1888\n",
      "Epoch [3/25] Step [2055/2250] Loss: 0.1428\n",
      "Epoch [3/25] Step [2070/2250] Loss: 0.1921\n",
      "Epoch [3/25] Step [2085/2250] Loss: 0.1407\n",
      "Epoch [3/25] Step [2100/2250] Loss: 0.1434\n",
      "Epoch [3/25] Step [2115/2250] Loss: 0.1888\n",
      "Epoch [3/25] Step [2130/2250] Loss: 0.2333\n",
      "Epoch [3/25] Step [2145/2250] Loss: 0.1048\n",
      "Epoch [3/25] Step [2160/2250] Loss: 0.1049\n",
      "Epoch [3/25] Step [2175/2250] Loss: 0.2636\n",
      "Epoch [3/25] Step [2190/2250] Loss: 0.2103\n",
      "Epoch [3/25] Step [2205/2250] Loss: 0.1197\n",
      "Epoch [3/25] Step [2220/2250] Loss: 0.1257\n",
      "Epoch [3/25] Step [2235/2250] Loss: 0.0925\n",
      "Epoch [3/25] completed in 1764.16s\n",
      "Train Accuracy: 0.9218, Validation Accuracy: 0.9144\n",
      "\n",
      "âœ… Best model updated and saved with Val Acc: 0.9144\n",
      "\n",
      "Epoch [4/25] Step [0/2250] Loss: 0.0884\n",
      "Epoch [4/25] Step [15/2250] Loss: 0.0485\n",
      "Epoch [4/25] Step [30/2250] Loss: 0.1658\n",
      "Epoch [4/25] Step [45/2250] Loss: 0.2009\n",
      "Epoch [4/25] Step [60/2250] Loss: 0.1444\n",
      "Epoch [4/25] Step [75/2250] Loss: 0.0827\n",
      "Epoch [4/25] Step [90/2250] Loss: 0.2068\n",
      "Epoch [4/25] Step [105/2250] Loss: 0.1327\n",
      "Epoch [4/25] Step [120/2250] Loss: 0.0157\n",
      "Epoch [4/25] Step [135/2250] Loss: 0.2135\n",
      "Epoch [4/25] Step [150/2250] Loss: 0.0275\n",
      "Epoch [4/25] Step [165/2250] Loss: 0.1903\n",
      "Epoch [4/25] Step [180/2250] Loss: 0.1093\n",
      "Epoch [4/25] Step [195/2250] Loss: 0.0650\n",
      "Epoch [4/25] Step [210/2250] Loss: 0.1342\n",
      "Epoch [4/25] Step [225/2250] Loss: 0.1715\n",
      "Epoch [4/25] Step [240/2250] Loss: 0.0668\n",
      "Epoch [4/25] Step [255/2250] Loss: 0.1761\n",
      "Epoch [4/25] Step [270/2250] Loss: 0.0546\n",
      "Epoch [4/25] Step [285/2250] Loss: 0.1186\n",
      "Epoch [4/25] Step [300/2250] Loss: 0.2424\n",
      "Epoch [4/25] Step [315/2250] Loss: 0.2634\n",
      "Epoch [4/25] Step [330/2250] Loss: 0.3363\n",
      "Epoch [4/25] Step [345/2250] Loss: 0.2422\n",
      "Epoch [4/25] Step [360/2250] Loss: 0.1141\n",
      "Epoch [4/25] Step [375/2250] Loss: 0.0501\n",
      "Epoch [4/25] Step [390/2250] Loss: 0.0205\n",
      "Epoch [4/25] Step [405/2250] Loss: 0.0967\n",
      "Epoch [4/25] Step [420/2250] Loss: 0.0310\n",
      "Epoch [4/25] Step [435/2250] Loss: 0.0509\n",
      "Epoch [4/25] Step [450/2250] Loss: 0.0243\n",
      "Epoch [4/25] Step [465/2250] Loss: 0.3097\n",
      "Epoch [4/25] Step [480/2250] Loss: 0.1139\n",
      "Epoch [4/25] Step [495/2250] Loss: 0.1882\n",
      "Epoch [4/25] Step [510/2250] Loss: 0.1032\n",
      "Epoch [4/25] Step [525/2250] Loss: 0.0876\n",
      "Epoch [4/25] Step [540/2250] Loss: 0.2400\n",
      "Epoch [4/25] Step [555/2250] Loss: 0.0471\n",
      "Epoch [4/25] Step [570/2250] Loss: 0.1161\n",
      "Epoch [4/25] Step [585/2250] Loss: 0.0870\n",
      "Epoch [4/25] Step [600/2250] Loss: 0.0994\n",
      "Epoch [4/25] Step [615/2250] Loss: 0.1825\n",
      "Epoch [4/25] Step [630/2250] Loss: 0.0971\n",
      "Epoch [4/25] Step [645/2250] Loss: 0.1019\n",
      "Epoch [4/25] Step [660/2250] Loss: 0.0799\n",
      "Epoch [4/25] Step [675/2250] Loss: 0.0962\n",
      "Epoch [4/25] Step [690/2250] Loss: 0.1161\n",
      "Epoch [4/25] Step [705/2250] Loss: 0.0780\n",
      "Epoch [4/25] Step [720/2250] Loss: 0.0932\n",
      "Epoch [4/25] Step [735/2250] Loss: 0.3183\n",
      "Epoch [4/25] Step [750/2250] Loss: 0.0279\n",
      "Epoch [4/25] Step [765/2250] Loss: 0.1552\n",
      "Epoch [4/25] Step [780/2250] Loss: 0.0770\n",
      "Epoch [4/25] Step [795/2250] Loss: 0.3218\n",
      "Epoch [4/25] Step [810/2250] Loss: 0.0967\n",
      "Epoch [4/25] Step [825/2250] Loss: 0.1052\n",
      "Epoch [4/25] Step [840/2250] Loss: 0.1508\n",
      "Epoch [4/25] Step [855/2250] Loss: 0.1324\n",
      "Epoch [4/25] Step [870/2250] Loss: 0.2407\n",
      "Epoch [4/25] Step [885/2250] Loss: 0.0292\n",
      "Epoch [4/25] Step [900/2250] Loss: 0.2249\n",
      "Epoch [4/25] Step [915/2250] Loss: 0.0682\n",
      "Epoch [4/25] Step [930/2250] Loss: 0.0969\n",
      "Epoch [4/25] Step [945/2250] Loss: 0.0441\n",
      "Epoch [4/25] Step [960/2250] Loss: 0.2782\n",
      "Epoch [4/25] Step [975/2250] Loss: 0.1634\n",
      "Epoch [4/25] Step [990/2250] Loss: 0.0519\n",
      "Epoch [4/25] Step [1005/2250] Loss: 0.1545\n",
      "Epoch [4/25] Step [1020/2250] Loss: 0.0699\n",
      "Epoch [4/25] Step [1035/2250] Loss: 0.1376\n",
      "Epoch [4/25] Step [1050/2250] Loss: 0.1512\n",
      "Epoch [4/25] Step [1065/2250] Loss: 0.1788\n",
      "Epoch [4/25] Step [1080/2250] Loss: 0.0438\n",
      "Epoch [4/25] Step [1095/2250] Loss: 0.2614\n",
      "Epoch [4/25] Step [1110/2250] Loss: 0.1468\n",
      "Epoch [4/25] Step [1125/2250] Loss: 0.0941\n",
      "Epoch [4/25] Step [1140/2250] Loss: 0.4604\n",
      "Epoch [4/25] Step [1155/2250] Loss: 0.0309\n",
      "Epoch [4/25] Step [1170/2250] Loss: 0.1573\n",
      "Epoch [4/25] Step [1185/2250] Loss: 0.2246\n",
      "Epoch [4/25] Step [1200/2250] Loss: 0.1552\n",
      "Epoch [4/25] Step [1215/2250] Loss: 0.1478\n",
      "Epoch [4/25] Step [1230/2250] Loss: 0.1380\n",
      "Epoch [4/25] Step [1245/2250] Loss: 0.1026\n",
      "Epoch [4/25] Step [1260/2250] Loss: 0.1161\n",
      "Epoch [4/25] Step [1275/2250] Loss: 0.1274\n",
      "Epoch [4/25] Step [1290/2250] Loss: 0.0401\n",
      "Epoch [4/25] Step [1305/2250] Loss: 0.1400\n",
      "Epoch [4/25] Step [1320/2250] Loss: 0.1726\n",
      "Epoch [4/25] Step [1335/2250] Loss: 0.1646\n",
      "Epoch [4/25] Step [1350/2250] Loss: 0.3157\n",
      "Epoch [4/25] Step [1365/2250] Loss: 0.0406\n",
      "Epoch [4/25] Step [1380/2250] Loss: 0.1032\n",
      "Epoch [4/25] Step [1395/2250] Loss: 0.1856\n",
      "Epoch [4/25] Step [1410/2250] Loss: 0.1528\n",
      "Epoch [4/25] Step [1425/2250] Loss: 0.1536\n",
      "Epoch [4/25] Step [1440/2250] Loss: 0.1258\n",
      "Epoch [4/25] Step [1455/2250] Loss: 0.0468\n",
      "Epoch [4/25] Step [1470/2250] Loss: 0.1603\n",
      "Epoch [4/25] Step [1485/2250] Loss: 0.0501\n",
      "Epoch [4/25] Step [1500/2250] Loss: 0.1842\n",
      "Epoch [4/25] Step [1515/2250] Loss: 0.1173\n",
      "Epoch [4/25] Step [1530/2250] Loss: 0.2264\n",
      "Epoch [4/25] Step [1545/2250] Loss: 0.3091\n",
      "Epoch [4/25] Step [1560/2250] Loss: 0.1048\n",
      "Epoch [4/25] Step [1575/2250] Loss: 0.2293\n",
      "Epoch [4/25] Step [1590/2250] Loss: 0.0635\n",
      "Epoch [4/25] Step [1605/2250] Loss: 0.0782\n",
      "Epoch [4/25] Step [1620/2250] Loss: 0.0681\n",
      "Epoch [4/25] Step [1635/2250] Loss: 0.0500\n",
      "Epoch [4/25] Step [1650/2250] Loss: 0.0776\n",
      "Epoch [4/25] Step [1665/2250] Loss: 0.1338\n",
      "Epoch [4/25] Step [1680/2250] Loss: 0.2181\n",
      "Epoch [4/25] Step [1695/2250] Loss: 0.1063\n",
      "Epoch [4/25] Step [1710/2250] Loss: 0.3008\n",
      "Epoch [4/25] Step [1725/2250] Loss: 0.1818\n",
      "Epoch [4/25] Step [1740/2250] Loss: 0.1481\n",
      "Epoch [4/25] Step [1755/2250] Loss: 0.1347\n",
      "Epoch [4/25] Step [1770/2250] Loss: 0.2805\n",
      "Epoch [4/25] Step [1785/2250] Loss: 0.0353\n",
      "Epoch [4/25] Step [1800/2250] Loss: 0.1041\n",
      "Epoch [4/25] Step [1815/2250] Loss: 0.1480\n",
      "Epoch [4/25] Step [1830/2250] Loss: 0.1753\n",
      "Epoch [4/25] Step [1845/2250] Loss: 0.1703\n",
      "Epoch [4/25] Step [1860/2250] Loss: 0.0407\n",
      "Epoch [4/25] Step [1875/2250] Loss: 0.2771\n",
      "Epoch [4/25] Step [1890/2250] Loss: 0.1190\n",
      "Epoch [4/25] Step [1905/2250] Loss: 0.1268\n",
      "Epoch [4/25] Step [1920/2250] Loss: 0.1722\n",
      "Epoch [4/25] Step [1935/2250] Loss: 0.1848\n",
      "Epoch [4/25] Step [1950/2250] Loss: 0.0437\n",
      "Epoch [4/25] Step [1965/2250] Loss: 0.0959\n",
      "Epoch [4/25] Step [1980/2250] Loss: 0.1482\n",
      "Epoch [4/25] Step [1995/2250] Loss: 0.4408\n",
      "Epoch [4/25] Step [2010/2250] Loss: 0.1138\n",
      "Epoch [4/25] Step [2025/2250] Loss: 0.0699\n",
      "Epoch [4/25] Step [2040/2250] Loss: 0.1423\n",
      "Epoch [4/25] Step [2055/2250] Loss: 0.1329\n",
      "Epoch [4/25] Step [2070/2250] Loss: 0.2057\n",
      "Epoch [4/25] Step [2085/2250] Loss: 0.0494\n",
      "Epoch [4/25] Step [2100/2250] Loss: 0.1091\n",
      "Epoch [4/25] Step [2115/2250] Loss: 0.1891\n",
      "Epoch [4/25] Step [2130/2250] Loss: 0.0567\n",
      "Epoch [4/25] Step [2145/2250] Loss: 0.0823\n",
      "Epoch [4/25] Step [2160/2250] Loss: 0.1557\n",
      "Epoch [4/25] Step [2175/2250] Loss: 0.2351\n",
      "Epoch [4/25] Step [2190/2250] Loss: 0.0747\n",
      "Epoch [4/25] Step [2205/2250] Loss: 0.1719\n",
      "Epoch [4/25] Step [2220/2250] Loss: 0.1790\n",
      "Epoch [4/25] Step [2235/2250] Loss: 0.4819\n",
      "Epoch [4/25] completed in 1763.23s\n",
      "Train Accuracy: 0.9460, Validation Accuracy: 0.9130\n",
      "\n",
      "Epoch [5/25] Step [0/2250] Loss: 0.0951\n",
      "Epoch [5/25] Step [15/2250] Loss: 0.1677\n",
      "Epoch [5/25] Step [30/2250] Loss: 0.1065\n",
      "Epoch [5/25] Step [45/2250] Loss: 0.0910\n",
      "Epoch [5/25] Step [60/2250] Loss: 0.2758\n",
      "Epoch [5/25] Step [75/2250] Loss: 0.0435\n",
      "Epoch [5/25] Step [90/2250] Loss: 0.0692\n",
      "Epoch [5/25] Step [105/2250] Loss: 0.1003\n",
      "Epoch [5/25] Step [120/2250] Loss: 0.2500\n",
      "Epoch [5/25] Step [135/2250] Loss: 0.0759\n",
      "Epoch [5/25] Step [150/2250] Loss: 0.2431\n",
      "Epoch [5/25] Step [165/2250] Loss: 0.1023\n",
      "Epoch [5/25] Step [180/2250] Loss: 0.1067\n",
      "Epoch [5/25] Step [195/2250] Loss: 0.0701\n",
      "Epoch [5/25] Step [210/2250] Loss: 0.1111\n",
      "Epoch [5/25] Step [225/2250] Loss: 0.0475\n",
      "Epoch [5/25] Step [240/2250] Loss: 0.0650\n",
      "Epoch [5/25] Step [255/2250] Loss: 0.0357\n",
      "Epoch [5/25] Step [270/2250] Loss: 0.0741\n",
      "Epoch [5/25] Step [285/2250] Loss: 0.0882\n",
      "Epoch [5/25] Step [300/2250] Loss: 0.0515\n",
      "Epoch [5/25] Step [315/2250] Loss: 0.1074\n",
      "Epoch [5/25] Step [330/2250] Loss: 0.0108\n",
      "Epoch [5/25] Step [345/2250] Loss: 0.0191\n",
      "Epoch [5/25] Step [360/2250] Loss: 0.2600\n",
      "Epoch [5/25] Step [375/2250] Loss: 0.0574\n",
      "Epoch [5/25] Step [390/2250] Loss: 0.0389\n",
      "Epoch [5/25] Step [405/2250] Loss: 0.1149\n",
      "Epoch [5/25] Step [420/2250] Loss: 0.0443\n",
      "Epoch [5/25] Step [435/2250] Loss: 0.0525\n",
      "Epoch [5/25] Step [450/2250] Loss: 0.1042\n",
      "Epoch [5/25] Step [465/2250] Loss: 0.2236\n",
      "Epoch [5/25] Step [480/2250] Loss: 0.0873\n",
      "Epoch [5/25] Step [495/2250] Loss: 0.0630\n",
      "Epoch [5/25] Step [510/2250] Loss: 0.1411\n",
      "Epoch [5/25] Step [525/2250] Loss: 0.0348\n",
      "Epoch [5/25] Step [540/2250] Loss: 0.0234\n",
      "Epoch [5/25] Step [555/2250] Loss: 0.1270\n",
      "Epoch [5/25] Step [570/2250] Loss: 0.1070\n",
      "Epoch [5/25] Step [585/2250] Loss: 0.2016\n",
      "Epoch [5/25] Step [600/2250] Loss: 0.1329\n",
      "Epoch [5/25] Step [615/2250] Loss: 0.1098\n",
      "Epoch [5/25] Step [630/2250] Loss: 0.1530\n",
      "Epoch [5/25] Step [645/2250] Loss: 0.1237\n",
      "Epoch [5/25] Step [660/2250] Loss: 0.0830\n",
      "Epoch [5/25] Step [675/2250] Loss: 0.0236\n",
      "Epoch [5/25] Step [690/2250] Loss: 0.0932\n",
      "Epoch [5/25] Step [705/2250] Loss: 0.1372\n",
      "Epoch [5/25] Step [720/2250] Loss: 0.1941\n",
      "Epoch [5/25] Step [735/2250] Loss: 0.0709\n",
      "Epoch [5/25] Step [750/2250] Loss: 0.0791\n",
      "Epoch [5/25] Step [765/2250] Loss: 0.0813\n",
      "Epoch [5/25] Step [780/2250] Loss: 0.1935\n",
      "Epoch [5/25] Step [795/2250] Loss: 0.0551\n",
      "Epoch [5/25] Step [810/2250] Loss: 0.0470\n",
      "Epoch [5/25] Step [825/2250] Loss: 0.1523\n",
      "Epoch [5/25] Step [840/2250] Loss: 0.3029\n",
      "Epoch [5/25] Step [855/2250] Loss: 0.0294\n",
      "Epoch [5/25] Step [870/2250] Loss: 0.1942\n",
      "Epoch [5/25] Step [885/2250] Loss: 0.2339\n",
      "Epoch [5/25] Step [900/2250] Loss: 0.1589\n",
      "Epoch [5/25] Step [915/2250] Loss: 0.0177\n",
      "Epoch [5/25] Step [930/2250] Loss: 0.1134\n",
      "Epoch [5/25] Step [945/2250] Loss: 0.0099\n",
      "Epoch [5/25] Step [960/2250] Loss: 0.1007\n",
      "Epoch [5/25] Step [975/2250] Loss: 0.0191\n",
      "Epoch [5/25] Step [990/2250] Loss: 0.0585\n",
      "Epoch [5/25] Step [1005/2250] Loss: 0.1778\n",
      "Epoch [5/25] Step [1020/2250] Loss: 0.0270\n",
      "Epoch [5/25] Step [1035/2250] Loss: 0.0581\n",
      "Epoch [5/25] Step [1050/2250] Loss: 0.0970\n",
      "Epoch [5/25] Step [1065/2250] Loss: 0.3116\n",
      "Epoch [5/25] Step [1080/2250] Loss: 0.0779\n",
      "Epoch [5/25] Step [1095/2250] Loss: 0.0555\n",
      "Epoch [5/25] Step [1110/2250] Loss: 0.1708\n",
      "Epoch [5/25] Step [1125/2250] Loss: 0.0640\n",
      "Epoch [5/25] Step [1140/2250] Loss: 0.1725\n",
      "Epoch [5/25] Step [1155/2250] Loss: 0.0717\n",
      "Epoch [5/25] Step [1170/2250] Loss: 0.0107\n",
      "Epoch [5/25] Step [1185/2250] Loss: 0.1060\n",
      "Epoch [5/25] Step [1200/2250] Loss: 0.5267\n",
      "Epoch [5/25] Step [1215/2250] Loss: 0.2279\n",
      "Epoch [5/25] Step [1230/2250] Loss: 0.1570\n",
      "Epoch [5/25] Step [1245/2250] Loss: 0.0788\n",
      "Epoch [5/25] Step [1260/2250] Loss: 0.0733\n",
      "Epoch [5/25] Step [1275/2250] Loss: 0.0703\n",
      "Epoch [5/25] Step [1290/2250] Loss: 0.0246\n",
      "Epoch [5/25] Step [1305/2250] Loss: 0.0487\n",
      "Epoch [5/25] Step [1320/2250] Loss: 0.1125\n",
      "Epoch [5/25] Step [1335/2250] Loss: 0.0441\n",
      "Epoch [5/25] Step [1350/2250] Loss: 0.2666\n",
      "Epoch [5/25] Step [1365/2250] Loss: 0.0395\n",
      "Epoch [5/25] Step [1380/2250] Loss: 0.0947\n",
      "Epoch [5/25] Step [1395/2250] Loss: 0.0826\n",
      "Epoch [5/25] Step [1410/2250] Loss: 0.1896\n",
      "Epoch [5/25] Step [1425/2250] Loss: 0.0443\n",
      "Epoch [5/25] Step [1440/2250] Loss: 0.1256\n",
      "Epoch [5/25] Step [1455/2250] Loss: 0.0613\n",
      "Epoch [5/25] Step [1470/2250] Loss: 0.0323\n",
      "Epoch [5/25] Step [1485/2250] Loss: 0.0667\n",
      "Epoch [5/25] Step [1500/2250] Loss: 0.0893\n",
      "Epoch [5/25] Step [1515/2250] Loss: 0.1610\n",
      "Epoch [5/25] Step [1530/2250] Loss: 0.2184\n",
      "Epoch [5/25] Step [1545/2250] Loss: 0.0136\n",
      "Epoch [5/25] Step [1560/2250] Loss: 0.2584\n",
      "Epoch [5/25] Step [1575/2250] Loss: 0.2045\n",
      "Epoch [5/25] Step [1590/2250] Loss: 0.2345\n",
      "Epoch [5/25] Step [1605/2250] Loss: 0.2231\n",
      "Epoch [5/25] Step [1620/2250] Loss: 0.1569\n",
      "Epoch [5/25] Step [1635/2250] Loss: 0.0643\n",
      "Epoch [5/25] Step [1650/2250] Loss: 0.0663\n",
      "Epoch [5/25] Step [1665/2250] Loss: 0.0166\n",
      "Epoch [5/25] Step [1680/2250] Loss: 0.2039\n",
      "Epoch [5/25] Step [1695/2250] Loss: 0.0892\n",
      "Epoch [5/25] Step [1710/2250] Loss: 0.0742\n",
      "Epoch [5/25] Step [1725/2250] Loss: 0.1472\n",
      "Epoch [5/25] Step [1740/2250] Loss: 0.1084\n",
      "Epoch [5/25] Step [1755/2250] Loss: 0.3281\n",
      "Epoch [5/25] Step [1770/2250] Loss: 0.2139\n",
      "Epoch [5/25] Step [1785/2250] Loss: 0.0395\n",
      "Epoch [5/25] Step [1800/2250] Loss: 0.0382\n",
      "Epoch [5/25] Step [1815/2250] Loss: 0.0792\n",
      "Epoch [5/25] Step [1830/2250] Loss: 0.0152\n",
      "Epoch [5/25] Step [1845/2250] Loss: 0.0704\n",
      "Epoch [5/25] Step [1860/2250] Loss: 0.2676\n",
      "Epoch [5/25] Step [1875/2250] Loss: 0.0078\n",
      "Epoch [5/25] Step [1890/2250] Loss: 0.1053\n",
      "Epoch [5/25] Step [1905/2250] Loss: 0.0918\n",
      "Epoch [5/25] Step [1920/2250] Loss: 0.1620\n",
      "Epoch [5/25] Step [1935/2250] Loss: 0.0862\n",
      "Epoch [5/25] Step [1950/2250] Loss: 0.1022\n",
      "Epoch [5/25] Step [1965/2250] Loss: 0.0881\n",
      "Epoch [5/25] Step [1980/2250] Loss: 0.0769\n",
      "Epoch [5/25] Step [1995/2250] Loss: 0.1503\n",
      "Epoch [5/25] Step [2010/2250] Loss: 0.1348\n",
      "Epoch [5/25] Step [2025/2250] Loss: 0.0740\n",
      "Epoch [5/25] Step [2040/2250] Loss: 0.0887\n",
      "Epoch [5/25] Step [2055/2250] Loss: 0.0174\n",
      "Epoch [5/25] Step [2070/2250] Loss: 0.0547\n",
      "Epoch [5/25] Step [2085/2250] Loss: 0.1778\n",
      "Epoch [5/25] Step [2100/2250] Loss: 0.1051\n",
      "Epoch [5/25] Step [2115/2250] Loss: 0.0960\n",
      "Epoch [5/25] Step [2130/2250] Loss: 0.0037\n",
      "Epoch [5/25] Step [2145/2250] Loss: 0.0124\n",
      "Epoch [5/25] Step [2160/2250] Loss: 0.1751\n",
      "Epoch [5/25] Step [2175/2250] Loss: 0.0580\n",
      "Epoch [5/25] Step [2190/2250] Loss: 0.0646\n",
      "Epoch [5/25] Step [2205/2250] Loss: 0.1969\n",
      "Epoch [5/25] Step [2220/2250] Loss: 0.2358\n",
      "Epoch [5/25] Step [2235/2250] Loss: 0.0494\n",
      "Epoch [5/25] completed in 1762.80s\n",
      "Train Accuracy: 0.9583, Validation Accuracy: 0.9312\n",
      "\n",
      "âœ… Best model updated and saved with Val Acc: 0.9312\n",
      "\n",
      "Epoch [6/25] Step [0/2250] Loss: 0.0368\n",
      "Epoch [6/25] Step [15/2250] Loss: 0.0660\n",
      "Epoch [6/25] Step [30/2250] Loss: 0.0488\n",
      "Epoch [6/25] Step [45/2250] Loss: 0.0260\n",
      "Epoch [6/25] Step [60/2250] Loss: 0.0271\n",
      "Epoch [6/25] Step [75/2250] Loss: 0.1196\n",
      "Epoch [6/25] Step [90/2250] Loss: 0.0168\n",
      "Epoch [6/25] Step [105/2250] Loss: 0.0042\n",
      "Epoch [6/25] Step [120/2250] Loss: 0.0523\n",
      "Epoch [6/25] Step [135/2250] Loss: 0.0336\n",
      "Epoch [6/25] Step [150/2250] Loss: 0.3144\n",
      "Epoch [6/25] Step [165/2250] Loss: 0.0865\n",
      "Epoch [6/25] Step [180/2250] Loss: 0.0733\n",
      "Epoch [6/25] Step [195/2250] Loss: 0.0274\n",
      "Epoch [6/25] Step [210/2250] Loss: 0.0305\n",
      "Epoch [6/25] Step [225/2250] Loss: 0.1037\n",
      "Epoch [6/25] Step [240/2250] Loss: 0.1187\n",
      "Epoch [6/25] Step [255/2250] Loss: 0.0122\n",
      "Epoch [6/25] Step [270/2250] Loss: 0.2830\n",
      "Epoch [6/25] Step [285/2250] Loss: 0.0801\n",
      "Epoch [6/25] Step [300/2250] Loss: 0.0651\n",
      "Epoch [6/25] Step [315/2250] Loss: 0.0699\n",
      "Epoch [6/25] Step [330/2250] Loss: 0.0810\n",
      "Epoch [6/25] Step [345/2250] Loss: 0.0450\n",
      "Epoch [6/25] Step [360/2250] Loss: 0.0377\n",
      "Epoch [6/25] Step [375/2250] Loss: 0.2051\n",
      "Epoch [6/25] Step [390/2250] Loss: 0.0284\n",
      "Epoch [6/25] Step [405/2250] Loss: 0.0033\n",
      "Epoch [6/25] Step [420/2250] Loss: 0.0112\n",
      "Epoch [6/25] Step [435/2250] Loss: 0.0190\n",
      "Epoch [6/25] Step [450/2250] Loss: 0.0060\n",
      "Epoch [6/25] Step [465/2250] Loss: 0.0802\n",
      "Epoch [6/25] Step [480/2250] Loss: 0.1287\n",
      "Epoch [6/25] Step [495/2250] Loss: 0.0139\n",
      "Epoch [6/25] Step [510/2250] Loss: 0.0779\n",
      "Epoch [6/25] Step [525/2250] Loss: 0.1658\n",
      "Epoch [6/25] Step [540/2250] Loss: 0.0021\n",
      "Epoch [6/25] Step [555/2250] Loss: 0.0174\n",
      "Epoch [6/25] Step [570/2250] Loss: 0.0355\n",
      "Epoch [6/25] Step [585/2250] Loss: 0.0647\n",
      "Epoch [6/25] Step [600/2250] Loss: 0.1615\n",
      "Epoch [6/25] Step [615/2250] Loss: 0.0285\n",
      "Epoch [6/25] Step [630/2250] Loss: 0.0385\n",
      "Epoch [6/25] Step [645/2250] Loss: 0.0154\n",
      "Epoch [6/25] Step [660/2250] Loss: 0.0778\n",
      "Epoch [6/25] Step [675/2250] Loss: 0.1704\n",
      "Epoch [6/25] Step [690/2250] Loss: 0.0254\n",
      "Epoch [6/25] Step [705/2250] Loss: 0.1974\n",
      "Epoch [6/25] Step [720/2250] Loss: 0.1598\n",
      "Epoch [6/25] Step [735/2250] Loss: 0.1461\n",
      "Epoch [6/25] Step [750/2250] Loss: 0.0713\n",
      "Epoch [6/25] Step [765/2250] Loss: 0.1513\n",
      "Epoch [6/25] Step [780/2250] Loss: 0.2253\n",
      "Epoch [6/25] Step [795/2250] Loss: 0.0683\n",
      "Epoch [6/25] Step [810/2250] Loss: 0.1437\n",
      "Epoch [6/25] Step [825/2250] Loss: 0.0393\n",
      "Epoch [6/25] Step [840/2250] Loss: 0.0577\n",
      "Epoch [6/25] Step [855/2250] Loss: 0.0760\n",
      "Epoch [6/25] Step [870/2250] Loss: 0.0164\n",
      "Epoch [6/25] Step [885/2250] Loss: 0.1231\n",
      "Epoch [6/25] Step [900/2250] Loss: 0.0214\n",
      "Epoch [6/25] Step [915/2250] Loss: 0.1205\n",
      "Epoch [6/25] Step [930/2250] Loss: 0.0192\n",
      "Epoch [6/25] Step [945/2250] Loss: 0.0621\n",
      "Epoch [6/25] Step [960/2250] Loss: 0.0554\n",
      "Epoch [6/25] Step [975/2250] Loss: 0.0770\n",
      "Epoch [6/25] Step [990/2250] Loss: 0.2252\n",
      "Epoch [6/25] Step [1005/2250] Loss: 0.0564\n",
      "Epoch [6/25] Step [1020/2250] Loss: 0.1273\n",
      "Epoch [6/25] Step [1035/2250] Loss: 0.1277\n",
      "Epoch [6/25] Step [1050/2250] Loss: 0.0292\n",
      "Epoch [6/25] Step [1065/2250] Loss: 0.0338\n",
      "Epoch [6/25] Step [1080/2250] Loss: 0.0502\n",
      "Epoch [6/25] Step [1095/2250] Loss: 0.1757\n",
      "Epoch [6/25] Step [1110/2250] Loss: 0.0448\n",
      "Epoch [6/25] Step [1125/2250] Loss: 0.1615\n",
      "Epoch [6/25] Step [1140/2250] Loss: 0.1725\n",
      "Epoch [6/25] Step [1155/2250] Loss: 0.0250\n",
      "Epoch [6/25] Step [1170/2250] Loss: 0.0441\n",
      "Epoch [6/25] Step [1185/2250] Loss: 0.1455\n",
      "Epoch [6/25] Step [1200/2250] Loss: 0.2012\n",
      "Epoch [6/25] Step [1215/2250] Loss: 0.1248\n",
      "Epoch [6/25] Step [1230/2250] Loss: 0.0824\n",
      "Epoch [6/25] Step [1245/2250] Loss: 0.0394\n",
      "Epoch [6/25] Step [1260/2250] Loss: 0.2114\n",
      "Epoch [6/25] Step [1275/2250] Loss: 0.1031\n",
      "Epoch [6/25] Step [1290/2250] Loss: 0.1921\n",
      "Epoch [6/25] Step [1305/2250] Loss: 0.0526\n",
      "Epoch [6/25] Step [1320/2250] Loss: 0.0658\n",
      "Epoch [6/25] Step [1335/2250] Loss: 0.0310\n",
      "Epoch [6/25] Step [1350/2250] Loss: 0.0441\n",
      "Epoch [6/25] Step [1365/2250] Loss: 0.0854\n",
      "Epoch [6/25] Step [1380/2250] Loss: 0.0819\n",
      "Epoch [6/25] Step [1395/2250] Loss: 0.0315\n",
      "Epoch [6/25] Step [1410/2250] Loss: 0.0858\n",
      "Epoch [6/25] Step [1425/2250] Loss: 0.1426\n",
      "Epoch [6/25] Step [1440/2250] Loss: 0.0158\n",
      "Epoch [6/25] Step [1455/2250] Loss: 0.0374\n",
      "Epoch [6/25] Step [1470/2250] Loss: 0.0516\n",
      "Epoch [6/25] Step [1485/2250] Loss: 0.1628\n",
      "Epoch [6/25] Step [1500/2250] Loss: 0.0688\n",
      "Epoch [6/25] Step [1515/2250] Loss: 0.1272\n",
      "Epoch [6/25] Step [1530/2250] Loss: 0.0244\n",
      "Epoch [6/25] Step [1545/2250] Loss: 0.0345\n",
      "Epoch [6/25] Step [1560/2250] Loss: 0.0976\n",
      "Epoch [6/25] Step [1575/2250] Loss: 0.1855\n",
      "Epoch [6/25] Step [1590/2250] Loss: 0.3524\n",
      "Epoch [6/25] Step [1605/2250] Loss: 0.1093\n",
      "Epoch [6/25] Step [1620/2250] Loss: 0.0111\n",
      "Epoch [6/25] Step [1635/2250] Loss: 0.0498\n",
      "Epoch [6/25] Step [1650/2250] Loss: 0.1989\n",
      "Epoch [6/25] Step [1665/2250] Loss: 0.1030\n",
      "Epoch [6/25] Step [1680/2250] Loss: 0.1200\n",
      "Epoch [6/25] Step [1695/2250] Loss: 0.0278\n",
      "Epoch [6/25] Step [1710/2250] Loss: 0.1258\n",
      "Epoch [6/25] Step [1725/2250] Loss: 0.0505\n",
      "Epoch [6/25] Step [1740/2250] Loss: 0.0656\n",
      "Epoch [6/25] Step [1755/2250] Loss: 0.0384\n",
      "Epoch [6/25] Step [1770/2250] Loss: 0.2071\n",
      "Epoch [6/25] Step [1785/2250] Loss: 0.2107\n",
      "Epoch [6/25] Step [1800/2250] Loss: 0.0648\n",
      "Epoch [6/25] Step [1815/2250] Loss: 0.0974\n",
      "Epoch [6/25] Step [1830/2250] Loss: 0.0182\n",
      "Epoch [6/25] Step [1845/2250] Loss: 0.0024\n",
      "Epoch [6/25] Step [1860/2250] Loss: 0.0532\n",
      "Epoch [6/25] Step [1875/2250] Loss: 0.0841\n",
      "Epoch [6/25] Step [1890/2250] Loss: 0.0788\n",
      "Epoch [6/25] Step [1905/2250] Loss: 0.3200\n",
      "Epoch [6/25] Step [1920/2250] Loss: 0.0438\n",
      "Epoch [6/25] Step [1935/2250] Loss: 0.0839\n",
      "Epoch [6/25] Step [1950/2250] Loss: 0.2048\n",
      "Epoch [6/25] Step [1965/2250] Loss: 0.1141\n",
      "Epoch [6/25] Step [1980/2250] Loss: 0.0615\n",
      "Epoch [6/25] Step [1995/2250] Loss: 0.1938\n",
      "Epoch [6/25] Step [2010/2250] Loss: 0.0435\n",
      "Epoch [6/25] Step [2025/2250] Loss: 0.0406\n",
      "Epoch [6/25] Step [2040/2250] Loss: 0.0427\n",
      "Epoch [6/25] Step [2055/2250] Loss: 0.0637\n",
      "Epoch [6/25] Step [2070/2250] Loss: 0.0843\n",
      "Epoch [6/25] Step [2085/2250] Loss: 0.0700\n",
      "Epoch [6/25] Step [2100/2250] Loss: 0.0033\n",
      "Epoch [6/25] Step [2115/2250] Loss: 0.0365\n",
      "Epoch [6/25] Step [2130/2250] Loss: 0.0730\n",
      "Epoch [6/25] Step [2145/2250] Loss: 0.2630\n",
      "Epoch [6/25] Step [2160/2250] Loss: 0.1005\n",
      "Epoch [6/25] Step [2175/2250] Loss: 0.1183\n",
      "Epoch [6/25] Step [2190/2250] Loss: 0.0498\n",
      "Epoch [6/25] Step [2205/2250] Loss: 0.0962\n",
      "Epoch [6/25] Step [2220/2250] Loss: 0.1600\n",
      "Epoch [6/25] Step [2235/2250] Loss: 0.0416\n",
      "Epoch [6/25] completed in 1763.62s\n",
      "Train Accuracy: 0.9683, Validation Accuracy: 0.9380\n",
      "\n",
      "âœ… Best model updated and saved with Val Acc: 0.9380\n",
      "\n",
      "Epoch [7/25] Step [0/2250] Loss: 0.0857\n",
      "Epoch [7/25] Step [15/2250] Loss: 0.0585\n",
      "Epoch [7/25] Step [30/2250] Loss: 0.1738\n",
      "Epoch [7/25] Step [45/2250] Loss: 0.0093\n",
      "Epoch [7/25] Step [60/2250] Loss: 0.0322\n",
      "Epoch [7/25] Step [75/2250] Loss: 0.0128\n",
      "Epoch [7/25] Step [90/2250] Loss: 0.1084\n",
      "Epoch [7/25] Step [105/2250] Loss: 0.0209\n",
      "Epoch [7/25] Step [120/2250] Loss: 0.0693\n",
      "Epoch [7/25] Step [135/2250] Loss: 0.1032\n",
      "Epoch [7/25] Step [150/2250] Loss: 0.0257\n",
      "Epoch [7/25] Step [165/2250] Loss: 0.0612\n",
      "Epoch [7/25] Step [180/2250] Loss: 0.0156\n",
      "Epoch [7/25] Step [195/2250] Loss: 0.0526\n",
      "Epoch [7/25] Step [210/2250] Loss: 0.0415\n",
      "Epoch [7/25] Step [225/2250] Loss: 0.0708\n",
      "Epoch [7/25] Step [240/2250] Loss: 0.0157\n",
      "Epoch [7/25] Step [255/2250] Loss: 0.2442\n",
      "Epoch [7/25] Step [270/2250] Loss: 0.0282\n",
      "Epoch [7/25] Step [285/2250] Loss: 0.0149\n",
      "Epoch [7/25] Step [300/2250] Loss: 0.0086\n",
      "Epoch [7/25] Step [315/2250] Loss: 0.0220\n",
      "Epoch [7/25] Step [330/2250] Loss: 0.0461\n",
      "Epoch [7/25] Step [345/2250] Loss: 0.0268\n",
      "Epoch [7/25] Step [360/2250] Loss: 0.2129\n",
      "Epoch [7/25] Step [375/2250] Loss: 0.0695\n",
      "Epoch [7/25] Step [390/2250] Loss: 0.0862\n",
      "Epoch [7/25] Step [405/2250] Loss: 0.0951\n",
      "Epoch [7/25] Step [420/2250] Loss: 0.0540\n",
      "Epoch [7/25] Step [435/2250] Loss: 0.0242\n",
      "Epoch [7/25] Step [450/2250] Loss: 0.0312\n",
      "Epoch [7/25] Step [465/2250] Loss: 0.0264\n",
      "Epoch [7/25] Step [480/2250] Loss: 0.0188\n",
      "Epoch [7/25] Step [495/2250] Loss: 0.0260\n",
      "Epoch [7/25] Step [510/2250] Loss: 0.0727\n",
      "Epoch [7/25] Step [525/2250] Loss: 0.1523\n",
      "Epoch [7/25] Step [540/2250] Loss: 0.0298\n",
      "Epoch [7/25] Step [555/2250] Loss: 0.0553\n",
      "Epoch [7/25] Step [570/2250] Loss: 0.0186\n",
      "Epoch [7/25] Step [585/2250] Loss: 0.0634\n",
      "Epoch [7/25] Step [600/2250] Loss: 0.2496\n",
      "Epoch [7/25] Step [615/2250] Loss: 0.0120\n",
      "Epoch [7/25] Step [630/2250] Loss: 0.0164\n",
      "Epoch [7/25] Step [645/2250] Loss: 0.0359\n",
      "Epoch [7/25] Step [660/2250] Loss: 0.0117\n",
      "Epoch [7/25] Step [675/2250] Loss: 0.0276\n",
      "Epoch [7/25] Step [690/2250] Loss: 0.0174\n",
      "Epoch [7/25] Step [705/2250] Loss: 0.2806\n",
      "Epoch [7/25] Step [720/2250] Loss: 0.0144\n",
      "Epoch [7/25] Step [735/2250] Loss: 0.0439\n",
      "Epoch [7/25] Step [750/2250] Loss: 0.0421\n",
      "Epoch [7/25] Step [765/2250] Loss: 0.0462\n",
      "Epoch [7/25] Step [780/2250] Loss: 0.0346\n",
      "Epoch [7/25] Step [795/2250] Loss: 0.0050\n",
      "Epoch [7/25] Step [810/2250] Loss: 0.0836\n",
      "Epoch [7/25] Step [825/2250] Loss: 0.0607\n",
      "Epoch [7/25] Step [840/2250] Loss: 0.0984\n",
      "Epoch [7/25] Step [855/2250] Loss: 0.0050\n",
      "Epoch [7/25] Step [870/2250] Loss: 0.0808\n",
      "Epoch [7/25] Step [885/2250] Loss: 0.0126\n",
      "Epoch [7/25] Step [900/2250] Loss: 0.0672\n",
      "Epoch [7/25] Step [915/2250] Loss: 0.0046\n",
      "Epoch [7/25] Step [930/2250] Loss: 0.1022\n",
      "Epoch [7/25] Step [945/2250] Loss: 0.0585\n",
      "Epoch [7/25] Step [960/2250] Loss: 0.0595\n",
      "Epoch [7/25] Step [975/2250] Loss: 0.0300\n",
      "Epoch [7/25] Step [990/2250] Loss: 0.0723\n",
      "Epoch [7/25] Step [1005/2250] Loss: 0.0043\n",
      "Epoch [7/25] Step [1020/2250] Loss: 0.0181\n",
      "Epoch [7/25] Step [1035/2250] Loss: 0.0283\n",
      "Epoch [7/25] Step [1050/2250] Loss: 0.0874\n",
      "Epoch [7/25] Step [1065/2250] Loss: 0.0460\n",
      "Epoch [7/25] Step [1080/2250] Loss: 0.0626\n",
      "Epoch [7/25] Step [1095/2250] Loss: 0.0077\n",
      "Epoch [7/25] Step [1110/2250] Loss: 0.0298\n",
      "Epoch [7/25] Step [1125/2250] Loss: 0.1426\n",
      "Epoch [7/25] Step [1140/2250] Loss: 0.0081\n",
      "Epoch [7/25] Step [1155/2250] Loss: 0.0854\n",
      "Epoch [7/25] Step [1170/2250] Loss: 0.0747\n",
      "Epoch [7/25] Step [1185/2250] Loss: 0.0813\n",
      "Epoch [7/25] Step [1200/2250] Loss: 0.0885\n",
      "Epoch [7/25] Step [1215/2250] Loss: 0.0462\n",
      "Epoch [7/25] Step [1230/2250] Loss: 0.1944\n",
      "Epoch [7/25] Step [1245/2250] Loss: 0.0661\n",
      "Epoch [7/25] Step [1260/2250] Loss: 0.1556\n",
      "Epoch [7/25] Step [1275/2250] Loss: 0.0442\n",
      "Epoch [7/25] Step [1290/2250] Loss: 0.0487\n",
      "Epoch [7/25] Step [1305/2250] Loss: 0.1626\n",
      "Epoch [7/25] Step [1320/2250] Loss: 0.0619\n",
      "Epoch [7/25] Step [1335/2250] Loss: 0.0378\n",
      "Epoch [7/25] Step [1350/2250] Loss: 0.1587\n",
      "Epoch [7/25] Step [1365/2250] Loss: 0.0137\n",
      "Epoch [7/25] Step [1380/2250] Loss: 0.0424\n",
      "Epoch [7/25] Step [1395/2250] Loss: 0.0396\n",
      "Epoch [7/25] Step [1410/2250] Loss: 0.0389\n",
      "Epoch [7/25] Step [1425/2250] Loss: 0.0568\n",
      "Epoch [7/25] Step [1440/2250] Loss: 0.0006\n",
      "Epoch [7/25] Step [1455/2250] Loss: 0.0230\n",
      "Epoch [7/25] Step [1470/2250] Loss: 0.0109\n",
      "Epoch [7/25] Step [1485/2250] Loss: 0.0075\n",
      "Epoch [7/25] Step [1500/2250] Loss: 0.2563\n",
      "Epoch [7/25] Step [1515/2250] Loss: 0.0578\n",
      "Epoch [7/25] Step [1530/2250] Loss: 0.0786\n",
      "Epoch [7/25] Step [1545/2250] Loss: 0.1588\n",
      "Epoch [7/25] Step [1560/2250] Loss: 0.0033\n",
      "Epoch [7/25] Step [1575/2250] Loss: 0.0110\n",
      "Epoch [7/25] Step [1590/2250] Loss: 0.0043\n",
      "Epoch [7/25] Step [1605/2250] Loss: 0.0172\n",
      "Epoch [7/25] Step [1620/2250] Loss: 0.0295\n",
      "Epoch [7/25] Step [1635/2250] Loss: 0.0547\n",
      "Epoch [7/25] Step [1650/2250] Loss: 0.1063\n",
      "Epoch [7/25] Step [1665/2250] Loss: 0.0400\n",
      "Epoch [7/25] Step [1680/2250] Loss: 0.0394\n",
      "Epoch [7/25] Step [1695/2250] Loss: 0.0453\n",
      "Epoch [7/25] Step [1710/2250] Loss: 0.1858\n",
      "Epoch [7/25] Step [1725/2250] Loss: 0.0820\n",
      "Epoch [7/25] Step [1740/2250] Loss: 0.1328\n",
      "Epoch [7/25] Step [1755/2250] Loss: 0.0523\n",
      "Epoch [7/25] Step [1770/2250] Loss: 0.0384\n",
      "Epoch [7/25] Step [1785/2250] Loss: 0.1495\n",
      "Epoch [7/25] Step [1800/2250] Loss: 0.0369\n",
      "Epoch [7/25] Step [1815/2250] Loss: 0.1446\n",
      "Epoch [7/25] Step [1830/2250] Loss: 0.0107\n",
      "Epoch [7/25] Step [1845/2250] Loss: 0.0432\n",
      "Epoch [7/25] Step [1860/2250] Loss: 0.0232\n",
      "Epoch [7/25] Step [1875/2250] Loss: 0.0062\n",
      "Epoch [7/25] Step [1890/2250] Loss: 0.0499\n",
      "Epoch [7/25] Step [1905/2250] Loss: 0.0223\n",
      "Epoch [7/25] Step [1920/2250] Loss: 0.0180\n",
      "Epoch [7/25] Step [1935/2250] Loss: 0.2287\n",
      "Epoch [7/25] Step [1950/2250] Loss: 0.0328\n",
      "Epoch [7/25] Step [1965/2250] Loss: 0.0641\n",
      "Epoch [7/25] Step [1980/2250] Loss: 0.0923\n",
      "Epoch [7/25] Step [1995/2250] Loss: 0.1496\n",
      "Epoch [7/25] Step [2010/2250] Loss: 0.0135\n",
      "Epoch [7/25] Step [2025/2250] Loss: 0.0359\n",
      "Epoch [7/25] Step [2040/2250] Loss: 0.0302\n",
      "Epoch [7/25] Step [2055/2250] Loss: 0.1532\n",
      "Epoch [7/25] Step [2070/2250] Loss: 0.3327\n",
      "Epoch [7/25] Step [2085/2250] Loss: 0.0851\n",
      "Epoch [7/25] Step [2100/2250] Loss: 0.0749\n",
      "Epoch [7/25] Step [2115/2250] Loss: 0.0570\n",
      "Epoch [7/25] Step [2130/2250] Loss: 0.1356\n",
      "Epoch [7/25] Step [2145/2250] Loss: 0.0121\n",
      "Epoch [7/25] Step [2160/2250] Loss: 0.0583\n",
      "Epoch [7/25] Step [2175/2250] Loss: 0.0713\n",
      "Epoch [7/25] Step [2190/2250] Loss: 0.0245\n",
      "Epoch [7/25] Step [2205/2250] Loss: 0.1258\n",
      "Epoch [7/25] Step [2220/2250] Loss: 0.1210\n",
      "Epoch [7/25] Step [2235/2250] Loss: 0.0531\n",
      "Epoch [7/25] completed in 1763.12s\n",
      "Train Accuracy: 0.9735, Validation Accuracy: 0.9394\n",
      "\n",
      "âœ… Best model updated and saved with Val Acc: 0.9394\n",
      "\n",
      "Epoch [8/25] Step [0/2250] Loss: 0.0144\n",
      "Epoch [8/25] Step [15/2250] Loss: 0.0574\n",
      "Epoch [8/25] Step [30/2250] Loss: 0.3271\n",
      "Epoch [8/25] Step [45/2250] Loss: 0.0513\n",
      "Epoch [8/25] Step [60/2250] Loss: 0.0449\n",
      "Epoch [8/25] Step [75/2250] Loss: 0.2397\n",
      "Epoch [8/25] Step [90/2250] Loss: 0.2990\n",
      "Epoch [8/25] Step [105/2250] Loss: 0.0284\n",
      "Epoch [8/25] Step [120/2250] Loss: 0.0504\n",
      "Epoch [8/25] Step [135/2250] Loss: 0.0067\n",
      "Epoch [8/25] Step [150/2250] Loss: 0.0284\n",
      "Epoch [8/25] Step [165/2250] Loss: 0.0198\n",
      "Epoch [8/25] Step [180/2250] Loss: 0.0138\n",
      "Epoch [8/25] Step [195/2250] Loss: 0.0583\n",
      "Epoch [8/25] Step [210/2250] Loss: 0.1178\n",
      "Epoch [8/25] Step [225/2250] Loss: 0.0616\n",
      "Epoch [8/25] Step [240/2250] Loss: 0.0159\n",
      "Epoch [8/25] Step [255/2250] Loss: 0.1927\n",
      "Epoch [8/25] Step [270/2250] Loss: 0.0173\n",
      "Epoch [8/25] Step [285/2250] Loss: 0.0287\n",
      "Epoch [8/25] Step [300/2250] Loss: 0.0030\n",
      "Epoch [8/25] Step [315/2250] Loss: 0.0172\n",
      "Epoch [8/25] Step [330/2250] Loss: 0.0055\n",
      "Epoch [8/25] Step [345/2250] Loss: 0.0084\n",
      "Epoch [8/25] Step [360/2250] Loss: 0.0184\n",
      "Epoch [8/25] Step [375/2250] Loss: 0.1429\n",
      "Epoch [8/25] Step [390/2250] Loss: 0.0170\n",
      "Epoch [8/25] Step [405/2250] Loss: 0.0541\n",
      "Epoch [8/25] Step [420/2250] Loss: 0.1267\n",
      "Epoch [8/25] Step [435/2250] Loss: 0.0073\n",
      "Epoch [8/25] Step [450/2250] Loss: 0.0650\n",
      "Epoch [8/25] Step [465/2250] Loss: 0.2682\n",
      "Epoch [8/25] Step [480/2250] Loss: 0.0334\n",
      "Epoch [8/25] Step [495/2250] Loss: 0.0192\n",
      "Epoch [8/25] Step [510/2250] Loss: 0.1019\n",
      "Epoch [8/25] Step [525/2250] Loss: 0.0202\n",
      "Epoch [8/25] Step [540/2250] Loss: 0.0044\n",
      "Epoch [8/25] Step [555/2250] Loss: 0.0810\n",
      "Epoch [8/25] Step [570/2250] Loss: 0.1606\n",
      "Epoch [8/25] Step [585/2250] Loss: 0.0714\n",
      "Epoch [8/25] Step [600/2250] Loss: 0.0225\n",
      "Epoch [8/25] Step [615/2250] Loss: 0.0438\n",
      "Epoch [8/25] Step [630/2250] Loss: 0.2113\n",
      "Epoch [8/25] Step [645/2250] Loss: 0.1433\n",
      "Epoch [8/25] Step [660/2250] Loss: 0.0112\n",
      "Epoch [8/25] Step [675/2250] Loss: 0.1751\n",
      "Epoch [8/25] Step [690/2250] Loss: 0.0869\n",
      "Epoch [8/25] Step [705/2250] Loss: 0.0287\n",
      "Epoch [8/25] Step [720/2250] Loss: 0.0215\n",
      "Epoch [8/25] Step [735/2250] Loss: 0.0069\n",
      "Epoch [8/25] Step [750/2250] Loss: 0.0693\n",
      "Epoch [8/25] Step [765/2250] Loss: 0.0179\n",
      "Epoch [8/25] Step [780/2250] Loss: 0.0771\n",
      "Epoch [8/25] Step [795/2250] Loss: 0.0996\n",
      "Epoch [8/25] Step [810/2250] Loss: 0.0264\n",
      "Epoch [8/25] Step [825/2250] Loss: 0.0541\n",
      "Epoch [8/25] Step [840/2250] Loss: 0.0277\n",
      "Epoch [8/25] Step [855/2250] Loss: 0.0494\n",
      "Epoch [8/25] Step [870/2250] Loss: 0.0335\n",
      "Epoch [8/25] Step [885/2250] Loss: 0.0120\n",
      "Epoch [8/25] Step [900/2250] Loss: 0.0265\n",
      "Epoch [8/25] Step [915/2250] Loss: 0.0871\n",
      "Epoch [8/25] Step [930/2250] Loss: 0.0733\n",
      "Epoch [8/25] Step [945/2250] Loss: 0.0040\n",
      "Epoch [8/25] Step [960/2250] Loss: 0.1614\n",
      "Epoch [8/25] Step [975/2250] Loss: 0.0905\n",
      "Epoch [8/25] Step [990/2250] Loss: 0.0772\n",
      "Epoch [8/25] Step [1005/2250] Loss: 0.0457\n",
      "Epoch [8/25] Step [1020/2250] Loss: 0.0218\n",
      "Epoch [8/25] Step [1035/2250] Loss: 0.1486\n",
      "Epoch [8/25] Step [1050/2250] Loss: 0.0325\n",
      "Epoch [8/25] Step [1065/2250] Loss: 0.0082\n",
      "Epoch [8/25] Step [1080/2250] Loss: 0.0154\n",
      "Epoch [8/25] Step [1095/2250] Loss: 0.0543\n",
      "Epoch [8/25] Step [1110/2250] Loss: 0.0220\n",
      "Epoch [8/25] Step [1125/2250] Loss: 0.0090\n",
      "Epoch [8/25] Step [1140/2250] Loss: 0.0056\n",
      "Epoch [8/25] Step [1155/2250] Loss: 0.0127\n",
      "Epoch [8/25] Step [1170/2250] Loss: 0.0437\n",
      "Epoch [8/25] Step [1185/2250] Loss: 0.0232\n",
      "Epoch [8/25] Step [1200/2250] Loss: 0.0341\n",
      "Epoch [8/25] Step [1215/2250] Loss: 0.1981\n",
      "Epoch [8/25] Step [1230/2250] Loss: 0.0323\n",
      "Epoch [8/25] Step [1245/2250] Loss: 0.0632\n",
      "Epoch [8/25] Step [1260/2250] Loss: 0.0766\n",
      "Epoch [8/25] Step [1275/2250] Loss: 0.0559\n",
      "Epoch [8/25] Step [1290/2250] Loss: 0.0407\n",
      "Epoch [8/25] Step [1305/2250] Loss: 0.0380\n",
      "Epoch [8/25] Step [1320/2250] Loss: 0.0370\n",
      "Epoch [8/25] Step [1335/2250] Loss: 0.0407\n",
      "Epoch [8/25] Step [1350/2250] Loss: 0.0781\n",
      "Epoch [8/25] Step [1365/2250] Loss: 0.2289\n",
      "Epoch [8/25] Step [1380/2250] Loss: 0.1146\n",
      "Epoch [8/25] Step [1395/2250] Loss: 0.0445\n",
      "Epoch [8/25] Step [1410/2250] Loss: 0.0761\n",
      "Epoch [8/25] Step [1425/2250] Loss: 0.0035\n",
      "Epoch [8/25] Step [1440/2250] Loss: 0.0022\n",
      "Epoch [8/25] Step [1455/2250] Loss: 0.0038\n",
      "Epoch [8/25] Step [1470/2250] Loss: 0.0107\n",
      "Epoch [8/25] Step [1485/2250] Loss: 0.0832\n",
      "Epoch [8/25] Step [1500/2250] Loss: 0.0481\n",
      "Epoch [8/25] Step [1515/2250] Loss: 0.0015\n",
      "Epoch [8/25] Step [1530/2250] Loss: 0.0801\n",
      "Epoch [8/25] Step [1545/2250] Loss: 0.0159\n",
      "Epoch [8/25] Step [1560/2250] Loss: 0.0575\n",
      "Epoch [8/25] Step [1575/2250] Loss: 0.0285\n",
      "Epoch [8/25] Step [1590/2250] Loss: 0.0582\n",
      "Epoch [8/25] Step [1605/2250] Loss: 0.0117\n",
      "Epoch [8/25] Step [1620/2250] Loss: 0.1121\n",
      "Epoch [8/25] Step [1635/2250] Loss: 0.0463\n",
      "Epoch [8/25] Step [1650/2250] Loss: 0.0471\n",
      "Epoch [8/25] Step [1665/2250] Loss: 0.0064\n",
      "Epoch [8/25] Step [1680/2250] Loss: 0.0683\n",
      "Epoch [8/25] Step [1695/2250] Loss: 0.0187\n",
      "Epoch [8/25] Step [1710/2250] Loss: 0.0458\n",
      "Epoch [8/25] Step [1725/2250] Loss: 0.0148\n",
      "Epoch [8/25] Step [1740/2250] Loss: 0.0192\n",
      "Epoch [8/25] Step [1755/2250] Loss: 0.0795\n",
      "Epoch [8/25] Step [1770/2250] Loss: 0.0217\n",
      "Epoch [8/25] Step [1785/2250] Loss: 0.2357\n",
      "Epoch [8/25] Step [1800/2250] Loss: 0.0323\n",
      "Epoch [8/25] Step [1815/2250] Loss: 0.1145\n",
      "Epoch [8/25] Step [1830/2250] Loss: 0.0336\n",
      "Epoch [8/25] Step [1845/2250] Loss: 0.0063\n",
      "Epoch [8/25] Step [1860/2250] Loss: 0.0069\n",
      "Epoch [8/25] Step [1875/2250] Loss: 0.0209\n",
      "Epoch [8/25] Step [1890/2250] Loss: 0.0170\n",
      "Epoch [8/25] Step [1905/2250] Loss: 0.2786\n",
      "Epoch [8/25] Step [1920/2250] Loss: 0.0742\n",
      "Epoch [8/25] Step [1935/2250] Loss: 0.0067\n",
      "Epoch [8/25] Step [1950/2250] Loss: 0.0166\n",
      "Epoch [8/25] Step [1965/2250] Loss: 0.0377\n",
      "Epoch [8/25] Step [1980/2250] Loss: 0.0197\n",
      "Epoch [8/25] Step [1995/2250] Loss: 0.0882\n",
      "Epoch [8/25] Step [2010/2250] Loss: 0.0004\n",
      "Epoch [8/25] Step [2025/2250] Loss: 0.2619\n",
      "Epoch [8/25] Step [2040/2250] Loss: 0.1999\n",
      "Epoch [8/25] Step [2055/2250] Loss: 0.0511\n",
      "Epoch [8/25] Step [2070/2250] Loss: 0.0504\n",
      "Epoch [8/25] Step [2085/2250] Loss: 0.0773\n",
      "Epoch [8/25] Step [2100/2250] Loss: 0.0368\n",
      "Epoch [8/25] Step [2115/2250] Loss: 0.0402\n",
      "Epoch [8/25] Step [2130/2250] Loss: 0.0144\n",
      "Epoch [8/25] Step [2145/2250] Loss: 0.2012\n",
      "Epoch [8/25] Step [2160/2250] Loss: 0.1224\n",
      "Epoch [8/25] Step [2175/2250] Loss: 0.0153\n",
      "Epoch [8/25] Step [2190/2250] Loss: 0.0524\n",
      "Epoch [8/25] Step [2205/2250] Loss: 0.0385\n",
      "Epoch [8/25] Step [2220/2250] Loss: 0.0053\n",
      "Epoch [8/25] Step [2235/2250] Loss: 0.0321\n",
      "Epoch [8/25] completed in 1762.51s\n",
      "Train Accuracy: 0.9782, Validation Accuracy: 0.9380\n",
      "\n",
      "Epoch [9/25] Step [0/2250] Loss: 0.0127\n",
      "Epoch [9/25] Step [15/2250] Loss: 0.0763\n",
      "Epoch [9/25] Step [30/2250] Loss: 0.0065\n",
      "Epoch [9/25] Step [45/2250] Loss: 0.1392\n",
      "Epoch [9/25] Step [60/2250] Loss: 0.0076\n",
      "Epoch [9/25] Step [75/2250] Loss: 0.0225\n",
      "Epoch [9/25] Step [90/2250] Loss: 0.2115\n",
      "Epoch [9/25] Step [105/2250] Loss: 0.0932\n",
      "Epoch [9/25] Step [120/2250] Loss: 0.0871\n",
      "Epoch [9/25] Step [135/2250] Loss: 0.0436\n",
      "Epoch [9/25] Step [150/2250] Loss: 0.0644\n",
      "Epoch [9/25] Step [165/2250] Loss: 0.0048\n",
      "Epoch [9/25] Step [180/2250] Loss: 0.0282\n",
      "Epoch [9/25] Step [195/2250] Loss: 0.0265\n",
      "Epoch [9/25] Step [210/2250] Loss: 0.0104\n",
      "Epoch [9/25] Step [225/2250] Loss: 0.0578\n",
      "Epoch [9/25] Step [240/2250] Loss: 0.0080\n",
      "Epoch [9/25] Step [255/2250] Loss: 0.0065\n",
      "Epoch [9/25] Step [270/2250] Loss: 0.0552\n",
      "Epoch [9/25] Step [285/2250] Loss: 0.0089\n",
      "Epoch [9/25] Step [300/2250] Loss: 0.2630\n",
      "Epoch [9/25] Step [315/2250] Loss: 0.0235\n",
      "Epoch [9/25] Step [330/2250] Loss: 0.0820\n",
      "Epoch [9/25] Step [345/2250] Loss: 0.1481\n",
      "Epoch [9/25] Step [360/2250] Loss: 0.0039\n",
      "Epoch [9/25] Step [375/2250] Loss: 0.0835\n",
      "Epoch [9/25] Step [390/2250] Loss: 0.0537\n",
      "Epoch [9/25] Step [405/2250] Loss: 0.0197\n",
      "Epoch [9/25] Step [420/2250] Loss: 0.0211\n",
      "Epoch [9/25] Step [435/2250] Loss: 0.0728\n",
      "Epoch [9/25] Step [450/2250] Loss: 0.0050\n",
      "Epoch [9/25] Step [465/2250] Loss: 0.1311\n",
      "Epoch [9/25] Step [480/2250] Loss: 0.0650\n",
      "Epoch [9/25] Step [495/2250] Loss: 0.0026\n",
      "Epoch [9/25] Step [510/2250] Loss: 0.0900\n",
      "Epoch [9/25] Step [525/2250] Loss: 0.1173\n",
      "Epoch [9/25] Step [540/2250] Loss: 0.0256\n",
      "Epoch [9/25] Step [555/2250] Loss: 0.0135\n",
      "Epoch [9/25] Step [570/2250] Loss: 0.0078\n",
      "Epoch [9/25] Step [585/2250] Loss: 0.0026\n",
      "Epoch [9/25] Step [600/2250] Loss: 0.0709\n",
      "Epoch [9/25] Step [615/2250] Loss: 0.0439\n",
      "Epoch [9/25] Step [630/2250] Loss: 0.0168\n",
      "Epoch [9/25] Step [645/2250] Loss: 0.0224\n",
      "Epoch [9/25] Step [660/2250] Loss: 0.0311\n",
      "Epoch [9/25] Step [675/2250] Loss: 0.0186\n",
      "Epoch [9/25] Step [690/2250] Loss: 0.0162\n",
      "Epoch [9/25] Step [705/2250] Loss: 0.0014\n",
      "Epoch [9/25] Step [720/2250] Loss: 0.0010\n",
      "Epoch [9/25] Step [735/2250] Loss: 0.0118\n",
      "Epoch [9/25] Step [750/2250] Loss: 0.0293\n",
      "Epoch [9/25] Step [765/2250] Loss: 0.0131\n",
      "Epoch [9/25] Step [780/2250] Loss: 0.1154\n",
      "Epoch [9/25] Step [795/2250] Loss: 0.0196\n",
      "Epoch [9/25] Step [810/2250] Loss: 0.1608\n",
      "Epoch [9/25] Step [825/2250] Loss: 0.0211\n",
      "Epoch [9/25] Step [840/2250] Loss: 0.0229\n",
      "Epoch [9/25] Step [855/2250] Loss: 0.0595\n",
      "Epoch [9/25] Step [870/2250] Loss: 0.0484\n",
      "Epoch [9/25] Step [885/2250] Loss: 0.0792\n",
      "Epoch [9/25] Step [900/2250] Loss: 0.0021\n",
      "Epoch [9/25] Step [915/2250] Loss: 0.0046\n",
      "Epoch [9/25] Step [930/2250] Loss: 0.0029\n",
      "Epoch [9/25] Step [945/2250] Loss: 0.0194\n",
      "Epoch [9/25] Step [960/2250] Loss: 0.0127\n",
      "Epoch [9/25] Step [975/2250] Loss: 0.0063\n",
      "Epoch [9/25] Step [990/2250] Loss: 0.1386\n",
      "Epoch [9/25] Step [1005/2250] Loss: 0.0366\n",
      "Epoch [9/25] Step [1020/2250] Loss: 0.0390\n",
      "Epoch [9/25] Step [1035/2250] Loss: 0.0345\n",
      "Epoch [9/25] Step [1050/2250] Loss: 0.0404\n",
      "Epoch [9/25] Step [1065/2250] Loss: 0.0061\n",
      "Epoch [9/25] Step [1080/2250] Loss: 0.0343\n",
      "Epoch [9/25] Step [1095/2250] Loss: 0.0490\n",
      "Epoch [9/25] Step [1110/2250] Loss: 0.2132\n",
      "Epoch [9/25] Step [1125/2250] Loss: 0.0031\n",
      "Epoch [9/25] Step [1140/2250] Loss: 0.0014\n",
      "Epoch [9/25] Step [1155/2250] Loss: 0.0647\n",
      "Epoch [9/25] Step [1170/2250] Loss: 0.0599\n",
      "Epoch [9/25] Step [1185/2250] Loss: 0.0165\n",
      "Epoch [9/25] Step [1200/2250] Loss: 0.0678\n",
      "Epoch [9/25] Step [1215/2250] Loss: 0.1280\n",
      "Epoch [9/25] Step [1230/2250] Loss: 0.0284\n",
      "Epoch [9/25] Step [1245/2250] Loss: 0.0752\n",
      "Epoch [9/25] Step [1260/2250] Loss: 0.0147\n",
      "Epoch [9/25] Step [1275/2250] Loss: 0.1078\n",
      "Epoch [9/25] Step [1290/2250] Loss: 0.2120\n",
      "Epoch [9/25] Step [1305/2250] Loss: 0.1060\n",
      "Epoch [9/25] Step [1320/2250] Loss: 0.0072\n",
      "Epoch [9/25] Step [1335/2250] Loss: 0.0547\n",
      "Epoch [9/25] Step [1350/2250] Loss: 0.0235\n",
      "Epoch [9/25] Step [1365/2250] Loss: 0.0151\n",
      "Epoch [9/25] Step [1380/2250] Loss: 0.0058\n",
      "Epoch [9/25] Step [1395/2250] Loss: 0.0366\n",
      "Epoch [9/25] Step [1410/2250] Loss: 0.0028\n",
      "Epoch [9/25] Step [1425/2250] Loss: 0.1362\n",
      "Epoch [9/25] Step [1440/2250] Loss: 0.0195\n",
      "Epoch [9/25] Step [1455/2250] Loss: 0.0083\n",
      "Epoch [9/25] Step [1470/2250] Loss: 0.0826\n",
      "Epoch [9/25] Step [1485/2250] Loss: 0.0532\n",
      "Epoch [9/25] Step [1500/2250] Loss: 0.0282\n",
      "Epoch [9/25] Step [1515/2250] Loss: 0.0747\n",
      "Epoch [9/25] Step [1530/2250] Loss: 0.0341\n",
      "Epoch [9/25] Step [1545/2250] Loss: 0.0173\n",
      "Epoch [9/25] Step [1560/2250] Loss: 0.0830\n",
      "Epoch [9/25] Step [1575/2250] Loss: 0.0008\n",
      "Epoch [9/25] Step [1590/2250] Loss: 0.0030\n",
      "Epoch [9/25] Step [1605/2250] Loss: 0.0083\n",
      "Epoch [9/25] Step [1620/2250] Loss: 0.1695\n",
      "Epoch [9/25] Step [1635/2250] Loss: 0.0153\n",
      "Epoch [9/25] Step [1650/2250] Loss: 0.1131\n",
      "Epoch [9/25] Step [1665/2250] Loss: 0.1112\n",
      "Epoch [9/25] Step [1680/2250] Loss: 0.1956\n",
      "Epoch [9/25] Step [1695/2250] Loss: 0.0060\n",
      "Epoch [9/25] Step [1710/2250] Loss: 0.0252\n",
      "Epoch [9/25] Step [1725/2250] Loss: 0.2184\n",
      "Epoch [9/25] Step [1740/2250] Loss: 0.0272\n",
      "Epoch [9/25] Step [1755/2250] Loss: 0.0026\n",
      "Epoch [9/25] Step [1770/2250] Loss: 0.0367\n",
      "Epoch [9/25] Step [1785/2250] Loss: 0.1526\n",
      "Epoch [9/25] Step [1800/2250] Loss: 0.2355\n",
      "Epoch [9/25] Step [1815/2250] Loss: 0.0615\n",
      "Epoch [9/25] Step [1830/2250] Loss: 0.0186\n",
      "Epoch [9/25] Step [1845/2250] Loss: 0.0242\n",
      "Epoch [9/25] Step [1860/2250] Loss: 0.0331\n",
      "Epoch [9/25] Step [1875/2250] Loss: 0.0039\n",
      "Epoch [9/25] Step [1890/2250] Loss: 0.1226\n",
      "Epoch [9/25] Step [1905/2250] Loss: 0.0867\n",
      "Epoch [9/25] Step [1920/2250] Loss: 0.1937\n",
      "Epoch [9/25] Step [1935/2250] Loss: 0.0123\n",
      "Epoch [9/25] Step [1950/2250] Loss: 0.0238\n",
      "Epoch [9/25] Step [1965/2250] Loss: 0.0453\n",
      "Epoch [9/25] Step [1980/2250] Loss: 0.0560\n",
      "Epoch [9/25] Step [1995/2250] Loss: 0.0865\n",
      "Epoch [9/25] Step [2010/2250] Loss: 0.0169\n",
      "Epoch [9/25] Step [2025/2250] Loss: 0.0391\n",
      "Epoch [9/25] Step [2040/2250] Loss: 0.0015\n",
      "Epoch [9/25] Step [2055/2250] Loss: 0.0423\n",
      "Epoch [9/25] Step [2070/2250] Loss: 0.0038\n",
      "Epoch [9/25] Step [2085/2250] Loss: 0.0566\n",
      "Epoch [9/25] Step [2100/2250] Loss: 0.0117\n",
      "Epoch [9/25] Step [2115/2250] Loss: 0.0177\n",
      "Epoch [9/25] Step [2130/2250] Loss: 0.0336\n",
      "Epoch [9/25] Step [2145/2250] Loss: 0.0576\n",
      "Epoch [9/25] Step [2160/2250] Loss: 0.0212\n",
      "Epoch [9/25] Step [2175/2250] Loss: 0.1086\n",
      "Epoch [9/25] Step [2190/2250] Loss: 0.0128\n",
      "Epoch [9/25] Step [2205/2250] Loss: 0.0586\n",
      "Epoch [9/25] Step [2220/2250] Loss: 0.0107\n",
      "Epoch [9/25] Step [2235/2250] Loss: 0.1802\n",
      "Epoch [9/25] completed in 1762.43s\n",
      "Train Accuracy: 0.9798, Validation Accuracy: 0.9354\n",
      "\n",
      "Epoch [10/25] Step [0/2250] Loss: 0.0402\n",
      "Epoch [10/25] Step [15/2250] Loss: 0.0527\n",
      "Epoch [10/25] Step [30/2250] Loss: 0.0023\n",
      "Epoch [10/25] Step [45/2250] Loss: 0.0035\n",
      "Epoch [10/25] Step [60/2250] Loss: 0.0192\n",
      "Epoch [10/25] Step [75/2250] Loss: 0.0133\n",
      "Epoch [10/25] Step [90/2250] Loss: 0.0146\n",
      "Epoch [10/25] Step [105/2250] Loss: 0.0057\n",
      "Epoch [10/25] Step [120/2250] Loss: 0.0050\n",
      "Epoch [10/25] Step [135/2250] Loss: 0.0025\n",
      "Epoch [10/25] Step [150/2250] Loss: 0.0129\n",
      "Epoch [10/25] Step [165/2250] Loss: 0.0564\n",
      "Epoch [10/25] Step [180/2250] Loss: 0.0227\n",
      "Epoch [10/25] Step [195/2250] Loss: 0.0108\n",
      "Epoch [10/25] Step [210/2250] Loss: 0.0209\n",
      "Epoch [10/25] Step [225/2250] Loss: 0.4712\n",
      "Epoch [10/25] Step [240/2250] Loss: 0.1163\n",
      "Epoch [10/25] Step [255/2250] Loss: 0.2166\n",
      "Epoch [10/25] Step [270/2250] Loss: 0.0647\n",
      "Epoch [10/25] Step [285/2250] Loss: 0.0180\n",
      "Epoch [10/25] Step [300/2250] Loss: 0.0039\n",
      "Epoch [10/25] Step [315/2250] Loss: 0.0139\n",
      "Epoch [10/25] Step [330/2250] Loss: 0.1757\n",
      "Epoch [10/25] Step [345/2250] Loss: 0.0017\n",
      "Epoch [10/25] Step [360/2250] Loss: 0.0665\n",
      "Epoch [10/25] Step [375/2250] Loss: 0.0160\n",
      "Epoch [10/25] Step [390/2250] Loss: 0.0118\n",
      "Epoch [10/25] Step [405/2250] Loss: 0.0019\n",
      "Epoch [10/25] Step [420/2250] Loss: 0.0007\n",
      "Epoch [10/25] Step [435/2250] Loss: 0.0138\n",
      "Epoch [10/25] Step [450/2250] Loss: 0.0053\n",
      "Epoch [10/25] Step [465/2250] Loss: 0.0217\n",
      "Epoch [10/25] Step [480/2250] Loss: 0.0111\n",
      "Epoch [10/25] Step [495/2250] Loss: 0.0017\n",
      "Epoch [10/25] Step [510/2250] Loss: 0.0059\n",
      "Epoch [10/25] Step [525/2250] Loss: 0.0025\n",
      "Epoch [10/25] Step [540/2250] Loss: 0.0056\n",
      "Epoch [10/25] Step [555/2250] Loss: 0.0353\n",
      "Epoch [10/25] Step [570/2250] Loss: 0.0293\n",
      "Epoch [10/25] Step [585/2250] Loss: 0.0082\n",
      "Epoch [10/25] Step [600/2250] Loss: 0.1951\n",
      "Epoch [10/25] Step [615/2250] Loss: 0.0245\n",
      "Epoch [10/25] Step [630/2250] Loss: 0.0578\n",
      "Epoch [10/25] Step [645/2250] Loss: 0.0403\n",
      "Epoch [10/25] Step [660/2250] Loss: 0.0544\n",
      "Epoch [10/25] Step [675/2250] Loss: 0.0061\n",
      "Epoch [10/25] Step [690/2250] Loss: 0.0052\n",
      "Epoch [10/25] Step [705/2250] Loss: 0.0272\n",
      "Epoch [10/25] Step [720/2250] Loss: 0.0076\n",
      "Epoch [10/25] Step [735/2250] Loss: 0.0205\n",
      "Epoch [10/25] Step [750/2250] Loss: 0.0035\n",
      "Epoch [10/25] Step [765/2250] Loss: 0.0008\n",
      "Epoch [10/25] Step [780/2250] Loss: 0.0057\n",
      "Epoch [10/25] Step [795/2250] Loss: 0.0516\n",
      "Epoch [10/25] Step [810/2250] Loss: 0.0080\n",
      "Epoch [10/25] Step [825/2250] Loss: 0.0149\n",
      "Epoch [10/25] Step [840/2250] Loss: 0.0264\n",
      "Epoch [10/25] Step [855/2250] Loss: 0.0047\n",
      "Epoch [10/25] Step [870/2250] Loss: 0.0186\n",
      "Epoch [10/25] Step [885/2250] Loss: 0.1636\n",
      "Epoch [10/25] Step [900/2250] Loss: 0.0172\n",
      "Epoch [10/25] Step [915/2250] Loss: 0.1321\n",
      "Epoch [10/25] Step [930/2250] Loss: 0.0056\n",
      "Epoch [10/25] Step [945/2250] Loss: 0.0497\n",
      "Epoch [10/25] Step [960/2250] Loss: 0.1010\n",
      "Epoch [10/25] Step [975/2250] Loss: 0.1448\n",
      "Epoch [10/25] Step [990/2250] Loss: 0.0175\n",
      "Epoch [10/25] Step [1005/2250] Loss: 0.0516\n",
      "Epoch [10/25] Step [1020/2250] Loss: 0.0408\n",
      "Epoch [10/25] Step [1035/2250] Loss: 0.0718\n",
      "Epoch [10/25] Step [1050/2250] Loss: 0.0268\n",
      "Epoch [10/25] Step [1065/2250] Loss: 0.0046\n",
      "Epoch [10/25] Step [1080/2250] Loss: 0.0588\n",
      "Epoch [10/25] Step [1095/2250] Loss: 0.0176\n",
      "Epoch [10/25] Step [1110/2250] Loss: 0.0291\n",
      "Epoch [10/25] Step [1125/2250] Loss: 0.0791\n",
      "Epoch [10/25] Step [1140/2250] Loss: 0.1478\n",
      "Epoch [10/25] Step [1155/2250] Loss: 0.3664\n",
      "Epoch [10/25] Step [1170/2250] Loss: 0.0030\n",
      "Epoch [10/25] Step [1185/2250] Loss: 0.1421\n",
      "Epoch [10/25] Step [1200/2250] Loss: 0.0102\n",
      "Epoch [10/25] Step [1215/2250] Loss: 0.0187\n",
      "Epoch [10/25] Step [1230/2250] Loss: 0.0046\n",
      "Epoch [10/25] Step [1245/2250] Loss: 0.1209\n",
      "Epoch [10/25] Step [1260/2250] Loss: 0.0495\n",
      "Epoch [10/25] Step [1275/2250] Loss: 0.0099\n",
      "Epoch [10/25] Step [1290/2250] Loss: 0.0141\n",
      "Epoch [10/25] Step [1305/2250] Loss: 0.0193\n",
      "Epoch [10/25] Step [1320/2250] Loss: 0.0144\n",
      "Epoch [10/25] Step [1335/2250] Loss: 0.0686\n",
      "Epoch [10/25] Step [1350/2250] Loss: 0.0282\n",
      "Epoch [10/25] Step [1365/2250] Loss: 0.0275\n",
      "Epoch [10/25] Step [1380/2250] Loss: 0.0244\n",
      "Epoch [10/25] Step [1395/2250] Loss: 0.0033\n",
      "Epoch [10/25] Step [1410/2250] Loss: 0.1184\n",
      "Epoch [10/25] Step [1425/2250] Loss: 0.0432\n",
      "Epoch [10/25] Step [1440/2250] Loss: 0.0171\n",
      "Epoch [10/25] Step [1455/2250] Loss: 0.0122\n",
      "Epoch [10/25] Step [1470/2250] Loss: 0.0084\n",
      "Epoch [10/25] Step [1485/2250] Loss: 0.0281\n",
      "Epoch [10/25] Step [1500/2250] Loss: 0.0385\n",
      "Epoch [10/25] Step [1515/2250] Loss: 0.0632\n",
      "Epoch [10/25] Step [1530/2250] Loss: 0.0817\n",
      "Epoch [10/25] Step [1545/2250] Loss: 0.0152\n",
      "Epoch [10/25] Step [1560/2250] Loss: 0.1133\n",
      "Epoch [10/25] Step [1575/2250] Loss: 0.0692\n",
      "Epoch [10/25] Step [1590/2250] Loss: 0.0028\n",
      "Epoch [10/25] Step [1605/2250] Loss: 0.0398\n",
      "Epoch [10/25] Step [1620/2250] Loss: 0.0016\n",
      "Epoch [10/25] Step [1635/2250] Loss: 0.0491\n",
      "Epoch [10/25] Step [1650/2250] Loss: 0.0034\n",
      "Epoch [10/25] Step [1665/2250] Loss: 0.1748\n",
      "Epoch [10/25] Step [1680/2250] Loss: 0.0088\n",
      "Epoch [10/25] Step [1695/2250] Loss: 0.0237\n",
      "Epoch [10/25] Step [1710/2250] Loss: 0.1408\n",
      "Epoch [10/25] Step [1725/2250] Loss: 0.0646\n",
      "Epoch [10/25] Step [1740/2250] Loss: 0.1663\n",
      "Epoch [10/25] Step [1755/2250] Loss: 0.1112\n",
      "Epoch [10/25] Step [1770/2250] Loss: 0.2193\n",
      "Epoch [10/25] Step [1785/2250] Loss: 0.0013\n",
      "Epoch [10/25] Step [1800/2250] Loss: 0.0235\n",
      "Epoch [10/25] Step [1815/2250] Loss: 0.0303\n",
      "Epoch [10/25] Step [1830/2250] Loss: 0.0103\n",
      "Epoch [10/25] Step [1845/2250] Loss: 0.0141\n",
      "Epoch [10/25] Step [1860/2250] Loss: 0.1054\n",
      "Epoch [10/25] Step [1875/2250] Loss: 0.0589\n",
      "Epoch [10/25] Step [1890/2250] Loss: 0.0194\n",
      "Epoch [10/25] Step [1905/2250] Loss: 0.1232\n",
      "Epoch [10/25] Step [1920/2250] Loss: 0.0390\n",
      "Epoch [10/25] Step [1935/2250] Loss: 0.0660\n",
      "Epoch [10/25] Step [1950/2250] Loss: 0.0182\n",
      "Epoch [10/25] Step [1965/2250] Loss: 0.0511\n",
      "Epoch [10/25] Step [1980/2250] Loss: 0.0132\n",
      "Epoch [10/25] Step [1995/2250] Loss: 0.0149\n",
      "Epoch [10/25] Step [2010/2250] Loss: 0.0169\n",
      "Epoch [10/25] Step [2025/2250] Loss: 0.0108\n",
      "Epoch [10/25] Step [2040/2250] Loss: 0.0589\n",
      "Epoch [10/25] Step [2055/2250] Loss: 0.0319\n",
      "Epoch [10/25] Step [2070/2250] Loss: 0.0521\n",
      "Epoch [10/25] Step [2085/2250] Loss: 0.0454\n",
      "Epoch [10/25] Step [2100/2250] Loss: 0.0330\n",
      "Epoch [10/25] Step [2115/2250] Loss: 0.0040\n",
      "Epoch [10/25] Step [2130/2250] Loss: 0.0795\n",
      "Epoch [10/25] Step [2145/2250] Loss: 0.0134\n",
      "Epoch [10/25] Step [2160/2250] Loss: 0.0239\n",
      "Epoch [10/25] Step [2175/2250] Loss: 0.0983\n",
      "Epoch [10/25] Step [2190/2250] Loss: 0.0093\n",
      "Epoch [10/25] Step [2205/2250] Loss: 0.0147\n",
      "Epoch [10/25] Step [2220/2250] Loss: 0.1313\n",
      "Epoch [10/25] Step [2235/2250] Loss: 0.0577\n",
      "Epoch [10/25] completed in 1927.48s\n",
      "Train Accuracy: 0.9828, Validation Accuracy: 0.9379\n",
      "\n",
      "Epoch [11/25] Step [0/2250] Loss: 0.0561\n",
      "Epoch [11/25] Step [15/2250] Loss: 0.0670\n",
      "Epoch [11/25] Step [30/2250] Loss: 0.1085\n",
      "Epoch [11/25] Step [45/2250] Loss: 0.0085\n",
      "Epoch [11/25] Step [60/2250] Loss: 0.0187\n",
      "Epoch [11/25] Step [75/2250] Loss: 0.0217\n",
      "Epoch [11/25] Step [90/2250] Loss: 0.1764\n",
      "Epoch [11/25] Step [105/2250] Loss: 0.0047\n",
      "Epoch [11/25] Step [120/2250] Loss: 0.0466\n",
      "Epoch [11/25] Step [135/2250] Loss: 0.0463\n",
      "Epoch [11/25] Step [150/2250] Loss: 0.0103\n",
      "Epoch [11/25] Step [165/2250] Loss: 0.0088\n",
      "Epoch [11/25] Step [180/2250] Loss: 0.0010\n",
      "Epoch [11/25] Step [195/2250] Loss: 0.0457\n",
      "Epoch [11/25] Step [210/2250] Loss: 0.0038\n",
      "Epoch [11/25] Step [225/2250] Loss: 0.0079\n",
      "Epoch [11/25] Step [240/2250] Loss: 0.0009\n",
      "Epoch [11/25] Step [255/2250] Loss: 0.0802\n",
      "Epoch [11/25] Step [270/2250] Loss: 0.0018\n",
      "Epoch [11/25] Step [285/2250] Loss: 0.0003\n",
      "Epoch [11/25] Step [300/2250] Loss: 0.0465\n",
      "Epoch [11/25] Step [315/2250] Loss: 0.0061\n",
      "Epoch [11/25] Step [330/2250] Loss: 0.0192\n",
      "Epoch [11/25] Step [345/2250] Loss: 0.0215\n",
      "Epoch [11/25] Step [360/2250] Loss: 0.0163\n",
      "Epoch [11/25] Step [375/2250] Loss: 0.0002\n",
      "Epoch [11/25] Step [390/2250] Loss: 0.0278\n",
      "Epoch [11/25] Step [405/2250] Loss: 0.1216\n",
      "Epoch [11/25] Step [420/2250] Loss: 0.0071\n",
      "Epoch [11/25] Step [435/2250] Loss: 0.0883\n",
      "Epoch [11/25] Step [450/2250] Loss: 0.0054\n",
      "Epoch [11/25] Step [465/2250] Loss: 0.0012\n",
      "Epoch [11/25] Step [480/2250] Loss: 0.0087\n",
      "Epoch [11/25] Step [495/2250] Loss: 0.0827\n",
      "Epoch [11/25] Step [510/2250] Loss: 0.1854\n",
      "Epoch [11/25] Step [525/2250] Loss: 0.1157\n",
      "Epoch [11/25] Step [540/2250] Loss: 0.0032\n",
      "Epoch [11/25] Step [555/2250] Loss: 0.0086\n",
      "Epoch [11/25] Step [570/2250] Loss: 0.1174\n",
      "Epoch [11/25] Step [585/2250] Loss: 0.0301\n",
      "Epoch [11/25] Step [600/2250] Loss: 0.0029\n",
      "Epoch [11/25] Step [615/2250] Loss: 0.1434\n",
      "Epoch [11/25] Step [630/2250] Loss: 0.0422\n",
      "Epoch [11/25] Step [645/2250] Loss: 0.0094\n",
      "Epoch [11/25] Step [660/2250] Loss: 0.0768\n",
      "Epoch [11/25] Step [675/2250] Loss: 0.0645\n",
      "Epoch [11/25] Step [690/2250] Loss: 0.0661\n",
      "Epoch [11/25] Step [705/2250] Loss: 0.0387\n",
      "Epoch [11/25] Step [720/2250] Loss: 0.0006\n",
      "Epoch [11/25] Step [735/2250] Loss: 0.0671\n",
      "Epoch [11/25] Step [750/2250] Loss: 0.0424\n",
      "Epoch [11/25] Step [765/2250] Loss: 0.0220\n",
      "Epoch [11/25] Step [780/2250] Loss: 0.0185\n",
      "Epoch [11/25] Step [795/2250] Loss: 0.0385\n",
      "Epoch [11/25] Step [810/2250] Loss: 0.0588\n",
      "Epoch [11/25] Step [825/2250] Loss: 0.0222\n",
      "Epoch [11/25] Step [840/2250] Loss: 0.0139\n",
      "Epoch [11/25] Step [855/2250] Loss: 0.0023\n",
      "Epoch [11/25] Step [870/2250] Loss: 0.0130\n",
      "Epoch [11/25] Step [885/2250] Loss: 0.0038\n",
      "Epoch [11/25] Step [900/2250] Loss: 0.0210\n",
      "Epoch [11/25] Step [915/2250] Loss: 0.0044\n",
      "Epoch [11/25] Step [930/2250] Loss: 0.1659\n",
      "Epoch [11/25] Step [945/2250] Loss: 0.0347\n",
      "Epoch [11/25] Step [960/2250] Loss: 0.0027\n",
      "Epoch [11/25] Step [975/2250] Loss: 0.1504\n",
      "Epoch [11/25] Step [990/2250] Loss: 0.0128\n",
      "Epoch [11/25] Step [1005/2250] Loss: 0.0026\n",
      "Epoch [11/25] Step [1020/2250] Loss: 0.1541\n",
      "Epoch [11/25] Step [1035/2250] Loss: 0.0054\n",
      "Epoch [11/25] Step [1050/2250] Loss: 0.0091\n",
      "Epoch [11/25] Step [1065/2250] Loss: 0.0409\n",
      "Epoch [11/25] Step [1080/2250] Loss: 0.0682\n",
      "Epoch [11/25] Step [1095/2250] Loss: 0.0027\n",
      "Epoch [11/25] Step [1110/2250] Loss: 0.0381\n",
      "Epoch [11/25] Step [1125/2250] Loss: 0.0133\n",
      "Epoch [11/25] Step [1140/2250] Loss: 0.0586\n",
      "Epoch [11/25] Step [1155/2250] Loss: 0.0138\n",
      "Epoch [11/25] Step [1170/2250] Loss: 0.0703\n",
      "Epoch [11/25] Step [1185/2250] Loss: 0.0556\n",
      "Epoch [11/25] Step [1200/2250] Loss: 0.1579\n",
      "Epoch [11/25] Step [1215/2250] Loss: 0.0359\n",
      "Epoch [11/25] Step [1230/2250] Loss: 0.0113\n",
      "Epoch [11/25] Step [1245/2250] Loss: 0.0079\n",
      "Epoch [11/25] Step [1260/2250] Loss: 0.0213\n",
      "Epoch [11/25] Step [1275/2250] Loss: 0.0405\n",
      "Epoch [11/25] Step [1290/2250] Loss: 0.0042\n",
      "Epoch [11/25] Step [1305/2250] Loss: 0.1447\n",
      "Epoch [11/25] Step [1320/2250] Loss: 0.1335\n",
      "Epoch [11/25] Step [1335/2250] Loss: 0.0118\n",
      "Epoch [11/25] Step [1350/2250] Loss: 0.0132\n",
      "Epoch [11/25] Step [1365/2250] Loss: 0.0003\n",
      "Epoch [11/25] Step [1380/2250] Loss: 0.1187\n",
      "Epoch [11/25] Step [1395/2250] Loss: 0.0426\n",
      "Epoch [11/25] Step [1410/2250] Loss: 0.0362\n",
      "Epoch [11/25] Step [1425/2250] Loss: 0.2210\n",
      "Epoch [11/25] Step [1440/2250] Loss: 0.0484\n",
      "Epoch [11/25] Step [1455/2250] Loss: 0.0488\n",
      "Epoch [11/25] Step [1470/2250] Loss: 0.0724\n",
      "Epoch [11/25] Step [1485/2250] Loss: 0.1788\n",
      "Epoch [11/25] Step [1500/2250] Loss: 0.0417\n",
      "Epoch [11/25] Step [1515/2250] Loss: 0.0250\n",
      "Epoch [11/25] Step [1530/2250] Loss: 0.0145\n",
      "Epoch [11/25] Step [1545/2250] Loss: 0.1141\n",
      "Epoch [11/25] Step [1560/2250] Loss: 0.0671\n",
      "Epoch [11/25] Step [1575/2250] Loss: 0.0226\n",
      "Epoch [11/25] Step [1590/2250] Loss: 0.0240\n",
      "Epoch [11/25] Step [1605/2250] Loss: 0.0302\n",
      "Epoch [11/25] Step [1620/2250] Loss: 0.0350\n",
      "Epoch [11/25] Step [1635/2250] Loss: 0.0080\n",
      "Epoch [11/25] Step [1650/2250] Loss: 0.0143\n",
      "Epoch [11/25] Step [1665/2250] Loss: 0.0151\n",
      "Epoch [11/25] Step [1680/2250] Loss: 0.0408\n",
      "Epoch [11/25] Step [1695/2250] Loss: 0.0033\n",
      "Epoch [11/25] Step [1710/2250] Loss: 0.0238\n",
      "Epoch [11/25] Step [1725/2250] Loss: 0.0132\n",
      "Epoch [11/25] Step [1740/2250] Loss: 0.0084\n",
      "Epoch [11/25] Step [1755/2250] Loss: 0.0158\n",
      "Epoch [11/25] Step [1770/2250] Loss: 0.0115\n",
      "Epoch [11/25] Step [1785/2250] Loss: 0.0163\n",
      "Epoch [11/25] Step [1800/2250] Loss: 0.0026\n",
      "Epoch [11/25] Step [1815/2250] Loss: 0.1965\n",
      "Epoch [11/25] Step [1830/2250] Loss: 0.0072\n",
      "Epoch [11/25] Step [1845/2250] Loss: 0.0086\n",
      "Epoch [11/25] Step [1860/2250] Loss: 0.1078\n",
      "Epoch [11/25] Step [1875/2250] Loss: 0.0188\n",
      "Epoch [11/25] Step [1890/2250] Loss: 0.0759\n",
      "Epoch [11/25] Step [1905/2250] Loss: 0.0309\n",
      "Epoch [11/25] Step [1920/2250] Loss: 0.0007\n",
      "Epoch [11/25] Step [1935/2250] Loss: 0.1195\n",
      "Epoch [11/25] Step [1950/2250] Loss: 0.0480\n",
      "Epoch [11/25] Step [1965/2250] Loss: 0.0020\n",
      "Epoch [11/25] Step [1980/2250] Loss: 0.0034\n",
      "Epoch [11/25] Step [1995/2250] Loss: 0.0218\n",
      "Epoch [11/25] Step [2010/2250] Loss: 0.0020\n",
      "Epoch [11/25] Step [2025/2250] Loss: 0.0255\n",
      "Epoch [11/25] Step [2040/2250] Loss: 0.0086\n",
      "Epoch [11/25] Step [2055/2250] Loss: 0.0104\n",
      "Epoch [11/25] Step [2070/2250] Loss: 0.0007\n",
      "Epoch [11/25] Step [2085/2250] Loss: 0.0281\n",
      "Epoch [11/25] Step [2100/2250] Loss: 0.2179\n",
      "Epoch [11/25] Step [2115/2250] Loss: 0.0142\n",
      "Epoch [11/25] Step [2130/2250] Loss: 0.0201\n",
      "Epoch [11/25] Step [2145/2250] Loss: 0.0120\n",
      "Epoch [11/25] Step [2160/2250] Loss: 0.0162\n",
      "Epoch [11/25] Step [2175/2250] Loss: 0.0253\n",
      "Epoch [11/25] Step [2190/2250] Loss: 0.0653\n",
      "Epoch [11/25] Step [2205/2250] Loss: 0.0053\n",
      "Epoch [11/25] Step [2220/2250] Loss: 0.0112\n",
      "Epoch [11/25] Step [2235/2250] Loss: 0.0252\n",
      "Epoch [11/25] completed in 1997.98s\n",
      "Train Accuracy: 0.9844, Validation Accuracy: 0.9404\n",
      "\n",
      "âœ… Best model updated and saved with Val Acc: 0.9404\n",
      "\n",
      "Epoch [12/25] Step [0/2250] Loss: 0.0202\n",
      "Epoch [12/25] Step [15/2250] Loss: 0.0022\n",
      "Epoch [12/25] Step [30/2250] Loss: 0.0015\n",
      "Epoch [12/25] Step [45/2250] Loss: 0.0051\n",
      "Epoch [12/25] Step [60/2250] Loss: 0.0233\n",
      "Epoch [12/25] Step [75/2250] Loss: 0.0077\n",
      "Epoch [12/25] Step [90/2250] Loss: 0.0256\n",
      "Epoch [12/25] Step [105/2250] Loss: 0.0005\n",
      "Epoch [12/25] Step [120/2250] Loss: 0.1081\n",
      "Epoch [12/25] Step [135/2250] Loss: 0.0323\n",
      "Epoch [12/25] Step [150/2250] Loss: 0.0220\n",
      "Epoch [12/25] Step [165/2250] Loss: 0.0207\n",
      "Epoch [12/25] Step [180/2250] Loss: 0.0182\n",
      "Epoch [12/25] Step [195/2250] Loss: 0.0565\n",
      "Epoch [12/25] Step [210/2250] Loss: 0.0103\n",
      "Epoch [12/25] Step [225/2250] Loss: 0.0031\n",
      "Epoch [12/25] Step [240/2250] Loss: 0.0338\n",
      "Epoch [12/25] Step [255/2250] Loss: 0.0003\n",
      "Epoch [12/25] Step [270/2250] Loss: 0.0458\n",
      "Epoch [12/25] Step [285/2250] Loss: 0.0245\n",
      "Epoch [12/25] Step [300/2250] Loss: 0.0826\n",
      "Epoch [12/25] Step [315/2250] Loss: 0.0029\n",
      "Epoch [12/25] Step [330/2250] Loss: 0.0619\n",
      "Epoch [12/25] Step [345/2250] Loss: 0.0039\n",
      "Epoch [12/25] Step [360/2250] Loss: 0.0663\n",
      "Epoch [12/25] Step [375/2250] Loss: 0.0735\n",
      "Epoch [12/25] Step [390/2250] Loss: 0.0562\n",
      "Epoch [12/25] Step [405/2250] Loss: 0.0071\n",
      "Epoch [12/25] Step [420/2250] Loss: 0.1792\n",
      "Epoch [12/25] Step [435/2250] Loss: 0.2036\n",
      "Epoch [12/25] Step [450/2250] Loss: 0.0026\n",
      "Epoch [12/25] Step [465/2250] Loss: 0.0707\n",
      "Epoch [12/25] Step [480/2250] Loss: 0.0610\n",
      "Epoch [12/25] Step [495/2250] Loss: 0.0772\n",
      "Epoch [12/25] Step [510/2250] Loss: 0.0003\n",
      "Epoch [12/25] Step [525/2250] Loss: 0.0197\n",
      "Epoch [12/25] Step [540/2250] Loss: 0.0072\n",
      "Epoch [12/25] Step [555/2250] Loss: 0.1107\n",
      "Epoch [12/25] Step [570/2250] Loss: 0.0400\n",
      "Epoch [12/25] Step [585/2250] Loss: 0.1330\n",
      "Epoch [12/25] Step [600/2250] Loss: 0.0814\n",
      "Epoch [12/25] Step [615/2250] Loss: 0.0059\n",
      "Epoch [12/25] Step [630/2250] Loss: 0.0070\n",
      "Epoch [12/25] Step [645/2250] Loss: 0.0098\n",
      "Epoch [12/25] Step [660/2250] Loss: 0.0789\n",
      "Epoch [12/25] Step [675/2250] Loss: 0.0336\n",
      "Epoch [12/25] Step [690/2250] Loss: 0.0591\n",
      "Epoch [12/25] Step [705/2250] Loss: 0.0291\n",
      "Epoch [12/25] Step [720/2250] Loss: 0.0001\n",
      "Epoch [12/25] Step [735/2250] Loss: 0.0184\n",
      "Epoch [12/25] Step [750/2250] Loss: 0.0429\n",
      "Epoch [12/25] Step [765/2250] Loss: 0.0015\n",
      "Epoch [12/25] Step [780/2250] Loss: 0.0255\n",
      "Epoch [12/25] Step [795/2250] Loss: 0.0668\n",
      "Epoch [12/25] Step [810/2250] Loss: 0.2741\n",
      "Epoch [12/25] Step [825/2250] Loss: 0.0016\n",
      "Epoch [12/25] Step [840/2250] Loss: 0.0181\n",
      "Epoch [12/25] Step [855/2250] Loss: 0.0383\n",
      "Epoch [12/25] Step [870/2250] Loss: 0.1162\n",
      "Epoch [12/25] Step [885/2250] Loss: 0.0056\n",
      "Epoch [12/25] Step [900/2250] Loss: 0.0048\n",
      "Epoch [12/25] Step [915/2250] Loss: 0.0092\n",
      "Epoch [12/25] Step [930/2250] Loss: 0.0048\n",
      "Epoch [12/25] Step [945/2250] Loss: 0.0546\n",
      "Epoch [12/25] Step [960/2250] Loss: 0.0290\n",
      "Epoch [12/25] Step [975/2250] Loss: 0.0156\n",
      "Epoch [12/25] Step [990/2250] Loss: 0.0307\n",
      "Epoch [12/25] Step [1005/2250] Loss: 0.1150\n",
      "Epoch [12/25] Step [1020/2250] Loss: 0.0549\n",
      "Epoch [12/25] Step [1035/2250] Loss: 0.0033\n",
      "Epoch [12/25] Step [1050/2250] Loss: 0.0229\n",
      "Epoch [12/25] Step [1065/2250] Loss: 0.0381\n",
      "Epoch [12/25] Step [1080/2250] Loss: 0.0991\n",
      "Epoch [12/25] Step [1095/2250] Loss: 0.0656\n",
      "Epoch [12/25] Step [1110/2250] Loss: 0.0304\n",
      "Epoch [12/25] Step [1125/2250] Loss: 0.1620\n",
      "Epoch [12/25] Step [1140/2250] Loss: 0.0512\n",
      "Epoch [12/25] Step [1155/2250] Loss: 0.0036\n",
      "Epoch [12/25] Step [1170/2250] Loss: 0.0627\n",
      "Epoch [12/25] Step [1185/2250] Loss: 0.0061\n",
      "Epoch [12/25] Step [1200/2250] Loss: 0.0008\n",
      "Epoch [12/25] Step [1215/2250] Loss: 0.1497\n",
      "Epoch [12/25] Step [1230/2250] Loss: 0.0006\n",
      "Epoch [12/25] Step [1245/2250] Loss: 0.2048\n",
      "Epoch [12/25] Step [1260/2250] Loss: 0.0416\n",
      "Epoch [12/25] Step [1275/2250] Loss: 0.0035\n",
      "Epoch [12/25] Step [1290/2250] Loss: 0.0987\n",
      "Epoch [12/25] Step [1305/2250] Loss: 0.0760\n",
      "Epoch [12/25] Step [1320/2250] Loss: 0.0221\n",
      "Epoch [12/25] Step [1335/2250] Loss: 0.0111\n",
      "Epoch [12/25] Step [1350/2250] Loss: 0.0359\n",
      "Epoch [12/25] Step [1365/2250] Loss: 0.0049\n",
      "Epoch [12/25] Step [1380/2250] Loss: 0.0158\n",
      "Epoch [12/25] Step [1395/2250] Loss: 0.0233\n",
      "Epoch [12/25] Step [1410/2250] Loss: 0.1262\n",
      "Epoch [12/25] Step [1425/2250] Loss: 0.0035\n",
      "Epoch [12/25] Step [1440/2250] Loss: 0.0205\n",
      "Epoch [12/25] Step [1455/2250] Loss: 0.0061\n",
      "Epoch [12/25] Step [1470/2250] Loss: 0.0219\n",
      "Epoch [12/25] Step [1485/2250] Loss: 0.0005\n",
      "Epoch [12/25] Step [1500/2250] Loss: 0.0032\n",
      "Epoch [12/25] Step [1515/2250] Loss: 0.1037\n",
      "Epoch [12/25] Step [1530/2250] Loss: 0.0009\n",
      "Epoch [12/25] Step [1545/2250] Loss: 0.0096\n",
      "Epoch [12/25] Step [1560/2250] Loss: 0.0015\n",
      "Epoch [12/25] Step [1575/2250] Loss: 0.0012\n",
      "Epoch [12/25] Step [1590/2250] Loss: 0.0057\n",
      "Epoch [12/25] Step [1605/2250] Loss: 0.0329\n",
      "Epoch [12/25] Step [1620/2250] Loss: 0.0031\n",
      "Epoch [12/25] Step [1635/2250] Loss: 0.0257\n",
      "Epoch [12/25] Step [1650/2250] Loss: 0.0527\n",
      "Epoch [12/25] Step [1665/2250] Loss: 0.0767\n",
      "Epoch [12/25] Step [1680/2250] Loss: 0.0172\n",
      "Epoch [12/25] Step [1695/2250] Loss: 0.0070\n",
      "Epoch [12/25] Step [1710/2250] Loss: 0.0012\n",
      "Epoch [12/25] Step [1725/2250] Loss: 0.0467\n",
      "Epoch [12/25] Step [1740/2250] Loss: 0.0706\n",
      "Epoch [12/25] Step [1755/2250] Loss: 0.0006\n",
      "Epoch [12/25] Step [1770/2250] Loss: 0.0047\n",
      "Epoch [12/25] Step [1785/2250] Loss: 0.0104\n",
      "Epoch [12/25] Step [1800/2250] Loss: 0.0234\n",
      "Epoch [12/25] Step [1815/2250] Loss: 0.1215\n",
      "Epoch [12/25] Step [1830/2250] Loss: 0.0189\n",
      "Epoch [12/25] Step [1845/2250] Loss: 0.0044\n",
      "Epoch [12/25] Step [1860/2250] Loss: 0.3042\n",
      "Epoch [12/25] Step [1875/2250] Loss: 0.0018\n",
      "Epoch [12/25] Step [1890/2250] Loss: 0.0216\n",
      "Epoch [12/25] Step [1905/2250] Loss: 0.0120\n",
      "Epoch [12/25] Step [1920/2250] Loss: 0.0026\n",
      "Epoch [12/25] Step [1935/2250] Loss: 0.0029\n",
      "Epoch [12/25] Step [1950/2250] Loss: 0.0382\n",
      "Epoch [12/25] Step [1965/2250] Loss: 0.0412\n",
      "Epoch [12/25] Step [1980/2250] Loss: 0.0103\n",
      "Epoch [12/25] Step [1995/2250] Loss: 0.0023\n",
      "Epoch [12/25] Step [2010/2250] Loss: 0.0564\n",
      "Epoch [12/25] Step [2025/2250] Loss: 0.0469\n",
      "Epoch [12/25] Step [2040/2250] Loss: 0.0147\n",
      "Epoch [12/25] Step [2055/2250] Loss: 0.0805\n",
      "Epoch [12/25] Step [2070/2250] Loss: 0.0378\n",
      "Epoch [12/25] Step [2085/2250] Loss: 0.2500\n",
      "Epoch [12/25] Step [2100/2250] Loss: 0.0176\n",
      "Epoch [12/25] Step [2115/2250] Loss: 0.0092\n",
      "Epoch [12/25] Step [2130/2250] Loss: 0.0241\n",
      "Epoch [12/25] Step [2145/2250] Loss: 0.0302\n",
      "Epoch [12/25] Step [2160/2250] Loss: 0.0020\n",
      "Epoch [12/25] Step [2175/2250] Loss: 0.0094\n",
      "Epoch [12/25] Step [2190/2250] Loss: 0.0028\n",
      "Epoch [12/25] Step [2205/2250] Loss: 0.0482\n",
      "Epoch [12/25] Step [2220/2250] Loss: 0.0010\n",
      "Epoch [12/25] Step [2235/2250] Loss: 0.0493\n",
      "Epoch [12/25] completed in 2003.66s\n",
      "Train Accuracy: 0.9851, Validation Accuracy: 0.9375\n",
      "\n",
      "Epoch [13/25] Step [0/2250] Loss: 0.0059\n",
      "Epoch [13/25] Step [15/2250] Loss: 0.0598\n",
      "Epoch [13/25] Step [30/2250] Loss: 0.0053\n",
      "Epoch [13/25] Step [45/2250] Loss: 0.0083\n",
      "Epoch [13/25] Step [60/2250] Loss: 0.0069\n",
      "Epoch [13/25] Step [75/2250] Loss: 0.0032\n",
      "Epoch [13/25] Step [90/2250] Loss: 0.0115\n",
      "Epoch [13/25] Step [105/2250] Loss: 0.0242\n",
      "Epoch [13/25] Step [120/2250] Loss: 0.0351\n",
      "Epoch [13/25] Step [135/2250] Loss: 0.0682\n",
      "Epoch [13/25] Step [150/2250] Loss: 0.0230\n",
      "Epoch [13/25] Step [165/2250] Loss: 0.0007\n",
      "Epoch [13/25] Step [180/2250] Loss: 0.1011\n",
      "Epoch [13/25] Step [195/2250] Loss: 0.0999\n",
      "Epoch [13/25] Step [210/2250] Loss: 0.0004\n",
      "Epoch [13/25] Step [225/2250] Loss: 0.1923\n",
      "Epoch [13/25] Step [240/2250] Loss: 0.0652\n",
      "Epoch [13/25] Step [255/2250] Loss: 0.0026\n",
      "Epoch [13/25] Step [270/2250] Loss: 0.1067\n",
      "Epoch [13/25] Step [285/2250] Loss: 0.0042\n",
      "Epoch [13/25] Step [300/2250] Loss: 0.1389\n",
      "Epoch [13/25] Step [315/2250] Loss: 0.0030\n",
      "Epoch [13/25] Step [330/2250] Loss: 0.0213\n",
      "Epoch [13/25] Step [345/2250] Loss: 0.0002\n",
      "Epoch [13/25] Step [360/2250] Loss: 0.0004\n",
      "Epoch [13/25] Step [375/2250] Loss: 0.0198\n",
      "Epoch [13/25] Step [390/2250] Loss: 0.0030\n",
      "Epoch [13/25] Step [405/2250] Loss: 0.0015\n",
      "Epoch [13/25] Step [420/2250] Loss: 0.0016\n",
      "Epoch [13/25] Step [435/2250] Loss: 0.0112\n",
      "Epoch [13/25] Step [450/2250] Loss: 0.0082\n",
      "Epoch [13/25] Step [465/2250] Loss: 0.0275\n",
      "Epoch [13/25] Step [480/2250] Loss: 0.1079\n",
      "Epoch [13/25] Step [495/2250] Loss: 0.0004\n",
      "Epoch [13/25] Step [510/2250] Loss: 0.0176\n",
      "Epoch [13/25] Step [525/2250] Loss: 0.0323\n",
      "Epoch [13/25] Step [540/2250] Loss: 0.0145\n",
      "Epoch [13/25] Step [555/2250] Loss: 0.1679\n",
      "Epoch [13/25] Step [570/2250] Loss: 0.0079\n",
      "Epoch [13/25] Step [585/2250] Loss: 0.0398\n",
      "Epoch [13/25] Step [600/2250] Loss: 0.0029\n",
      "Epoch [13/25] Step [615/2250] Loss: 0.2982\n",
      "Epoch [13/25] Step [630/2250] Loss: 0.0105\n",
      "Epoch [13/25] Step [645/2250] Loss: 0.0384\n",
      "Epoch [13/25] Step [660/2250] Loss: 0.0438\n",
      "Epoch [13/25] Step [675/2250] Loss: 0.0076\n",
      "Epoch [13/25] Step [690/2250] Loss: 0.0513\n",
      "Epoch [13/25] Step [705/2250] Loss: 0.0152\n",
      "Epoch [13/25] Step [720/2250] Loss: 0.0129\n",
      "Epoch [13/25] Step [735/2250] Loss: 0.0220\n",
      "Epoch [13/25] Step [750/2250] Loss: 0.0073\n",
      "Epoch [13/25] Step [765/2250] Loss: 0.0034\n",
      "Epoch [13/25] Step [780/2250] Loss: 0.3503\n",
      "Epoch [13/25] Step [795/2250] Loss: 0.1263\n",
      "Epoch [13/25] Step [810/2250] Loss: 0.0236\n",
      "Epoch [13/25] Step [825/2250] Loss: 0.0018\n",
      "Epoch [13/25] Step [840/2250] Loss: 0.0441\n",
      "Epoch [13/25] Step [855/2250] Loss: 0.0132\n",
      "Epoch [13/25] Step [870/2250] Loss: 0.0346\n",
      "Epoch [13/25] Step [885/2250] Loss: 0.0817\n",
      "Epoch [13/25] Step [900/2250] Loss: 0.0499\n",
      "Epoch [13/25] Step [915/2250] Loss: 0.2565\n",
      "Epoch [13/25] Step [930/2250] Loss: 0.0041\n",
      "Epoch [13/25] Step [945/2250] Loss: 0.0067\n",
      "Epoch [13/25] Step [960/2250] Loss: 0.0004\n",
      "Epoch [13/25] Step [975/2250] Loss: 0.0057\n",
      "Epoch [13/25] Step [990/2250] Loss: 0.0629\n",
      "Epoch [13/25] Step [1005/2250] Loss: 0.1752\n",
      "Epoch [13/25] Step [1020/2250] Loss: 0.0043\n",
      "Epoch [13/25] Step [1035/2250] Loss: 0.0298\n",
      "Epoch [13/25] Step [1050/2250] Loss: 0.0245\n",
      "Epoch [13/25] Step [1065/2250] Loss: 0.0013\n",
      "Epoch [13/25] Step [1080/2250] Loss: 0.0008\n",
      "Epoch [13/25] Step [1095/2250] Loss: 0.0109\n",
      "Epoch [13/25] Step [1110/2250] Loss: 0.0765\n",
      "Epoch [13/25] Step [1125/2250] Loss: 0.0509\n",
      "Epoch [13/25] Step [1140/2250] Loss: 0.0022\n",
      "Epoch [13/25] Step [1155/2250] Loss: 0.0072\n",
      "Epoch [13/25] Step [1170/2250] Loss: 0.0076\n",
      "Epoch [13/25] Step [1185/2250] Loss: 0.2823\n",
      "Epoch [13/25] Step [1200/2250] Loss: 0.0481\n",
      "Epoch [13/25] Step [1215/2250] Loss: 0.0034\n",
      "Epoch [13/25] Step [1230/2250] Loss: 0.0026\n",
      "Epoch [13/25] Step [1245/2250] Loss: 0.0114\n",
      "Epoch [13/25] Step [1260/2250] Loss: 0.0191\n",
      "Epoch [13/25] Step [1275/2250] Loss: 0.0004\n",
      "Epoch [13/25] Step [1290/2250] Loss: 0.0054\n",
      "Epoch [13/25] Step [1305/2250] Loss: 0.0615\n",
      "Epoch [13/25] Step [1320/2250] Loss: 0.0623\n",
      "Epoch [13/25] Step [1335/2250] Loss: 0.0193\n",
      "Epoch [13/25] Step [1350/2250] Loss: 0.0074\n",
      "Epoch [13/25] Step [1365/2250] Loss: 0.0638\n",
      "Epoch [13/25] Step [1380/2250] Loss: 0.1924\n",
      "Epoch [13/25] Step [1395/2250] Loss: 0.0264\n",
      "Epoch [13/25] Step [1410/2250] Loss: 0.0728\n",
      "Epoch [13/25] Step [1425/2250] Loss: 0.0085\n",
      "Epoch [13/25] Step [1440/2250] Loss: 0.0008\n",
      "Epoch [13/25] Step [1455/2250] Loss: 0.0419\n",
      "Epoch [13/25] Step [1470/2250] Loss: 0.0652\n",
      "Epoch [13/25] Step [1485/2250] Loss: 0.0084\n",
      "Epoch [13/25] Step [1500/2250] Loss: 0.0024\n",
      "Epoch [13/25] Step [1515/2250] Loss: 0.0001\n",
      "Epoch [13/25] Step [1530/2250] Loss: 0.2327\n",
      "Epoch [13/25] Step [1545/2250] Loss: 0.0424\n",
      "Epoch [13/25] Step [1560/2250] Loss: 0.0010\n",
      "Epoch [13/25] Step [1575/2250] Loss: 0.0020\n",
      "Epoch [13/25] Step [1590/2250] Loss: 0.0209\n",
      "Epoch [13/25] Step [1605/2250] Loss: 0.0003\n",
      "Epoch [13/25] Step [1620/2250] Loss: 0.0005\n",
      "Epoch [13/25] Step [1635/2250] Loss: 0.0153\n",
      "Epoch [13/25] Step [1650/2250] Loss: 0.0132\n",
      "Epoch [13/25] Step [1665/2250] Loss: 0.0900\n",
      "Epoch [13/25] Step [1680/2250] Loss: 0.0199\n",
      "Epoch [13/25] Step [1695/2250] Loss: 0.0169\n",
      "Epoch [13/25] Step [1710/2250] Loss: 0.0325\n",
      "Epoch [13/25] Step [1725/2250] Loss: 0.1706\n",
      "Epoch [13/25] Step [1740/2250] Loss: 0.0883\n",
      "Epoch [13/25] Step [1755/2250] Loss: 0.0146\n",
      "Epoch [13/25] Step [1770/2250] Loss: 0.0889\n",
      "Epoch [13/25] Step [1785/2250] Loss: 0.0190\n",
      "Epoch [13/25] Step [1800/2250] Loss: 0.1151\n",
      "Epoch [13/25] Step [1815/2250] Loss: 0.0053\n",
      "Epoch [13/25] Step [1830/2250] Loss: 0.0321\n",
      "Epoch [13/25] Step [1845/2250] Loss: 0.0124\n",
      "Epoch [13/25] Step [1860/2250] Loss: 0.0044\n",
      "Epoch [13/25] Step [1875/2250] Loss: 0.0094\n",
      "Epoch [13/25] Step [1890/2250] Loss: 0.0285\n",
      "Epoch [13/25] Step [1905/2250] Loss: 0.0014\n",
      "Epoch [13/25] Step [1920/2250] Loss: 0.0123\n",
      "Epoch [13/25] Step [1935/2250] Loss: 0.0088\n",
      "Epoch [13/25] Step [1950/2250] Loss: 0.0070\n",
      "Epoch [13/25] Step [1965/2250] Loss: 0.0189\n",
      "Epoch [13/25] Step [1980/2250] Loss: 0.0130\n",
      "Epoch [13/25] Step [1995/2250] Loss: 0.1043\n",
      "Epoch [13/25] Step [2010/2250] Loss: 0.0507\n",
      "Epoch [13/25] Step [2025/2250] Loss: 0.0027\n",
      "Epoch [13/25] Step [2040/2250] Loss: 0.0692\n",
      "Epoch [13/25] Step [2055/2250] Loss: 0.1246\n",
      "Epoch [13/25] Step [2070/2250] Loss: 0.0188\n",
      "Epoch [13/25] Step [2085/2250] Loss: 0.0206\n",
      "Epoch [13/25] Step [2100/2250] Loss: 0.0080\n",
      "Epoch [13/25] Step [2115/2250] Loss: 0.0026\n",
      "Epoch [13/25] Step [2130/2250] Loss: 0.0752\n",
      "Epoch [13/25] Step [2145/2250] Loss: 0.0482\n",
      "Epoch [13/25] Step [2160/2250] Loss: 0.0273\n",
      "Epoch [13/25] Step [2175/2250] Loss: 0.0067\n",
      "Epoch [13/25] Step [2190/2250] Loss: 0.0297\n",
      "Epoch [13/25] Step [2205/2250] Loss: 0.0121\n",
      "Epoch [13/25] Step [2220/2250] Loss: 0.0951\n",
      "Epoch [13/25] Step [2235/2250] Loss: 0.0173\n",
      "Epoch [13/25] completed in 1997.68s\n",
      "Train Accuracy: 0.9865, Validation Accuracy: 0.9449\n",
      "\n",
      "âœ… Best model updated and saved with Val Acc: 0.9449\n",
      "\n",
      "Epoch [14/25] Step [0/2250] Loss: 0.0092\n",
      "Epoch [14/25] Step [15/2250] Loss: 0.0072\n",
      "Epoch [14/25] Step [30/2250] Loss: 0.0282\n",
      "Epoch [14/25] Step [45/2250] Loss: 0.0387\n",
      "Epoch [14/25] Step [60/2250] Loss: 0.0555\n",
      "Epoch [14/25] Step [75/2250] Loss: 0.0974\n",
      "Epoch [14/25] Step [90/2250] Loss: 0.0979\n",
      "Epoch [14/25] Step [105/2250] Loss: 0.0029\n",
      "Epoch [14/25] Step [120/2250] Loss: 0.0019\n",
      "Epoch [14/25] Step [135/2250] Loss: 0.0134\n",
      "Epoch [14/25] Step [150/2250] Loss: 0.0026\n",
      "Epoch [14/25] Step [165/2250] Loss: 0.0026\n",
      "Epoch [14/25] Step [180/2250] Loss: 0.0195\n",
      "Epoch [14/25] Step [195/2250] Loss: 0.2657\n",
      "Epoch [14/25] Step [210/2250] Loss: 0.0206\n",
      "Epoch [14/25] Step [225/2250] Loss: 0.0256\n",
      "Epoch [14/25] Step [240/2250] Loss: 0.0018\n",
      "Epoch [14/25] Step [255/2250] Loss: 0.1203\n",
      "Epoch [14/25] Step [270/2250] Loss: 0.0077\n",
      "Epoch [14/25] Step [285/2250] Loss: 0.0021\n",
      "Epoch [14/25] Step [300/2250] Loss: 0.2854\n",
      "Epoch [14/25] Step [315/2250] Loss: 0.0203\n",
      "Epoch [14/25] Step [330/2250] Loss: 0.0034\n",
      "Epoch [14/25] Step [345/2250] Loss: 0.0220\n",
      "Epoch [14/25] Step [360/2250] Loss: 0.0924\n",
      "Epoch [14/25] Step [375/2250] Loss: 0.0056\n",
      "Epoch [14/25] Step [390/2250] Loss: 0.0487\n",
      "Epoch [14/25] Step [405/2250] Loss: 0.0429\n",
      "Epoch [14/25] Step [420/2250] Loss: 0.0027\n",
      "Epoch [14/25] Step [435/2250] Loss: 0.0026\n",
      "Epoch [14/25] Step [450/2250] Loss: 0.0625\n",
      "Epoch [14/25] Step [465/2250] Loss: 0.0084\n",
      "Epoch [14/25] Step [480/2250] Loss: 0.0010\n",
      "Epoch [14/25] Step [495/2250] Loss: 0.0698\n",
      "Epoch [14/25] Step [510/2250] Loss: 0.0011\n",
      "Epoch [14/25] Step [525/2250] Loss: 0.0418\n",
      "Epoch [14/25] Step [540/2250] Loss: 0.0012\n",
      "Epoch [14/25] Step [555/2250] Loss: 0.1451\n",
      "Epoch [14/25] Step [570/2250] Loss: 0.0237\n",
      "Epoch [14/25] Step [585/2250] Loss: 0.0157\n",
      "Epoch [14/25] Step [600/2250] Loss: 0.0037\n",
      "Epoch [14/25] Step [615/2250] Loss: 0.0595\n",
      "Epoch [14/25] Step [630/2250] Loss: 0.0098\n",
      "Epoch [14/25] Step [645/2250] Loss: 0.0061\n",
      "Epoch [14/25] Step [660/2250] Loss: 0.0084\n",
      "Epoch [14/25] Step [675/2250] Loss: 0.0085\n",
      "Epoch [14/25] Step [690/2250] Loss: 0.0107\n",
      "Epoch [14/25] Step [705/2250] Loss: 0.0076\n",
      "Epoch [14/25] Step [720/2250] Loss: 0.0110\n",
      "Epoch [14/25] Step [735/2250] Loss: 0.0360\n",
      "Epoch [14/25] Step [750/2250] Loss: 0.0172\n",
      "Epoch [14/25] Step [765/2250] Loss: 0.0020\n",
      "Epoch [14/25] Step [780/2250] Loss: 0.0189\n",
      "Epoch [14/25] Step [795/2250] Loss: 0.0004\n",
      "Epoch [14/25] Step [810/2250] Loss: 0.0332\n",
      "Epoch [14/25] Step [825/2250] Loss: 0.0303\n",
      "Epoch [14/25] Step [840/2250] Loss: 0.0001\n",
      "Epoch [14/25] Step [855/2250] Loss: 0.0768\n",
      "Epoch [14/25] Step [870/2250] Loss: 0.0247\n",
      "Epoch [14/25] Step [885/2250] Loss: 0.0012\n",
      "Epoch [14/25] Step [900/2250] Loss: 0.0905\n",
      "Epoch [14/25] Step [915/2250] Loss: 0.1980\n",
      "Epoch [14/25] Step [930/2250] Loss: 0.0071\n",
      "Epoch [14/25] Step [945/2250] Loss: 0.0129\n",
      "Epoch [14/25] Step [960/2250] Loss: 0.0430\n",
      "Epoch [14/25] Step [975/2250] Loss: 0.0485\n",
      "Epoch [14/25] Step [990/2250] Loss: 0.0092\n",
      "Epoch [14/25] Step [1005/2250] Loss: 0.0356\n",
      "Epoch [14/25] Step [1020/2250] Loss: 0.0138\n",
      "Epoch [14/25] Step [1035/2250] Loss: 0.0058\n",
      "Epoch [14/25] Step [1050/2250] Loss: 0.0090\n",
      "Epoch [14/25] Step [1065/2250] Loss: 0.0268\n",
      "Epoch [14/25] Step [1080/2250] Loss: 0.0283\n",
      "Epoch [14/25] Step [1095/2250] Loss: 0.0006\n",
      "Epoch [14/25] Step [1110/2250] Loss: 0.0125\n",
      "Epoch [14/25] Step [1125/2250] Loss: 0.0002\n",
      "Epoch [14/25] Step [1140/2250] Loss: 0.0428\n",
      "Epoch [14/25] Step [1155/2250] Loss: 0.0073\n",
      "Epoch [14/25] Step [1170/2250] Loss: 0.0077\n",
      "Epoch [14/25] Step [1185/2250] Loss: 0.2719\n",
      "Epoch [14/25] Step [1200/2250] Loss: 0.0019\n",
      "Epoch [14/25] Step [1215/2250] Loss: 0.0492\n",
      "Epoch [14/25] Step [1230/2250] Loss: 0.0013\n",
      "Epoch [14/25] Step [1245/2250] Loss: 0.0008\n",
      "Epoch [14/25] Step [1260/2250] Loss: 0.0310\n",
      "Epoch [14/25] Step [1275/2250] Loss: 0.0139\n",
      "Epoch [14/25] Step [1290/2250] Loss: 0.0010\n",
      "Epoch [14/25] Step [1305/2250] Loss: 0.0042\n",
      "Epoch [14/25] Step [1320/2250] Loss: 0.3462\n",
      "Epoch [14/25] Step [1335/2250] Loss: 0.0171\n",
      "Epoch [14/25] Step [1350/2250] Loss: 0.0229\n",
      "Epoch [14/25] Step [1365/2250] Loss: 0.0134\n",
      "Epoch [14/25] Step [1380/2250] Loss: 0.0031\n",
      "Epoch [14/25] Step [1395/2250] Loss: 0.0020\n",
      "Epoch [14/25] Step [1410/2250] Loss: 0.1151\n",
      "Epoch [14/25] Step [1425/2250] Loss: 0.0146\n",
      "Epoch [14/25] Step [1440/2250] Loss: 0.0511\n",
      "Epoch [14/25] Step [1455/2250] Loss: 0.0598\n",
      "Epoch [14/25] Step [1470/2250] Loss: 0.0984\n",
      "Epoch [14/25] Step [1485/2250] Loss: 0.0027\n",
      "Epoch [14/25] Step [1500/2250] Loss: 0.0127\n",
      "Epoch [14/25] Step [1515/2250] Loss: 0.0015\n",
      "Epoch [14/25] Step [1530/2250] Loss: 0.0367\n",
      "Epoch [14/25] Step [1545/2250] Loss: 0.0759\n",
      "Epoch [14/25] Step [1560/2250] Loss: 0.0853\n",
      "Epoch [14/25] Step [1575/2250] Loss: 0.0056\n",
      "Epoch [14/25] Step [1590/2250] Loss: 0.0450\n",
      "Epoch [14/25] Step [1605/2250] Loss: 0.0469\n",
      "Epoch [14/25] Step [1620/2250] Loss: 0.0176\n",
      "Epoch [14/25] Step [1635/2250] Loss: 0.0264\n",
      "Epoch [14/25] Step [1650/2250] Loss: 0.0113\n",
      "Epoch [14/25] Step [1665/2250] Loss: 0.0014\n",
      "Epoch [14/25] Step [1680/2250] Loss: 0.0006\n",
      "Epoch [14/25] Step [1695/2250] Loss: 0.0764\n",
      "Epoch [14/25] Step [1710/2250] Loss: 0.0798\n",
      "Epoch [14/25] Step [1725/2250] Loss: 0.0248\n",
      "Epoch [14/25] Step [1740/2250] Loss: 0.2336\n",
      "Epoch [14/25] Step [1755/2250] Loss: 0.0160\n",
      "Epoch [14/25] Step [1770/2250] Loss: 0.0031\n",
      "Epoch [14/25] Step [1785/2250] Loss: 0.0076\n",
      "Epoch [14/25] Step [1800/2250] Loss: 0.0024\n",
      "Epoch [14/25] Step [1815/2250] Loss: 0.0195\n",
      "Epoch [14/25] Step [1830/2250] Loss: 0.1183\n",
      "Epoch [14/25] Step [1845/2250] Loss: 0.0777\n",
      "Epoch [14/25] Step [1860/2250] Loss: 0.0311\n",
      "Epoch [14/25] Step [1875/2250] Loss: 0.0060\n",
      "Epoch [14/25] Step [1890/2250] Loss: 0.2089\n",
      "Epoch [14/25] Step [1905/2250] Loss: 0.0031\n",
      "Epoch [14/25] Step [1920/2250] Loss: 0.0071\n",
      "Epoch [14/25] Step [1935/2250] Loss: 0.0520\n",
      "Epoch [14/25] Step [1950/2250] Loss: 0.0414\n",
      "Epoch [14/25] Step [1965/2250] Loss: 0.1304\n",
      "Epoch [14/25] Step [1980/2250] Loss: 0.0205\n",
      "Epoch [14/25] Step [1995/2250] Loss: 0.0081\n",
      "Epoch [14/25] Step [2010/2250] Loss: 0.0037\n",
      "Epoch [14/25] Step [2025/2250] Loss: 0.3100\n",
      "Epoch [14/25] Step [2040/2250] Loss: 0.1127\n",
      "Epoch [14/25] Step [2055/2250] Loss: 0.0277\n",
      "Epoch [14/25] Step [2070/2250] Loss: 0.0007\n",
      "Epoch [14/25] Step [2085/2250] Loss: 0.0240\n",
      "Epoch [14/25] Step [2100/2250] Loss: 0.0850\n",
      "Epoch [14/25] Step [2115/2250] Loss: 0.0343\n",
      "Epoch [14/25] Step [2130/2250] Loss: 0.0135\n",
      "Epoch [14/25] Step [2145/2250] Loss: 0.0361\n",
      "Epoch [14/25] Step [2160/2250] Loss: 0.0379\n",
      "Epoch [14/25] Step [2175/2250] Loss: 0.0030\n",
      "Epoch [14/25] Step [2190/2250] Loss: 0.0249\n",
      "Epoch [14/25] Step [2205/2250] Loss: 0.0217\n",
      "Epoch [14/25] Step [2220/2250] Loss: 0.0441\n",
      "Epoch [14/25] Step [2235/2250] Loss: 0.0029\n",
      "Epoch [14/25] completed in 1999.40s\n",
      "Train Accuracy: 0.9879, Validation Accuracy: 0.9425\n",
      "\n",
      "Epoch [15/25] Step [0/2250] Loss: 0.0116\n",
      "Epoch [15/25] Step [15/2250] Loss: 0.0042\n",
      "Epoch [15/25] Step [30/2250] Loss: 0.0456\n",
      "Epoch [15/25] Step [45/2250] Loss: 0.0155\n",
      "Epoch [15/25] Step [60/2250] Loss: 0.0668\n",
      "Epoch [15/25] Step [75/2250] Loss: 0.0030\n",
      "Epoch [15/25] Step [90/2250] Loss: 0.0038\n",
      "Epoch [15/25] Step [105/2250] Loss: 0.0307\n",
      "Epoch [15/25] Step [120/2250] Loss: 0.0018\n",
      "Epoch [15/25] Step [135/2250] Loss: 0.0261\n",
      "Epoch [15/25] Step [150/2250] Loss: 0.0404\n",
      "Epoch [15/25] Step [165/2250] Loss: 0.0147\n",
      "Epoch [15/25] Step [180/2250] Loss: 0.1667\n",
      "Epoch [15/25] Step [195/2250] Loss: 0.0245\n",
      "Epoch [15/25] Step [210/2250] Loss: 0.0126\n",
      "Epoch [15/25] Step [225/2250] Loss: 0.0703\n",
      "Epoch [15/25] Step [240/2250] Loss: 0.1166\n",
      "Epoch [15/25] Step [255/2250] Loss: 0.0028\n",
      "Epoch [15/25] Step [270/2250] Loss: 0.0367\n",
      "Epoch [15/25] Step [285/2250] Loss: 0.0010\n",
      "Epoch [15/25] Step [300/2250] Loss: 0.0062\n",
      "Epoch [15/25] Step [315/2250] Loss: 0.0590\n",
      "Epoch [15/25] Step [330/2250] Loss: 0.1025\n",
      "Epoch [15/25] Step [345/2250] Loss: 0.4531\n",
      "Epoch [15/25] Step [360/2250] Loss: 0.0078\n",
      "Epoch [15/25] Step [375/2250] Loss: 0.0081\n",
      "Epoch [15/25] Step [390/2250] Loss: 0.0312\n",
      "Epoch [15/25] Step [405/2250] Loss: 0.0317\n",
      "Epoch [15/25] Step [420/2250] Loss: 0.0205\n",
      "Epoch [15/25] Step [435/2250] Loss: 0.0014\n",
      "Epoch [15/25] Step [450/2250] Loss: 0.0015\n",
      "Epoch [15/25] Step [465/2250] Loss: 0.0025\n",
      "Epoch [15/25] Step [480/2250] Loss: 0.0021\n",
      "Epoch [15/25] Step [495/2250] Loss: 0.1036\n",
      "Epoch [15/25] Step [510/2250] Loss: 0.0102\n",
      "Epoch [15/25] Step [525/2250] Loss: 0.0073\n",
      "Epoch [15/25] Step [540/2250] Loss: 0.1052\n",
      "Epoch [15/25] Step [555/2250] Loss: 0.0172\n",
      "Epoch [15/25] Step [570/2250] Loss: 0.0126\n",
      "Epoch [15/25] Step [585/2250] Loss: 0.0010\n",
      "Epoch [15/25] Step [600/2250] Loss: 0.0037\n",
      "Epoch [15/25] Step [615/2250] Loss: 0.0023\n",
      "Epoch [15/25] Step [630/2250] Loss: 0.0997\n",
      "Epoch [15/25] Step [645/2250] Loss: 0.0015\n",
      "Epoch [15/25] Step [660/2250] Loss: 0.0024\n",
      "Epoch [15/25] Step [675/2250] Loss: 0.0036\n",
      "Epoch [15/25] Step [690/2250] Loss: 0.0155\n",
      "Epoch [15/25] Step [705/2250] Loss: 0.0023\n",
      "Epoch [15/25] Step [720/2250] Loss: 0.0172\n",
      "Epoch [15/25] Step [735/2250] Loss: 0.0067\n",
      "Epoch [15/25] Step [750/2250] Loss: 0.0166\n",
      "Epoch [15/25] Step [765/2250] Loss: 0.0004\n",
      "Epoch [15/25] Step [780/2250] Loss: 0.0062\n",
      "Epoch [15/25] Step [795/2250] Loss: 0.0031\n",
      "Epoch [15/25] Step [810/2250] Loss: 0.0066\n",
      "Epoch [15/25] Step [825/2250] Loss: 0.1071\n",
      "Epoch [15/25] Step [840/2250] Loss: 0.0021\n",
      "Epoch [15/25] Step [855/2250] Loss: 0.0064\n",
      "Epoch [15/25] Step [870/2250] Loss: 0.0033\n",
      "Epoch [15/25] Step [885/2250] Loss: 0.1379\n",
      "Epoch [15/25] Step [900/2250] Loss: 0.0346\n",
      "Epoch [15/25] Step [915/2250] Loss: 0.0221\n",
      "Epoch [15/25] Step [930/2250] Loss: 0.0036\n",
      "Epoch [15/25] Step [945/2250] Loss: 0.0031\n",
      "Epoch [15/25] Step [960/2250] Loss: 0.0230\n",
      "Epoch [15/25] Step [975/2250] Loss: 0.0038\n",
      "Epoch [15/25] Step [990/2250] Loss: 0.0049\n",
      "Epoch [15/25] Step [1005/2250] Loss: 0.0388\n",
      "Epoch [15/25] Step [1020/2250] Loss: 0.0039\n",
      "Epoch [15/25] Step [1035/2250] Loss: 0.0065\n",
      "Epoch [15/25] Step [1050/2250] Loss: 0.0162\n",
      "Epoch [15/25] Step [1065/2250] Loss: 0.0036\n",
      "Epoch [15/25] Step [1080/2250] Loss: 0.0875\n",
      "Epoch [15/25] Step [1095/2250] Loss: 0.0723\n",
      "Epoch [15/25] Step [1110/2250] Loss: 0.0087\n",
      "Epoch [15/25] Step [1125/2250] Loss: 0.0141\n",
      "Epoch [15/25] Step [1140/2250] Loss: 0.0600\n",
      "Epoch [15/25] Step [1155/2250] Loss: 0.0159\n",
      "Epoch [15/25] Step [1170/2250] Loss: 0.0004\n",
      "Epoch [15/25] Step [1185/2250] Loss: 0.0532\n",
      "Epoch [15/25] Step [1200/2250] Loss: 0.0067\n",
      "Epoch [15/25] Step [1215/2250] Loss: 0.0531\n",
      "Epoch [15/25] Step [1230/2250] Loss: 0.0105\n",
      "Epoch [15/25] Step [1245/2250] Loss: 0.0015\n",
      "Epoch [15/25] Step [1260/2250] Loss: 0.0102\n",
      "Epoch [15/25] Step [1275/2250] Loss: 0.0006\n",
      "Epoch [15/25] Step [1290/2250] Loss: 0.0017\n",
      "Epoch [15/25] Step [1305/2250] Loss: 0.0641\n",
      "Epoch [15/25] Step [1320/2250] Loss: 0.0314\n",
      "Epoch [15/25] Step [1335/2250] Loss: 0.0215\n",
      "Epoch [15/25] Step [1350/2250] Loss: 0.1116\n",
      "Epoch [15/25] Step [1365/2250] Loss: 0.0122\n",
      "Epoch [15/25] Step [1380/2250] Loss: 0.0021\n",
      "Epoch [15/25] Step [1395/2250] Loss: 0.1998\n",
      "Epoch [15/25] Step [1410/2250] Loss: 0.0567\n",
      "Epoch [15/25] Step [1425/2250] Loss: 0.0000\n",
      "Epoch [15/25] Step [1440/2250] Loss: 0.0028\n",
      "Epoch [15/25] Step [1455/2250] Loss: 0.0111\n",
      "Epoch [15/25] Step [1470/2250] Loss: 0.0054\n",
      "Epoch [15/25] Step [1485/2250] Loss: 0.0016\n",
      "Epoch [15/25] Step [1500/2250] Loss: 0.0056\n",
      "Epoch [15/25] Step [1515/2250] Loss: 0.0075\n",
      "Epoch [15/25] Step [1530/2250] Loss: 0.0004\n",
      "Epoch [15/25] Step [1545/2250] Loss: 0.0003\n",
      "Epoch [15/25] Step [1560/2250] Loss: 0.0169\n",
      "Epoch [15/25] Step [1575/2250] Loss: 0.0157\n",
      "Epoch [15/25] Step [1590/2250] Loss: 0.0023\n",
      "Epoch [15/25] Step [1605/2250] Loss: 0.0646\n",
      "Epoch [15/25] Step [1620/2250] Loss: 0.1398\n",
      "Epoch [15/25] Step [1635/2250] Loss: 0.0057\n",
      "Epoch [15/25] Step [1650/2250] Loss: 0.0096\n",
      "Epoch [15/25] Step [1665/2250] Loss: 0.0025\n",
      "Epoch [15/25] Step [1680/2250] Loss: 0.0016\n",
      "Epoch [15/25] Step [1695/2250] Loss: 0.0028\n",
      "Epoch [15/25] Step [1710/2250] Loss: 0.0108\n",
      "Epoch [15/25] Step [1725/2250] Loss: 0.0704\n",
      "Epoch [15/25] Step [1740/2250] Loss: 0.0054\n",
      "Epoch [15/25] Step [1755/2250] Loss: 0.0005\n",
      "Epoch [15/25] Step [1770/2250] Loss: 0.0075\n",
      "Epoch [15/25] Step [1785/2250] Loss: 0.0714\n",
      "Epoch [15/25] Step [1800/2250] Loss: 0.0103\n",
      "Epoch [15/25] Step [1815/2250] Loss: 0.0010\n",
      "Epoch [15/25] Step [1830/2250] Loss: 0.0014\n",
      "Epoch [15/25] Step [1845/2250] Loss: 0.0082\n",
      "Epoch [15/25] Step [1860/2250] Loss: 0.0040\n",
      "Epoch [15/25] Step [1875/2250] Loss: 0.1239\n",
      "Epoch [15/25] Step [1890/2250] Loss: 0.0545\n",
      "Epoch [15/25] Step [1905/2250] Loss: 0.0040\n",
      "Epoch [15/25] Step [1920/2250] Loss: 0.0375\n",
      "Epoch [15/25] Step [1935/2250] Loss: 0.0086\n",
      "Epoch [15/25] Step [1950/2250] Loss: 0.0488\n",
      "Epoch [15/25] Step [1965/2250] Loss: 0.0004\n",
      "Epoch [15/25] Step [1980/2250] Loss: 0.0576\n",
      "Epoch [15/25] Step [1995/2250] Loss: 0.0040\n",
      "Epoch [15/25] Step [2010/2250] Loss: 0.0007\n",
      "Epoch [15/25] Step [2025/2250] Loss: 0.0333\n",
      "Epoch [15/25] Step [2040/2250] Loss: 0.0048\n",
      "Epoch [15/25] Step [2055/2250] Loss: 0.0051\n",
      "Epoch [15/25] Step [2070/2250] Loss: 0.0016\n",
      "Epoch [15/25] Step [2085/2250] Loss: 0.1649\n",
      "Epoch [15/25] Step [2100/2250] Loss: 0.0010\n",
      "Epoch [15/25] Step [2115/2250] Loss: 0.0477\n",
      "Epoch [15/25] Step [2130/2250] Loss: 0.0039\n",
      "Epoch [15/25] Step [2145/2250] Loss: 0.0553\n",
      "Epoch [15/25] Step [2160/2250] Loss: 0.0017\n",
      "Epoch [15/25] Step [2175/2250] Loss: 0.1055\n",
      "Epoch [15/25] Step [2190/2250] Loss: 0.1208\n",
      "Epoch [15/25] Step [2205/2250] Loss: 0.1632\n",
      "Epoch [15/25] Step [2220/2250] Loss: 0.0020\n",
      "Epoch [15/25] Step [2235/2250] Loss: 0.0014\n",
      "Epoch [15/25] completed in 1997.93s\n",
      "Train Accuracy: 0.9886, Validation Accuracy: 0.9470\n",
      "\n",
      "âœ… Best model updated and saved with Val Acc: 0.9470\n",
      "\n",
      "Epoch [16/25] Step [0/2250] Loss: 0.0395\n",
      "Epoch [16/25] Step [15/2250] Loss: 0.0053\n",
      "Epoch [16/25] Step [30/2250] Loss: 0.0099\n",
      "Epoch [16/25] Step [45/2250] Loss: 0.0042\n",
      "Epoch [16/25] Step [60/2250] Loss: 0.0220\n",
      "Epoch [16/25] Step [75/2250] Loss: 0.0236\n",
      "Epoch [16/25] Step [90/2250] Loss: 0.1078\n",
      "Epoch [16/25] Step [105/2250] Loss: 0.0050\n",
      "Epoch [16/25] Step [120/2250] Loss: 0.1724\n",
      "Epoch [16/25] Step [135/2250] Loss: 0.0093\n",
      "Epoch [16/25] Step [150/2250] Loss: 0.0074\n",
      "Epoch [16/25] Step [165/2250] Loss: 0.0198\n",
      "Epoch [16/25] Step [180/2250] Loss: 0.0720\n",
      "Epoch [16/25] Step [195/2250] Loss: 0.0490\n",
      "Epoch [16/25] Step [210/2250] Loss: 0.0007\n",
      "Epoch [16/25] Step [225/2250] Loss: 0.0032\n",
      "Epoch [16/25] Step [240/2250] Loss: 0.1203\n",
      "Epoch [16/25] Step [255/2250] Loss: 0.0067\n",
      "Epoch [16/25] Step [270/2250] Loss: 0.0200\n",
      "Epoch [16/25] Step [285/2250] Loss: 0.0087\n",
      "Epoch [16/25] Step [300/2250] Loss: 0.0023\n",
      "Epoch [16/25] Step [315/2250] Loss: 0.0163\n",
      "Epoch [16/25] Step [330/2250] Loss: 0.0413\n",
      "Epoch [16/25] Step [345/2250] Loss: 0.0348\n",
      "Epoch [16/25] Step [360/2250] Loss: 0.0183\n",
      "Epoch [16/25] Step [375/2250] Loss: 0.0081\n",
      "Epoch [16/25] Step [390/2250] Loss: 0.0007\n",
      "Epoch [16/25] Step [405/2250] Loss: 0.0004\n",
      "Epoch [16/25] Step [420/2250] Loss: 0.0018\n",
      "Epoch [16/25] Step [435/2250] Loss: 0.0115\n",
      "Epoch [16/25] Step [450/2250] Loss: 0.0017\n",
      "Epoch [16/25] Step [465/2250] Loss: 0.0163\n",
      "Epoch [16/25] Step [480/2250] Loss: 0.0008\n",
      "Epoch [16/25] Step [495/2250] Loss: 0.0107\n",
      "Epoch [16/25] Step [510/2250] Loss: 0.0004\n",
      "Epoch [16/25] Step [525/2250] Loss: 0.0043\n",
      "Epoch [16/25] Step [540/2250] Loss: 0.0017\n",
      "Epoch [16/25] Step [555/2250] Loss: 0.0141\n",
      "Epoch [16/25] Step [570/2250] Loss: 0.0010\n",
      "Epoch [16/25] Step [585/2250] Loss: 0.0061\n",
      "Epoch [16/25] Step [600/2250] Loss: 0.1423\n",
      "Epoch [16/25] Step [615/2250] Loss: 0.0009\n",
      "Epoch [16/25] Step [630/2250] Loss: 0.0078\n",
      "Epoch [16/25] Step [645/2250] Loss: 0.0093\n",
      "Epoch [16/25] Step [660/2250] Loss: 0.0857\n",
      "Epoch [16/25] Step [675/2250] Loss: 0.0209\n",
      "Epoch [16/25] Step [690/2250] Loss: 0.0204\n",
      "Epoch [16/25] Step [705/2250] Loss: 0.0040\n",
      "Epoch [16/25] Step [720/2250] Loss: 0.0023\n",
      "Epoch [16/25] Step [735/2250] Loss: 0.0096\n",
      "Epoch [16/25] Step [750/2250] Loss: 0.0053\n",
      "Epoch [16/25] Step [765/2250] Loss: 0.0010\n",
      "Epoch [16/25] Step [780/2250] Loss: 0.0557\n",
      "Epoch [16/25] Step [795/2250] Loss: 0.0023\n",
      "Epoch [16/25] Step [810/2250] Loss: 0.1081\n",
      "Epoch [16/25] Step [825/2250] Loss: 0.0433\n",
      "Epoch [16/25] Step [840/2250] Loss: 0.0765\n",
      "Epoch [16/25] Step [855/2250] Loss: 0.0340\n",
      "Epoch [16/25] Step [870/2250] Loss: 0.0034\n",
      "Epoch [16/25] Step [885/2250] Loss: 0.0403\n",
      "Epoch [16/25] Step [900/2250] Loss: 0.0139\n",
      "Epoch [16/25] Step [915/2250] Loss: 0.0016\n",
      "Epoch [16/25] Step [930/2250] Loss: 0.0162\n",
      "Epoch [16/25] Step [945/2250] Loss: 0.0106\n",
      "Epoch [16/25] Step [960/2250] Loss: 0.0014\n",
      "Epoch [16/25] Step [975/2250] Loss: 0.0249\n",
      "Epoch [16/25] Step [990/2250] Loss: 0.0311\n",
      "Epoch [16/25] Step [1005/2250] Loss: 0.0117\n",
      "Epoch [16/25] Step [1020/2250] Loss: 0.0116\n",
      "Epoch [16/25] Step [1035/2250] Loss: 0.0126\n",
      "Epoch [16/25] Step [1050/2250] Loss: 0.0002\n",
      "Epoch [16/25] Step [1065/2250] Loss: 0.0016\n",
      "Epoch [16/25] Step [1080/2250] Loss: 0.0340\n",
      "Epoch [16/25] Step [1095/2250] Loss: 0.0003\n",
      "Epoch [16/25] Step [1110/2250] Loss: 0.0011\n",
      "Epoch [16/25] Step [1125/2250] Loss: 0.0010\n",
      "Epoch [16/25] Step [1140/2250] Loss: 0.0007\n",
      "Epoch [16/25] Step [1155/2250] Loss: 0.0018\n",
      "Epoch [16/25] Step [1170/2250] Loss: 0.1210\n",
      "Epoch [16/25] Step [1185/2250] Loss: 0.0024\n",
      "Epoch [16/25] Step [1200/2250] Loss: 0.0032\n",
      "Epoch [16/25] Step [1215/2250] Loss: 0.0007\n",
      "Epoch [16/25] Step [1230/2250] Loss: 0.0269\n",
      "Epoch [16/25] Step [1245/2250] Loss: 0.0326\n",
      "Epoch [16/25] Step [1260/2250] Loss: 0.0003\n",
      "Epoch [16/25] Step [1275/2250] Loss: 0.0105\n",
      "Epoch [16/25] Step [1290/2250] Loss: 0.0017\n",
      "Epoch [16/25] Step [1305/2250] Loss: 0.0199\n",
      "Epoch [16/25] Step [1320/2250] Loss: 0.0013\n",
      "Epoch [16/25] Step [1335/2250] Loss: 0.0027\n",
      "Epoch [16/25] Step [1350/2250] Loss: 0.1590\n",
      "Epoch [16/25] Step [1365/2250] Loss: 0.0131\n",
      "Epoch [16/25] Step [1380/2250] Loss: 0.1183\n",
      "Epoch [16/25] Step [1395/2250] Loss: 0.6571\n",
      "Epoch [16/25] Step [1410/2250] Loss: 0.0373\n",
      "Epoch [16/25] Step [1425/2250] Loss: 0.0090\n",
      "Epoch [16/25] Step [1440/2250] Loss: 0.0074\n",
      "Epoch [16/25] Step [1455/2250] Loss: 0.0007\n",
      "Epoch [16/25] Step [1470/2250] Loss: 0.0001\n",
      "Epoch [16/25] Step [1485/2250] Loss: 0.0026\n",
      "Epoch [16/25] Step [1500/2250] Loss: 0.0447\n",
      "Epoch [16/25] Step [1515/2250] Loss: 0.0111\n",
      "Epoch [16/25] Step [1530/2250] Loss: 0.0167\n",
      "Epoch [16/25] Step [1545/2250] Loss: 0.0605\n",
      "Epoch [16/25] Step [1560/2250] Loss: 0.0018\n",
      "Epoch [16/25] Step [1575/2250] Loss: 0.0266\n",
      "Epoch [16/25] Step [1590/2250] Loss: 0.0456\n",
      "Epoch [16/25] Step [1605/2250] Loss: 0.0342\n",
      "Epoch [16/25] Step [1620/2250] Loss: 0.0084\n",
      "Epoch [16/25] Step [1635/2250] Loss: 0.0059\n",
      "Epoch [16/25] Step [1650/2250] Loss: 0.0203\n",
      "Epoch [16/25] Step [1665/2250] Loss: 0.0004\n",
      "Epoch [16/25] Step [1680/2250] Loss: 0.0390\n",
      "Epoch [16/25] Step [1695/2250] Loss: 0.0230\n",
      "Epoch [16/25] Step [1710/2250] Loss: 0.0071\n",
      "Epoch [16/25] Step [1725/2250] Loss: 0.0118\n",
      "Epoch [16/25] Step [1740/2250] Loss: 0.0155\n",
      "Epoch [16/25] Step [1755/2250] Loss: 0.0032\n",
      "Epoch [16/25] Step [1770/2250] Loss: 0.0309\n",
      "Epoch [16/25] Step [1785/2250] Loss: 0.0282\n",
      "Epoch [16/25] Step [1800/2250] Loss: 0.0008\n",
      "Epoch [16/25] Step [1815/2250] Loss: 0.0124\n",
      "Epoch [16/25] Step [1830/2250] Loss: 0.0064\n",
      "Epoch [16/25] Step [1845/2250] Loss: 0.1144\n",
      "Epoch [16/25] Step [1860/2250] Loss: 0.0010\n",
      "Epoch [16/25] Step [1875/2250] Loss: 0.0022\n",
      "Epoch [16/25] Step [1890/2250] Loss: 0.0019\n",
      "Epoch [16/25] Step [1905/2250] Loss: 0.0010\n",
      "Epoch [16/25] Step [1920/2250] Loss: 0.0546\n",
      "Epoch [16/25] Step [1935/2250] Loss: 0.1372\n",
      "Epoch [16/25] Step [1950/2250] Loss: 0.0595\n",
      "Epoch [16/25] Step [1965/2250] Loss: 0.0001\n",
      "Epoch [16/25] Step [1980/2250] Loss: 0.0017\n",
      "Epoch [16/25] Step [1995/2250] Loss: 0.2142\n",
      "Epoch [16/25] Step [2010/2250] Loss: 0.0011\n",
      "Epoch [16/25] Step [2025/2250] Loss: 0.0078\n",
      "Epoch [16/25] Step [2040/2250] Loss: 0.0113\n",
      "Epoch [16/25] Step [2055/2250] Loss: 0.0143\n",
      "Epoch [16/25] Step [2070/2250] Loss: 0.0179\n",
      "Epoch [16/25] Step [2085/2250] Loss: 0.0011\n",
      "Epoch [16/25] Step [2100/2250] Loss: 0.0957\n",
      "Epoch [16/25] Step [2115/2250] Loss: 0.0043\n",
      "Epoch [16/25] Step [2130/2250] Loss: 0.0016\n",
      "Epoch [16/25] Step [2145/2250] Loss: 0.3750\n",
      "Epoch [16/25] Step [2160/2250] Loss: 0.0027\n",
      "Epoch [16/25] Step [2175/2250] Loss: 0.0195\n",
      "Epoch [16/25] Step [2190/2250] Loss: 0.0016\n",
      "Epoch [16/25] Step [2205/2250] Loss: 0.0010\n",
      "Epoch [16/25] Step [2220/2250] Loss: 0.0036\n",
      "Epoch [16/25] Step [2235/2250] Loss: 0.0019\n",
      "Epoch [16/25] completed in 2005.47s\n",
      "Train Accuracy: 0.9890, Validation Accuracy: 0.9456\n",
      "\n",
      "Epoch [17/25] Step [0/2250] Loss: 0.0313\n",
      "Epoch [17/25] Step [15/2250] Loss: 0.0008\n",
      "Epoch [17/25] Step [30/2250] Loss: 0.0042\n",
      "Epoch [17/25] Step [45/2250] Loss: 0.0052\n",
      "Epoch [17/25] Step [60/2250] Loss: 0.0015\n",
      "Epoch [17/25] Step [75/2250] Loss: 0.0026\n",
      "Epoch [17/25] Step [90/2250] Loss: 0.0067\n",
      "Epoch [17/25] Step [105/2250] Loss: 0.0006\n",
      "Epoch [17/25] Step [120/2250] Loss: 0.0005\n",
      "Epoch [17/25] Step [135/2250] Loss: 0.0029\n",
      "Epoch [17/25] Step [150/2250] Loss: 0.0419\n",
      "Epoch [17/25] Step [165/2250] Loss: 0.0027\n",
      "Epoch [17/25] Step [180/2250] Loss: 0.0291\n",
      "Epoch [17/25] Step [195/2250] Loss: 0.0037\n",
      "Epoch [17/25] Step [210/2250] Loss: 0.0018\n",
      "Epoch [17/25] Step [225/2250] Loss: 0.0003\n",
      "Epoch [17/25] Step [240/2250] Loss: 0.0045\n",
      "Epoch [17/25] Step [255/2250] Loss: 0.0007\n",
      "Epoch [17/25] Step [270/2250] Loss: 0.0559\n",
      "Epoch [17/25] Step [285/2250] Loss: 0.0004\n",
      "Epoch [17/25] Step [300/2250] Loss: 0.0358\n",
      "Epoch [17/25] Step [315/2250] Loss: 0.0823\n",
      "Epoch [17/25] Step [330/2250] Loss: 0.0304\n",
      "Epoch [17/25] Step [345/2250] Loss: 0.0354\n",
      "Epoch [17/25] Step [360/2250] Loss: 0.0482\n",
      "Epoch [17/25] Step [375/2250] Loss: 0.1512\n",
      "Epoch [17/25] Step [390/2250] Loss: 0.0002\n",
      "Epoch [17/25] Step [405/2250] Loss: 0.0048\n",
      "Epoch [17/25] Step [420/2250] Loss: 0.0879\n",
      "Epoch [17/25] Step [435/2250] Loss: 0.0032\n",
      "Epoch [17/25] Step [450/2250] Loss: 0.0100\n",
      "Epoch [17/25] Step [465/2250] Loss: 0.0014\n",
      "Epoch [17/25] Step [480/2250] Loss: 0.0236\n",
      "Epoch [17/25] Step [495/2250] Loss: 0.0353\n",
      "Epoch [17/25] Step [510/2250] Loss: 0.0156\n",
      "Epoch [17/25] Step [525/2250] Loss: 0.0033\n",
      "Epoch [17/25] Step [540/2250] Loss: 0.0042\n",
      "Epoch [17/25] Step [555/2250] Loss: 0.0138\n",
      "Epoch [17/25] Step [570/2250] Loss: 0.0002\n",
      "Epoch [17/25] Step [585/2250] Loss: 0.0029\n",
      "Epoch [17/25] Step [600/2250] Loss: 0.0149\n",
      "Epoch [17/25] Step [615/2250] Loss: 0.0310\n",
      "Epoch [17/25] Step [630/2250] Loss: 0.0008\n",
      "Epoch [17/25] Step [645/2250] Loss: 0.0004\n",
      "Epoch [17/25] Step [660/2250] Loss: 0.0002\n",
      "Epoch [17/25] Step [675/2250] Loss: 0.0017\n",
      "Epoch [17/25] Step [690/2250] Loss: 0.0000\n",
      "Epoch [17/25] Step [705/2250] Loss: 0.1041\n",
      "Epoch [17/25] Step [720/2250] Loss: 0.0034\n",
      "Epoch [17/25] Step [735/2250] Loss: 0.0034\n",
      "Epoch [17/25] Step [750/2250] Loss: 0.0495\n",
      "Epoch [17/25] Step [765/2250] Loss: 0.0044\n",
      "Epoch [17/25] Step [780/2250] Loss: 0.0265\n",
      "Epoch [17/25] Step [795/2250] Loss: 0.0019\n",
      "Epoch [17/25] Step [810/2250] Loss: 0.0012\n",
      "Epoch [17/25] Step [825/2250] Loss: 0.0066\n",
      "Epoch [17/25] Step [840/2250] Loss: 0.0030\n",
      "Epoch [17/25] Step [855/2250] Loss: 0.0167\n",
      "Epoch [17/25] Step [870/2250] Loss: 0.0251\n",
      "Epoch [17/25] Step [885/2250] Loss: 0.0015\n",
      "Epoch [17/25] Step [900/2250] Loss: 0.0413\n",
      "Epoch [17/25] Step [915/2250] Loss: 0.0115\n",
      "Epoch [17/25] Step [930/2250] Loss: 0.0008\n",
      "Epoch [17/25] Step [945/2250] Loss: 0.0547\n",
      "Epoch [17/25] Step [960/2250] Loss: 0.0370\n",
      "Epoch [17/25] Step [975/2250] Loss: 0.0533\n",
      "Epoch [17/25] Step [990/2250] Loss: 0.0420\n",
      "Epoch [17/25] Step [1005/2250] Loss: 0.0005\n",
      "Epoch [17/25] Step [1020/2250] Loss: 0.0000\n",
      "Epoch [17/25] Step [1035/2250] Loss: 0.0007\n",
      "Epoch [17/25] Step [1050/2250] Loss: 0.0031\n",
      "Epoch [17/25] Step [1065/2250] Loss: 0.0463\n",
      "Epoch [17/25] Step [1080/2250] Loss: 0.0060\n",
      "Epoch [17/25] Step [1095/2250] Loss: 0.0257\n",
      "Epoch [17/25] Step [1110/2250] Loss: 0.0321\n",
      "Epoch [17/25] Step [1125/2250] Loss: 0.0032\n",
      "Epoch [17/25] Step [1140/2250] Loss: 0.0066\n",
      "Epoch [17/25] Step [1155/2250] Loss: 0.0001\n",
      "Epoch [17/25] Step [1170/2250] Loss: 0.0011\n",
      "Epoch [17/25] Step [1185/2250] Loss: 0.0114\n",
      "Epoch [17/25] Step [1200/2250] Loss: 0.0238\n",
      "Epoch [17/25] Step [1215/2250] Loss: 0.0146\n",
      "Epoch [17/25] Step [1230/2250] Loss: 0.1303\n",
      "Epoch [17/25] Step [1245/2250] Loss: 0.0466\n",
      "Epoch [17/25] Step [1260/2250] Loss: 0.0025\n",
      "Epoch [17/25] Step [1275/2250] Loss: 0.0550\n",
      "Epoch [17/25] Step [1290/2250] Loss: 0.0002\n",
      "Epoch [17/25] Step [1305/2250] Loss: 0.0104\n",
      "Epoch [17/25] Step [1320/2250] Loss: 0.0059\n",
      "Epoch [17/25] Step [1335/2250] Loss: 0.0041\n",
      "Epoch [17/25] Step [1350/2250] Loss: 0.0007\n",
      "Epoch [17/25] Step [1365/2250] Loss: 0.0006\n",
      "Epoch [17/25] Step [1380/2250] Loss: 0.0429\n",
      "Epoch [17/25] Step [1395/2250] Loss: 0.0017\n",
      "Epoch [17/25] Step [1410/2250] Loss: 0.0041\n",
      "Epoch [17/25] Step [1425/2250] Loss: 0.0709\n",
      "Epoch [17/25] Step [1440/2250] Loss: 0.1408\n",
      "Epoch [17/25] Step [1455/2250] Loss: 0.0196\n",
      "Epoch [17/25] Step [1470/2250] Loss: 0.1390\n",
      "Epoch [17/25] Step [1485/2250] Loss: 0.0001\n",
      "Epoch [17/25] Step [1500/2250] Loss: 0.0018\n",
      "Epoch [17/25] Step [1515/2250] Loss: 0.0394\n",
      "Epoch [17/25] Step [1530/2250] Loss: 0.0020\n",
      "Epoch [17/25] Step [1545/2250] Loss: 0.0015\n",
      "Epoch [17/25] Step [1560/2250] Loss: 0.3038\n",
      "Epoch [17/25] Step [1575/2250] Loss: 0.0032\n",
      "Epoch [17/25] Step [1590/2250] Loss: 0.0073\n",
      "Epoch [17/25] Step [1605/2250] Loss: 0.0135\n",
      "Epoch [17/25] Step [1620/2250] Loss: 0.0083\n",
      "Epoch [17/25] Step [1635/2250] Loss: 0.0223\n",
      "Epoch [17/25] Step [1650/2250] Loss: 0.0692\n",
      "Epoch [17/25] Step [1665/2250] Loss: 0.1079\n",
      "Epoch [17/25] Step [1680/2250] Loss: 0.0137\n",
      "Epoch [17/25] Step [1695/2250] Loss: 0.0050\n",
      "Epoch [17/25] Step [1710/2250] Loss: 0.0074\n",
      "Epoch [17/25] Step [1725/2250] Loss: 0.0082\n",
      "Epoch [17/25] Step [1740/2250] Loss: 0.0013\n",
      "Epoch [17/25] Step [1755/2250] Loss: 0.0091\n",
      "Epoch [17/25] Step [1770/2250] Loss: 0.0479\n",
      "Epoch [17/25] Step [1785/2250] Loss: 0.0010\n",
      "Epoch [17/25] Step [1800/2250] Loss: 0.0102\n",
      "Epoch [17/25] Step [1815/2250] Loss: 0.0840\n",
      "Epoch [17/25] Step [1830/2250] Loss: 0.0266\n",
      "Epoch [17/25] Step [1845/2250] Loss: 0.0245\n",
      "Epoch [17/25] Step [1860/2250] Loss: 0.0016\n",
      "Epoch [17/25] Step [1875/2250] Loss: 0.0005\n",
      "Epoch [17/25] Step [1890/2250] Loss: 0.0227\n",
      "Epoch [17/25] Step [1905/2250] Loss: 0.0088\n",
      "Epoch [17/25] Step [1920/2250] Loss: 0.0047\n",
      "Epoch [17/25] Step [1935/2250] Loss: 0.0023\n",
      "Epoch [17/25] Step [1950/2250] Loss: 0.0125\n",
      "Epoch [17/25] Step [1965/2250] Loss: 0.2531\n",
      "Epoch [17/25] Step [1980/2250] Loss: 0.0764\n",
      "Epoch [17/25] Step [1995/2250] Loss: 0.0122\n",
      "Epoch [17/25] Step [2010/2250] Loss: 0.0018\n",
      "Epoch [17/25] Step [2025/2250] Loss: 0.0140\n",
      "Epoch [17/25] Step [2040/2250] Loss: 0.0080\n",
      "Epoch [17/25] Step [2055/2250] Loss: 0.0101\n",
      "Epoch [17/25] Step [2070/2250] Loss: 0.0145\n",
      "Epoch [17/25] Step [2085/2250] Loss: 0.0551\n",
      "Epoch [17/25] Step [2100/2250] Loss: 0.0002\n",
      "Epoch [17/25] Step [2115/2250] Loss: 0.1044\n",
      "Epoch [17/25] Step [2130/2250] Loss: 0.0002\n",
      "Epoch [17/25] Step [2145/2250] Loss: 0.0416\n",
      "Epoch [17/25] Step [2160/2250] Loss: 0.0014\n",
      "Epoch [17/25] Step [2175/2250] Loss: 0.0158\n",
      "Epoch [17/25] Step [2190/2250] Loss: 0.0133\n",
      "Epoch [17/25] Step [2205/2250] Loss: 0.0785\n",
      "Epoch [17/25] Step [2220/2250] Loss: 0.2401\n",
      "Epoch [17/25] Step [2235/2250] Loss: 0.0039\n",
      "Epoch [17/25] completed in 2007.74s\n",
      "Train Accuracy: 0.9896, Validation Accuracy: 0.9351\n",
      "\n",
      "Epoch [18/25] Step [0/2250] Loss: 0.0041\n",
      "Epoch [18/25] Step [15/2250] Loss: 0.0389\n",
      "Epoch [18/25] Step [30/2250] Loss: 0.0032\n",
      "Epoch [18/25] Step [45/2250] Loss: 0.0265\n",
      "Epoch [18/25] Step [60/2250] Loss: 0.0033\n",
      "Epoch [18/25] Step [75/2250] Loss: 0.0320\n",
      "Epoch [18/25] Step [90/2250] Loss: 0.0018\n",
      "Epoch [18/25] Step [105/2250] Loss: 0.0751\n",
      "Epoch [18/25] Step [120/2250] Loss: 0.1704\n",
      "Epoch [18/25] Step [135/2250] Loss: 0.0251\n",
      "Epoch [18/25] Step [150/2250] Loss: 0.0038\n",
      "Epoch [18/25] Step [165/2250] Loss: 0.0175\n",
      "Epoch [18/25] Step [180/2250] Loss: 0.0015\n",
      "Epoch [18/25] Step [195/2250] Loss: 0.0025\n",
      "Epoch [18/25] Step [210/2250] Loss: 0.0163\n",
      "Epoch [18/25] Step [225/2250] Loss: 0.0003\n",
      "Epoch [18/25] Step [240/2250] Loss: 0.0001\n",
      "Epoch [18/25] Step [255/2250] Loss: 0.0007\n",
      "Epoch [18/25] Step [270/2250] Loss: 0.0036\n",
      "Epoch [18/25] Step [285/2250] Loss: 0.1330\n",
      "Epoch [18/25] Step [300/2250] Loss: 0.0059\n",
      "Epoch [18/25] Step [315/2250] Loss: 0.0050\n",
      "Epoch [18/25] Step [330/2250] Loss: 0.0007\n",
      "Epoch [18/25] Step [345/2250] Loss: 0.0026\n",
      "Epoch [18/25] Step [360/2250] Loss: 0.1351\n",
      "Epoch [18/25] Step [375/2250] Loss: 0.0034\n",
      "Epoch [18/25] Step [390/2250] Loss: 0.0062\n",
      "Epoch [18/25] Step [405/2250] Loss: 0.0056\n",
      "Epoch [18/25] Step [420/2250] Loss: 0.0056\n",
      "Epoch [18/25] Step [435/2250] Loss: 0.0033\n",
      "Epoch [18/25] Step [450/2250] Loss: 0.0014\n",
      "Epoch [18/25] Step [465/2250] Loss: 0.0007\n",
      "Epoch [18/25] Step [480/2250] Loss: 0.0066\n",
      "Epoch [18/25] Step [495/2250] Loss: 0.0026\n",
      "Epoch [18/25] Step [510/2250] Loss: 0.0120\n",
      "Epoch [18/25] Step [525/2250] Loss: 0.1638\n",
      "Epoch [18/25] Step [540/2250] Loss: 0.0018\n",
      "Epoch [18/25] Step [555/2250] Loss: 0.0675\n",
      "Epoch [18/25] Step [570/2250] Loss: 0.0039\n",
      "Epoch [18/25] Step [585/2250] Loss: 0.0100\n",
      "Epoch [18/25] Step [600/2250] Loss: 0.1216\n",
      "Epoch [18/25] Step [615/2250] Loss: 0.1116\n",
      "Epoch [18/25] Step [630/2250] Loss: 0.0005\n",
      "Epoch [18/25] Step [645/2250] Loss: 0.0411\n",
      "Epoch [18/25] Step [660/2250] Loss: 0.0111\n",
      "Epoch [18/25] Step [675/2250] Loss: 0.0003\n",
      "Epoch [18/25] Step [690/2250] Loss: 0.0133\n",
      "Epoch [18/25] Step [705/2250] Loss: 0.0305\n",
      "Epoch [18/25] Step [720/2250] Loss: 0.0014\n",
      "Epoch [18/25] Step [735/2250] Loss: 0.0013\n",
      "Epoch [18/25] Step [750/2250] Loss: 0.1565\n",
      "Epoch [18/25] Step [765/2250] Loss: 0.0000\n",
      "Epoch [18/25] Step [780/2250] Loss: 0.0003\n",
      "Epoch [18/25] Step [795/2250] Loss: 0.0925\n",
      "Epoch [18/25] Step [810/2250] Loss: 0.0010\n",
      "Epoch [18/25] Step [825/2250] Loss: 0.0000\n",
      "Epoch [18/25] Step [840/2250] Loss: 0.0208\n",
      "Epoch [18/25] Step [855/2250] Loss: 0.0126\n",
      "Epoch [18/25] Step [870/2250] Loss: 0.0088\n",
      "Epoch [18/25] Step [885/2250] Loss: 0.0003\n",
      "Epoch [18/25] Step [900/2250] Loss: 0.0057\n",
      "Epoch [18/25] Step [915/2250] Loss: 0.0003\n",
      "Epoch [18/25] Step [930/2250] Loss: 0.0007\n",
      "Epoch [18/25] Step [945/2250] Loss: 0.2828\n",
      "Epoch [18/25] Step [960/2250] Loss: 0.0007\n",
      "Epoch [18/25] Step [975/2250] Loss: 0.0046\n",
      "Epoch [18/25] Step [990/2250] Loss: 0.0721\n",
      "Epoch [18/25] Step [1005/2250] Loss: 0.0503\n",
      "Epoch [18/25] Step [1020/2250] Loss: 0.0100\n",
      "Epoch [18/25] Step [1035/2250] Loss: 0.0007\n",
      "Epoch [18/25] Step [1050/2250] Loss: 0.0118\n",
      "Epoch [18/25] Step [1065/2250] Loss: 0.0058\n",
      "Epoch [18/25] Step [1080/2250] Loss: 0.0009\n",
      "Epoch [18/25] Step [1095/2250] Loss: 0.0019\n",
      "Epoch [18/25] Step [1110/2250] Loss: 0.0692\n",
      "Epoch [18/25] Step [1125/2250] Loss: 0.0085\n",
      "Epoch [18/25] Step [1140/2250] Loss: 0.0250\n",
      "Epoch [18/25] Step [1155/2250] Loss: 0.0267\n",
      "Epoch [18/25] Step [1170/2250] Loss: 0.0368\n",
      "Epoch [18/25] Step [1185/2250] Loss: 0.0068\n",
      "Epoch [18/25] Step [1200/2250] Loss: 0.0036\n",
      "Epoch [18/25] Step [1215/2250] Loss: 0.0004\n",
      "Epoch [18/25] Step [1230/2250] Loss: 0.0409\n",
      "Epoch [18/25] Step [1245/2250] Loss: 0.0099\n",
      "Epoch [18/25] Step [1260/2250] Loss: 0.0129\n",
      "Epoch [18/25] Step [1275/2250] Loss: 0.0672\n",
      "Epoch [18/25] Step [1290/2250] Loss: 0.0041\n",
      "Epoch [18/25] Step [1305/2250] Loss: 0.0804\n",
      "Epoch [18/25] Step [1320/2250] Loss: 0.0415\n",
      "Epoch [18/25] Step [1335/2250] Loss: 0.0239\n",
      "Epoch [18/25] Step [1350/2250] Loss: 0.0030\n",
      "Epoch [18/25] Step [1365/2250] Loss: 0.0179\n",
      "Epoch [18/25] Step [1380/2250] Loss: 0.0174\n",
      "Epoch [18/25] Step [1395/2250] Loss: 0.0068\n",
      "Epoch [18/25] Step [1410/2250] Loss: 0.0063\n",
      "Epoch [18/25] Step [1425/2250] Loss: 0.0121\n",
      "Epoch [18/25] Step [1440/2250] Loss: 0.0475\n",
      "Epoch [18/25] Step [1455/2250] Loss: 0.0019\n",
      "Epoch [18/25] Step [1470/2250] Loss: 0.0787\n",
      "Epoch [18/25] Step [1485/2250] Loss: 0.0426\n",
      "Epoch [18/25] Step [1500/2250] Loss: 0.0433\n",
      "Epoch [18/25] Step [1515/2250] Loss: 0.0174\n",
      "Epoch [18/25] Step [1530/2250] Loss: 0.1533\n",
      "Epoch [18/25] Step [1545/2250] Loss: 0.1835\n",
      "Epoch [18/25] Step [1560/2250] Loss: 0.0058\n",
      "Epoch [18/25] Step [1575/2250] Loss: 0.0125\n",
      "Epoch [18/25] Step [1590/2250] Loss: 0.0003\n",
      "Epoch [18/25] Step [1605/2250] Loss: 0.1220\n",
      "Epoch [18/25] Step [1620/2250] Loss: 0.0060\n",
      "Epoch [18/25] Step [1635/2250] Loss: 0.0008\n",
      "Epoch [18/25] Step [1650/2250] Loss: 0.1351\n",
      "Epoch [18/25] Step [1665/2250] Loss: 0.0301\n",
      "Epoch [18/25] Step [1680/2250] Loss: 0.2065\n",
      "Epoch [18/25] Step [1695/2250] Loss: 0.0023\n",
      "Epoch [18/25] Step [1710/2250] Loss: 0.0023\n",
      "Epoch [18/25] Step [1725/2250] Loss: 0.0113\n",
      "Epoch [18/25] Step [1740/2250] Loss: 0.0051\n",
      "Epoch [18/25] Step [1755/2250] Loss: 0.1368\n",
      "Epoch [18/25] Step [1770/2250] Loss: 0.0232\n",
      "Epoch [18/25] Step [1785/2250] Loss: 0.0406\n",
      "Epoch [18/25] Step [1800/2250] Loss: 0.0007\n",
      "Epoch [18/25] Step [1815/2250] Loss: 0.0004\n",
      "Epoch [18/25] Step [1830/2250] Loss: 0.0146\n",
      "Epoch [18/25] Step [1845/2250] Loss: 0.0005\n",
      "Epoch [18/25] Step [1860/2250] Loss: 0.0796\n",
      "Epoch [18/25] Step [1875/2250] Loss: 0.0039\n",
      "Epoch [18/25] Step [1890/2250] Loss: 0.0041\n",
      "Epoch [18/25] Step [1905/2250] Loss: 0.0577\n",
      "Epoch [18/25] Step [1920/2250] Loss: 0.0005\n",
      "Epoch [18/25] Step [1935/2250] Loss: 0.0234\n",
      "Epoch [18/25] Step [1950/2250] Loss: 0.0034\n",
      "Epoch [18/25] Step [1965/2250] Loss: 0.0416\n",
      "Epoch [18/25] Step [1980/2250] Loss: 0.0041\n",
      "Epoch [18/25] Step [1995/2250] Loss: 0.0014\n",
      "Epoch [18/25] Step [2010/2250] Loss: 0.0401\n",
      "Epoch [18/25] Step [2025/2250] Loss: 0.0001\n",
      "Epoch [18/25] Step [2040/2250] Loss: 0.1362\n",
      "Epoch [18/25] Step [2055/2250] Loss: 0.0036\n",
      "Epoch [18/25] Step [2070/2250] Loss: 0.0404\n",
      "Epoch [18/25] Step [2085/2250] Loss: 0.0031\n",
      "Epoch [18/25] Step [2100/2250] Loss: 0.0006\n",
      "Epoch [18/25] Step [2115/2250] Loss: 0.0010\n",
      "Epoch [18/25] Step [2130/2250] Loss: 0.0163\n",
      "Epoch [18/25] Step [2145/2250] Loss: 0.0633\n",
      "Epoch [18/25] Step [2160/2250] Loss: 0.0835\n",
      "Epoch [18/25] Step [2175/2250] Loss: 0.0006\n",
      "Epoch [18/25] Step [2190/2250] Loss: 0.1794\n",
      "Epoch [18/25] Step [2205/2250] Loss: 0.0989\n",
      "Epoch [18/25] Step [2220/2250] Loss: 0.0895\n",
      "Epoch [18/25] Step [2235/2250] Loss: 0.0014\n",
      "Epoch [18/25] completed in 2009.65s\n",
      "Train Accuracy: 0.9903, Validation Accuracy: 0.9435\n",
      "\n",
      "Epoch [19/25] Step [0/2250] Loss: 0.0005\n",
      "Epoch [19/25] Step [15/2250] Loss: 0.0068\n",
      "Epoch [19/25] Step [30/2250] Loss: 0.0014\n",
      "Epoch [19/25] Step [45/2250] Loss: 0.0020\n",
      "Epoch [19/25] Step [60/2250] Loss: 0.0018\n",
      "Epoch [19/25] Step [75/2250] Loss: 0.0605\n",
      "Epoch [19/25] Step [90/2250] Loss: 0.0001\n",
      "Epoch [19/25] Step [105/2250] Loss: 0.0019\n",
      "Epoch [19/25] Step [120/2250] Loss: 0.0338\n",
      "Epoch [19/25] Step [135/2250] Loss: 0.0010\n",
      "Epoch [19/25] Step [150/2250] Loss: 0.2091\n",
      "Epoch [19/25] Step [165/2250] Loss: 0.0107\n",
      "Epoch [19/25] Step [180/2250] Loss: 0.0003\n",
      "Epoch [19/25] Step [195/2250] Loss: 0.0031\n",
      "Epoch [19/25] Step [210/2250] Loss: 0.0008\n",
      "Epoch [19/25] Step [225/2250] Loss: 0.1564\n",
      "Epoch [19/25] Step [240/2250] Loss: 0.0104\n",
      "Epoch [19/25] Step [255/2250] Loss: 0.0227\n",
      "Epoch [19/25] Step [270/2250] Loss: 0.0047\n",
      "Epoch [19/25] Step [285/2250] Loss: 0.0002\n",
      "Epoch [19/25] Step [300/2250] Loss: 0.0069\n",
      "Epoch [19/25] Step [315/2250] Loss: 0.0010\n",
      "Epoch [19/25] Step [330/2250] Loss: 0.0264\n",
      "Epoch [19/25] Step [345/2250] Loss: 0.0324\n",
      "Epoch [19/25] Step [360/2250] Loss: 0.0192\n",
      "Epoch [19/25] Step [375/2250] Loss: 0.0071\n",
      "Epoch [19/25] Step [390/2250] Loss: 0.0517\n",
      "Epoch [19/25] Step [405/2250] Loss: 0.0790\n",
      "Epoch [19/25] Step [420/2250] Loss: 0.0138\n",
      "Epoch [19/25] Step [435/2250] Loss: 0.0037\n",
      "Epoch [19/25] Step [450/2250] Loss: 0.0049\n",
      "Epoch [19/25] Step [465/2250] Loss: 0.0017\n",
      "Epoch [19/25] Step [480/2250] Loss: 0.0036\n",
      "Epoch [19/25] Step [495/2250] Loss: 0.0020\n",
      "Epoch [19/25] Step [510/2250] Loss: 0.0078\n",
      "Epoch [19/25] Step [525/2250] Loss: 0.0019\n",
      "Epoch [19/25] Step [540/2250] Loss: 0.0166\n",
      "Epoch [19/25] Step [555/2250] Loss: 0.0007\n",
      "Epoch [19/25] Step [570/2250] Loss: 0.0001\n",
      "Epoch [19/25] Step [585/2250] Loss: 0.0032\n",
      "Epoch [19/25] Step [600/2250] Loss: 0.0042\n",
      "Epoch [19/25] Step [615/2250] Loss: 0.0094\n",
      "Epoch [19/25] Step [630/2250] Loss: 0.0066\n",
      "Epoch [19/25] Step [645/2250] Loss: 0.0359\n",
      "Epoch [19/25] Step [660/2250] Loss: 0.0061\n",
      "Epoch [19/25] Step [675/2250] Loss: 0.0012\n",
      "Epoch [19/25] Step [690/2250] Loss: 0.0016\n",
      "Epoch [19/25] Step [705/2250] Loss: 0.0047\n",
      "Epoch [19/25] Step [720/2250] Loss: 0.0028\n",
      "Epoch [19/25] Step [735/2250] Loss: 0.0031\n",
      "Epoch [19/25] Step [750/2250] Loss: 0.0096\n",
      "Epoch [19/25] Step [765/2250] Loss: 0.0463\n",
      "Epoch [19/25] Step [780/2250] Loss: 0.0058\n",
      "Epoch [19/25] Step [795/2250] Loss: 0.0630\n",
      "Epoch [19/25] Step [810/2250] Loss: 0.0015\n",
      "Epoch [19/25] Step [825/2250] Loss: 0.0548\n",
      "Epoch [19/25] Step [840/2250] Loss: 0.0019\n",
      "Epoch [19/25] Step [855/2250] Loss: 0.0553\n",
      "Epoch [19/25] Step [870/2250] Loss: 0.0172\n",
      "Epoch [19/25] Step [885/2250] Loss: 0.0013\n",
      "Epoch [19/25] Step [900/2250] Loss: 0.0068\n",
      "Epoch [19/25] Step [915/2250] Loss: 0.0683\n",
      "Epoch [19/25] Step [930/2250] Loss: 0.0083\n",
      "Epoch [19/25] Step [945/2250] Loss: 0.1024\n",
      "Epoch [19/25] Step [960/2250] Loss: 0.0127\n",
      "Epoch [19/25] Step [975/2250] Loss: 0.0205\n",
      "Epoch [19/25] Step [990/2250] Loss: 0.0001\n",
      "Epoch [19/25] Step [1005/2250] Loss: 0.0024\n",
      "Epoch [19/25] Step [1020/2250] Loss: 0.0117\n",
      "Epoch [19/25] Step [1035/2250] Loss: 0.0509\n",
      "Epoch [19/25] Step [1050/2250] Loss: 0.0128\n",
      "Epoch [19/25] Step [1065/2250] Loss: 0.0079\n",
      "Epoch [19/25] Step [1080/2250] Loss: 0.0019\n",
      "Epoch [19/25] Step [1095/2250] Loss: 0.0002\n",
      "Epoch [19/25] Step [1110/2250] Loss: 0.0017\n",
      "Epoch [19/25] Step [1125/2250] Loss: 0.0008\n",
      "Epoch [19/25] Step [1140/2250] Loss: 0.0312\n",
      "Epoch [19/25] Step [1155/2250] Loss: 0.0262\n",
      "Epoch [19/25] Step [1170/2250] Loss: 0.0180\n",
      "Epoch [19/25] Step [1185/2250] Loss: 0.0074\n",
      "Epoch [19/25] Step [1200/2250] Loss: 0.0044\n",
      "Epoch [19/25] Step [1215/2250] Loss: 0.0173\n",
      "Epoch [19/25] Step [1230/2250] Loss: 0.0486\n",
      "Epoch [19/25] Step [1245/2250] Loss: 0.1787\n",
      "Epoch [19/25] Step [1260/2250] Loss: 0.0436\n",
      "Epoch [19/25] Step [1275/2250] Loss: 0.0017\n",
      "Epoch [19/25] Step [1290/2250] Loss: 0.0031\n",
      "Epoch [19/25] Step [1305/2250] Loss: 0.0024\n",
      "Epoch [19/25] Step [1320/2250] Loss: 0.0010\n",
      "Epoch [19/25] Step [1335/2250] Loss: 0.0978\n",
      "Epoch [19/25] Step [1350/2250] Loss: 0.1407\n",
      "Epoch [19/25] Step [1365/2250] Loss: 0.0065\n",
      "Epoch [19/25] Step [1380/2250] Loss: 0.0050\n",
      "Epoch [19/25] Step [1395/2250] Loss: 0.0058\n",
      "Epoch [19/25] Step [1410/2250] Loss: 0.0410\n",
      "Epoch [19/25] Step [1425/2250] Loss: 0.0028\n",
      "Epoch [19/25] Step [1440/2250] Loss: 0.0062\n",
      "Epoch [19/25] Step [1455/2250] Loss: 0.0018\n",
      "Epoch [19/25] Step [1470/2250] Loss: 0.0190\n",
      "Epoch [19/25] Step [1485/2250] Loss: 0.0021\n",
      "Epoch [19/25] Step [1500/2250] Loss: 0.0471\n",
      "Epoch [19/25] Step [1515/2250] Loss: 0.0147\n",
      "Epoch [19/25] Step [1530/2250] Loss: 0.0001\n",
      "Epoch [19/25] Step [1545/2250] Loss: 0.0834\n",
      "Epoch [19/25] Step [1560/2250] Loss: 0.0346\n",
      "Epoch [19/25] Step [1575/2250] Loss: 0.0530\n",
      "Epoch [19/25] Step [1590/2250] Loss: 0.0097\n",
      "Epoch [19/25] Step [1605/2250] Loss: 0.0092\n",
      "Epoch [19/25] Step [1620/2250] Loss: 0.0013\n",
      "Epoch [19/25] Step [1635/2250] Loss: 0.0018\n",
      "Epoch [19/25] Step [1650/2250] Loss: 0.0004\n",
      "Epoch [19/25] Step [1665/2250] Loss: 0.0150\n",
      "Epoch [19/25] Step [1680/2250] Loss: 0.0038\n",
      "Epoch [19/25] Step [1695/2250] Loss: 0.0745\n",
      "Epoch [19/25] Step [1710/2250] Loss: 0.0027\n",
      "Epoch [19/25] Step [1725/2250] Loss: 0.0001\n",
      "Epoch [19/25] Step [1740/2250] Loss: 0.0147\n",
      "Epoch [19/25] Step [1755/2250] Loss: 0.0006\n",
      "Epoch [19/25] Step [1770/2250] Loss: 0.0082\n",
      "Epoch [19/25] Step [1785/2250] Loss: 0.0375\n",
      "Epoch [19/25] Step [1800/2250] Loss: 0.0064\n",
      "Epoch [19/25] Step [1815/2250] Loss: 0.0285\n",
      "Epoch [19/25] Step [1830/2250] Loss: 0.0002\n",
      "Epoch [19/25] Step [1845/2250] Loss: 0.0270\n",
      "Epoch [19/25] Step [1860/2250] Loss: 0.0001\n",
      "Epoch [19/25] Step [1875/2250] Loss: 0.0003\n",
      "Epoch [19/25] Step [1890/2250] Loss: 0.0171\n",
      "Epoch [19/25] Step [1905/2250] Loss: 0.1236\n",
      "Epoch [19/25] Step [1920/2250] Loss: 0.0004\n",
      "Epoch [19/25] Step [1935/2250] Loss: 0.0020\n",
      "Epoch [19/25] Step [1950/2250] Loss: 0.1492\n",
      "Epoch [19/25] Step [1965/2250] Loss: 0.0284\n",
      "Epoch [19/25] Step [1980/2250] Loss: 0.0108\n",
      "Epoch [19/25] Step [1995/2250] Loss: 0.0079\n",
      "Epoch [19/25] Step [2010/2250] Loss: 0.1418\n",
      "Epoch [19/25] Step [2025/2250] Loss: 0.1418\n",
      "Epoch [19/25] Step [2040/2250] Loss: 0.0009\n",
      "Epoch [19/25] Step [2055/2250] Loss: 0.0739\n",
      "Epoch [19/25] Step [2070/2250] Loss: 0.0545\n",
      "Epoch [19/25] Step [2085/2250] Loss: 0.0018\n",
      "Epoch [19/25] Step [2100/2250] Loss: 0.0201\n",
      "Epoch [19/25] Step [2115/2250] Loss: 0.0400\n",
      "Epoch [19/25] Step [2130/2250] Loss: 0.0052\n",
      "Epoch [19/25] Step [2145/2250] Loss: 0.0902\n",
      "Epoch [19/25] Step [2160/2250] Loss: 0.0012\n",
      "Epoch [19/25] Step [2175/2250] Loss: 0.0068\n",
      "Epoch [19/25] Step [2190/2250] Loss: 0.0026\n",
      "Epoch [19/25] Step [2205/2250] Loss: 0.0018\n",
      "Epoch [19/25] Step [2220/2250] Loss: 0.1449\n",
      "Epoch [19/25] Step [2235/2250] Loss: 0.1553\n",
      "Epoch [19/25] completed in 2006.45s\n",
      "Train Accuracy: 0.9902, Validation Accuracy: 0.9465\n",
      "\n",
      "Epoch [20/25] Step [0/2250] Loss: 0.0906\n",
      "Epoch [20/25] Step [15/2250] Loss: 0.0051\n",
      "Epoch [20/25] Step [30/2250] Loss: 0.0020\n",
      "Epoch [20/25] Step [45/2250] Loss: 0.0013\n",
      "Epoch [20/25] Step [60/2250] Loss: 0.0010\n",
      "Epoch [20/25] Step [75/2250] Loss: 0.1243\n",
      "Epoch [20/25] Step [90/2250] Loss: 0.0032\n",
      "Epoch [20/25] Step [105/2250] Loss: 0.0646\n",
      "Epoch [20/25] Step [120/2250] Loss: 0.0062\n",
      "Epoch [20/25] Step [135/2250] Loss: 0.0007\n",
      "Epoch [20/25] Step [150/2250] Loss: 0.0026\n",
      "Epoch [20/25] Step [165/2250] Loss: 0.0020\n",
      "Epoch [20/25] Step [180/2250] Loss: 0.0314\n",
      "Epoch [20/25] Step [195/2250] Loss: 0.0130\n",
      "Epoch [20/25] Step [210/2250] Loss: 0.0016\n",
      "Epoch [20/25] Step [225/2250] Loss: 0.0004\n",
      "Epoch [20/25] Step [240/2250] Loss: 0.0665\n",
      "Epoch [20/25] Step [255/2250] Loss: 0.0331\n",
      "Epoch [20/25] Step [270/2250] Loss: 0.0009\n",
      "Epoch [20/25] Step [285/2250] Loss: 0.0004\n",
      "Epoch [20/25] Step [300/2250] Loss: 0.0019\n",
      "Epoch [20/25] Step [315/2250] Loss: 0.0008\n",
      "Epoch [20/25] Step [330/2250] Loss: 0.0124\n",
      "Epoch [20/25] Step [345/2250] Loss: 0.0076\n",
      "Epoch [20/25] Step [360/2250] Loss: 0.0016\n",
      "Epoch [20/25] Step [375/2250] Loss: 0.0006\n",
      "Epoch [20/25] Step [390/2250] Loss: 0.0134\n",
      "Epoch [20/25] Step [405/2250] Loss: 0.0200\n",
      "Epoch [20/25] Step [420/2250] Loss: 0.1167\n",
      "Epoch [20/25] Step [435/2250] Loss: 0.0011\n",
      "Epoch [20/25] Step [450/2250] Loss: 0.0008\n",
      "Epoch [20/25] Step [465/2250] Loss: 0.0194\n",
      "Epoch [20/25] Step [480/2250] Loss: 0.0016\n",
      "Epoch [20/25] Step [495/2250] Loss: 0.0026\n",
      "Epoch [20/25] Step [510/2250] Loss: 0.0020\n",
      "Epoch [20/25] Step [525/2250] Loss: 0.0406\n",
      "Epoch [20/25] Step [540/2250] Loss: 0.0002\n",
      "Epoch [20/25] Step [555/2250] Loss: 0.0006\n",
      "Epoch [20/25] Step [570/2250] Loss: 0.0121\n",
      "Epoch [20/25] Step [585/2250] Loss: 0.0012\n",
      "Epoch [20/25] Step [600/2250] Loss: 0.0055\n",
      "Epoch [20/25] Step [615/2250] Loss: 0.0758\n",
      "Epoch [20/25] Step [630/2250] Loss: 0.1386\n",
      "Epoch [20/25] Step [645/2250] Loss: 0.0135\n",
      "Epoch [20/25] Step [660/2250] Loss: 0.0003\n",
      "Epoch [20/25] Step [675/2250] Loss: 0.0174\n",
      "Epoch [20/25] Step [690/2250] Loss: 0.4295\n",
      "Epoch [20/25] Step [705/2250] Loss: 0.0003\n",
      "Epoch [20/25] Step [720/2250] Loss: 0.0082\n",
      "Epoch [20/25] Step [735/2250] Loss: 0.0080\n",
      "Epoch [20/25] Step [750/2250] Loss: 0.0000\n",
      "Epoch [20/25] Step [765/2250] Loss: 0.0305\n",
      "Epoch [20/25] Step [780/2250] Loss: 0.1280\n",
      "Epoch [20/25] Step [795/2250] Loss: 0.0213\n",
      "Epoch [20/25] Step [810/2250] Loss: 0.0051\n",
      "Epoch [20/25] Step [825/2250] Loss: 0.0395\n",
      "Epoch [20/25] Step [840/2250] Loss: 0.0039\n",
      "Epoch [20/25] Step [855/2250] Loss: 0.0013\n",
      "Epoch [20/25] Step [870/2250] Loss: 0.0376\n",
      "Epoch [20/25] Step [885/2250] Loss: 0.0171\n",
      "Epoch [20/25] Step [900/2250] Loss: 0.0085\n",
      "Epoch [20/25] Step [915/2250] Loss: 0.0101\n",
      "Epoch [20/25] Step [930/2250] Loss: 0.0139\n",
      "Epoch [20/25] Step [945/2250] Loss: 0.0002\n",
      "Epoch [20/25] Step [960/2250] Loss: 0.0565\n",
      "Epoch [20/25] Step [975/2250] Loss: 0.0040\n",
      "Epoch [20/25] Step [990/2250] Loss: 0.0278\n",
      "Epoch [20/25] Step [1005/2250] Loss: 0.0104\n",
      "Epoch [20/25] Step [1020/2250] Loss: 0.0928\n",
      "Epoch [20/25] Step [1035/2250] Loss: 0.0219\n",
      "Epoch [20/25] Step [1050/2250] Loss: 0.0076\n",
      "Epoch [20/25] Step [1065/2250] Loss: 0.0202\n",
      "Epoch [20/25] Step [1080/2250] Loss: 0.0417\n",
      "Epoch [20/25] Step [1095/2250] Loss: 0.0087\n",
      "Epoch [20/25] Step [1110/2250] Loss: 0.0047\n",
      "Epoch [20/25] Step [1125/2250] Loss: 0.0760\n",
      "Epoch [20/25] Step [1140/2250] Loss: 0.0107\n",
      "Epoch [20/25] Step [1155/2250] Loss: 0.0017\n",
      "Epoch [20/25] Step [1170/2250] Loss: 0.1405\n",
      "Epoch [20/25] Step [1185/2250] Loss: 0.0004\n",
      "Epoch [20/25] Step [1200/2250] Loss: 0.0001\n",
      "Epoch [20/25] Step [1215/2250] Loss: 0.2507\n",
      "Epoch [20/25] Step [1230/2250] Loss: 0.0166\n",
      "Epoch [20/25] Step [1245/2250] Loss: 0.1029\n",
      "Epoch [20/25] Step [1260/2250] Loss: 0.0026\n",
      "Epoch [20/25] Step [1275/2250] Loss: 0.0047\n",
      "Epoch [20/25] Step [1290/2250] Loss: 0.0007\n",
      "Epoch [20/25] Step [1305/2250] Loss: 0.0019\n",
      "Epoch [20/25] Step [1320/2250] Loss: 0.0046\n",
      "Epoch [20/25] Step [1335/2250] Loss: 0.0014\n",
      "Epoch [20/25] Step [1350/2250] Loss: 0.0094\n",
      "Epoch [20/25] Step [1365/2250] Loss: 0.0013\n",
      "Epoch [20/25] Step [1380/2250] Loss: 0.0058\n",
      "Epoch [20/25] Step [1395/2250] Loss: 0.0021\n",
      "Epoch [20/25] Step [1410/2250] Loss: 0.0250\n",
      "Epoch [20/25] Step [1425/2250] Loss: 0.0005\n",
      "Epoch [20/25] Step [1440/2250] Loss: 0.0003\n",
      "Epoch [20/25] Step [1455/2250] Loss: 0.0021\n",
      "Epoch [20/25] Step [1470/2250] Loss: 0.0078\n",
      "Epoch [20/25] Step [1485/2250] Loss: 0.0024\n",
      "Epoch [20/25] Step [1500/2250] Loss: 0.0005\n",
      "Epoch [20/25] Step [1515/2250] Loss: 0.0084\n",
      "Epoch [20/25] Step [1530/2250] Loss: 0.1438\n",
      "Epoch [20/25] Step [1545/2250] Loss: 0.0803\n",
      "Epoch [20/25] Step [1560/2250] Loss: 0.0049\n",
      "Epoch [20/25] Step [1575/2250] Loss: 0.0244\n",
      "Epoch [20/25] Step [1590/2250] Loss: 0.0006\n",
      "Epoch [20/25] Step [1605/2250] Loss: 0.0031\n",
      "Epoch [20/25] Step [1620/2250] Loss: 0.2549\n",
      "Epoch [20/25] Step [1635/2250] Loss: 0.0679\n",
      "Epoch [20/25] Step [1650/2250] Loss: 0.0030\n",
      "Epoch [20/25] Step [1665/2250] Loss: 0.0029\n",
      "Epoch [20/25] Step [1680/2250] Loss: 0.0069\n",
      "Epoch [20/25] Step [1695/2250] Loss: 0.0162\n",
      "Epoch [20/25] Step [1710/2250] Loss: 0.0019\n",
      "Epoch [20/25] Step [1725/2250] Loss: 0.0000\n",
      "Epoch [20/25] Step [1740/2250] Loss: 0.0004\n",
      "Epoch [20/25] Step [1755/2250] Loss: 0.0032\n",
      "Epoch [20/25] Step [1770/2250] Loss: 0.0022\n",
      "Epoch [20/25] Step [1785/2250] Loss: 0.0502\n",
      "Epoch [20/25] Step [1800/2250] Loss: 0.0895\n",
      "Epoch [20/25] Step [1815/2250] Loss: 0.0382\n",
      "Epoch [20/25] Step [1830/2250] Loss: 0.0008\n",
      "Epoch [20/25] Step [1845/2250] Loss: 0.0009\n",
      "Epoch [20/25] Step [1860/2250] Loss: 0.0051\n",
      "Epoch [20/25] Step [1875/2250] Loss: 0.0026\n",
      "Epoch [20/25] Step [1890/2250] Loss: 0.0028\n",
      "Epoch [20/25] Step [1905/2250] Loss: 0.0021\n",
      "Epoch [20/25] Step [1920/2250] Loss: 0.0029\n",
      "Epoch [20/25] Step [1935/2250] Loss: 0.0178\n",
      "Epoch [20/25] Step [1950/2250] Loss: 0.0033\n",
      "Epoch [20/25] Step [1965/2250] Loss: 0.0396\n",
      "Epoch [20/25] Step [1980/2250] Loss: 0.0093\n",
      "Epoch [20/25] Step [1995/2250] Loss: 0.0247\n",
      "Epoch [20/25] Step [2010/2250] Loss: 0.0045\n",
      "Epoch [20/25] Step [2025/2250] Loss: 0.0032\n",
      "Epoch [20/25] Step [2040/2250] Loss: 0.0026\n",
      "Epoch [20/25] Step [2055/2250] Loss: 0.0521\n",
      "Epoch [20/25] Step [2070/2250] Loss: 0.1555\n",
      "Epoch [20/25] Step [2085/2250] Loss: 0.0091\n",
      "Epoch [20/25] Step [2100/2250] Loss: 0.0013\n",
      "Epoch [20/25] Step [2115/2250] Loss: 0.0178\n",
      "Epoch [20/25] Step [2130/2250] Loss: 0.0217\n",
      "Epoch [20/25] Step [2145/2250] Loss: 0.0166\n",
      "Epoch [20/25] Step [2160/2250] Loss: 0.0002\n",
      "Epoch [20/25] Step [2175/2250] Loss: 0.0040\n",
      "Epoch [20/25] Step [2190/2250] Loss: 0.0003\n",
      "Epoch [20/25] Step [2205/2250] Loss: 0.0232\n",
      "Epoch [20/25] Step [2220/2250] Loss: 0.0003\n",
      "Epoch [20/25] Step [2235/2250] Loss: 0.0006\n",
      "Epoch [20/25] completed in 2009.57s\n",
      "Train Accuracy: 0.9913, Validation Accuracy: 0.9463\n",
      "\n",
      "Epoch [21/25] Step [0/2250] Loss: 0.0024\n",
      "Epoch [21/25] Step [15/2250] Loss: 0.0054\n",
      "Epoch [21/25] Step [30/2250] Loss: 0.0026\n",
      "Epoch [21/25] Step [45/2250] Loss: 0.0026\n",
      "Epoch [21/25] Step [60/2250] Loss: 0.0018\n",
      "Epoch [21/25] Step [75/2250] Loss: 0.0007\n",
      "Epoch [21/25] Step [90/2250] Loss: 0.0075\n",
      "Epoch [21/25] Step [105/2250] Loss: 0.0028\n",
      "Epoch [21/25] Step [120/2250] Loss: 0.0000\n",
      "Epoch [21/25] Step [135/2250] Loss: 0.0463\n",
      "Epoch [21/25] Step [150/2250] Loss: 0.0024\n",
      "Epoch [21/25] Step [165/2250] Loss: 0.0007\n",
      "Epoch [21/25] Step [180/2250] Loss: 0.0038\n",
      "Epoch [21/25] Step [195/2250] Loss: 0.0563\n",
      "Epoch [21/25] Step [210/2250] Loss: 0.0009\n",
      "Epoch [21/25] Step [225/2250] Loss: 0.0006\n",
      "Epoch [21/25] Step [240/2250] Loss: 0.0043\n",
      "Epoch [21/25] Step [255/2250] Loss: 0.0348\n",
      "Epoch [21/25] Step [270/2250] Loss: 0.0393\n",
      "Epoch [21/25] Step [285/2250] Loss: 0.0025\n",
      "Epoch [21/25] Step [300/2250] Loss: 0.0274\n",
      "Epoch [21/25] Step [315/2250] Loss: 0.0020\n",
      "Epoch [21/25] Step [330/2250] Loss: 0.0034\n",
      "Epoch [21/25] Step [345/2250] Loss: 0.0008\n",
      "Epoch [21/25] Step [360/2250] Loss: 0.0017\n",
      "Epoch [21/25] Step [375/2250] Loss: 0.0075\n",
      "Epoch [21/25] Step [390/2250] Loss: 0.0160\n",
      "Epoch [21/25] Step [405/2250] Loss: 0.0027\n",
      "Epoch [21/25] Step [420/2250] Loss: 0.0678\n",
      "Epoch [21/25] Step [435/2250] Loss: 0.0051\n",
      "Epoch [21/25] Step [450/2250] Loss: 0.0013\n",
      "Epoch [21/25] Step [465/2250] Loss: 0.0209\n",
      "Epoch [21/25] Step [480/2250] Loss: 0.0011\n",
      "Epoch [21/25] Step [495/2250] Loss: 0.0009\n",
      "Epoch [21/25] Step [510/2250] Loss: 0.0018\n",
      "Epoch [21/25] Step [525/2250] Loss: 0.0592\n",
      "Epoch [21/25] Step [540/2250] Loss: 0.0026\n",
      "Epoch [21/25] Step [555/2250] Loss: 0.0001\n",
      "Epoch [21/25] Step [570/2250] Loss: 0.0002\n",
      "Epoch [21/25] Step [585/2250] Loss: 0.0048\n",
      "Epoch [21/25] Step [600/2250] Loss: 0.1523\n",
      "Epoch [21/25] Step [615/2250] Loss: 0.0074\n",
      "Epoch [21/25] Step [630/2250] Loss: 0.0002\n",
      "Epoch [21/25] Step [645/2250] Loss: 0.0006\n",
      "Epoch [21/25] Step [660/2250] Loss: 0.0004\n",
      "Epoch [21/25] Step [675/2250] Loss: 0.0217\n",
      "Epoch [21/25] Step [690/2250] Loss: 0.0003\n",
      "Epoch [21/25] Step [705/2250] Loss: 0.0017\n",
      "Epoch [21/25] Step [720/2250] Loss: 0.0085\n",
      "Epoch [21/25] Step [735/2250] Loss: 0.0488\n",
      "Epoch [21/25] Step [750/2250] Loss: 0.0134\n",
      "Epoch [21/25] Step [765/2250] Loss: 0.0072\n",
      "Epoch [21/25] Step [780/2250] Loss: 0.0045\n",
      "Epoch [21/25] Step [795/2250] Loss: 0.0037\n",
      "Epoch [21/25] Step [810/2250] Loss: 0.0002\n",
      "Epoch [21/25] Step [825/2250] Loss: 0.0253\n",
      "Epoch [21/25] Step [840/2250] Loss: 0.0004\n",
      "Epoch [21/25] Step [855/2250] Loss: 0.0001\n",
      "Epoch [21/25] Step [870/2250] Loss: 0.0573\n",
      "Epoch [21/25] Step [885/2250] Loss: 0.0163\n",
      "Epoch [21/25] Step [900/2250] Loss: 0.0010\n",
      "Epoch [21/25] Step [915/2250] Loss: 0.1531\n",
      "Epoch [21/25] Step [930/2250] Loss: 0.0020\n",
      "Epoch [21/25] Step [945/2250] Loss: 0.0007\n",
      "Epoch [21/25] Step [960/2250] Loss: 0.0005\n",
      "Epoch [21/25] Step [975/2250] Loss: 0.0875\n",
      "Epoch [21/25] Step [990/2250] Loss: 0.0066\n",
      "Epoch [21/25] Step [1005/2250] Loss: 0.0563\n",
      "Epoch [21/25] Step [1020/2250] Loss: 0.1056\n",
      "Epoch [21/25] Step [1035/2250] Loss: 0.0477\n",
      "Epoch [21/25] Step [1050/2250] Loss: 0.0286\n",
      "Epoch [21/25] Step [1065/2250] Loss: 0.0022\n",
      "Epoch [21/25] Step [1080/2250] Loss: 0.0189\n",
      "Epoch [21/25] Step [1095/2250] Loss: 0.0008\n",
      "Epoch [21/25] Step [1110/2250] Loss: 0.0775\n",
      "Epoch [21/25] Step [1125/2250] Loss: 0.0011\n",
      "Epoch [21/25] Step [1140/2250] Loss: 0.0010\n",
      "Epoch [21/25] Step [1155/2250] Loss: 0.0004\n",
      "Epoch [21/25] Step [1170/2250] Loss: 0.2393\n",
      "Epoch [21/25] Step [1185/2250] Loss: 0.0891\n",
      "Epoch [21/25] Step [1200/2250] Loss: 0.0011\n",
      "Epoch [21/25] Step [1215/2250] Loss: 0.0018\n",
      "Epoch [21/25] Step [1230/2250] Loss: 0.0125\n",
      "Epoch [21/25] Step [1245/2250] Loss: 0.0619\n",
      "Epoch [21/25] Step [1260/2250] Loss: 0.0090\n",
      "Epoch [21/25] Step [1275/2250] Loss: 0.0022\n",
      "Epoch [21/25] Step [1290/2250] Loss: 0.0008\n",
      "Epoch [21/25] Step [1305/2250] Loss: 0.0008\n",
      "Epoch [21/25] Step [1320/2250] Loss: 0.0003\n",
      "Epoch [21/25] Step [1335/2250] Loss: 0.1441\n",
      "Epoch [21/25] Step [1350/2250] Loss: 0.0328\n",
      "Epoch [21/25] Step [1365/2250] Loss: 0.0037\n",
      "Epoch [21/25] Step [1380/2250] Loss: 0.0076\n",
      "Epoch [21/25] Step [1395/2250] Loss: 0.0000\n",
      "Epoch [21/25] Step [1410/2250] Loss: 0.0056\n",
      "Epoch [21/25] Step [1425/2250] Loss: 0.0164\n",
      "Epoch [21/25] Step [1440/2250] Loss: 0.0014\n",
      "Epoch [21/25] Step [1455/2250] Loss: 0.0001\n",
      "Epoch [21/25] Step [1470/2250] Loss: 0.1588\n",
      "Epoch [21/25] Step [1485/2250] Loss: 0.0006\n",
      "Epoch [21/25] Step [1500/2250] Loss: 0.0283\n",
      "Epoch [21/25] Step [1515/2250] Loss: 0.0875\n",
      "Epoch [21/25] Step [1530/2250] Loss: 0.0018\n",
      "Epoch [21/25] Step [1545/2250] Loss: 0.0013\n",
      "Epoch [21/25] Step [1560/2250] Loss: 0.0856\n",
      "Epoch [21/25] Step [1575/2250] Loss: 0.0129\n",
      "Epoch [21/25] Step [1590/2250] Loss: 0.0758\n",
      "Epoch [21/25] Step [1605/2250] Loss: 0.1079\n",
      "Epoch [21/25] Step [1620/2250] Loss: 0.0330\n",
      "Epoch [21/25] Step [1635/2250] Loss: 0.3682\n",
      "Epoch [21/25] Step [1650/2250] Loss: 0.0445\n",
      "Epoch [21/25] Step [1665/2250] Loss: 0.0003\n",
      "Epoch [21/25] Step [1680/2250] Loss: 0.0196\n",
      "Epoch [21/25] Step [1695/2250] Loss: 0.0013\n",
      "Epoch [21/25] Step [1710/2250] Loss: 0.1555\n",
      "Epoch [21/25] Step [1725/2250] Loss: 0.0133\n",
      "Epoch [21/25] Step [1740/2250] Loss: 0.0230\n",
      "Epoch [21/25] Step [1755/2250] Loss: 0.0005\n",
      "Epoch [21/25] Step [1770/2250] Loss: 0.0252\n",
      "Epoch [21/25] Step [1785/2250] Loss: 0.0113\n",
      "Epoch [21/25] Step [1800/2250] Loss: 0.0353\n",
      "Epoch [21/25] Step [1815/2250] Loss: 0.0067\n",
      "Epoch [21/25] Step [1830/2250] Loss: 0.0502\n",
      "Epoch [21/25] Step [1845/2250] Loss: 0.0719\n",
      "Epoch [21/25] Step [1860/2250] Loss: 0.0028\n",
      "Epoch [21/25] Step [1875/2250] Loss: 0.0004\n",
      "Epoch [21/25] Step [1890/2250] Loss: 0.0003\n",
      "Epoch [21/25] Step [1905/2250] Loss: 0.0129\n",
      "Epoch [21/25] Step [1920/2250] Loss: 0.0152\n",
      "Epoch [21/25] Step [1935/2250] Loss: 0.0059\n",
      "Epoch [21/25] Step [1950/2250] Loss: 0.1300\n",
      "Epoch [21/25] Step [1965/2250] Loss: 0.0709\n",
      "Epoch [21/25] Step [1980/2250] Loss: 0.0011\n",
      "Epoch [21/25] Step [1995/2250] Loss: 0.0036\n",
      "Epoch [21/25] Step [2010/2250] Loss: 0.0189\n",
      "Epoch [21/25] Step [2025/2250] Loss: 0.0010\n",
      "Epoch [21/25] Step [2040/2250] Loss: 0.0278\n",
      "Epoch [21/25] Step [2055/2250] Loss: 0.0000\n",
      "Epoch [21/25] Step [2070/2250] Loss: 0.0202\n",
      "Epoch [21/25] Step [2085/2250] Loss: 0.0045\n",
      "Epoch [21/25] Step [2100/2250] Loss: 0.0379\n",
      "Epoch [21/25] Step [2115/2250] Loss: 0.0168\n",
      "Epoch [21/25] Step [2130/2250] Loss: 0.0041\n",
      "Epoch [21/25] Step [2145/2250] Loss: 0.0185\n",
      "Epoch [21/25] Step [2160/2250] Loss: 0.0025\n",
      "Epoch [21/25] Step [2175/2250] Loss: 0.0012\n",
      "Epoch [21/25] Step [2190/2250] Loss: 0.0074\n",
      "Epoch [21/25] Step [2205/2250] Loss: 0.0242\n",
      "Epoch [21/25] Step [2220/2250] Loss: 0.0088\n",
      "Epoch [21/25] Step [2235/2250] Loss: 0.0013\n",
      "Epoch [21/25] completed in 2007.19s\n",
      "Train Accuracy: 0.9916, Validation Accuracy: 0.9499\n",
      "\n",
      "âœ… Best model updated and saved with Val Acc: 0.9499\n",
      "\n",
      "Epoch [22/25] Step [0/2250] Loss: 0.0026\n",
      "Epoch [22/25] Step [15/2250] Loss: 0.0006\n",
      "Epoch [22/25] Step [30/2250] Loss: 0.0183\n",
      "Epoch [22/25] Step [45/2250] Loss: 0.0227\n",
      "Epoch [22/25] Step [60/2250] Loss: 0.1368\n",
      "Epoch [22/25] Step [75/2250] Loss: 0.0709\n",
      "Epoch [22/25] Step [90/2250] Loss: 0.0045\n",
      "Epoch [22/25] Step [105/2250] Loss: 0.0018\n",
      "Epoch [22/25] Step [120/2250] Loss: 0.0117\n",
      "Epoch [22/25] Step [135/2250] Loss: 0.0023\n",
      "Epoch [22/25] Step [150/2250] Loss: 0.0020\n",
      "Epoch [22/25] Step [165/2250] Loss: 0.0009\n",
      "Epoch [22/25] Step [180/2250] Loss: 0.0011\n",
      "Epoch [22/25] Step [195/2250] Loss: 0.1325\n",
      "Epoch [22/25] Step [210/2250] Loss: 0.0132\n",
      "Epoch [22/25] Step [225/2250] Loss: 0.0038\n",
      "Epoch [22/25] Step [240/2250] Loss: 0.0002\n",
      "Epoch [22/25] Step [255/2250] Loss: 0.0135\n",
      "Epoch [22/25] Step [270/2250] Loss: 0.0080\n",
      "Epoch [22/25] Step [285/2250] Loss: 0.0052\n",
      "Epoch [22/25] Step [300/2250] Loss: 0.0024\n",
      "Epoch [22/25] Step [315/2250] Loss: 0.0000\n",
      "Epoch [22/25] Step [330/2250] Loss: 0.0012\n",
      "Epoch [22/25] Step [345/2250] Loss: 0.0008\n",
      "Epoch [22/25] Step [360/2250] Loss: 0.0119\n",
      "Epoch [22/25] Step [375/2250] Loss: 0.1259\n",
      "Epoch [22/25] Step [390/2250] Loss: 0.0004\n",
      "Epoch [22/25] Step [405/2250] Loss: 0.0002\n",
      "Epoch [22/25] Step [420/2250] Loss: 0.0011\n",
      "Epoch [22/25] Step [435/2250] Loss: 0.0066\n",
      "Epoch [22/25] Step [450/2250] Loss: 0.0132\n",
      "Epoch [22/25] Step [465/2250] Loss: 0.0274\n",
      "Epoch [22/25] Step [480/2250] Loss: 0.1076\n",
      "Epoch [22/25] Step [495/2250] Loss: 0.0120\n",
      "Epoch [22/25] Step [510/2250] Loss: 0.1109\n",
      "Epoch [22/25] Step [525/2250] Loss: 0.0152\n",
      "Epoch [22/25] Step [540/2250] Loss: 0.0086\n",
      "Epoch [22/25] Step [555/2250] Loss: 0.0213\n",
      "Epoch [22/25] Step [570/2250] Loss: 0.1960\n",
      "Epoch [22/25] Step [585/2250] Loss: 0.0245\n",
      "Epoch [22/25] Step [600/2250] Loss: 0.0104\n",
      "Epoch [22/25] Step [615/2250] Loss: 0.0121\n",
      "Epoch [22/25] Step [630/2250] Loss: 0.0054\n",
      "Epoch [22/25] Step [645/2250] Loss: 0.0223\n",
      "Epoch [22/25] Step [660/2250] Loss: 0.0022\n",
      "Epoch [22/25] Step [675/2250] Loss: 0.0010\n",
      "Epoch [22/25] Step [690/2250] Loss: 0.0282\n",
      "Epoch [22/25] Step [705/2250] Loss: 0.0060\n",
      "Epoch [22/25] Step [720/2250] Loss: 0.0037\n",
      "Epoch [22/25] Step [735/2250] Loss: 0.0375\n",
      "Epoch [22/25] Step [750/2250] Loss: 0.0045\n",
      "Epoch [22/25] Step [765/2250] Loss: 0.0020\n",
      "Epoch [22/25] Step [780/2250] Loss: 0.0513\n",
      "Epoch [22/25] Step [795/2250] Loss: 0.0259\n",
      "Epoch [22/25] Step [810/2250] Loss: 0.0005\n",
      "Epoch [22/25] Step [825/2250] Loss: 0.0120\n",
      "Epoch [22/25] Step [840/2250] Loss: 0.0941\n",
      "Epoch [22/25] Step [855/2250] Loss: 0.0008\n",
      "Epoch [22/25] Step [870/2250] Loss: 0.0005\n",
      "Epoch [22/25] Step [885/2250] Loss: 0.0011\n",
      "Epoch [22/25] Step [900/2250] Loss: 0.0040\n",
      "Epoch [22/25] Step [915/2250] Loss: 0.0132\n",
      "Epoch [22/25] Step [930/2250] Loss: 0.1168\n",
      "Epoch [22/25] Step [945/2250] Loss: 0.0004\n",
      "Epoch [22/25] Step [960/2250] Loss: 0.0008\n",
      "Epoch [22/25] Step [975/2250] Loss: 0.0376\n",
      "Epoch [22/25] Step [990/2250] Loss: 0.0091\n",
      "Epoch [22/25] Step [1005/2250] Loss: 0.0757\n",
      "Epoch [22/25] Step [1020/2250] Loss: 0.1347\n",
      "Epoch [22/25] Step [1035/2250] Loss: 0.0069\n",
      "Epoch [22/25] Step [1050/2250] Loss: 0.0030\n",
      "Epoch [22/25] Step [1065/2250] Loss: 0.0070\n",
      "Epoch [22/25] Step [1080/2250] Loss: 0.0007\n",
      "Epoch [22/25] Step [1095/2250] Loss: 0.0001\n",
      "Epoch [22/25] Step [1110/2250] Loss: 0.0095\n",
      "Epoch [22/25] Step [1125/2250] Loss: 0.0003\n",
      "Epoch [22/25] Step [1140/2250] Loss: 0.0083\n",
      "Epoch [22/25] Step [1155/2250] Loss: 0.1200\n",
      "Epoch [22/25] Step [1170/2250] Loss: 0.0001\n",
      "Epoch [22/25] Step [1185/2250] Loss: 0.0936\n",
      "Epoch [22/25] Step [1200/2250] Loss: 0.0001\n",
      "Epoch [22/25] Step [1215/2250] Loss: 0.0002\n",
      "Epoch [22/25] Step [1230/2250] Loss: 0.0037\n",
      "Epoch [22/25] Step [1245/2250] Loss: 0.0074\n",
      "Epoch [22/25] Step [1260/2250] Loss: 0.0347\n",
      "Epoch [22/25] Step [1275/2250] Loss: 0.0035\n",
      "Epoch [22/25] Step [1290/2250] Loss: 0.0002\n",
      "Epoch [22/25] Step [1305/2250] Loss: 0.0131\n",
      "Epoch [22/25] Step [1320/2250] Loss: 0.0006\n",
      "Epoch [22/25] Step [1335/2250] Loss: 0.0860\n",
      "Epoch [22/25] Step [1350/2250] Loss: 0.0243\n",
      "Epoch [22/25] Step [1365/2250] Loss: 0.1148\n",
      "Epoch [22/25] Step [1380/2250] Loss: 0.0169\n",
      "Epoch [22/25] Step [1395/2250] Loss: 0.0015\n",
      "Epoch [22/25] Step [1410/2250] Loss: 0.0016\n",
      "Epoch [22/25] Step [1425/2250] Loss: 0.0023\n",
      "Epoch [22/25] Step [1440/2250] Loss: 0.0191\n",
      "Epoch [22/25] Step [1455/2250] Loss: 0.0188\n",
      "Epoch [22/25] Step [1470/2250] Loss: 0.0027\n",
      "Epoch [22/25] Step [1485/2250] Loss: 0.0132\n",
      "Epoch [22/25] Step [1500/2250] Loss: 0.0264\n",
      "Epoch [22/25] Step [1515/2250] Loss: 0.0021\n",
      "Epoch [22/25] Step [1530/2250] Loss: 0.1426\n",
      "Epoch [22/25] Step [1545/2250] Loss: 0.0024\n",
      "Epoch [22/25] Step [1560/2250] Loss: 0.0019\n",
      "Epoch [22/25] Step [1575/2250] Loss: 0.0053\n",
      "Epoch [22/25] Step [1590/2250] Loss: 0.0018\n",
      "Epoch [22/25] Step [1605/2250] Loss: 0.0009\n",
      "Epoch [22/25] Step [1620/2250] Loss: 0.0020\n",
      "Epoch [22/25] Step [1635/2250] Loss: 0.0008\n",
      "Epoch [22/25] Step [1650/2250] Loss: 0.0054\n",
      "Epoch [22/25] Step [1665/2250] Loss: 0.0021\n",
      "Epoch [22/25] Step [1680/2250] Loss: 0.0304\n",
      "Epoch [22/25] Step [1695/2250] Loss: 0.0020\n",
      "Epoch [22/25] Step [1710/2250] Loss: 0.0144\n",
      "Epoch [22/25] Step [1725/2250] Loss: 0.0161\n",
      "Epoch [22/25] Step [1740/2250] Loss: 0.0518\n",
      "Epoch [22/25] Step [1755/2250] Loss: 0.0117\n",
      "Epoch [22/25] Step [1770/2250] Loss: 0.0225\n",
      "Epoch [22/25] Step [1785/2250] Loss: 0.0033\n",
      "Epoch [22/25] Step [1800/2250] Loss: 0.0156\n",
      "Epoch [22/25] Step [1815/2250] Loss: 0.0011\n",
      "Epoch [22/25] Step [1830/2250] Loss: 0.3672\n",
      "Epoch [22/25] Step [1845/2250] Loss: 0.0322\n",
      "Epoch [22/25] Step [1860/2250] Loss: 0.0017\n",
      "Epoch [22/25] Step [1875/2250] Loss: 0.0001\n",
      "Epoch [22/25] Step [1890/2250] Loss: 0.0086\n",
      "Epoch [22/25] Step [1905/2250] Loss: 0.0997\n",
      "Epoch [22/25] Step [1920/2250] Loss: 0.0010\n",
      "Epoch [22/25] Step [1935/2250] Loss: 0.0056\n",
      "Epoch [22/25] Step [1950/2250] Loss: 0.0014\n",
      "Epoch [22/25] Step [1965/2250] Loss: 0.0049\n",
      "Epoch [22/25] Step [1980/2250] Loss: 0.0002\n",
      "Epoch [22/25] Step [1995/2250] Loss: 0.0066\n",
      "Epoch [22/25] Step [2010/2250] Loss: 0.0177\n",
      "Epoch [22/25] Step [2025/2250] Loss: 0.0259\n",
      "Epoch [22/25] Step [2040/2250] Loss: 0.0014\n",
      "Epoch [22/25] Step [2055/2250] Loss: 0.0117\n",
      "Epoch [22/25] Step [2070/2250] Loss: 0.0013\n",
      "Epoch [22/25] Step [2085/2250] Loss: 0.0022\n",
      "Epoch [22/25] Step [2100/2250] Loss: 0.0010\n",
      "Epoch [22/25] Step [2115/2250] Loss: 0.0066\n",
      "Epoch [22/25] Step [2130/2250] Loss: 0.0055\n",
      "Epoch [22/25] Step [2145/2250] Loss: 0.0583\n",
      "Epoch [22/25] Step [2160/2250] Loss: 0.0574\n",
      "Epoch [22/25] Step [2175/2250] Loss: 0.0407\n",
      "Epoch [22/25] Step [2190/2250] Loss: 0.0090\n",
      "Epoch [22/25] Step [2205/2250] Loss: 0.0018\n",
      "Epoch [22/25] Step [2220/2250] Loss: 0.0138\n",
      "Epoch [22/25] Step [2235/2250] Loss: 0.0008\n",
      "Epoch [22/25] completed in 2011.00s\n",
      "Train Accuracy: 0.9916, Validation Accuracy: 0.9489\n",
      "\n",
      "Epoch [23/25] Step [0/2250] Loss: 0.0100\n",
      "Epoch [23/25] Step [15/2250] Loss: 0.0000\n",
      "Epoch [23/25] Step [30/2250] Loss: 0.0287\n",
      "Epoch [23/25] Step [45/2250] Loss: 0.0043\n",
      "Epoch [23/25] Step [60/2250] Loss: 0.0034\n",
      "Epoch [23/25] Step [75/2250] Loss: 0.0489\n",
      "Epoch [23/25] Step [90/2250] Loss: 0.0000\n",
      "Epoch [23/25] Step [105/2250] Loss: 0.0020\n",
      "Epoch [23/25] Step [120/2250] Loss: 0.0120\n",
      "Epoch [23/25] Step [135/2250] Loss: 0.0033\n",
      "Epoch [23/25] Step [150/2250] Loss: 0.1738\n",
      "Epoch [23/25] Step [165/2250] Loss: 0.0016\n",
      "Epoch [23/25] Step [180/2250] Loss: 0.0265\n",
      "Epoch [23/25] Step [195/2250] Loss: 0.0020\n",
      "Epoch [23/25] Step [210/2250] Loss: 0.0228\n",
      "Epoch [23/25] Step [225/2250] Loss: 0.0135\n",
      "Epoch [23/25] Step [240/2250] Loss: 0.0007\n",
      "Epoch [23/25] Step [255/2250] Loss: 0.0029\n",
      "Epoch [23/25] Step [270/2250] Loss: 0.0030\n",
      "Epoch [23/25] Step [285/2250] Loss: 0.0737\n",
      "Epoch [23/25] Step [300/2250] Loss: 0.0025\n",
      "Epoch [23/25] Step [315/2250] Loss: 0.0591\n",
      "Epoch [23/25] Step [330/2250] Loss: 0.0008\n",
      "Epoch [23/25] Step [345/2250] Loss: 0.0271\n",
      "Epoch [23/25] Step [360/2250] Loss: 0.0009\n",
      "Epoch [23/25] Step [375/2250] Loss: 0.0160\n",
      "Epoch [23/25] Step [390/2250] Loss: 0.0002\n",
      "Epoch [23/25] Step [405/2250] Loss: 0.0197\n",
      "Epoch [23/25] Step [420/2250] Loss: 0.0112\n",
      "Epoch [23/25] Step [435/2250] Loss: 0.0001\n",
      "Epoch [23/25] Step [450/2250] Loss: 0.2619\n",
      "Epoch [23/25] Step [465/2250] Loss: 0.0013\n",
      "Epoch [23/25] Step [480/2250] Loss: 0.0439\n",
      "Epoch [23/25] Step [495/2250] Loss: 0.0090\n",
      "Epoch [23/25] Step [510/2250] Loss: 0.0002\n",
      "Epoch [23/25] Step [525/2250] Loss: 0.0022\n",
      "Epoch [23/25] Step [540/2250] Loss: 0.0469\n",
      "Epoch [23/25] Step [555/2250] Loss: 0.0169\n",
      "Epoch [23/25] Step [570/2250] Loss: 0.0009\n",
      "Epoch [23/25] Step [585/2250] Loss: 0.0009\n",
      "Epoch [23/25] Step [600/2250] Loss: 0.0006\n",
      "Epoch [23/25] Step [615/2250] Loss: 0.0017\n",
      "Epoch [23/25] Step [630/2250] Loss: 0.0106\n",
      "Epoch [23/25] Step [645/2250] Loss: 0.0036\n",
      "Epoch [23/25] Step [660/2250] Loss: 0.0062\n",
      "Epoch [23/25] Step [675/2250] Loss: 0.1156\n",
      "Epoch [23/25] Step [690/2250] Loss: 0.1878\n",
      "Epoch [23/25] Step [705/2250] Loss: 0.0546\n",
      "Epoch [23/25] Step [720/2250] Loss: 0.0041\n",
      "Epoch [23/25] Step [735/2250] Loss: 0.0730\n",
      "Epoch [23/25] Step [750/2250] Loss: 0.0002\n",
      "Epoch [23/25] Step [765/2250] Loss: 0.0004\n",
      "Epoch [23/25] Step [780/2250] Loss: 0.0012\n",
      "Epoch [23/25] Step [795/2250] Loss: 0.0025\n",
      "Epoch [23/25] Step [810/2250] Loss: 0.0015\n",
      "Epoch [23/25] Step [825/2250] Loss: 0.0048\n",
      "Epoch [23/25] Step [840/2250] Loss: 0.0006\n",
      "Epoch [23/25] Step [855/2250] Loss: 0.0002\n",
      "Epoch [23/25] Step [870/2250] Loss: 0.0062\n",
      "Epoch [23/25] Step [885/2250] Loss: 0.0274\n",
      "Epoch [23/25] Step [900/2250] Loss: 0.0035\n",
      "Epoch [23/25] Step [915/2250] Loss: 0.0048\n",
      "Epoch [23/25] Step [930/2250] Loss: 0.0044\n",
      "Epoch [23/25] Step [945/2250] Loss: 0.0019\n",
      "Epoch [23/25] Step [960/2250] Loss: 0.0037\n",
      "Epoch [23/25] Step [975/2250] Loss: 0.0183\n",
      "Epoch [23/25] Step [990/2250] Loss: 0.0006\n",
      "Epoch [23/25] Step [1005/2250] Loss: 0.0002\n",
      "Epoch [23/25] Step [1020/2250] Loss: 0.0003\n",
      "Epoch [23/25] Step [1035/2250] Loss: 0.0011\n",
      "Epoch [23/25] Step [1050/2250] Loss: 0.0140\n",
      "Epoch [23/25] Step [1065/2250] Loss: 0.0009\n",
      "Epoch [23/25] Step [1080/2250] Loss: 0.0140\n",
      "Epoch [23/25] Step [1095/2250] Loss: 0.0074\n",
      "Epoch [23/25] Step [1110/2250] Loss: 0.0747\n",
      "Epoch [23/25] Step [1125/2250] Loss: 0.0031\n",
      "Epoch [23/25] Step [1140/2250] Loss: 0.0146\n",
      "Epoch [23/25] Step [1155/2250] Loss: 0.1265\n",
      "Epoch [23/25] Step [1170/2250] Loss: 0.0113\n",
      "Epoch [23/25] Step [1185/2250] Loss: 0.0759\n",
      "Epoch [23/25] Step [1200/2250] Loss: 0.1019\n",
      "Epoch [23/25] Step [1215/2250] Loss: 0.0067\n",
      "Epoch [23/25] Step [1230/2250] Loss: 0.0072\n",
      "Epoch [23/25] Step [1245/2250] Loss: 0.0510\n",
      "Epoch [23/25] Step [1260/2250] Loss: 0.1315\n",
      "Epoch [23/25] Step [1275/2250] Loss: 0.1114\n",
      "Epoch [23/25] Step [1290/2250] Loss: 0.0193\n",
      "Epoch [23/25] Step [1305/2250] Loss: 0.0012\n",
      "Epoch [23/25] Step [1320/2250] Loss: 0.0041\n",
      "Epoch [23/25] Step [1335/2250] Loss: 0.0118\n",
      "Epoch [23/25] Step [1350/2250] Loss: 0.0077\n",
      "Epoch [23/25] Step [1365/2250] Loss: 0.0149\n",
      "Epoch [23/25] Step [1380/2250] Loss: 0.0072\n",
      "Epoch [23/25] Step [1395/2250] Loss: 0.0002\n",
      "Epoch [23/25] Step [1410/2250] Loss: 0.0007\n",
      "Epoch [23/25] Step [1425/2250] Loss: 0.0201\n",
      "Epoch [23/25] Step [1440/2250] Loss: 0.0035\n",
      "Epoch [23/25] Step [1455/2250] Loss: 0.0053\n",
      "Epoch [23/25] Step [1470/2250] Loss: 0.0017\n",
      "Epoch [23/25] Step [1485/2250] Loss: 0.0037\n",
      "Epoch [23/25] Step [1500/2250] Loss: 0.0136\n",
      "Epoch [23/25] Step [1515/2250] Loss: 0.0012\n",
      "Epoch [23/25] Step [1530/2250] Loss: 0.0007\n",
      "Epoch [23/25] Step [1545/2250] Loss: 0.0018\n",
      "Epoch [23/25] Step [1560/2250] Loss: 0.0040\n",
      "Epoch [23/25] Step [1575/2250] Loss: 0.0072\n",
      "Epoch [23/25] Step [1590/2250] Loss: 0.0012\n",
      "Epoch [23/25] Step [1605/2250] Loss: 0.0030\n",
      "Epoch [23/25] Step [1620/2250] Loss: 0.1138\n",
      "Epoch [23/25] Step [1635/2250] Loss: 0.0076\n",
      "Epoch [23/25] Step [1650/2250] Loss: 0.0660\n",
      "Epoch [23/25] Step [1665/2250] Loss: 0.0012\n",
      "Epoch [23/25] Step [1680/2250] Loss: 0.0132\n",
      "Epoch [23/25] Step [1695/2250] Loss: 0.0003\n",
      "Epoch [23/25] Step [1710/2250] Loss: 0.0294\n",
      "Epoch [23/25] Step [1725/2250] Loss: 0.0004\n",
      "Epoch [23/25] Step [1740/2250] Loss: 0.0011\n",
      "Epoch [23/25] Step [1755/2250] Loss: 0.0001\n",
      "Epoch [23/25] Step [1770/2250] Loss: 0.0005\n",
      "Epoch [23/25] Step [1785/2250] Loss: 0.0004\n",
      "Epoch [23/25] Step [1800/2250] Loss: 0.0350\n",
      "Epoch [23/25] Step [1815/2250] Loss: 0.0004\n",
      "Epoch [23/25] Step [1830/2250] Loss: 0.0048\n",
      "Epoch [23/25] Step [1845/2250] Loss: 0.0259\n",
      "Epoch [23/25] Step [1860/2250] Loss: 0.0018\n",
      "Epoch [23/25] Step [1875/2250] Loss: 0.0027\n",
      "Epoch [23/25] Step [1890/2250] Loss: 0.0038\n",
      "Epoch [23/25] Step [1905/2250] Loss: 0.0423\n",
      "Epoch [23/25] Step [1920/2250] Loss: 0.1357\n",
      "Epoch [23/25] Step [1935/2250] Loss: 0.0675\n",
      "Epoch [23/25] Step [1950/2250] Loss: 0.0623\n",
      "Epoch [23/25] Step [1965/2250] Loss: 0.2115\n",
      "Epoch [23/25] Step [1980/2250] Loss: 0.1029\n",
      "Epoch [23/25] Step [1995/2250] Loss: 0.0097\n",
      "Epoch [23/25] Step [2010/2250] Loss: 0.0082\n",
      "Epoch [23/25] Step [2025/2250] Loss: 0.0007\n",
      "Epoch [23/25] Step [2040/2250] Loss: 0.0124\n",
      "Epoch [23/25] Step [2055/2250] Loss: 0.0015\n",
      "Epoch [23/25] Step [2070/2250] Loss: 0.0104\n",
      "Epoch [23/25] Step [2085/2250] Loss: 0.0002\n",
      "Epoch [23/25] Step [2100/2250] Loss: 0.0004\n",
      "Epoch [23/25] Step [2115/2250] Loss: 0.0006\n",
      "Epoch [23/25] Step [2130/2250] Loss: 0.0843\n",
      "Epoch [23/25] Step [2145/2250] Loss: 0.0002\n",
      "Epoch [23/25] Step [2160/2250] Loss: 0.2722\n",
      "Epoch [23/25] Step [2175/2250] Loss: 0.0758\n",
      "Epoch [23/25] Step [2190/2250] Loss: 0.0819\n",
      "Epoch [23/25] Step [2205/2250] Loss: 0.0009\n",
      "Epoch [23/25] Step [2220/2250] Loss: 0.0045\n",
      "Epoch [23/25] Step [2235/2250] Loss: 0.0127\n",
      "Epoch [23/25] completed in 2010.52s\n",
      "Train Accuracy: 0.9922, Validation Accuracy: 0.9479\n",
      "\n",
      "Epoch [24/25] Step [0/2250] Loss: 0.0040\n",
      "Epoch [24/25] Step [15/2250] Loss: 0.0159\n",
      "Epoch [24/25] Step [30/2250] Loss: 0.0207\n",
      "Epoch [24/25] Step [45/2250] Loss: 0.0018\n",
      "Epoch [24/25] Step [60/2250] Loss: 0.0007\n",
      "Epoch [24/25] Step [75/2250] Loss: 0.0002\n",
      "Epoch [24/25] Step [90/2250] Loss: 0.0166\n",
      "Epoch [24/25] Step [105/2250] Loss: 0.0215\n",
      "Epoch [24/25] Step [120/2250] Loss: 0.0006\n",
      "Epoch [24/25] Step [135/2250] Loss: 0.0002\n",
      "Epoch [24/25] Step [150/2250] Loss: 0.0028\n",
      "Epoch [24/25] Step [165/2250] Loss: 0.0001\n",
      "Epoch [24/25] Step [180/2250] Loss: 0.0302\n",
      "Epoch [24/25] Step [195/2250] Loss: 0.0001\n",
      "Epoch [24/25] Step [210/2250] Loss: 0.0006\n",
      "Epoch [24/25] Step [225/2250] Loss: 0.0012\n",
      "Epoch [24/25] Step [240/2250] Loss: 0.0011\n",
      "Epoch [24/25] Step [255/2250] Loss: 0.0261\n",
      "Epoch [24/25] Step [270/2250] Loss: 0.0000\n",
      "Epoch [24/25] Step [285/2250] Loss: 0.0001\n",
      "Epoch [24/25] Step [300/2250] Loss: 0.0010\n",
      "Epoch [24/25] Step [315/2250] Loss: 0.0152\n",
      "Epoch [24/25] Step [330/2250] Loss: 0.0007\n",
      "Epoch [24/25] Step [345/2250] Loss: 0.0405\n",
      "Epoch [24/25] Step [360/2250] Loss: 0.0157\n",
      "Epoch [24/25] Step [375/2250] Loss: 0.0017\n",
      "Epoch [24/25] Step [390/2250] Loss: 0.0051\n",
      "Epoch [24/25] Step [405/2250] Loss: 0.0005\n",
      "Epoch [24/25] Step [420/2250] Loss: 0.0441\n",
      "Epoch [24/25] Step [435/2250] Loss: 0.0108\n",
      "Epoch [24/25] Step [450/2250] Loss: 0.0568\n",
      "Epoch [24/25] Step [465/2250] Loss: 0.0070\n",
      "Epoch [24/25] Step [480/2250] Loss: 0.0002\n",
      "Epoch [24/25] Step [495/2250] Loss: 0.0010\n",
      "Epoch [24/25] Step [510/2250] Loss: 0.0000\n",
      "Epoch [24/25] Step [525/2250] Loss: 0.0003\n",
      "Epoch [24/25] Step [540/2250] Loss: 0.0001\n",
      "Epoch [24/25] Step [555/2250] Loss: 0.0096\n",
      "Epoch [24/25] Step [570/2250] Loss: 0.0001\n",
      "Epoch [24/25] Step [585/2250] Loss: 0.0000\n",
      "Epoch [24/25] Step [600/2250] Loss: 0.0018\n",
      "Epoch [24/25] Step [615/2250] Loss: 0.0021\n",
      "Epoch [24/25] Step [630/2250] Loss: 0.0921\n",
      "Epoch [24/25] Step [645/2250] Loss: 0.0797\n",
      "Epoch [24/25] Step [660/2250] Loss: 0.0003\n",
      "Epoch [24/25] Step [675/2250] Loss: 0.0072\n",
      "Epoch [24/25] Step [690/2250] Loss: 0.0108\n",
      "Epoch [24/25] Step [705/2250] Loss: 0.0012\n",
      "Epoch [24/25] Step [720/2250] Loss: 0.0031\n",
      "Epoch [24/25] Step [735/2250] Loss: 0.1780\n",
      "Epoch [24/25] Step [750/2250] Loss: 0.0366\n",
      "Epoch [24/25] Step [765/2250] Loss: 0.0285\n",
      "Epoch [24/25] Step [780/2250] Loss: 0.0006\n",
      "Epoch [24/25] Step [795/2250] Loss: 0.0020\n",
      "Epoch [24/25] Step [810/2250] Loss: 0.0036\n",
      "Epoch [24/25] Step [825/2250] Loss: 0.0138\n",
      "Epoch [24/25] Step [840/2250] Loss: 0.0020\n",
      "Epoch [24/25] Step [855/2250] Loss: 0.0005\n",
      "Epoch [24/25] Step [870/2250] Loss: 0.0007\n",
      "Epoch [24/25] Step [885/2250] Loss: 0.0038\n",
      "Epoch [24/25] Step [900/2250] Loss: 0.0043\n",
      "Epoch [24/25] Step [915/2250] Loss: 0.0079\n",
      "Epoch [24/25] Step [930/2250] Loss: 0.0002\n",
      "Epoch [24/25] Step [945/2250] Loss: 0.0002\n",
      "Epoch [24/25] Step [960/2250] Loss: 0.0635\n",
      "Epoch [24/25] Step [975/2250] Loss: 0.0765\n",
      "Epoch [24/25] Step [990/2250] Loss: 0.0001\n",
      "Epoch [24/25] Step [1005/2250] Loss: 0.0008\n",
      "Epoch [24/25] Step [1020/2250] Loss: 0.0103\n",
      "Epoch [24/25] Step [1035/2250] Loss: 0.0510\n",
      "Epoch [24/25] Step [1050/2250] Loss: 0.0004\n",
      "Epoch [24/25] Step [1065/2250] Loss: 0.0101\n",
      "Epoch [24/25] Step [1080/2250] Loss: 0.0005\n",
      "Epoch [24/25] Step [1095/2250] Loss: 0.0005\n",
      "Epoch [24/25] Step [1110/2250] Loss: 0.2710\n",
      "Epoch [24/25] Step [1125/2250] Loss: 0.0035\n",
      "Epoch [24/25] Step [1140/2250] Loss: 0.0008\n",
      "Epoch [24/25] Step [1155/2250] Loss: 0.0089\n",
      "Epoch [24/25] Step [1170/2250] Loss: 0.0383\n",
      "Epoch [24/25] Step [1185/2250] Loss: 0.0615\n",
      "Epoch [24/25] Step [1200/2250] Loss: 0.0040\n",
      "Epoch [24/25] Step [1215/2250] Loss: 0.0016\n",
      "Epoch [24/25] Step [1230/2250] Loss: 0.0035\n",
      "Epoch [24/25] Step [1245/2250] Loss: 0.0002\n",
      "Epoch [24/25] Step [1260/2250] Loss: 0.0132\n",
      "Epoch [24/25] Step [1275/2250] Loss: 0.0003\n",
      "Epoch [24/25] Step [1290/2250] Loss: 0.0098\n",
      "Epoch [24/25] Step [1305/2250] Loss: 0.0001\n",
      "Epoch [24/25] Step [1320/2250] Loss: 0.0002\n",
      "Epoch [24/25] Step [1335/2250] Loss: 0.0008\n",
      "Epoch [24/25] Step [1350/2250] Loss: 0.0022\n",
      "Epoch [24/25] Step [1365/2250] Loss: 0.0001\n",
      "Epoch [24/25] Step [1380/2250] Loss: 0.0182\n",
      "Epoch [24/25] Step [1395/2250] Loss: 0.0016\n",
      "Epoch [24/25] Step [1410/2250] Loss: 0.0005\n",
      "Epoch [24/25] Step [1425/2250] Loss: 0.0235\n",
      "Epoch [24/25] Step [1440/2250] Loss: 0.0023\n",
      "Epoch [24/25] Step [1455/2250] Loss: 0.0000\n",
      "Epoch [24/25] Step [1470/2250] Loss: 0.4776\n",
      "Epoch [24/25] Step [1485/2250] Loss: 0.0842\n",
      "Epoch [24/25] Step [1500/2250] Loss: 0.0386\n",
      "Epoch [24/25] Step [1515/2250] Loss: 0.0179\n",
      "Epoch [24/25] Step [1530/2250] Loss: 0.0017\n",
      "Epoch [24/25] Step [1545/2250] Loss: 0.0000\n",
      "Epoch [24/25] Step [1560/2250] Loss: 0.0009\n",
      "Epoch [24/25] Step [1575/2250] Loss: 0.0736\n",
      "Epoch [24/25] Step [1590/2250] Loss: 0.0085\n",
      "Epoch [24/25] Step [1605/2250] Loss: 0.0005\n",
      "Epoch [24/25] Step [1620/2250] Loss: 0.0010\n",
      "Epoch [24/25] Step [1635/2250] Loss: 0.0122\n",
      "Epoch [24/25] Step [1650/2250] Loss: 0.0001\n",
      "Epoch [24/25] Step [1665/2250] Loss: 0.0020\n",
      "Epoch [24/25] Step [1680/2250] Loss: 0.0001\n",
      "Epoch [24/25] Step [1695/2250] Loss: 0.0212\n",
      "Epoch [24/25] Step [1710/2250] Loss: 0.0045\n",
      "Epoch [24/25] Step [1725/2250] Loss: 0.0071\n",
      "Epoch [24/25] Step [1740/2250] Loss: 0.0084\n",
      "Epoch [24/25] Step [1755/2250] Loss: 0.0010\n",
      "Epoch [24/25] Step [1770/2250] Loss: 0.0021\n",
      "Epoch [24/25] Step [1785/2250] Loss: 0.0002\n",
      "Epoch [24/25] Step [1800/2250] Loss: 0.0002\n",
      "Epoch [24/25] Step [1815/2250] Loss: 0.0182\n",
      "Epoch [24/25] Step [1830/2250] Loss: 0.0183\n",
      "Epoch [24/25] Step [1845/2250] Loss: 0.0003\n",
      "Epoch [24/25] Step [1860/2250] Loss: 0.0611\n",
      "Epoch [24/25] Step [1875/2250] Loss: 0.0057\n",
      "Epoch [24/25] Step [1890/2250] Loss: 0.0002\n",
      "Epoch [24/25] Step [1905/2250] Loss: 0.0030\n",
      "Epoch [24/25] Step [1920/2250] Loss: 0.0094\n",
      "Epoch [24/25] Step [1935/2250] Loss: 0.0001\n",
      "Epoch [24/25] Step [1950/2250] Loss: 0.0291\n",
      "Epoch [24/25] Step [1965/2250] Loss: 0.0015\n",
      "Epoch [24/25] Step [1980/2250] Loss: 0.0006\n",
      "Epoch [24/25] Step [1995/2250] Loss: 0.0237\n",
      "Epoch [24/25] Step [2010/2250] Loss: 0.1771\n",
      "Epoch [24/25] Step [2025/2250] Loss: 0.0010\n",
      "Epoch [24/25] Step [2040/2250] Loss: 0.0922\n",
      "Epoch [24/25] Step [2055/2250] Loss: 0.0022\n",
      "Epoch [24/25] Step [2070/2250] Loss: 0.0014\n",
      "Epoch [24/25] Step [2085/2250] Loss: 0.0001\n",
      "Epoch [24/25] Step [2100/2250] Loss: 0.0674\n",
      "Epoch [24/25] Step [2115/2250] Loss: 0.1565\n",
      "Epoch [24/25] Step [2130/2250] Loss: 0.2288\n",
      "Epoch [24/25] Step [2145/2250] Loss: 0.0183\n",
      "Epoch [24/25] Step [2160/2250] Loss: 0.0003\n",
      "Epoch [24/25] Step [2175/2250] Loss: 0.0004\n",
      "Epoch [24/25] Step [2190/2250] Loss: 0.0037\n",
      "Epoch [24/25] Step [2205/2250] Loss: 0.0240\n",
      "Epoch [24/25] Step [2220/2250] Loss: 0.0691\n",
      "Epoch [24/25] Step [2235/2250] Loss: 0.0003\n",
      "Epoch [24/25] completed in 2007.25s\n",
      "Train Accuracy: 0.9930, Validation Accuracy: 0.9487\n",
      "\n",
      "Epoch [25/25] Step [0/2250] Loss: 0.0000\n",
      "Epoch [25/25] Step [15/2250] Loss: 0.0013\n",
      "Epoch [25/25] Step [30/2250] Loss: 0.0373\n",
      "Epoch [25/25] Step [45/2250] Loss: 0.0026\n",
      "Epoch [25/25] Step [60/2250] Loss: 0.0002\n",
      "Epoch [25/25] Step [75/2250] Loss: 0.0000\n",
      "Epoch [25/25] Step [90/2250] Loss: 0.0084\n",
      "Epoch [25/25] Step [105/2250] Loss: 0.0001\n",
      "Epoch [25/25] Step [120/2250] Loss: 0.0003\n",
      "Epoch [25/25] Step [135/2250] Loss: 0.0045\n",
      "Epoch [25/25] Step [150/2250] Loss: 0.1182\n",
      "Epoch [25/25] Step [165/2250] Loss: 0.0008\n",
      "Epoch [25/25] Step [180/2250] Loss: 0.0018\n",
      "Epoch [25/25] Step [195/2250] Loss: 0.0005\n",
      "Epoch [25/25] Step [210/2250] Loss: 0.0043\n",
      "Epoch [25/25] Step [225/2250] Loss: 0.0070\n",
      "Epoch [25/25] Step [240/2250] Loss: 0.0891\n",
      "Epoch [25/25] Step [255/2250] Loss: 0.0009\n",
      "Epoch [25/25] Step [270/2250] Loss: 0.0012\n",
      "Epoch [25/25] Step [285/2250] Loss: 0.0116\n",
      "Epoch [25/25] Step [300/2250] Loss: 0.0001\n",
      "Epoch [25/25] Step [315/2250] Loss: 0.0000\n",
      "Epoch [25/25] Step [330/2250] Loss: 0.0012\n",
      "Epoch [25/25] Step [345/2250] Loss: 0.0024\n",
      "Epoch [25/25] Step [360/2250] Loss: 0.0044\n",
      "Epoch [25/25] Step [375/2250] Loss: 0.1078\n",
      "Epoch [25/25] Step [390/2250] Loss: 0.0037\n",
      "Epoch [25/25] Step [405/2250] Loss: 0.1636\n",
      "Epoch [25/25] Step [420/2250] Loss: 0.0000\n",
      "Epoch [25/25] Step [435/2250] Loss: 0.0009\n",
      "Epoch [25/25] Step [450/2250] Loss: 0.0088\n",
      "Epoch [25/25] Step [465/2250] Loss: 0.0000\n",
      "Epoch [25/25] Step [480/2250] Loss: 0.0223\n",
      "Epoch [25/25] Step [495/2250] Loss: 0.2316\n",
      "Epoch [25/25] Step [510/2250] Loss: 0.0511\n",
      "Epoch [25/25] Step [525/2250] Loss: 0.0013\n",
      "Epoch [25/25] Step [540/2250] Loss: 0.0024\n",
      "Epoch [25/25] Step [555/2250] Loss: 0.0475\n",
      "Epoch [25/25] Step [570/2250] Loss: 0.0112\n",
      "Epoch [25/25] Step [585/2250] Loss: 0.0377\n",
      "Epoch [25/25] Step [600/2250] Loss: 0.0030\n",
      "Epoch [25/25] Step [615/2250] Loss: 0.0221\n",
      "Epoch [25/25] Step [630/2250] Loss: 0.0346\n",
      "Epoch [25/25] Step [645/2250] Loss: 0.0361\n",
      "Epoch [25/25] Step [660/2250] Loss: 0.0066\n",
      "Epoch [25/25] Step [675/2250] Loss: 0.0009\n",
      "Epoch [25/25] Step [690/2250] Loss: 0.0018\n",
      "Epoch [25/25] Step [705/2250] Loss: 0.3968\n",
      "Epoch [25/25] Step [720/2250] Loss: 0.0459\n",
      "Epoch [25/25] Step [735/2250] Loss: 0.0121\n",
      "Epoch [25/25] Step [750/2250] Loss: 0.0011\n",
      "Epoch [25/25] Step [765/2250] Loss: 0.0005\n",
      "Epoch [25/25] Step [780/2250] Loss: 0.0373\n",
      "Epoch [25/25] Step [795/2250] Loss: 0.1272\n",
      "Epoch [25/25] Step [810/2250] Loss: 0.1431\n",
      "Epoch [25/25] Step [825/2250] Loss: 0.0019\n",
      "Epoch [25/25] Step [840/2250] Loss: 0.0142\n",
      "Epoch [25/25] Step [855/2250] Loss: 0.0565\n",
      "Epoch [25/25] Step [870/2250] Loss: 0.0833\n",
      "Epoch [25/25] Step [885/2250] Loss: 0.0006\n",
      "Epoch [25/25] Step [900/2250] Loss: 0.0136\n",
      "Epoch [25/25] Step [915/2250] Loss: 0.1388\n",
      "Epoch [25/25] Step [930/2250] Loss: 0.0523\n",
      "Epoch [25/25] Step [945/2250] Loss: 0.0105\n",
      "Epoch [25/25] Step [960/2250] Loss: 0.0005\n",
      "Epoch [25/25] Step [975/2250] Loss: 0.0005\n",
      "Epoch [25/25] Step [990/2250] Loss: 0.0000\n",
      "Epoch [25/25] Step [1005/2250] Loss: 0.0020\n",
      "Epoch [25/25] Step [1020/2250] Loss: 0.0006\n",
      "Epoch [25/25] Step [1035/2250] Loss: 0.0059\n",
      "Epoch [25/25] Step [1050/2250] Loss: 0.0785\n",
      "Epoch [25/25] Step [1065/2250] Loss: 0.1653\n",
      "Epoch [25/25] Step [1080/2250] Loss: 0.0240\n",
      "Epoch [25/25] Step [1095/2250] Loss: 0.0019\n",
      "Epoch [25/25] Step [1110/2250] Loss: 0.0003\n",
      "Epoch [25/25] Step [1125/2250] Loss: 0.0128\n",
      "Epoch [25/25] Step [1140/2250] Loss: 0.0215\n",
      "Epoch [25/25] Step [1155/2250] Loss: 0.0036\n",
      "Epoch [25/25] Step [1170/2250] Loss: 0.0064\n",
      "Epoch [25/25] Step [1185/2250] Loss: 0.0571\n",
      "Epoch [25/25] Step [1200/2250] Loss: 0.0053\n",
      "Epoch [25/25] Step [1215/2250] Loss: 0.0021\n",
      "Epoch [25/25] Step [1230/2250] Loss: 0.0651\n",
      "Epoch [25/25] Step [1245/2250] Loss: 0.0001\n",
      "Epoch [25/25] Step [1260/2250] Loss: 0.1088\n",
      "Epoch [25/25] Step [1275/2250] Loss: 0.0017\n",
      "Epoch [25/25] Step [1290/2250] Loss: 0.0036\n",
      "Epoch [25/25] Step [1305/2250] Loss: 0.0003\n",
      "Epoch [25/25] Step [1320/2250] Loss: 0.0007\n",
      "Epoch [25/25] Step [1335/2250] Loss: 0.0048\n",
      "Epoch [25/25] Step [1350/2250] Loss: 0.0057\n",
      "Epoch [25/25] Step [1365/2250] Loss: 0.0005\n",
      "Epoch [25/25] Step [1380/2250] Loss: 0.0661\n",
      "Epoch [25/25] Step [1395/2250] Loss: 0.0034\n",
      "Epoch [25/25] Step [1410/2250] Loss: 0.0005\n",
      "Epoch [25/25] Step [1425/2250] Loss: 0.0133\n",
      "Epoch [25/25] Step [1440/2250] Loss: 0.0678\n",
      "Epoch [25/25] Step [1455/2250] Loss: 0.0009\n",
      "Epoch [25/25] Step [1470/2250] Loss: 0.0217\n",
      "Epoch [25/25] Step [1485/2250] Loss: 0.0029\n",
      "Epoch [25/25] Step [1500/2250] Loss: 0.0284\n",
      "Epoch [25/25] Step [1515/2250] Loss: 0.0020\n",
      "Epoch [25/25] Step [1530/2250] Loss: 0.0344\n",
      "Epoch [25/25] Step [1545/2250] Loss: 0.0005\n",
      "Epoch [25/25] Step [1560/2250] Loss: 0.0012\n",
      "Epoch [25/25] Step [1575/2250] Loss: 0.0137\n",
      "Epoch [25/25] Step [1590/2250] Loss: 0.0023\n",
      "Epoch [25/25] Step [1605/2250] Loss: 0.0030\n",
      "Epoch [25/25] Step [1620/2250] Loss: 0.0057\n",
      "Epoch [25/25] Step [1635/2250] Loss: 0.0001\n",
      "Epoch [25/25] Step [1650/2250] Loss: 0.1800\n",
      "Epoch [25/25] Step [1665/2250] Loss: 0.0001\n",
      "Epoch [25/25] Step [1680/2250] Loss: 0.0000\n",
      "Epoch [25/25] Step [1695/2250] Loss: 0.0002\n",
      "Epoch [25/25] Step [1710/2250] Loss: 0.0009\n",
      "Epoch [25/25] Step [1725/2250] Loss: 0.0179\n",
      "Epoch [25/25] Step [1740/2250] Loss: 0.0025\n",
      "Epoch [25/25] Step [1755/2250] Loss: 0.0033\n",
      "Epoch [25/25] Step [1770/2250] Loss: 0.0023\n",
      "Epoch [25/25] Step [1785/2250] Loss: 0.0004\n",
      "Epoch [25/25] Step [1800/2250] Loss: 0.0102\n",
      "Epoch [25/25] Step [1815/2250] Loss: 0.0519\n",
      "Epoch [25/25] Step [1830/2250] Loss: 0.0009\n",
      "Epoch [25/25] Step [1845/2250] Loss: 0.0001\n",
      "Epoch [25/25] Step [1860/2250] Loss: 0.0253\n",
      "Epoch [25/25] Step [1875/2250] Loss: 0.0216\n",
      "Epoch [25/25] Step [1890/2250] Loss: 0.0002\n",
      "Epoch [25/25] Step [1905/2250] Loss: 0.0006\n",
      "Epoch [25/25] Step [1920/2250] Loss: 0.0041\n",
      "Epoch [25/25] Step [1935/2250] Loss: 0.0000\n",
      "Epoch [25/25] Step [1950/2250] Loss: 0.0020\n",
      "Epoch [25/25] Step [1965/2250] Loss: 0.0061\n",
      "Epoch [25/25] Step [1980/2250] Loss: 0.0281\n",
      "Epoch [25/25] Step [1995/2250] Loss: 0.1447\n",
      "Epoch [25/25] Step [2010/2250] Loss: 0.0035\n",
      "Epoch [25/25] Step [2025/2250] Loss: 0.0618\n",
      "Epoch [25/25] Step [2040/2250] Loss: 0.0003\n",
      "Epoch [25/25] Step [2055/2250] Loss: 0.0021\n",
      "Epoch [25/25] Step [2070/2250] Loss: 0.0085\n",
      "Epoch [25/25] Step [2085/2250] Loss: 0.0002\n",
      "Epoch [25/25] Step [2100/2250] Loss: 0.0050\n",
      "Epoch [25/25] Step [2115/2250] Loss: 0.0064\n",
      "Epoch [25/25] Step [2130/2250] Loss: 0.0238\n",
      "Epoch [25/25] Step [2145/2250] Loss: 0.0023\n",
      "Epoch [25/25] Step [2160/2250] Loss: 0.0040\n",
      "Epoch [25/25] Step [2175/2250] Loss: 0.0082\n",
      "Epoch [25/25] Step [2190/2250] Loss: 0.0071\n",
      "Epoch [25/25] Step [2205/2250] Loss: 0.0067\n",
      "Epoch [25/25] Step [2220/2250] Loss: 0.0013\n",
      "Epoch [25/25] Step [2235/2250] Loss: 0.0005\n",
      "Epoch [25/25] completed in 2006.99s\n",
      "Train Accuracy: 0.9931, Validation Accuracy: 0.9563\n",
      "\n",
      "âœ… Best model updated and saved with Val Acc: 0.9563\n",
      "\n",
      "\n",
      "ðŸ“Š Final Test Set Evaluation:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Normal       0.95      0.96      0.96     11961\n",
      "      Stroke       0.96      0.95      0.95     12039\n",
      "\n",
      "    accuracy                           0.95     24000\n",
      "   macro avg       0.96      0.96      0.95     24000\n",
      "weighted avg       0.96      0.95      0.95     24000\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.955"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-10T01:35:20.436025Z",
     "start_time": "2025-06-10T01:35:20.432503Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "57ed36b0322e1f53",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
